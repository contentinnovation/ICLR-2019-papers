{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
        "author": "Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, Mohamed Elhoseiny, 1University of Oxford, 2Facebook AI Research arslan.chaudhry@eng.ox.ac.uk, {ranzato,mrf,elhoseiny}@fb.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hkf2_sC5FX"
        },
        "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (<a class=\"ref-link\" id=\"cLopez-Paz_2017_a\" href=\"#rLopez-Paz_2017_a\">Lopez-Paz & Ranzato, 2017</a>), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (<a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al., 2016</a>) and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.1"
    },
    "keywords": [
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "quadratic program",
            "url": "https://en.wikipedia.org/wiki/quadratic_program"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        }
    ],
    "abbreviations": {
        "A-GEM": "Averaged GEM",
        "LLL": "lifelong learning",
        "LCA": "Learning Curve Area",
        "QP": "quadratic program"
    },
    "highlights": [
        "Intelligent systems, whether they are natural or artificial, must be able to quickly adapt to changes in the environment and to quickly learn new skills by leveraging past experiences",
        "We studied the problem of efficient Lifelong Learning (LLL) in the case where the learner can only do a single pass over the input data stream",
        "We found that our approach, Averaged GEM, has the best tradeoff between average accuracy by the end of the learning experience and computational/memory cost",
        "We demonstrated that by using compositional task descriptors all methods can improve their few-shot performance, with Averaged GEM often being the best",
        "Our detailed experiments reported in Appendix E show that there is still a substantial performance gap between lifelong learning methods, including Averaged GEM, trained in a sequential learning setting and the same network trained in a non-sequential multi-task setting, despite seeing the same data samples",
        "While task descriptors do help in the few-shot learning regime, the Learning Curve Area performance gap between different methods is very small; suggesting a poor ability of current methods to transfer knowledge even when forgetting has been eliminated"
    ],
    "key_statements": [
        "Intelligent systems, whether they are natural or artificial, must be able to quickly adapt to changes in the environment and to quickly learn new skills by leveraging past experiences",
        "The lifelong learning (LLL) setting attempts at addressing this shortcoming, bringing machine learning closer to a more realistic human learning by acquiring new skills quickly with a small amount of training data, given the experience accumulated in the past",
        "This means that the learner should not be allowed to merely store all examples seen in the past nor should the learner be engaged in computations that would not be feasible in real-time, as the goal is to quickly learn from a stream of data",
        "We introduce a new metric to measure the speed of learning, which is useful to quantify the ability of a learning algorithm to learn a new task (Sec. 3)",
        "Notice how PROG-NN has the worst memory cost by the end of training - as its number of parameters grows super-linearly with the number of tasks",
        "PROG-NN has no forgetting by construction and Averaged GEM and GEM have the lowest forgetting among methods that use a fixed capacity architecture",
        "We conclude that Averaged GEM offers the best trade-off between average accuracy performance and efficiency in terms of sample, memory and computational cost",
        "In Fig. 5, we report the 0-shot performance of lifelong learning methods on Split CUB and Split AWA datasets over time, showing a clear advantage of using compositional task descriptors with joint embedding models, which is more significant for Averaged GEM",
        "We studied the problem of efficient Lifelong Learning (LLL) in the case where the learner can only do a single pass over the input data stream",
        "We found that our approach, Averaged GEM, has the best tradeoff between average accuracy by the end of the learning experience and computational/memory cost",
        "We demonstrated that by using compositional task descriptors all methods can improve their few-shot performance, with Averaged GEM often being the best",
        "Our detailed experiments reported in Appendix E show that there is still a substantial performance gap between lifelong learning methods, including Averaged GEM, trained in a sequential learning setting and the same network trained in a non-sequential multi-task setting, despite seeing the same data samples",
        "While task descriptors do help in the few-shot learning regime, the Learning Curve Area performance gap between different methods is very small; suggesting a poor ability of current methods to transfer knowledge even when forgetting has been eliminated"
    ],
    "summary": [
        "Intelligent systems, whether they are natural or artificial, must be able to quickly adapt to changes in the environment and to quickly learn new skills by leveraging past experiences.",
        "We propose an evaluation methodology and an algorithm that better match our desiderata, namely learning efficiently \u2013 in terms of training samples, time and memory \u2013 from a stream of tasks.",
        "The learner will have to learn and will be tested on an entirely new sequence of tasks and it will perform just a single pass over this data stream.",
        "Learning Curve Area (LCA \u2208 [0, 1]) Let us first define an average b-shot performance after the model has been trained for all the T tasks as: 1T",
        "The use of task descriptors improves average accuracy across the board as shown in Fig.2, with A-GEM a bit better than all the other methods we tried.",
        "All joint-embedding models using task descriptors have better LCA performance, this is the same across all methods including A-GEM.",
        "We conclude that A-GEM offers the best trade-off between average accuracy performance and efficiency in terms of sample, memory and computational cost.",
        "In Fig. 5, we report the 0-shot performance of LLL methods on Split CUB and Split AWA datasets over time, showing a clear advantage of using compositional task descriptors with joint embedding models, which is more significant for A-GEM.",
        "Another approach is to regularize parameters important to solve past tasks (<a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al, 2016</a>; <a class=\"ref-link\" id=\"cZenke_et+al_2017_a\" href=\"#rZenke_et+al_2017_a\">Zenke et al, 2017</a>; <a class=\"ref-link\" id=\"cChaudhry_et+al_2018_a\" href=\"#rChaudhry_et+al_2018_a\">Chaudhry et al, 2018</a>), which has been proven effective for over-parameterized models in the multiple epoch setting, while we focus on learning from few examples using memory efficient models.",
        "Methods based on episodic memory (<a class=\"ref-link\" id=\"cRebuffi_et+al_2017_a\" href=\"#rRebuffi_et+al_2017_a\">Rebuffi et al, 2017</a>; <a class=\"ref-link\" id=\"cLopez-Paz_2017_a\" href=\"#rLopez-Paz_2017_a\"><a class=\"ref-link\" id=\"cLopez-Paz_2017_a\" href=\"#rLopez-Paz_2017_a\">Lopez-Paz & Ranzato, 2017</a></a>) require a little bit more memory at training time but can work much better in the single pass setting we considered (<a class=\"ref-link\" id=\"cLopez-Paz_2017_a\" href=\"#rLopez-Paz_2017_a\"><a class=\"ref-link\" id=\"cLopez-Paz_2017_a\" href=\"#rLopez-Paz_2017_a\">Lopez-Paz & Ranzato, 2017</a></a>).",
        "We found that our approach, A-GEM, has the best tradeoff between average accuracy by the end of the learning experience and computational/memory cost.",
        "Our detailed experiments reported in Appendix E show that there is still a substantial performance gap between LLL methods, including A-GEM, trained in a sequential learning setting and the same network trained in a non-sequential multi-task setting, despite seeing the same data samples.",
        "While task descriptors do help in the few-shot learning regime, the LCA performance gap between different methods is very small; suggesting a poor ability of current methods to transfer knowledge even when forgetting has been eliminated.",
        "Addressing these two fundamental issues will be the focus of our future research"
    ],
    "headline": "We investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost",
    "reference_links": [
        {
            "id": "Aljundi_et+al_2017_a",
            "entry": "Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pp. 7120\u20137129, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aljundi%2C%20Rahaf%20Chakravarty%2C%20Punarjay%20Tuytelaars%2C%20Tinne%20Expert%20gate%3A%20Lifelong%20learning%20with%20a%20network%20of%20experts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aljundi%2C%20Rahaf%20Chakravarty%2C%20Punarjay%20Tuytelaars%2C%20Tinne%20Expert%20gate%3A%20Lifelong%20learning%20with%20a%20network%20of%20experts%202017"
        },
        {
            "id": "Aljundi_et+al_2018_a",
            "entry": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aljundi%2C%20Rahaf%20Babiloni%2C%20Francesca%20Elhoseiny%2C%20Mohamed%20Rohrbach%2C%20Marcus%20Memory%20aware%20synapses%3A%20Learning%20what%20%28not%29%20to%20forget%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aljundi%2C%20Rahaf%20Babiloni%2C%20Francesca%20Elhoseiny%2C%20Mohamed%20Rohrbach%2C%20Marcus%20Memory%20aware%20synapses%3A%20Learning%20what%20%28not%29%20to%20forget%202018"
        },
        {
            "id": "Baroni_et+al_2017_a",
            "entry": "Marco Baroni, Armand Joulin, Allan Jabri, German Kruszewski, Angeliki Lazaridou, Klemen Simonic, and Tomas Mikolov. Commai: Evaluating the first steps towards a useful general ai. arXiv preprint arXiv:1701.08954, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08954"
        },
        {
            "id": "Chang_et+al_2018_a",
            "entry": "Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. Automatically composing representation transformations as a means for generalization. In ICML workshop Neural Abstract Machines and Program Induction v2, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Michael%20Gupta%2C%20Abhishek%20Levine%2C%20Sergey%20Griffiths%2C%20Thomas%20L.%20Automatically%20composing%20representation%20transformations%20as%20a%20means%20for%20generalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Michael%20Gupta%2C%20Abhishek%20Levine%2C%20Sergey%20Griffiths%2C%20Thomas%20L.%20Automatically%20composing%20representation%20transformations%20as%20a%20means%20for%20generalization%202018"
        },
        {
            "id": "Chaudhry_et+al_2018_a",
            "entry": "Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhry%2C%20Arslan%20Dokania%2C%20Puneet%20K.%20Ajanthan%2C%20Thalaiyasingam%20Torr%2C%20Philip%20H.S.%20Riemannian%20walk%20for%20incremental%20learning%3A%20Understanding%20forgetting%20and%20intransigence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhry%2C%20Arslan%20Dokania%2C%20Puneet%20K.%20Ajanthan%2C%20Thalaiyasingam%20Torr%2C%20Philip%20H.S.%20Riemannian%20walk%20for%20incremental%20learning%3A%20Understanding%20forgetting%20and%20intransigence%202018"
        },
        {
            "id": "Elhoseiny_et+al_2017_a",
            "entry": "Mohamed Elhoseiny, Ahmed Elgammal, and Babak Saleh. Write a classifier: Predicting visual classifiers from unstructured text. IEEE TPAMI, 39(12):2539\u20132553, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elhoseiny%2C%20Mohamed%20Elgammal%2C%20Ahmed%20Saleh%2C%20Babak%20Write%20a%20classifier%3A%20Predicting%20visual%20classifiers%20from%20unstructured%20text%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elhoseiny%2C%20Mohamed%20Elgammal%2C%20Ahmed%20Saleh%2C%20Babak%20Write%20a%20classifier%3A%20Predicting%20visual%20classifiers%20from%20unstructured%20text%202017"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "Leslie_2018_a",
            "entry": "Leslie P. Kaelbling Ferran Alet, Tomas Lozano-Perez. Modular meta-learning. arXiv preprint arXiv:1806.10166v1, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10166v1"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Isele_et+al_2016_a",
            "entry": "David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI\u201916, pp. 1620\u20131626. AAAI Press, 2016. ISBN 978-1-57735-7704.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isele%2C%20David%20Rostami%2C%20Mohammad%20Eaton%2C%20Eric%20Using%20task%20features%20for%20zero-shot%20knowledge%20transfer%20in%20lifelong%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isele%2C%20David%20Rostami%2C%20Mohammad%20Eaton%2C%20Eric%20Using%20task%20features%20for%20zero-shot%20knowledge%20transfer%20in%20lifelong%20learning%202016"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America (PNAS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20C.%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20C.%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/kriz/cifar.html, 2009.",
            "url": "https://www.cs.toronto.edu/kriz/cifar.html"
        },
        {
            "id": "Lampert_et+al_2009_a",
            "entry": "Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 951\u2013958. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009"
        },
        {
            "id": "Lampert_et+al_2014_a",
            "entry": "Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453\u2013465, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Attribute-based%20classification%20for%20zero-shot%20visual%20object%20categorization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Attribute-based%20classification%20for%20zero-shot%20visual%20object%20categorization%202014"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Lopez-Paz_2017_a",
            "entry": "David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Gradient%20episodic%20memory%20for%20continuum%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Gradient%20episodic%20memory%20for%20continuum%20learning%202017"
        },
        {
            "id": "Nguyen_et+al_2018_a",
            "entry": "Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Cuong%20V.%20Li%2C%20Yingzhen%20Bui%2C%20Thang%20D.%20Turner%2C%20Richard%20E.%20Variational%20continual%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Cuong%20V.%20Li%2C%20Yingzhen%20Bui%2C%20Thang%20D.%20Turner%2C%20Richard%20E.%20Variational%20continual%20learning%202018"
        },
        {
            "id": "Rebuffi_et+al_2017_a",
            "entry": "S-V. Rebuffi, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classifier and representation learning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20S.-V.%20Kolesnikov%2C%20A.%20H%2C%20C.%20Lampert.%20iCaRL%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20S.-V.%20Kolesnikov%2C%20A.%20H%2C%20C.%20Lampert.%20iCaRL%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "Ring_1997_a",
            "entry": "Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77\u2013104, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ring%2C%20Mark%20B.%20Child%3A%20A%20first%20step%20towards%20continual%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ring%2C%20Mark%20B.%20Child%3A%20A%20first%20step%20towards%20continual%20learning%201997"
        },
        {
            "id": "Rosenbaum_et+al_2018_a",
            "entry": "Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenbaum%2C%20Clemens%20Klinger%2C%20Tim%20Riemer%2C%20Matthew%20Routing%20networks%3A%20Adaptive%20selection%20of%20non-linear%20functions%20for%20multi-task%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenbaum%2C%20Clemens%20Klinger%2C%20Tim%20Riemer%2C%20Matthew%20Routing%20networks%3A%20Adaptive%20selection%20of%20non-linear%20functions%20for%20multi-task%20learning%202018"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaul%2C%20T.%20Horgan%2C%20D.%20Gregor%2C%20K.%20Silver%2C%20D.%20Universal%20value%20function%20approximators%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaul%2C%20T.%20Horgan%2C%20D.%20Gregor%2C%20K.%20Silver%2C%20D.%20Universal%20value%20function%20approximators%202015"
        },
        {
            "id": "Schwarz_et+al_2018_a",
            "entry": "Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In International Conference in Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarz%2C%20Jonathan%20Luketina%2C%20Jelena%20Czarnecki%2C%20Wojciech%20M.%20Grabska-Barwinska%2C%20Agnieszka%20Progress%20and%20compress%3A%20A%20scalable%20framework%20for%20continual%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarz%2C%20Jonathan%20Luketina%2C%20Jelena%20Czarnecki%2C%20Wojciech%20M.%20Grabska-Barwinska%2C%20Agnieszka%20Progress%20and%20compress%3A%20A%20scalable%20framework%20for%20continual%20learning%202018"
        },
        {
            "id": "Shin_et+al_2017_a",
            "entry": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017"
        },
        {
            "id": "Sutton_et+al_2011_a",
            "entry": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. The 10th International Conference on Autonomous Agents and Multiagent Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Modayil%2C%20J.%20Delp%2C%20M.%20Degris%2C%20T.%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Modayil%2C%20J.%20Delp%2C%20M.%20Degris%2C%20T.%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011"
        },
        {
            "id": "Springer_2011_a",
            "entry": "Springer, 1998. C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 2018. Ju Xu and Zhanxing Zhu. Reinforced continual learning. In arXiv preprint arXiv:1805.12369v1, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12369v1"
        },
        {
            "id": "Zenke_et+al_2017_a",
            "entry": "F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, and Mohamed Elhoseiny. Large-scale visual relationship understanding. arXiv preprint arXiv:1804.10660, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.10660"
        }
    ]
}
