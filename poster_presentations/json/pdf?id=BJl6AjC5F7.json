{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TO REPRESENT EDITS",
        "author": "Pengcheng Yin,\u2217 Graham Neubig Language Technology Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {pcyin,gneubig}@cs.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJl6AjC5F7"
        },
        "abstract": "We introduce the problem of learning distributed representations of edits. By combining a \u201cneural editor\u201d with an \u201cedit encoder\u201d, our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem."
    },
    "keywords": [
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "abstract syntax tree",
            "url": "https://en.wikipedia.org/wiki/abstract_syntax_tree"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "source code",
            "url": "https://en.wikipedia.org/wiki/source_code"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "AST": "abstract syntax tree",
        "GGNN": "gated graph neural network"
    },
    "highlights": [
        "One great advantage of electronic storage of documents is the ease with which we can edit them, and edits are performed in a wide variety of contents",
        "As we are primarily interested in edits on text and source code in this work, we explored two architectures: a sequenceto-sequence model for text, and a graph-to-tree model for source code, whose known semantics we can leverage both on the encoder as well as on the decoder side",
        "Based on the observation that edits to source code often manipulate the syntax tree by moving expressions around, we extend the decoding model of <a class=\"ref-link\" id=\"cYin_2018_a\" href=\"#rYin_2018_a\">Yin & Neubig (2018</a>) by adding a facility to copy entire subtrees from the input",
        "Our work directly focuses on discriminative learning of representing edits and applying the learned edits for both sequential (NL) and structured data. Another similar line of research is \u201cretrieve-and-edit\u201d models for text generation (<a class=\"ref-link\" id=\"cHashimoto_et+al_2018_a\" href=\"#rHashimoto_et+al_2018_a\">Hashimoto et al, 2018</a>), where given an input data x, the target prediction y is generated by editing a similar target y that is retrieved based on the similarity between its source x and the input x. While these models typically require an \u201ceditor\u201d component to generate the output by exploiting the difference between similar inputs, they usually use the simpler bag-of-edits representations (<a class=\"ref-link\" id=\"cWu_et+al_2019_a\" href=\"#rWu_et+al_2019_a\">Wu et al, 2019</a>), or implicitly capture it via end-to-end neural networks (<a class=\"ref-link\" id=\"cContractor_et+al_2018_a\" href=\"#rContractor_et+al_2018_a\">Contractor et al, 2018</a>)",
        "While we have presented a set of initial models and metrics on the problem and obtained some first promising results, further development in both of these areas is needed"
    ],
    "key_statements": [
        "One great advantage of electronic storage of documents is the ease with which we can edit them, and edits are performed in a wide variety of contents",
        "As we are primarily interested in edits on text and source code in this work, we explored two architectures: a sequenceto-sequence model for text, and a graph-to-tree model for source code, whose known semantics we can leverage both on the encoder as well as on the decoder side",
        "Based on the observation that edits to source code often manipulate the syntax tree by moving expressions around, we extend the decoding model of <a class=\"ref-link\" id=\"cYin_2018_a\" href=\"#rYin_2018_a\">Yin & Neubig (2018</a>) by adding a facility to copy entire subtrees from the input",
        "Our work directly focuses on discriminative learning of representing edits and applying the learned edits for both sequential (NL) and structured data. Another similar line of research is \u201cretrieve-and-edit\u201d models for text generation (<a class=\"ref-link\" id=\"cHashimoto_et+al_2018_a\" href=\"#rHashimoto_et+al_2018_a\">Hashimoto et al, 2018</a>), where given an input data x, the target prediction y is generated by editing a similar target y that is retrieved based on the similarity between its source x and the input x. While these models typically require an \u201ceditor\u201d component to generate the output by exploiting the difference between similar inputs, they usually use the simpler bag-of-edits representations (<a class=\"ref-link\" id=\"cWu_et+al_2019_a\" href=\"#rWu_et+al_2019_a\">Wu et al, 2019</a>), or implicitly capture it via end-to-end neural networks (<a class=\"ref-link\" id=\"cContractor_et+al_2018_a\" href=\"#rContractor_et+al_2018_a\">Contractor et al, 2018</a>)",
        "While we have presented a set of initial models and metrics on the problem and obtained some first promising results, further development in both of these areas is needed"
    ],
    "summary": [
        "One great advantage of electronic storage of documents is the ease with which we can edit them, and edits are performed in a wide variety of contents.",
        "Visualizing Edits on Fixers Data In a first experiment, we train our sequential neural editor model on our GitHubEdits data and compute",
        "Example 1 shows that the neural edit models succeeded in representing syntactically and semantically similar edits, while the bag-of-words baseline relies purely on surface token overlap.",
        "We are interested in answering two research questions: First, how well can our neural editors generate x+ given the gold-standard edit representation f\u2206(x\u2212, x+)?",
        "To answer the first question, we trained our neural editor models on the WikiAtomicEdits and the GitHubEdits dataset, and evaluate the performance of encoding and applying edits on test sets.",
        "We evaluated the performance of our neural editor models with a simple \u201cBag-ofEdits\u201d edit encoding scheme, where f\u2206(x\u2212, x+) is modeled as the concatenation of two vectors, each representing the sum of the embeddings of added and deleted tokens in the edit, respectively.",
        "With our proposed sequence- and graph-based edit encoders, our neural editor models achieve reasonable end-to-end performance, surpassing systems using bag-of-edits representations.",
        "We use our high-quality C#Fixers dataset, and for each fixer category F of semantically similar edits, we randomly select a seed edit {x\u2212 \u2192 x+} \u2208 F , and use its edit representation f\u2206(x\u2212, x+) to predict the updated code for all examples in F , i.e., we have x+ = \u03b1(x\u2212, f\u2206(x\u2212, x+)), \u2200 {x\u2212 \u2192 x+} \u2208 F .",
        "Tab. 5 summarizes the results and reports the upper bound performance when using the goldstandard edit representation f\u2206(x\u2212, x+) to predict x+, and an approximation of the \u201clower bound\u201d accuracies using pre-trained Seq2Seq and Graph2Tree models without edit encoders.",
        "We found that our neural Graph2Tree editor with the sequential edit encoder significantly outperforms the Seq2Seq editor, even though Seq2Seq performs better when using gold-standard edit representations.",
        "Our work shares with <a class=\"ref-link\" id=\"cGuu_et+al_2017_a\" href=\"#rGuu_et+al_2017_a\"><a class=\"ref-link\" id=\"cGuu_et+al_2017_a\" href=\"#rGuu_et+al_2017_a\">Guu et al (2017</a></a>) in that (1) the posterior edit encoding model in <a class=\"ref-link\" id=\"cGuu_et+al_2017_a\" href=\"#rGuu_et+al_2017_a\"><a class=\"ref-link\" id=\"cGuu_et+al_2017_a\" href=\"#rGuu_et+al_2017_a\">Guu et al (2017</a></a>) is similar to our baseline \u201cbag-of-edits\u201d encoder in Sec. 4.4, and (2) the sequence-to-sequence sentence generation model given the prototype and edit representation is reminiscent of our Seq2Seq editor.",
        "While these models typically require an \u201ceditor\u201d component to generate the output by exploiting the difference between similar inputs, they usually use the simpler bag-of-edits representations (<a class=\"ref-link\" id=\"cWu_et+al_2019_a\" href=\"#rWu_et+al_2019_a\">Wu et al, 2019</a>), or implicitly capture it via end-to-end neural networks (<a class=\"ref-link\" id=\"cContractor_et+al_2018_a\" href=\"#rContractor_et+al_2018_a\">Contractor et al, 2018</a>)."
    ],
    "headline": "We introduce the problem of learning distributed representations of edits",
    "reference_links": [
        {
            "id": "Artstein_2008_a",
            "entry": "Ron Artstein and Massimo Poesio. Inter-coder agreement for computational linguistics. Computational Linguistics, 34:555\u2013596, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artstein%2C%20Ron%20Poesio%2C%20Massimo%20Inter-coder%20agreement%20for%20computational%20linguistics%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artstein%2C%20Ron%20Poesio%2C%20Massimo%20Inter-coder%20agreement%20for%20computational%20linguistics%202008"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. arXiv preprint arXiv:1802.03691, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03691"
        },
        {
            "id": "Contractor_et+al_2018_a",
            "entry": "Danish Contractor, Vineet Kumar, Sachindra Joshi, and Gaurav Pandey. Exemplar encoder-decoder for neural conversation generation. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Contractor%2C%20Danish%20Kumar%2C%20Vineet%20Joshi%2C%20Sachindra%20Pandey%2C%20Gaurav%20Exemplar%20encoder-decoder%20for%20neural%20conversation%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Contractor%2C%20Danish%20Kumar%2C%20Vineet%20Joshi%2C%20Sachindra%20Pandey%2C%20Gaurav%20Exemplar%20encoder-decoder%20for%20neural%20conversation%20generation%202018"
        },
        {
            "id": "Dai_2015_a",
            "entry": "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 3079\u20133087, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015"
        },
        {
            "id": "Faruqui_2018_a",
            "entry": "Manaal Faruqui and Dipanjan Das. Identifying Well-formed Natural Language Questions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faruqui%2C%20Manaal%20Das%2C%20Dipanjan%20Identifying%20Well-formed%20Natural%20Language%20Questions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faruqui%2C%20Manaal%20Das%2C%20Dipanjan%20Identifying%20Well-formed%20Natural%20Language%20Questions%202018"
        },
        {
            "id": "Faruqui_et+al_2018_b",
            "entry": "Manaal Faruqui, Ellie Pavlick, Ian Tenney, and Dipanjan Das. WikiAtomicEdits: A multilingual corpus of Wikipedia edits for modeling language and discourse. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faruqui%2C%20Manaal%20Pavlick%2C%20Ellie%20Tenney%2C%20Ian%20Das%2C%20Dipanjan%20WikiAtomicEdits%3A%20A%20multilingual%20corpus%20of%20Wikipedia%20edits%20for%20modeling%20language%20and%20discourse%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faruqui%2C%20Manaal%20Pavlick%2C%20Ellie%20Tenney%2C%20Ian%20Das%2C%20Dipanjan%20WikiAtomicEdits%3A%20A%20multilingual%20corpus%20of%20Wikipedia%20edits%20for%20modeling%20language%20and%20discourse%202018"
        },
        {
            "id": "Gilmer_et+al_2017_a",
            "entry": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.",
            "url": "http://www.deeplearningbook.org"
        },
        {
            "id": "Guu_et+al_2017_a",
            "entry": "Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. arXiv preprint arXiv:1709.08878, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08878"
        },
        {
            "id": "Hashimoto_et+al_2018_a",
            "entry": "Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S. Liang. A retrieve-and-edit framework for predicting structured outputs. In NeurIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hashimoto%2C%20Tatsunori%20B.%20Guu%2C%20Kelvin%20Oren%2C%20Yonatan%20Liang%2C%20Percy%20S.%20A%20retrieve-and-edit%20framework%20for%20predicting%20structured%20outputs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hashimoto%2C%20Tatsunori%20B.%20Guu%2C%20Kelvin%20Oren%2C%20Yonatan%20Liang%2C%20Percy%20S.%20A%20retrieve-and-edit%20framework%20for%20predicting%20structured%20outputs%202018"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05493"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qi%20Allamanis%2C%20Miltiadis%20Brockschmidt%2C%20Marc%20Gaunt%2C%20Alexander%20L.%20Constrained%20graph%20variational%20autoencoders%20for%20molecule%20design%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qi%20Allamanis%2C%20Miltiadis%20Brockschmidt%2C%20Marc%20Gaunt%2C%20Alexander%20L.%20Constrained%20graph%20variational%20autoencoders%20for%20molecule%20design%202018"
        },
        {
            "id": "Luong_et+al_2015_a",
            "entry": "Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015"
        },
        {
            "id": "Van_2008_a",
            "entry": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "Maddison_2014_a",
            "entry": "Chris Maddison and Daniel Tarlow. Structured generative models of natural source code. In International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20Tarlow%2C%20Daniel%20Structured%20generative%20models%20of%20natural%20source%20code%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20Tarlow%2C%20Daniel%20Structured%20generative%20models%20of%20natural%20source%20code%202014"
        },
        {
            "id": "Manning_et+al_2008_a",
            "entry": "Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719, 9780521865715.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manning%2C%20Christopher%20D.%20Raghavan%2C%20Prabhakar%20Schutze%2C%20Hinrich%20Introduction%20to%20Information%20Retrieval%202008"
        },
        {
            "id": "Nguyen_et+al_2013_a",
            "entry": "Hoan Anh Nguyen, Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen, and Hridesh Rajan. A study of repetitiveness of code changes in software evolution. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering, pp. 180\u2013190. IEEE Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Hoan%20Anh%20Nguyen%2C%20Anh%20Tuan%20Nguyen%2C%20Tung%20Thanh%20Nguyen%2C%20Tien%20N.%20A%20study%20of%20repetitiveness%20of%20code%20changes%20in%20software%20evolution%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Hoan%20Anh%20Nguyen%2C%20Anh%20Tuan%20Nguyen%2C%20Tung%20Thanh%20Nguyen%2C%20Tien%20N.%20A%20study%20of%20repetitiveness%20of%20code%20changes%20in%20software%20evolution%202013"
        },
        {
            "id": "Paletov_et+al_2018_a",
            "entry": "Rumen Paletov, Petar Tsankov, Veselin Raychev, and Martin Vechev. Inferring crypto API rules from code changes. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 450\u2013464. ACM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paletov%2C%20Rumen%20Tsankov%2C%20Petar%20Raychev%2C%20Veselin%20Vechev%2C%20Martin%20Inferring%20crypto%20API%20rules%20from%20code%20changes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paletov%2C%20Rumen%20Tsankov%2C%20Petar%20Raychev%2C%20Veselin%20Vechev%2C%20Martin%20Inferring%20crypto%20API%20rules%20from%20code%20changes%202018"
        },
        {
            "id": "Reudismam_2017_a",
            "entry": "Reudismam Rolim, Gustavo Soares, Loris D\u2019Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjorn Hartmann. Learning syntactic program transformations from examples. In Proceedings of the 39th International Conference on Software Engineering, pp. 404\u2013415. IEEE Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reudismam%20Rolim%20Gustavo%20Soares%20Loris%20DAntoni%20Oleksandr%20Polozov%20Sumit%20Gulwani%20Rohit%20Gheyi%20Ryo%20Suzuki%20and%20Bjorn%20Hartmann%20Learning%20syntactic%20program%20transformations%20from%20examples%20In%20Proceedings%20of%20the%2039th%20International%20Conference%20on%20Software%20Engineering%20pp%20404415%20IEEE%20Press%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reudismam%20Rolim%20Gustavo%20Soares%20Loris%20DAntoni%20Oleksandr%20Polozov%20Sumit%20Gulwani%20Rohit%20Gheyi%20Ryo%20Suzuki%20and%20Bjorn%20Hartmann%20Learning%20syntactic%20program%20transformations%20from%20examples%20In%20Proceedings%20of%20the%2039th%20International%20Conference%20on%20Software%20Engineering%20pp%20404415%20IEEE%20Press%202017"
        },
        {
            "id": "Rolim_et+al_2018_a",
            "entry": "Reudismam Rolim, Gustavo Soares, Rohit Gheyi, and Loris D\u2019Antoni. Learning quick fixes from code repositories. arXiv preprint arXiv:1803.03806, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03806"
        },
        {
            "id": "Simonovsky_2018_a",
            "entry": "Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using variational autoencoders. arXiv preprint arXiv:1802.03480, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03480"
        },
        {
            "id": "Sohn_et+al_2015_a",
            "entry": "Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Advances in Neural Information Processing Systems, pp. 3483\u20133491, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015"
        },
        {
            "id": "Veeriah_et+al_2015_a",
            "entry": "Vivek Veeriah, Naifan Zhuang, and Guo-Jun Qi. Differential recurrent neural networks for action recognition. In Proceedings of the IEEE international conference on computer vision, pp. 4041\u2013 4049, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veeriah%2C%20Vivek%20Zhuang%2C%20Naifan%20Qi%2C%20Guo-Jun%20Differential%20recurrent%20neural%20networks%20for%20action%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veeriah%2C%20Vivek%20Zhuang%2C%20Naifan%20Qi%2C%20Guo-Jun%20Differential%20recurrent%20neural%20networks%20for%20action%20recognition%202015"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692\u20132700, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Fortunato%2C%20Meire%20Jaitly%2C%20Navdeep%20Pointer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Fortunato%2C%20Meire%20Jaitly%2C%20Navdeep%20Pointer%20networks%202015"
        },
        {
            "id": "Wu_et+al_2019_a",
            "entry": "Yu Ping Wu, Furu Wei, Shaohan Huang, Zhoujun Li, and Ming Zhou. Response generation by context-aware prototype editing. 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yu%20Ping%20Wei%2C%20Furu%20Huang%2C%20Shaohan%20Li%2C%20Zhoujun%20Response%20generation%20by%20context-aware%20prototype%20editing%202019"
        },
        {
            "id": "Yang_2000_a",
            "entry": "Diyi Yang, Aaron Halfaker, Robert Kraut, and Eduard Hovy. Identifying semantic edit intentions from revisions in wikipedia. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 2000\u20132010, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diyi%20Yang%20Aaron%20Halfaker%20Robert%20Kraut%20and%20Eduard%20Hovy%20Identifying%20semantic%20edit%20intentions%20from%20revisions%20in%20wikipedia%20In%20Proceedings%20of%20the%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pp%2020002010%202017a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diyi%20Yang%20Aaron%20Halfaker%20Robert%20Kraut%20and%20Eduard%20Hovy%20Identifying%20semantic%20edit%20intentions%20from%20revisions%20in%20wikipedia%20In%20Proceedings%20of%20the%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pp%2020002010%202017a"
        },
        {
            "id": "Yang_et+al_0000_a",
            "entry": "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. arXiv preprint arXiv:1702.08139, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08139"
        },
        {
            "id": "Yin_2017_a",
            "entry": "Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 440\u2013450, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Pengcheng%20Neubig%2C%20Graham%20A%20syntactic%20neural%20model%20for%20general-purpose%20code%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Pengcheng%20Neubig%2C%20Graham%20A%20syntactic%20neural%20model%20for%20general-purpose%20code%20generation%202017"
        },
        {
            "id": "Yin_2018_a",
            "entry": "Pengcheng Yin and Graham Neubig. TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) Demo Track, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Pengcheng%20Neubig%2C%20Graham%20TRANX%3A%20A%20transition-based%20neural%20abstract%20syntax%20parser%20for%20semantic%20parsing%20and%20code%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Pengcheng%20Neubig%2C%20Graham%20TRANX%3A%20A%20transition-based%20neural%20abstract%20syntax%20parser%20for%20semantic%20parsing%20and%20code%20generation%202018"
        },
        {
            "id": "Zhuang_2018_a",
            "entry": "Naifan Zhuang, The Duc Kieu, Guo-Jun Qi, and Kien A Hua. Deep differential recurrent neural networks. arXiv preprint arXiv:1804.04192, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04192"
        },
        {
            "id": "WikiAtomicEdits_2018_a",
            "entry": "WikiAtomicEdits We randomly sampled 1040K insertion examples from the English portion of WikiAtomicEdits (Faruqui et al., 2018) dataset, with a train, development and test splits of 1000K, 20K and 20K.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=WikiAtomicEdits%20We%20randomly%20sampled%201040K%20insertion%20examples%20from%20the%20English%20portion%20of%20WikiAtomicEdits%20Faruqui%20et%20al%202018%20dataset%20with%20a%20train%20development%20and%20test%20splits%20of%201000K%2020K%20and%2020K"
        }
    ]
}
