{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE CONVERGENCE OF A CLASS OF ADAM-TYPE ALGORITHMS FOR NON-CONVEX OPTIMIZATION",
        "author": "Xiangyi Chen, Sijia Liu, Ruoyu Sun\u00a7, Mingyi Hong, \u2020ECE, University of Minnesota - Twin Cities \u2021MIT-IBM Watson AI Lab, IBM Research \u00a7ISE, University of Illinois at Urbana-Champaign \u2020{chen,mhong}@umn.edu, \u2021sijia.liu@ibm.com, \u00a7ruoyus@illinois.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1x-x309tm"
        },
        "abstract": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the \u201cAdam-type\u201d, includes the popular algorithms such as Adam (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Ba, 2014</a>) , AMSGrad (<a class=\"ref-link\" id=\"cReddi_et+al_2018_a\" href=\"#rReddi_et+al_2018_a\">Reddi et al., 2018</a>) , AdaGrad (<a class=\"ref-link\" id=\"cDuchi_et+al_2011_a\" href=\"#rDuchi_et+al_2011_a\">Duchi et al., 2011</a>). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the conve\u221argence of the Adam-type methods, with a convergence rate of order O(log T / T ) for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior."
    },
    "keywords": [
        {
            "term": "adaptive learning rate",
            "url": "https://en.wikipedia.org/wiki/adaptive_learning_rate"
        },
        {
            "term": "learning rate",
            "url": "https://en.wikipedia.org/wiki/learning_rate"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "first order",
            "url": "https://en.wikipedia.org/wiki/first_order"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "convex problem",
            "url": "https://en.wikipedia.org/wiki/convex_problem"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "GD": "gradient descent",
        "SGD": "stochastic gradient descent",
        "CNNs": "convolutional neural networks"
    },
    "highlights": [
        "First-order optimization has witnessed tremendous progress in the last decade, especially to solve machine learning problems (<a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\"><a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\">Bottou et al, 2018</a></a>)",
        "Almost every first-order method obeys the following generic form (<a class=\"ref-link\" id=\"cBoyd_2004_a\" href=\"#rBoyd_2004_a\">Boyd & Vandenberghe, 2004</a>), xt+1 = xt \u2212 \u03b1t\u2206t, where xt denotes the solution updated at the tth iteration for t = 1, 2, . . . , T , T is the number of iterations, \u2206t is a certain descent direction, and \u03b1t > 0 is some learning rate",
        "These achievements fall into three categories: a) momentum methods (<a class=\"ref-link\" id=\"cNesterov_1983_a\" href=\"#rNesterov_1983_a\">Nesterov, 1983</a>; <a class=\"ref-link\" id=\"cPolyak_1964_a\" href=\"#rPolyak_1964_a\">Polyak, 1964</a>; <a class=\"ref-link\" id=\"cGhadimi_et+al_2015_a\" href=\"#rGhadimi_et+al_2015_a\">Ghadimi et al, 2015</a>) which carefully design the descent direction \u2206t; b) adaptive learning rate methods (Becker et al, 1988; <a class=\"ref-link\" id=\"cDuchi_et+al_2011_a\" href=\"#rDuchi_et+al_2011_a\">Duchi et al, 2011</a>; <a class=\"ref-link\" id=\"cZeiler_2012_a\" href=\"#rZeiler_2012_a\">Zeiler, 2012</a>; <a class=\"ref-link\" id=\"cDauphin_et+al_2015_a\" href=\"#rDauphin_et+al_2015_a\">Dauphin et al, 2015</a>) which determine good learning rates \u03b1t, and c) adaptive gradient methods that enjoy dual advantages of a) and b)",
        "We provided some mild conditions to ensure convergence of a class of Adam-type algorithms, which includes Adam, AMSGrad, AdaGrad, AdaFom, stochastic gradient descent, stochastic gradient descent with momentum as special cases",
        "We provide insights on how oscillation of effective stepsizes can affect convergence rate for the class of algorithms which could be beneficial for the design of future algorithms",
        "This paper focuses on unconstrained non-convex optimization problems, and one future direction is to study a more general setting of constrained non-convex optimization"
    ],
    "key_statements": [
        "First-order optimization has witnessed tremendous progress in the last decade, especially to solve machine learning problems (<a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\"><a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\">Bottou et al, 2018</a></a>)",
        "Almost every first-order method obeys the following generic form (<a class=\"ref-link\" id=\"cBoyd_2004_a\" href=\"#rBoyd_2004_a\">Boyd & Vandenberghe, 2004</a>), xt+1 = xt \u2212 \u03b1t\u2206t, where xt denotes the solution updated at the tth iteration for t = 1, 2, . . . , T , T is the number of iterations, \u2206t is a certain descent direction, and \u03b1t > 0 is some learning rate",
        "Recent works have proposed a variety of accelerated versions of gradient descent and stochastic gradient descent (<a class=\"ref-link\" id=\"cNesterov_2013_a\" href=\"#rNesterov_2013_a\">Nesterov, 2013</a>)",
        "These achievements fall into three categories: a) momentum methods (<a class=\"ref-link\" id=\"cNesterov_1983_a\" href=\"#rNesterov_1983_a\">Nesterov, 1983</a>; <a class=\"ref-link\" id=\"cPolyak_1964_a\" href=\"#rPolyak_1964_a\">Polyak, 1964</a>; <a class=\"ref-link\" id=\"cGhadimi_et+al_2015_a\" href=\"#rGhadimi_et+al_2015_a\">Ghadimi et al, 2015</a>) which carefully design the descent direction \u2206t; b) adaptive learning rate methods (Becker et al, 1988; <a class=\"ref-link\" id=\"cDuchi_et+al_2011_a\" href=\"#rDuchi_et+al_2011_a\">Duchi et al, 2011</a>; <a class=\"ref-link\" id=\"cZeiler_2012_a\" href=\"#rZeiler_2012_a\">Zeiler, 2012</a>; <a class=\"ref-link\" id=\"cDauphin_et+al_2015_a\" href=\"#rDauphin_et+al_2015_a\">Dauphin et al, 2015</a>) which determine good learning rates \u03b1t, and c) adaptive gradient methods that enjoy dual advantages of a) and b)",
        "To the best of our knowledge, the question that whether adaptive gradient methods such as Adam, AMSGrad, AdaGrad converge for non-convex problems is still open in theory",
        "Contributions Our work aims to build the theory to understand the behavior for a generic class of adaptive gradient methods for non-convex optimization",
        "(Generality) We consider a class of generalized Adam, referred to as the \u201cAdam-type\u201d, and we show for the first time that under suitable conditions about the stepsizes and algorithm parameters, this class of alg\u221aorithms all converge to first-order stationary solutions of the non-convex problem, with O convergence rate",
        "We provide interpretations of the convergence conditions to illustrate why under some circumstances, certain Adam-type algorithms can outperform stochastic gradient descent",
        "Our work focuses on the generic form of exponentially weighted stochastic gradient descent method presented in Algorithm 1, for which we name as generalized Adam due to its resemblance to the original Adam algorithm and many of its variants",
        "In contrast to stochastic gradient descent and Adam, AMSGrad converges in this case since both Term A and Term B grow slower than accumulation of effective stepsizes",
        "We observe that the performance of AdaFom lies between AMSGrad/Adam and AdaGrad. This is not surprising, since AdaFom can be regarded as a momentum version of AdaGrad but uses a simpler adaptive learning rate compared to AMSGrad/Adam",
        "We provided some mild conditions to ensure convergence of a class of Adam-type algorithms, which includes Adam, AMSGrad, AdaGrad, AdaFom, stochastic gradient descent, stochastic gradient descent with momentum as special cases",
        "We provide insights on how oscillation of effective stepsizes can affect convergence rate for the class of algorithms which could be beneficial for the design of future algorithms",
        "This paper focuses on unconstrained non-convex optimization problems, and one future direction is to study a more general setting of constrained non-convex optimization"
    ],
    "summary": [
        "First-order optimization has witnessed tremendous progress in the last decade, especially to solve machine learning problems (<a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\"><a class=\"ref-link\" id=\"cBottou_et+al_2018_a\" href=\"#rBottou_et+al_2018_a\">Bottou et al, 2018</a></a>).",
        "To the best of our knowledge, the question that whether adaptive gradient methods such as Adam, AMSGrad, AdaGrad converge for non-convex problems is still open in theory.",
        "(Generality) We consider a class of generalized Adam, referred to as the \u201cAdam-type\u201d, and we show for the first time that under suitable conditions about the stepsizes and algorithm parameters, this class of alg\u221aorithms all converge to first-order stationary solutions of the non-convex problem, with O convergence rate.",
        "The main technical challenge in analyzing the non-convex version of Adam-type algorithms is that the used update directions could no longer be unbiased estimates of the true gradients.",
        "We characterize how the effective stepsize parameters \u03b1t and vt affect convergence of Adam-type algorithms.",
        "When this term dominates the convergence speed in Theorem 3.1, it is possible that proper design of vt can help reduce this quantity compared with SGD (An example is provided in Appendix 6.1.1 to further illustrate this fact.) in certain cases.",
        "For this problem and Term A always ends up growing as fast as accumulation of effective stepsizes, which implies Adam only converges with diminishing stepsizes even in non-stochastic optimization.",
        "In contrast to SGD and Adam, AMSGrad converges in this case since both Term A and Term B grow slower than accumulation of effective stepsizes.",
        "We use an example to demonstrate the importance of the Term B for the convergence of Adam-type algorithms.",
        "3.3 CONVERGENCE OF AMSGRAD AND ADAFOM Theorem 3.1 provides a general approach for the design of the weighting sequence {vt} and the converge\u221ance analysis of Adam\u221a-type algorithms.",
        "Aside from checking convergence of an algorithm, Theorem 3.1 can provide convergence rates of AdaGrad and AMSGrad, which will be given as corollaries later.",
        "Our proposed convergence rate of AMSGrad matches the result in (<a class=\"ref-link\" id=\"cReddi_et+al_2018_a\" href=\"#rReddi_et+al_2018_a\">Reddi et al, 2018</a>) for stochastic convex optimization.",
        "We compare the empirical performance of Adam-type algorithms, including AMSGrad, Adam, AdaFom and AdaGrad, on training two convolutional neural networks (CNNs).",
        "This is not surprising, since AdaFom can be regarded as a momentum version of AdaGrad but uses a simpler adaptive learning rate compared to AMSGrad/Adam.",
        "We provided some mild conditions to ensure convergence of a class of Adam-type algorithms, which includes Adam, AMSGrad, AdaGrad, AdaFom, SGD, SGD with momentum as special cases.",
        "To the best of our knowledge, the convergence of Adam-type algorithm for non-convex problems was unknown before.",
        "This paper focuses on unconstrained non-convex optimization problems, and one future direction is to study a more general setting of constrained non-convex optimization"
    ],
    "headline": "This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients",
    "reference_links": [
        {
            "id": "Basu_et+al_2018_a",
            "entry": "A. Basu, S. De, A. Mukherjee, and E. Ullah. Convergence guarantees for rmsprop and adam in non-convex optimization and their comparison to nesterov acceleration on autoencoders. arXiv preprint arXiv:1807.06766, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06766"
        },
        {
            "id": "Becker_1988_a",
            "entry": "S. Becker, Y. Le Cun, et al. Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29\u201337. San Matteo, CA: Morgan Kaufmann, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Becker%2C%20S.%20Cun%2C%20Y.Le%20Improving%20the%20convergence%20of%20back-propagation%20learning%20with%20second%20order%20methods%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Becker%2C%20S.%20Cun%2C%20Y.Le%20Improving%20the%20convergence%20of%20back-propagation%20learning%20with%20second%20order%20methods%201988"
        },
        {
            "id": "Bernstein_et+al_2018_a",
            "entry": "J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: compressed optimisation for nonconvex problems. arXiv preprint arXiv:1802.04434, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04434"
        },
        {
            "id": "Bottou_et+al_2018_a",
            "entry": "L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20optimization%202004"
        },
        {
            "id": "Cartis_et+al_2010_a",
            "entry": "C. Cartis, N. I. Gould, and P. L. Toint. On the complexity of steepest descent, newton\u2019s and regularized newton\u2019s methods for nonconvex unconstrained optimization problems. Siam journal on optimization, 20(6): 2833\u20132852, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.I.%20Toint%2C%20P.L.%20On%20the%20complexity%20of%20steepest%20descent%2C%20newton%E2%80%99s%20and%20regularized%20newton%E2%80%99s%20methods%20for%20nonconvex%20unconstrained%20optimization%20problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.I.%20Toint%2C%20P.L.%20On%20the%20complexity%20of%20steepest%20descent%2C%20newton%E2%80%99s%20and%20regularized%20newton%E2%80%99s%20methods%20for%20nonconvex%20unconstrained%20optimization%20problems%202010"
        },
        {
            "id": "Chen_2018_a",
            "entry": "J. Chen and Q. Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. arXiv preprint arXiv:1806.06763, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06763"
        },
        {
            "id": "Dauphin_et+al_2015_a",
            "entry": "Y. Dauphin, H. de Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex optimization. In Advances in neural information processing systems, pp. 1504\u20131512, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.%20de%20Vries%2C%20H.%20Bengio%2C%20Y.%20Equilibrated%20adaptive%20learning%20rates%20for%20non-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.%20de%20Vries%2C%20H.%20Bengio%2C%20Y.%20Equilibrated%20adaptive%20learning%20rates%20for%20non-convex%20optimization%202015"
        },
        {
            "id": "Dozat_2016_a",
            "entry": "T. Dozat. Incorporating nesterov momentum into adam. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dozat%2C%20T.%20Incorporating%20nesterov%20momentum%20into%20adam%202016"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Ghadimi_et+al_2015_a",
            "entry": "E. Ghadimi, H. R. Feyzmahdavian, and M. Johansson. Global convergence of the heavy-ball method for convex optimization. In 2015 European Control Conference (ECC), pp. 310\u2013315. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20E.%20Feyzmahdavian%2C%20H.R.%20Johansson%2C%20M.%20Global%20convergence%20of%20the%20heavy-ball%20method%20for%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20E.%20Feyzmahdavian%2C%20H.R.%20Johansson%2C%20M.%20Global%20convergence%20of%20the%20heavy-ball%20method%20for%20convex%20optimization%202015"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "Ghadimi_2016_a",
            "entry": "S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59\u201399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "H. Huang, C. Wang, and B. Dong. Nostalgic adam: Weighing more of the past gradients when designing the adaptive learning rate. arXiv preprint arXiv:1805.07557, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07557"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10456"
        },
        {
            "id": "Johnson_2013_a",
            "entry": "R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pp. 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lei_et+al_2017_a",
            "entry": "L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pp. 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "Li_2018_a",
            "entry": "X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. arXiv preprint arXiv:1805.08114, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08114"
        },
        {
            "id": "Nemirovskii_et+al_1983_a",
            "entry": "A. Nemirovskii, D. B. Yudin, and E. R. Dawson. Problem complexity and method efficiency in optimization. 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovskii%2C%20A.%20Yudin%2C%20D.B.%20Dawson%2C%20E.R.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "Nesterov_1983_a",
            "entry": "Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1/k 2). In Doklady AN USSR, volume 269, pp. 543\u2013547, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20A%20method%20for%20unconstrained%20convex%20minimization%20problem%20with%20the%20rate%20of%20convergence%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20A%20method%20for%20unconstrained%20convex%20minimization%20problem%20with%20the%20rate%20of%20convergence%201983"
        },
        {
            "id": "Nesterov_2013_a",
            "entry": "Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "Ochs_et+al_2015_a",
            "entry": "P. Ochs, T. Brox, and T. Pock. ipiasco: Inertial proximal algorithm for strongly convex optimization. Journal of Mathematical Imaging and Vision, 53(2):171\u2013181, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ochs%2C%20P.%20Brox%2C%20T.%20Pock%2C%20T.%20ipiasco%3A%20Inertial%20proximal%20algorithm%20for%20strongly%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ochs%2C%20P.%20Brox%2C%20T.%20Pock%2C%20T.%20ipiasco%3A%20Inertial%20proximal%20algorithm%20for%20strongly%20convex%20optimization%202015"
        },
        {
            "id": "Polyak_1964_a",
            "entry": "P. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20P.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20P.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "Reddi_et+al_2016_a",
            "entry": "S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pp. 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Ward_et+al_2018_a",
            "entry": "R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01811"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "T. Yang, Q. Lin, and Z. Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.03257"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "Zhou_et+al_1808_a",
            "entry": "D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018. M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the",
            "arxiv_url": "https://arxiv.org/pdf/1808.05671"
        },
        {
            "id": "Zou_2003_a",
            "entry": "20th International Conference on Machine Learning (ICML-03), pp. 928\u2013936, 2003. F. Zou and L. Shen. On the convergence of adagrad with momentum for training deep neural networks. arXiv preprint arXiv:1808.03408, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.03408"
        }
    ]
}
