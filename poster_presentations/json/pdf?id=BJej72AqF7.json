{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A MAX-AFFINE SPLINE PERSPECTIVE OF RECURRENT NEURAL NETWORKS",
        "author": "Zichao Wang, Randall Balestriero & Richard G. Baraniuk Department of Electrical and Computer Engineering Rice University Houston, TX 77005, USA {zw,rb,richb}@rice.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJej72AqF7"
        },
        "abstract": "We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs). We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator. The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper. First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time. Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input. Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit 2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization. Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions. In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets."
    },
    "keywords": [
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "Backpropagation through time",
            "url": "https://en.wikipedia.org/wiki/Backpropagation_through_time"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "t-SNE",
            "url": "https://en.wikipedia.org/wiki/t-SNE"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent Neural Networks",
        "MASOs": "max-affine spline operators",
        "MASO": "max-affine spline operator",
        "BPTT": "Backpropagation through time",
        "GRU": "gated recurrent unit",
        "AUC": "area under the ROC curve"
    },
    "highlights": [
        "Recurrent neural networks (RNNs) are a powerful class of models for processing sequential inputs and a basic building block for more advanced models that have found success in challenging problems involving sequential data, including sequence classification , sequence generation), speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>), and image captioning (<a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\"><a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\">Mao et al, 2015</a></a>)",
        "We provide a new angle for understanding Recurrent Neural Networks using max-affine spline operators (MASOs) (<a class=\"ref-link\" id=\"cMagnani_2009_a\" href=\"#rMagnani_2009_a\">Magnani & Boyd, 2009</a>; <a class=\"ref-link\" id=\"cHannah_2013_a\" href=\"#rHannah_2013_a\">Hannah & Dunson, 2013</a>) from approximation theory",
        "We prove that an Recurrent Neural Networks with piecewise affine and convex nonlinearities can be rewritten as a composition of max-affine spline operators, making it a piecewise affine spline operator with an elegant analytical form (Section 3)",
        "We provide a new perspective on Recurrent Neural Networks dynamics by visualizing the evolution of the Recurrent Neural Networks input space partitioning through time (Section 4)",
        "We show the piecewise affine mapping in an Recurrent Neural Networks associated with a given input sequence corresponds to an input-dependent template, from which we can interpret the Recurrent Neural Networks as performing greedy template matching at every Recurrent Neural Networks cell (Section 5)",
        "We have developed and explored a novel perspective of Recurrent Neural Networks in terms of max-affine spline operators (MASOs)"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) are a powerful class of models for processing sequential inputs and a basic building block for more advanced models that have found success in challenging problems involving sequential data, including sequence classification , sequence generation), speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>), and image captioning (<a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\"><a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\">Mao et al, 2015</a></a>)",
        "We provide a new angle for understanding Recurrent Neural Networks using max-affine spline operators (MASOs) (<a class=\"ref-link\" id=\"cMagnani_2009_a\" href=\"#rMagnani_2009_a\">Magnani & Boyd, 2009</a>; <a class=\"ref-link\" id=\"cHannah_2013_a\" href=\"#rHannah_2013_a\">Hannah & Dunson, 2013</a>) from approximation theory",
        "We prove that an Recurrent Neural Networks with piecewise affine and convex nonlinearities can be rewritten as a composition of max-affine spline operators, making it a piecewise affine spline operator with an elegant analytical form (Section 3)",
        "We provide a new perspective on Recurrent Neural Networks dynamics by visualizing the evolution of the Recurrent Neural Networks input space partitioning through time (Section 4)",
        "We show the piecewise affine mapping in an Recurrent Neural Networks associated with a given input sequence corresponds to an input-dependent template, from which we can interpret the Recurrent Neural Networks as performing greedy template matching at every Recurrent Neural Networks cell (Section 5)",
        "We rigorously prove that using a random initial hidden state in an Recurrent Neural Networks corresponds to an explicit regularizer that can mollify exploding gradients",
        "Where h( ,t) is the hidden state at layer and time step t, h(0,t) := x(t) which is the input sequence, \u03c3 is an activation function and W ( ), Wr( ), and b( ) are time-invariant parameters at layer . h( ,0) is the initial hidden state at layer which needs to be set to some value beforehand to start the Recurrent Neural Networks recursive computation",
        "We focus on Recurrent Neural Networks with piecewise affine and convex nonlinearities in order to derive rigorous analytical results",
        "The max-affine spline operators viewpoint enables us to see how an Recurrent Neural Networks implicitly partitions its input sequence through time, which provides a new perspective of its dynamics",
        "We demonstrate the evolution of the partition codes of a one-layer ReLU Recurrent Neural Networks trained on the MNIST dataset, with each image flattened into a 1-dimensional sequence so that input at each time step is a single pixel",
        "Leveraging the max-affine spline operators view of Recurrent Neural Networks, we demonstrate that one can improve significantly over a zero initial hidden state by using a random initial hidden state",
        "Our key realization is that the gradient of the Recurrent Neural Networks output with respect to the initial hidden state h(0) features the term Ah from Theorem 3",
        "We have developed and explored a novel perspective of Recurrent Neural Networks in terms of max-affine spline operators (MASOs)",
        "The spline viewpoint suggested that the typical zero initial hidden state be replaced with a random one that mollifies the exploding gradient problem and improves generalization performance"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) are a powerful class of models for processing sequential inputs and a basic building block for more advanced models that have found success in challenging problems involving sequential data, including sequence classification , sequence generation), speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>), and image captioning (<a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\"><a class=\"ref-link\" id=\"cMao_et+al_2015_a\" href=\"#rMao_et+al_2015_a\">Mao et al, 2015</a></a>).",
        "The MASO formulation of RNNs enables us to theoretically justify the use of a random initial hidden state to improve RNN performance.",
        "We rigorously prove that using a random initial hidden state in an RNN corresponds to an explicit regularizer that can mollify exploding gradients.",
        "The output of the overall RNN is typically an affine transformation of the hidden state of the last layer L at time step t z(t) = W h(L,t) + b .",
        "The MASO viewpoint enables us to see how an RNN implicitly partitions its input sequence through time, which provides a new perspective of its dynamics.",
        "We demonstrate the evolution of the partition codes of a one-layer ReLU RNN trained on the MNIST dataset, with each image flattened into a 1-dimensional sequence so that input at each time step is a single pixel.",
        "This choice regularizes the affine slope parameter associated with the initial hidden state and mollifies the so-called exploding gradient problem (<a class=\"ref-link\" id=\"cPascanu_et+al_2013_a\" href=\"#rPascanu_et+al_2013_a\">Pascanu et al, 2013</a>).",
        "We first state our theoretical result that using random initial hidden state corresponds to an explicit regularization and discuss its impact on exploding gradients.",
        "Our key realization is that the gradient of the RNN output with respect to the initial hidden state h(0) features the term Ah from Theorem 3",
        "We report on the results of a number of experiments that indicate the significant performance gains that can be obtained using a random initial hidden state of properly chosen standard deviation \u03c3 .",
        "We used RMSprop to train ReLU RNNs of one and two layers with and without random initial hidden state on the MNIST, permuted MNIST4 and SST-2 datasets.",
        "Inspired by the results of the previous experiment, we integrated a random initial hidden state into some more complex RNN models.",
        "We first evaluate a one-layer gated recurrent unit (GRU) on the MNIST and permuted MNIST datasets, with a random and zero initial hidden state.",
        "These encouraging preliminary results suggest that, while more theoretical and empirical investigations are needed, a random initial hidden state can boost the performance of complicated RNN models that are not piecewise affine and convex.",
        "The spline viewpoint suggested that the typical zero initial hidden state be replaced with a random one that mollifies the exploding gradient problem and improves generalization performance.",
        "We can apply recent random matrix theory results (Martin & Mahoney, 2018) to the affine parameter ARNN to understand RNN training dynamics."
    ],
    "headline": "We develop a framework for understanding and improving recurrent neural networks using max-affine spline operators",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2016_a",
            "entry": "M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In Proc. Int. Conf. Mach. Learn. (ICML), volume 48, pp. 1120\u20131128, Jun. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016-06"
        },
        {
            "id": "Bahdanau_et+al_1409_a",
            "entry": "D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ArXiv e-prints, 1409.0473, September 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Balestriero_1805_a",
            "entry": "R. Balestriero and R. G. Baraniuk. Mad max: Affine spline insights into deep learning. ArXiv e-prints, 1805.06576, May 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06576"
        },
        {
            "id": "Balestriero_2018_a",
            "entry": "R. Balestriero and R. G. Baraniuk. From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference. ArXiv e-prints, 1810.09274, Oct 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1810.09274"
        },
        {
            "id": "Balestriero_2018_b",
            "entry": "R. Balestriero and R. G. Baraniuk. A spline theory of deep networks. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80, pp. 374\u2013383, Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balestriero%2C%20R.%20Baraniuk%2C%20R.G.%20A%20spline%20theory%20of%20deep%20networks%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balestriero%2C%20R.%20Baraniuk%2C%20R.G.%20A%20spline%20theory%20of%20deep%20networks%202018-07"
        },
        {
            "id": "Cakir_et+al_2017_a",
            "entry": "E. Cakir, S. Adavanne, G. Parascandolo, K. Drossos, and T. Virtanen. Convolutional recurrent neural networks for bird audio detection. In Eur. Signal Process. Conf. (EUSIPCO), pp. 1744\u20131748, Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cakir%2C%20E.%20Adavanne%2C%20S.%20Parascandolo%2C%20G.%20Drossos%2C%20K.%20Convolutional%20recurrent%20neural%20networks%20for%20bird%20audio%20detection%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cakir%2C%20E.%20Adavanne%2C%20S.%20Parascandolo%2C%20G.%20Drossos%2C%20K.%20Convolutional%20recurrent%20neural%20networks%20for%20bird%20audio%20detection%202017-08"
        },
        {
            "id": "Cooijmans_et+al_2017_a",
            "entry": "T. Cooijmans, N. Ballas, C. Laurent, C. Gulcehre, and A. C. Courville. Recurrent batch normalization. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cooijmans%2C%20T.%20Ballas%2C%20N.%20Laurent%2C%20C.%20Gulcehre%2C%20C.%20Recurrent%20batch%20normalization%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cooijmans%2C%20T.%20Ballas%2C%20N.%20Laurent%2C%20C.%20Gulcehre%2C%20C.%20Recurrent%20batch%20normalization%202017-04"
        },
        {
            "id": "Dieng_et+al_2018_a",
            "entry": "A. Dieng, R. Ranganath, J. Altosaar, and D. Blei. Noisin: Unbiased regularization for recurrent neural networks. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80, pp. 1252\u20131261, Jul. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieng%2C%20A.%20Ranganath%2C%20R.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Noisin%3A%20Unbiased%20regularization%20for%20recurrent%20neural%20networks%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieng%2C%20A.%20Ranganath%2C%20R.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Noisin%3A%20Unbiased%20regularization%20for%20recurrent%20neural%20networks%202018-07"
        },
        {
            "id": "Elman_1990_a",
            "entry": "J. L. Elman. Finding structure in time. Cogn. Sci., 14:179\u2013211, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elman%2C%20J.L.%20Finding%20structure%20in%20time%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elman%2C%20J.L.%20Finding%20structure%20in%20time%201990"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "X Glorot, Bordes A., and Y. Bengio. Deep sparse rectifier neural networks. In Proc. Int. Conf. Artificial Intell. and Statist. (AISTATS), volume 15, pp. 315\u2013323, Apr. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20A.%2C%20Bordes%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20A.%2C%20Bordes%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011-04"
        },
        {
            "id": "Graves_et+al_2013_a",
            "entry": "A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Proc. IEEE Int. Conf. Acoust., Speech and Signal Process. (ICASSP), pp. 6645\u20136649, May 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20A.%20Mohamed%2C%20A.%20Hinton%2C%20G.%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20A.%20Mohamed%2C%20A.%20Hinton%2C%20G.%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013-05"
        },
        {
            "id": "Hammer_2000_a",
            "entry": "B. Hammer. On the approximation capability of recurrent neural networks. Neurocomputing, 31(1): 107\u2013123, Mar. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hammer%2C%20B.%20On%20the%20approximation%20capability%20of%20recurrent%20neural%20networks%202000-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hammer%2C%20B.%20On%20the%20approximation%20capability%20of%20recurrent%20neural%20networks%202000-03"
        },
        {
            "id": "Hannah_2013_a",
            "entry": "L. A. Hannah and D. B. Dunson. Multivariate convex regression with adaptive partitioning. J. Mach. Learn. Res., 14:3261\u20133294, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hannah%2C%20L.A.%20Dunson%2C%20D.B.%20Multivariate%20convex%20regression%20with%20adaptive%20partitioning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hannah%2C%20L.A.%20Dunson%2C%20D.B.%20Multivariate%20convex%20regression%20with%20adaptive%20partitioning%202013"
        },
        {
            "id": "Helfrich_et+al_2018_a",
            "entry": "K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled Cayley transform. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80, pp. 1969\u20131978, Jul. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Helfrich%2C%20K.%20Willmott%2C%20D.%20Ye%2C%20Q.%20Orthogonal%20recurrent%20neural%20networks%20with%20scaled%20Cayley%20transform%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Helfrich%2C%20K.%20Willmott%2C%20D.%20Ye%2C%20Q.%20Orthogonal%20recurrent%20neural%20networks%20with%20scaled%20Cayley%20transform%202018-07"
        },
        {
            "id": "Henaff_et+al_2016_a",
            "entry": "M. Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In Proc. Int. Conf. Mach. Learn. (ICML), volume 48, pp. 2034\u20132042, Jun. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henaff%2C%20M.%20Szlam%2C%20A.%20LeCun%2C%20Y.%20Recurrent%20orthogonal%20networks%20and%20long-memory%20tasks%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henaff%2C%20M.%20Szlam%2C%20A.%20LeCun%2C%20Y.%20Recurrent%20orthogonal%20networks%20and%20long-memory%20tasks%202016-06"
        },
        {
            "id": "Hyland_2017_a",
            "entry": "S. L. Hyland and G. Ratsch. Learning unitary operators with help from u (n). In Proc. AAAI conf. Artificial Intell., pp. 2050\u20132058, Feb. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyland%2C%20S.L.%20Ratsch%2C%20G.%20Learning%20unitary%20operators%20with%20help%20from%20u%20%28n%29%202017-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyland%2C%20S.L.%20Ratsch%2C%20G.%20Learning%20unitary%20operators%20with%20help%20from%20u%20%28n%29%202017-02"
        },
        {
            "id": "Jing_et+al_2017_a",
            "entry": "L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljacic. Tunable efficient unitary neural networks (EUNN) and their application to RNNs. In Proc. Int. Conf. Mach. Learn. (ICML), volume 70, pp. 1733\u20131741, Aug. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20RNNs%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20RNNs%202017-08"
        },
        {
            "id": "Jose_et+al_2018_a",
            "entry": "C. Jose, M. Cisse, and F. Fleuret. Kronecker recurrent units. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jose%2C%20C.%20Cisse%2C%20M.%20Fleuret%2C%20F.%20Kronecker%20recurrent%20units%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jose%2C%20C.%20Cisse%2C%20M.%20Fleuret%2C%20F.%20Kronecker%20recurrent%20units%202018-04"
        },
        {
            "id": "Krueger_et+al_2017_a",
            "entry": "D Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, H. Larochelle, A. C. Courville, and C. Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krueger%2C%20D.%20Maharaj%2C%20T.%20Kramar%2C%20J.%20Pezeshki%2C%20M.%20Zoneout%3A%20Regularizing%20rnns%20by%20randomly%20preserving%20hidden%20activations%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krueger%2C%20D.%20Maharaj%2C%20T.%20Kramar%2C%20J.%20Pezeshki%2C%20M.%20Zoneout%3A%20Regularizing%20rnns%20by%20randomly%20preserving%20hidden%20activations%202017-04"
        },
        {
            "id": "Le_et+al_1504_a",
            "entry": "Q. V. Le, N. Jaitly, and G. E. Hinton. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. ArXiv e-prints, 1504.00941, Apr. 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "J. Li, X. Chen, E. Hovy, and D. Jurafsky. Visualizing and understanding neural models in NLP. In Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol. (NAACL HLT), pp. 681\u2013691, Jun. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Chen%2C%20X.%20Hovy%2C%20E.%20Jurafsky%2C%20D.%20Visualizing%20and%20understanding%20neural%20models%20in%20NLP%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Chen%2C%20X.%20Hovy%2C%20E.%20Jurafsky%2C%20D.%20Visualizing%20and%20understanding%20neural%20models%20in%20NLP%202016-06"
        },
        {
            "id": "Magnani_2009_a",
            "entry": "A. Magnani and S. P. Boyd. Convex piecewise-linear fitting. Optimization Eng., 10(1):1\u201317, Mar. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009-03"
        },
        {
            "id": "Mao_et+al_2015_a",
            "entry": "J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). In Proc. Int. Conf. Learn. Representations (ICLR), May 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20J.%20Xu%2C%20W.%20Yang%2C%20Y.%20Wang%2C%20J.%20Deep%20captioning%20with%20multimodal%20recurrent%20neural%20networks%20%28m-rnn%29%202015-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20J.%20Xu%2C%20W.%20Yang%2C%20Y.%20Wang%2C%20J.%20Deep%20captioning%20with%20multimodal%20recurrent%20neural%20networks%20%28m-rnn%29%202015-05"
        },
        {
            "id": "Martin_1810_a",
            "entry": "C. H. Martin and M. W. Mahoney. Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning. ArXiv e-prints, 1810.01075, Oct 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.01075"
        },
        {
            "id": "Mhammedi_et+al_2017_a",
            "entry": "Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In Proc. Int. Conf. Mach. Learn. (ICML), volume 70, pp. 2401\u20132409, Aug. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhammedi%2C%20Z.%20Hellicar%2C%20A.%20Rahman%2C%20A.%20Bailey%2C%20J.%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhammedi%2C%20Z.%20Hellicar%2C%20A.%20Rahman%2C%20A.%20Bailey%2C%20J.%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017-08"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 1310\u20131318, Jun. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013-06"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pp. 1532\u20131543, Oct. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10"
        },
        {
            "id": "Pham_et+al_2014_a",
            "entry": "V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout improves recurrent neural networks for handwriting recognition. In Proc. Int. Conf. Frontiers Handwriting Recognition (ICFHR), pp. 285\u2013290, Sept. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pham%2C%20V.%20Bluche%2C%20T.%20Kermorvant%2C%20C.%20Louradour%2C%20J.%20Dropout%20improves%20recurrent%20neural%20networks%20for%20handwriting%20recognition%202014-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pham%2C%20V.%20Bluche%2C%20T.%20Kermorvant%2C%20C.%20Louradour%2C%20J.%20Dropout%20improves%20recurrent%20neural%20networks%20for%20handwriting%20recognition%202014-09"
        },
        {
            "id": "Schafer_2006_a",
            "entry": "A. M. Schafer and H. G. Zimmermann. Recurrent neural networks are universal approximators. In Proc. Int. Conf. Artificial Neural Netw. (ICANN), pp. 632\u2013640, Sept. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schafer%2C%20A.M.%20Zimmermann%2C%20H.G.%20Recurrent%20neural%20networks%20are%20universal%20approximators%202006-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schafer%2C%20A.M.%20Zimmermann%2C%20H.G.%20Recurrent%20neural%20networks%20are%20universal%20approximators%202006-09"
        },
        {
            "id": "Siegelmann_1995_a",
            "entry": "H. T. Siegelmann and E. D. Sontag. On the computational power of neural nets. J. Comput. Syst. Sci., 50(1):132\u2013150, Feb. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siegelmann%2C%20H.T.%20Sontag%2C%20E.D.%20On%20the%20computational%20power%20of%20neural%20nets%201995-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siegelmann%2C%20H.T.%20Sontag%2C%20E.D.%20On%20the%20computational%20power%20of%20neural%20nets%201995-02"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pp. 1631\u20131642, Oct. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20R.%20Perelygin%2C%20A.%20Wu%2C%20J.%20Chuang%2C%20J.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20R.%20Perelygin%2C%20A.%20Wu%2C%20J.%20Chuang%2C%20J.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013-10"
        },
        {
            "id": "Stowell_2014_a",
            "entry": "D. Stowell and M. D. Plumbley. An open dataset for research on audio field recording archives: Freefield1010. In Proc. Audio Eng. Soc. 53rd Conf. Semantic Audio (AES53), pp. 1\u20137, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stowell%2C%20D.%20Plumbley%2C%20M.D.%20An%20open%20dataset%20for%20research%20on%20audio%20field%20recording%20archives%3A%20Freefield1010%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stowell%2C%20D.%20Plumbley%2C%20M.D.%20An%20open%20dataset%20for%20research%20on%20audio%20field%20recording%20archives%3A%20Freefield1010%202014"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proc. Annu. Meeting Assoc. Comput. Linguistics (ACL), pp. 1556\u20131566, Jul. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tai%2C%20Kai%20Sheng%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Improved%20semantic%20representations%20from%20tree-structured%20long%20short-term%20memory%20networks%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tai%2C%20Kai%20Sheng%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Improved%20semantic%20representations%20from%20tree-structured%20long%20short-term%20memory%20networks%202015-07"
        },
        {
            "id": "Talathi_2016_a",
            "entry": "S. S. Talathi and A. Vartak. Improving performance of recurrent network network with relu nonlinearity. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talathi%2C%20S.S.%20Vartak%2C%20A.%20Improving%20performance%20of%20recurrent%20network%20network%20with%20relu%20nonlinearity%202016-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talathi%2C%20S.S.%20Vartak%2C%20A.%20Improving%20performance%20of%20recurrent%20network%20network%20with%20relu%20nonlinearity%202016-04"
        },
        {
            "id": "Teng_et+al_2016_a",
            "entry": "Z. Teng, D. T. Vo, and Y. Zhang. Context-sensitive lexicon features for neural sentiment analysis. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pp. 1629\u20131638, Nov. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teng%2C%20Z.%20Vo%2C%20D.T.%20Zhang%2C%20Y.%20Context-sensitive%20lexicon%20features%20for%20neural%20sentiment%20analysis%202016-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teng%2C%20Z.%20Vo%2C%20D.T.%20Zhang%2C%20Y.%20Context-sensitive%20lexicon%20features%20for%20neural%20sentiment%20analysis%202016-11"
        },
        {
            "id": "Van_2008_a",
            "entry": "L. van der Maaten and G. Hinton. Visualizing data using t-SNE. J. Mach. Learn. Res., 9(Nov): 2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "Van_1992_a",
            "entry": "H. L. van Trees. Detection, Estimation, and Modulation Theory: Radar-Sonar Signal Processing and Gaussian Signals in Noise. Krieger Publishing Co., Inc., 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Trees.%20Detection%2C%20H.L.%20Estimation%20and%20Modulation%20Theory%3A%20Radar-Sonar%20Signal%20Processing%20and%20Gaussian%20Signals%20in%20Noise%201992"
        },
        {
            "id": "Van_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 H. L. van Trees. Detection, estimation, and modulation theory, part I. John Wiley & Sons, Inc., 2013. S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Proc. Advances",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Trees.%20Detection%2C%20H.L.%20estimation%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Trees.%20Detection%2C%20H.L.%20estimation%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Syst_2013_a",
            "entry": "Neural Inform. Process. Syst. (NIPS), volume 1, pp. 351\u2013359, Dec. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neural%20Inform%20Process%20Syst%20NIPS%20volume%201%20pp%20351359%20Dec%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neural%20Inform%20Process%20Syst%20NIPS%20volume%201%20pp%20351359%20Dec%202013"
        },
        {
            "id": "Wisdom_et+al_2016_a",
            "entry": "S. Wisdom, T. Powers, J. R. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. In Proc. Advances Neural Inform. Process. Syst. (NIPS), Dec. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20S.%20Powers%2C%20T.%20Hershey%2C%20J.R.%20Roux%2C%20J.Le%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20S.%20Powers%2C%20T.%20Hershey%2C%20J.R.%20Roux%2C%20J.Le%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016-12"
        },
        {
            "id": "Zaremba_et+al_1409_a",
            "entry": "W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent Neural Network Regularization. ArXiv e-prints, 1409.2329, September 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        },
        {
            "id": "E-Prints_1511_a",
            "entry": "ArXiv e-prints, 1511.08630, November 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08630"
        },
        {
            "id": "Neural_2012_a",
            "entry": "Neural Netw.: Tricks of the Trade, pp. 687\u2013707, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neural%20Netw%20Tricks%20of%20the%20Trade%20pp%20687707%202012"
        },
        {
            "id": "S_2014_a",
            "entry": "SST-2. The dataset7 consists of 6920, 872, 1821 sentences in the training, validation and test set, respectively. Total number of vocabulary is 17539, and average sentence length is 19.67. Each sentence is minimally processed into sequences of words and use a fixed-dimensional and trainable vector to represent each word. We initialize these vectors either randomly or using GloVe (Pennington et al., 2014). Due to the small size of the dataset, the phrases in each sentence that have semantic labels are also used as part of the training set in addition to the whole sentence during training. Dropout of 0.5 is applied to all experiments. Phrases are not used during validation and testing, i.e., we always use entire sentences during validation and testing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SST2%20The%20dataset7%20consists%20of%206920%20872%201821%20sentences%20in%20the%20training%20validation%20and%20test%20set%20respectively%20Total%20number%20of%20vocabulary%20is%2017539%20and%20average%20sentence%20length%20is%201967%20Each%20sentence%20is%20minimally%20processed%20into%20sequences%20of%20words%20and%20use%20a%20fixeddimensional%20and%20trainable%20vector%20to%20represent%20each%20word%20We%20initialize%20these%20vectors%20either%20randomly%20or%20using%20GloVe%20Pennington%20et%20al%202014%20Due%20to%20the%20small%20size%20of%20the%20dataset%20the%20phrases%20in%20each%20sentence%20that%20have%20semantic%20labels%20are%20also%20used%20as%20part%20of%20the%20training%20set%20in%20addition%20to%20the%20whole%20sentence%20during%20training%20Dropout%20of%2005%20is%20applied%20to%20all%20experiments%20Phrases%20are%20not%20used%20during%20validation%20and%20testing%20ie%20we%20always%20use%20entire%20sentences%20during%20validation%20and%20testing",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SST2%20The%20dataset7%20consists%20of%206920%20872%201821%20sentences%20in%20the%20training%20validation%20and%20test%20set%20respectively%20Total%20number%20of%20vocabulary%20is%2017539%20and%20average%20sentence%20length%20is%201967%20Each%20sentence%20is%20minimally%20processed%20into%20sequences%20of%20words%20and%20use%20a%20fixeddimensional%20and%20trainable%20vector%20to%20represent%20each%20word%20We%20initialize%20these%20vectors%20either%20randomly%20or%20using%20GloVe%20Pennington%20et%20al%202014%20Due%20to%20the%20small%20size%20of%20the%20dataset%20the%20phrases%20in%20each%20sentence%20that%20have%20semantic%20labels%20are%20also%20used%20as%20part%20of%20the%20training%20set%20in%20addition%20to%20the%20whole%20sentence%20during%20training%20Dropout%20of%2005%20is%20applied%20to%20all%20experiments%20Phrases%20are%20not%20used%20during%20validation%20and%20testing%20ie%20we%20always%20use%20entire%20sentences%20during%20validation%20and%20testing"
        },
        {
            "id": "Dataset_2014_a",
            "entry": "Bird Audio Dataset. The dataset8 consists of 7, 000 field recording signals of 10 seconds sampled at 44 kHz from the Freesound Stowell & Plumbley (2014) audio archive representing slightly less than 20 hours of audio signals. The audio waveforms are extracted from diverse scenes such as city, nature, train, voice, water, etc., some of which include bird sounds. The labels regarding the bird detection task can be found in the file freefield1010. Performance is measured by Area Under Curve (AUC) due to the unbalanced distribution of the classes. We preprocess every audio clip by first using short-time Fourier transform (STFT) with 40ms and 50% overlapping Hamming window to obtain audio spectrum and then by extracting 40 log mel-band energy features. After preprocessing, each input is of dimension D = 96 and T = 999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dataset%2C%20Bird%20Audio%20The%20dataset8%20consists%20of%207%2C%20000%20field%20recording%20signals%20of%2010%20seconds%20sampled%20at%2044%20kHz%20from%20the%20Freesound%20Stowell%20%26%20Plumbley%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dataset%2C%20Bird%20Audio%20The%20dataset8%20consists%20of%207%2C%20000%20field%20recording%20signals%20of%2010%20seconds%20sampled%20at%2044%20kHz%20from%20the%20Freesound%20Stowell%20%26%20Plumbley%202014"
        },
        {
            "id": "Setup_2008_a",
            "entry": "Setup of input space partitioning experiments. For the results in the main text, we use t-SNE visualization (van der Maaten & Hinton, 2008) with 2 dimensions and the default settings from the python sklearn package. Visualization is performed on the whole 10k test set images. For finding the nearest neighbors of examples in the SST-2 dataset, since the examples are of varying lengths, we constrain the distance comparison to within +/-10 words of the target sentence. When the sentence lengths are not the same, we simply pad the shorter ones to the longest one, then process it with RNN and finally calculate the distance as the 2 distance of the partition codes (i.e., concatenation of all hidden states) that RNN computes. We justify the comparison between examples of different lengths using padding by noting that batching examples and padding the examples to the longest example within a batch has been a common practice in modern natural language processing tasks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Setup%20of%20input%20space%20partitioning%20experiments%20For%20the%20results%20in%20the%20main%20text%20we%20use%20tSNE%20visualization%20van%20der%20Maaten%20%20Hinton%202008%20with%202%20dimensions%20and%20the%20default%20settings%20from%20the%20python%20sklearn%20package%20Visualization%20is%20performed%20on%20the%20whole%2010k%20test%20set%20images%20For%20finding%20the%20nearest%20neighbors%20of%20examples%20in%20the%20SST2%20dataset%20since%20the%20examples%20are%20of%20varying%20lengths%20we%20constrain%20the%20distance%20comparison%20to%20within%2010%20words%20of%20the%20target%20sentence%20When%20the%20sentence%20lengths%20are%20not%20the%20same%20we%20simply%20pad%20the%20shorter%20ones%20to%20the%20longest%20one%20then%20process%20it%20with%20RNN%20and%20finally%20calculate%20the%20distance%20as%20the%202%20distance%20of%20the%20partition%20codes%20ie%20concatenation%20of%20all%20hidden%20states%20that%20RNN%20computes%20We%20justify%20the%20comparison%20between%20examples%20of%20different%20lengths%20using%20padding%20by%20noting%20that%20batching%20examples%20and%20padding%20the%20examples%20to%20the%20longest%20example%20within%20a%20batch%20has%20been%20a%20common%20practice%20in%20modern%20natural%20language%20processing%20tasks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Setup%20of%20input%20space%20partitioning%20experiments%20For%20the%20results%20in%20the%20main%20text%20we%20use%20tSNE%20visualization%20van%20der%20Maaten%20%20Hinton%202008%20with%202%20dimensions%20and%20the%20default%20settings%20from%20the%20python%20sklearn%20package%20Visualization%20is%20performed%20on%20the%20whole%2010k%20test%20set%20images%20For%20finding%20the%20nearest%20neighbors%20of%20examples%20in%20the%20SST2%20dataset%20since%20the%20examples%20are%20of%20varying%20lengths%20we%20constrain%20the%20distance%20comparison%20to%20within%2010%20words%20of%20the%20target%20sentence%20When%20the%20sentence%20lengths%20are%20not%20the%20same%20we%20simply%20pad%20the%20shorter%20ones%20to%20the%20longest%20one%20then%20process%20it%20with%20RNN%20and%20finally%20calculate%20the%20distance%20as%20the%202%20distance%20of%20the%20partition%20codes%20ie%20concatenation%20of%20all%20hidden%20states%20that%20RNN%20computes%20We%20justify%20the%20comparison%20between%20examples%20of%20different%20lengths%20using%20padding%20by%20noting%20that%20batching%20examples%20and%20padding%20the%20examples%20to%20the%20longest%20example%20within%20a%20batch%20has%20been%20a%20common%20practice%20in%20modern%20natural%20language%20processing%20tasks"
        }
    ]
}
