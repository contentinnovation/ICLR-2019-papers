{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "FROM LANGUAGE TO GOALS: INVERSE REINFORCEMENT LEARNING FOR VISION-BASED INSTRUCTION FOLLOWING",
        "author": "Justin Fu ,\u2217Anoop Korattikara, Sergey Levine, Sergio Guadarrama Google AI {justinfu,kbanoop,slevine,sguada}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1lq1hRqYQ"
        },
        "abstract": "Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a languageconditioned policy leads to poor performance."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "fully-connected network",
            "url": "https://en.wikipedia.org/wiki/fully-connected_network"
        }
    ],
    "abbreviations": {
        "LC-RL": "LANGUAGE-CONDITIONED REWARD LEARNING",
        "MDP": "Markov decision process",
        "IRL": "Inverse reinforcement learning",
        "CNN": "convolutional neural network",
        "FC": "fully-connected network"
    },
    "highlights": [
        "While reinforcement learning provides a powerful and flexible framework for describing and solving control tasks, it requires the practitioner to specify objectives in terms of reward functions",
        "We investigate the feasibility of grounding free-form natural language commands as reward functions using inverse reinforcement learning (IRL)",
        "To create a more fair comparison, we compare against GAIL using a dynamic programming solver, and we see that the performance is comparable to LANGUAGE-CONDITIONED REWARD LEARNING on training environments, but performs significantly worse on test environments. These results are in line with our intuitions - GAIL and Inverse reinforcement learning are equivalent in training scenarios (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon, 2016</a>), but the discriminator of GAIL does not correspond to the true reward function, and performs worse when evaluated in novel environments",
        "We found that both LANGUAGE-CONDITIONED REWARD LEARNING and Reward Regression were able to learn reward functions which generalize to both novel tasks and novel house layouts, and both achieve significant performance over the policy-based approach",
        "We introduced LANGUAGE-CONDITIONED REWARD LEARNING, an algorithm for scalable training of language-conditioned reward functions represented by neural networks",
        "Our method restricts training to tractable domains with known dynamics, but learns a reward function which can be used with standard RL methods in environments with unknown dynamics"
    ],
    "key_statements": [
        "While reinforcement learning provides a powerful and flexible framework for describing and solving control tasks, it requires the practitioner to specify objectives in terms of reward functions",
        "Engineering reward functions is often done by experienced practitioners and researchers, and even can pose a significant challenge, such as when working with complex image-based observations",
        "A common approach to building natural lan- This is a simple example where the reward funcguage interfaces for reinforcement learning tion is easier to specify than the policy",
        "A simple example is shown in Figure 1, where an agent is tasked with navigating through a house",
        "We investigate the feasibility of grounding free-form natural language commands as reward functions using inverse reinforcement learning (IRL)",
        "In order to isolate the language-learning problem from the difficulties in solving reinforcement learning and adversarial learning problems, we base our method on an exact MaxEnt Inverse reinforcement learning (Ziebart, 2010) procedure, which requires full knowledge of environment dynamics to train a language-conditioned reward function represented by a deep neural network",
        "We evaluate our method on a dataset of realistic indoor house navigation and pick-and-place tasks using the SUNCG dataset, with natural language commands",
        "Our experiments show that policy-based approaches have worse generalization performance to new environments, because the policy must rely on zero-shot generalization at test time as we show in Section 6.3",
        "While in this paper we argue for the performance benefits of a reward-based approach, a reason one may want to adopt a policy-based approach over a reward-based one is if one cannot run RL to train a new policy in a new environment, such as for time or safety reasons",
        "We believe our work is the first to apply language-conditioned inverse reinforcement learning to environments with image observations and deep neural networks, and we show that our rewards generalize to novel tasks and environments",
        "A unique challenge of the language-conditioned Inverse reinforcement learning problem, compared to standard Inverse reinforcement learning, is that the goal is to learn a reward function that generalizes across multiple tasks",
        "We adopt a similar approach adapted for the language-Inverse reinforcement learning problem, and formalize the notion of a task, denoted by \u03be, as an Markov decision process, where individual tasks may not share the same state spaces, dynamics or reward functions",
        "At test time we can use standard model-free RL algorithms to learn the task from the inferred reward function in new environments",
        "LANGUAGE-CONDITIONED REWARD LEARNING refers to the language-conditioned Inverse reinforcement learning method outlined in Section 5, which takes as input demonstration and language pairs and learns a shared reward function across all tasks",
        "To create a more fair comparison, we compare against GAIL using a dynamic programming solver, and we see that the performance is comparable to LANGUAGE-CONDITIONED REWARD LEARNING on training environments, but performs significantly worse on test environments. These results are in line with our intuitions - GAIL and Inverse reinforcement learning are equivalent in training scenarios (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon, 2016</a>), but the discriminator of GAIL does not correspond to the true reward function, and performs worse when evaluated in novel environments",
        "Our main experimental results on reward learning are reported in Table 1, and experiments in reoptimizing the learned reward function are reported in Table 2",
        "We found that both LANGUAGE-CONDITIONED REWARD LEARNING and Reward Regression were able to learn reward functions which generalize to both novel tasks and novel house layouts, and both achieve significant performance over the policy-based approach",
        "We found that Reward Regression has superior performance when compared to LANGUAGE-CONDITIONED REWARD LEARNING, due to the fact that it uses oracle ground-truth supervision",
        "We introduced LANGUAGE-CONDITIONED REWARD LEARNING, an algorithm for scalable training of language-conditioned reward functions represented by neural networks",
        "Our method restricts training to tractable domains with known dynamics, but learns a reward function which can be used with standard RL methods in environments with unknown dynamics"
    ],
    "summary": [
        "While reinforcement learning provides a powerful and flexible framework for describing and solving control tasks, it requires the practitioner to specify objectives in terms of reward functions.",
        "Agents is to build language-conditioned policies that directly map observations and language commands to a sequence of actions that perform the desired task.",
        "In order to isolate the language-learning problem from the difficulties in solving reinforcement learning and adversarial learning problems, we base our method on an exact MaxEnt IRL (Ziebart, 2010) procedure, which requires full knowledge of environment dynamics to train a language-conditioned reward function represented by a deep neural network.",
        "We believe our work is the first to apply language-conditioned inverse reinforcement learning to environments with image observations and deep neural networks, and we show that our rewards generalize to novel tasks and environments.",
        "A unique challenge of the language-conditioned IRL problem, compared to standard IRL, is that the goal is to learn a reward function that generalizes across multiple tasks.",
        "We adopt a similar approach adapted for the language-IRL problem, and formalize the notion of a task, denoted by \u03be, as an MDP, where individual tasks may not share the same state spaces, dynamics or reward functions.",
        "In order to optimize this objective, we first require that all tasks share the same observation space and action space, and the reward to be a function of the observation, rather than of the state.",
        "At test time we can use standard model-free RL algorithms to learn the task from the inferred reward function in new environments.",
        "LC-RL refers to the language-conditioned IRL method outlined in Section 5, which takes as input demonstration and language pairs and learns a shared reward function across all tasks.",
        "We experiment with reoptimizing the learned reward using DQN (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a>), a sample-based RL method that does not require ground-truth knowledge of the environment dynamics.",
        "These results are in line with our intuitions - GAIL and IRL are equivalent in training scenarios (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon, 2016</a>), but the discriminator of GAIL does not correspond to the true reward function, and performs worse when evaluated in novel environments.",
        "We found that both LC-RL and Reward Regression were able to learn reward functions which generalize to both novel tasks and novel house layouts, and both achieve significant performance over the policy-based approach.",
        "With proper exploration methods, we believe that language-conditioned reward learning provides a performant and conceptually simple method for grounding language as concrete tasks an agent can perform within an interactive environment.",
        "We demonstrate that the reward-learning approach to instruction following outperforms the policy-learning when evaluated in test environments, because the reward-learning enables an agent to learn and interact within the test environment rather than relying on zero-shot policy transfer"
    ],
    "headline": "How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments",
    "reference_links": [
        {
            "id": "Anderson_et+al_2018_a",
            "entry": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian D. Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20Wu%2C%20Qi%20Teney%2C%20Damien%20Bruce%2C%20Jake%20Vision-and-language%20navigation%3A%20Interpreting%20visually-grounded%20navigation%20instructions%20in%20real%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20Wu%2C%20Qi%20Teney%2C%20Damien%20Bruce%2C%20Jake%20Vision-and-language%20navigation%3A%20Interpreting%20visually-grounded%20navigation%20instructions%20in%20real%20environments%202018"
        },
        {
            "id": "Argall_et+al_2009_a",
            "entry": "Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Bahdanau_et+al_2018_a",
            "entry": "D. Bahdanau, F. Hill, J. Leike, E. Hughes, P. Kohli, and E. Grefenstette. Learning to Follow Language Instructions with Adversarial Reward Induction. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Hill%2C%20F.%20Leike%2C%20J.%20Hughes%2C%20E.%20Learning%20to%20Follow%20Language%20Instructions%20with%20Adversarial%20Reward%20Induction.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Branavan_et+al_2009_a",
            "entry": "S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Branavan%2C%20S.R.K.%20Chen%2C%20Harr%20Zettlemoyer%2C%20Luke%20S.%20Barzilay%2C%20Regina%20Reinforcement%20learning%20for%20mapping%20instructions%20to%20actions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Branavan%2C%20S.R.K.%20Chen%2C%20Harr%20Zettlemoyer%2C%20Luke%20S.%20Barzilay%2C%20Regina%20Reinforcement%20learning%20for%20mapping%20instructions%20to%20actions%202009"
        },
        {
            "id": "Choi_2012_a",
            "entry": "Jaedeug Choi and Kee-eung Kim. Nonparametric bayesian inverse reinforcement learning for multiple reward functions. In Advances in Neural Information Processing Systems (NIPS). 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Jaedeug%20Kim%2C%20Kee-eung%20Nonparametric%20bayesian%20inverse%20reinforcement%20learning%20for%20multiple%20reward%20functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Jaedeug%20Kim%2C%20Kee-eung%20Nonparametric%20bayesian%20inverse%20reinforcement%20learning%20for%20multiple%20reward%20functions%202012"
        },
        {
            "id": "Christiano_et+al_2017_a",
            "entry": "P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christiano%2C%20P.%20Leike%2C%20J.%20Brown%2C%20T.B.%20Martic%2C%20M.%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christiano%2C%20P.%20Leike%2C%20J.%20Brown%2C%20T.B.%20Martic%2C%20M.%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017"
        },
        {
            "id": "Das_et+al_2018_a",
            "entry": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20Abhishek%20Datta%2C%20Samyak%20Gkioxari%2C%20Georgia%20Lee%2C%20Stefan%20Embodied%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20Abhishek%20Datta%2C%20Samyak%20Gkioxari%2C%20Georgia%20Lee%2C%20Stefan%20Embodied%20question%20answering%202018"
        },
        {
            "id": "Dimitrakakis_2012_a",
            "entry": "Christos Dimitrakakis and Constantin A. Rothkopf. European conference on recent advances in reinforcement learning (ewrl). In Scott Sanner and Marcus Hutter (eds.), Recent Advances in Reinforcement Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dimitrakakis%2C%20Christos%20Rothkopf%2C%20Constantin%20A.%20European%20conference%20on%20recent%20advances%20in%20reinforcement%20learning%20%28ewrl%29%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dimitrakakis%2C%20Christos%20Rothkopf%2C%20Constantin%20A.%20European%20conference%20on%20recent%20advances%20in%20reinforcement%20learning%20%28ewrl%29%202012"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. CoRR, abs/1611.03852, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03852"
        },
        {
            "id": "Forbes_et+al_2015_a",
            "entry": "Maxwell Forbes, Rajesh P. N. Rao, Luke Zettlemoyer, and Maya Cakmak. Robot programming by demonstration with situated spatial language understanding. In IEEE International Conference on Robotics and Automation (ICRA), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forbes%2C%20Maxwell%20Rao%2C%20Rajesh%20P.N.%20Zettlemoyer%2C%20Luke%20Cakmak%2C%20Maya%20Robot%20programming%20by%20demonstration%20with%20situated%20spatial%20language%20understanding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Forbes%2C%20Maxwell%20Rao%2C%20Rajesh%20P.N.%20Zettlemoyer%2C%20Luke%20Cakmak%2C%20Maya%20Robot%20programming%20by%20demonstration%20with%20situated%20spatial%20language%20understanding%202015"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018"
        },
        {
            "id": "Hermann_et+al_2017_a",
            "entry": "Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, and Phil Blunsom. Grounded language learning in a simulated 3d world. CoRR, abs/1706.06551, 2017. URL http://arxiv.org/abs/1706.06551.",
            "url": "http://arxiv.org/abs/1706.06551",
            "arxiv_url": "https://arxiv.org/pdf/1706.06551"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Li_2017_a",
            "entry": "Kun Li and Joel W. Burdick. Meta inverse reinforcement learning via maximum reward sharing for human motion analysis. CoRR, abs/1710.03592, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03592"
        },
        {
            "id": "Macglashan_et+al_2015_a",
            "entry": "James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael L. Littman, Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang. Grounding english commands to reward functions. In Robotics: Science and Systems (RSS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacGlashan%2C%20James%20Babes-Vroman%2C%20Monica%20desJardins%2C%20Marie%20Littman%2C%20Michael%20L.%20Grounding%20english%20commands%20to%20reward%20functions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacGlashan%2C%20James%20Babes-Vroman%2C%20Monica%20desJardins%2C%20Marie%20Littman%2C%20Michael%20L.%20Grounding%20english%20commands%20to%20reward%20functions%202015"
        },
        {
            "id": "Mei_et+al_2016_a",
            "entry": "Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI Conference on Artificial Intelligence (AAAI), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Hongyuan%20Bansal%2C%20Mohit%20Listen%2C%20Matthew%20R.Walter%20attend%20and%20walk%3A%20Neural%20mapping%20of%20navigational%20instructions%20to%20action%20sequences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Hongyuan%20Bansal%2C%20Mohit%20Listen%2C%20Matthew%20R.Walter%20attend%20and%20walk%3A%20Neural%20mapping%20of%20navigational%20instructions%20to%20action%20sequences%202016"
        },
        {
            "id": "Misra_et+al_2014_a",
            "entry": "Dipendra Kumar Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Contextsensitive grounding of natural language to manipulation instructions. In Robotics: Science and Systems (RSS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Dipendra%20Kumar%20Sung%2C%20Jaeyong%20Lee%2C%20Kevin%20Saxena%2C%20Ashutosh%20Tell%20me%20dave%3A%20Contextsensitive%20grounding%20of%20natural%20language%20to%20manipulation%20instructions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Dipendra%20Kumar%20Sung%2C%20Jaeyong%20Lee%2C%20Kevin%20Saxena%2C%20Ashutosh%20Tell%20me%20dave%3A%20Contextsensitive%20grounding%20of%20natural%20language%20to%20manipulation%20instructions%202014"
        },
        {
            "id": "Misra_et+al_2017_a",
            "entry": "Dipendra Kumar Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Dipendra%20Kumar%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Dipendra%20Kumar%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, feb 2015. ISSN 0028-0836.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning (ICML), 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Perez_et+al_2018_a",
            "entry": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perez%2C%20Ethan%20Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Dumoulin%2C%20Vincent%20Film%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018"
        },
        {
            "id": "Shah_et+al_2018_a",
            "entry": "Pararth Shah, Marek Fiser, Aleksandra Faust, J. Chase Kew, and Dilek Hakkani-Tur. Follownet: Robot navigation by following natural language directions with deep reinforcement learning. CoRR, abs/1805.06150, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06150"
        },
        {
            "id": "Song_et+al_2017_a",
            "entry": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. Proceedings of 29th IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Shuran%20Yu%2C%20Fisher%20Zeng%2C%20Andy%20Chang%2C%20Angel%20X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Shuran%20Yu%2C%20Fisher%20Zeng%2C%20Andy%20Chang%2C%20Angel%20X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017"
        },
        {
            "id": "Sung_et+al_2015_a",
            "entry": "Jaeyong Sung, Seok Hyun Jin, and Ashutosh Saxena. Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds. In International Symposium on Robotics Research (ISRR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sung%2C%20Jaeyong%20Jin%2C%20Seok%20Hyun%20Saxena%2C%20Ashutosh%20Robobarista%3A%20Object%20part%20based%20transfer%20of%20manipulation%20trajectories%20from%20crowd-sourcing%20in%203d%20pointclouds%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sung%2C%20Jaeyong%20Jin%2C%20Seok%20Hyun%20Saxena%2C%20Ashutosh%20Robobarista%3A%20Object%20part%20based%20transfer%20of%20manipulation%20trajectories%20from%20crowd-sourcing%20in%203d%20pointclouds%202015"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Stefanie%20Tellex%20Thomas%20Kollar%20Steven%20Dickerson%20Matthew%20R%20Walter%20Ashis%20Gopal%20Banerjee%20Seth"
        },
        {
            "id": "Teller_2011_a",
            "entry": "Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI\u201911, pp. 1507\u20131514. AAAI Press, 2011. URL http://dl.acm.org/citation.cfm?id=2900423.2900661. Hsiao-Yu Fish Tung, Adam W. Harley, Liang-Kang Huang, and Katerina Fragkiadaki. Reward learning from narrated demonstrations. CoRR, abs/1804.10692, 2018. Brian Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, Carnegie Mellon University, 2010. Brian Ziebart, Andrew Maas, Andrew Bagnell, and Anind Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2008.",
            "url": "http://dl.acm.org/citation.cfm?id=2900423.2900661",
            "arxiv_url": "https://arxiv.org/pdf/1804.10692"
        }
    ]
}
