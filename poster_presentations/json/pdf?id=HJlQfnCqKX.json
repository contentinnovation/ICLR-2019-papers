{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PREDICTING THE GENERALIZATION GAP IN DEEP NETWORKS WITH MARGIN DISTRIBUTIONS",
        "author": "Yiding Jiang \u2217, Dilip Krishnan, Hossein Mobahi, Samy Bengio Google AI {ydjiang, dilipkay, hmobahi, bengio}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJlQfnCqKX"
        },
        "abstract": "Recent research has demonstrated that deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap can be predicted from training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum). Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "cross entropy",
            "url": "https://en.wikipedia.org/wiki/cross_entropy"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "decision boundary",
            "url": "https://en.wikipedia.org/wiki/decision_boundary"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Generalization, the ability of a classifier to perform well on unseen examples, is a desideratum for progress towards real-world deployment of deep neural networks in domains such as autonomous cars and healthcare",
        "Deep networks can overfit to arbitrarily corrupted data (Zhang et al, 2016), and they are sensitive to small geometric transformations (<a class=\"ref-link\" id=\"cAzulay_2018_a\" href=\"#rAzulay_2018_a\">Azulay & Weiss, 2018</a>; <a class=\"ref-link\" id=\"cEngstrom_et+al_2017_a\" href=\"#rEngstrom_et+al_2017_a\">Engstrom et al, 2017</a>). These results have led to the important question about how the generalization gap of a deep network can be predicted using the training data and network parameters",
        "We propose a new quantity for predicting generalization gap of a feedforward neural network",
        "Using the notion of margin in support vector machines (<a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\">Vapnik, 1995</a>) and extension to deep networks (<a class=\"ref-link\" id=\"cElsayed_et+al_2018_a\" href=\"#rElsayed_et+al_2018_a\">Elsayed et al, 2018</a>), we develop a measure that shows a strong correlation with generalization gap and significantly outperforms recently developed theoretical bounds on",
        "We have presented a predictor for generalization gap based on margin distribution in deep networks and conducted extensive experiments to assess it",
        "Our results show that our scheme achieves a high adjusted coefficient of determination"
    ],
    "key_statements": [
        "Generalization, the ability of a classifier to perform well on unseen examples, is a desideratum for progress towards real-world deployment of deep neural networks in domains such as autonomous cars and healthcare",
        "Deep networks can overfit to arbitrarily corrupted data (Zhang et al, 2016), and they are sensitive to small geometric transformations (<a class=\"ref-link\" id=\"cAzulay_2018_a\" href=\"#rAzulay_2018_a\">Azulay & Weiss, 2018</a>; <a class=\"ref-link\" id=\"cEngstrom_et+al_2017_a\" href=\"#rEngstrom_et+al_2017_a\">Engstrom et al, 2017</a>). These results have led to the important question about how the generalization gap of a deep network can be predicted using the training data and network parameters",
        "We propose a new quantity for predicting generalization gap of a feedforward neural network",
        "Using the notion of margin in support vector machines (<a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\">Vapnik, 1995</a>) and extension to deep networks (<a class=\"ref-link\" id=\"cElsayed_et+al_2018_a\" href=\"#rElsayed_et+al_2018_a\">Elsayed et al, 2018</a>), we develop a measure that shows a strong correlation with generalization gap and significantly outperforms recently developed theoretical bounds on",
        "Our experiments reveal that margin distribution from all of the layers of the network contribute to prediction of generalization gap",
        "We have presented a predictor for generalization gap based on margin distribution in deep networks and conducted extensive experiments to assess it",
        "Our results show that our scheme achieves a high adjusted coefficient of determination"
    ],
    "summary": [
        "Generalization, the ability of a classifier to perform well on unseen examples, is a desideratum for progress towards real-world deployment of deep neural networks in domains such as autonomous cars and healthcare.",
        "These results have led to the important question about how the generalization gap of a deep network can be predicted using the training data and network parameters.",
        "Besides improvement in the prediction of the generalization gap, our work is distinct from recently developed bounds and margin definitions in a number of ways: 1.",
        "We argue that it is crucial to use margin information across layers and show that this significantly improves generalization gap prediction.",
        "3. The common definition of margin, as used in the recent bounds e.g. Neyshabur et al (2017b), or as extended to deep networks, is based on the closest distance of the training points to the decision boundary.",
        "A number of prior works such as <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al (2017</a>), Neyshabur et al (2017b), <a class=\"ref-link\" id=\"cLiu_et+al_2016_a\" href=\"#rLiu_et+al_2016_a\">Liu et al (2016</a>), <a class=\"ref-link\" id=\"cSun_et+al_2015_a\" href=\"#rSun_et+al_2015_a\">Sun et al (2015</a>), <a class=\"ref-link\" id=\"cSokolic_et+al_2016_a\" href=\"#rSokolic_et+al_2016_a\">Sokolic et al (2016</a>), and Liang et al (2017) have focused on analyzing or maximizing the margin at either the input or the output layer of a deep network.",
        "Our experiments reveal that margin distribution from all of the layers of the network contribute to prediction of generalization gap.",
        "We found constructing the total signature based on four evenly-spaced layers sufficiently captures the variation in the distributions and generalization gap, and makes the signature agnostic to the depth of the network.",
        "Our goal is to predict the generalization gap, i.e. the difference between training and test accuracy at the end of training, based on total signature \u03b8 of a trained model.",
        "We compute the depth-agnostic signature of the normalized margin distribution.",
        "Moment: Use the first 5 moments of the normalized margin distribution as signature instead of quartile statistics \u03b8 \u2208 R20 (Sec. 3); 5.",
        "The plots show that the Resnet achieves a better margin distribution, correlated with greater test accuracy, even though it was trained without data augmentation.",
        "The resulting normalized margin density plots clearly reflect the better generalization achieved by CIFAR-10: the densities at all layers are wider and shifted to the right.",
        "We have presented a predictor for generalization gap based on margin distribution in deep networks and conducted extensive experiments to assess it.",
        "The predictor uses normalized margin distribution across multiple layers of the network.",
        "Compared to the strong baseline of spectral complexity normalized output margin (<a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al, 2017</a>), our scheme exhibits much higher predictive power and can be applied to any feedforward network)."
    ],
    "headline": "We propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pp. 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Azulay_2018_a",
            "entry": "Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? arXiv preprint arXiv:1805.12177, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12177"
        },
        {
            "id": "Bartlett_1998_a",
            "entry": "P. L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525\u2013536, March 1998. ISSN 0018-9448. doi: 10.1109/18.661502.",
            "crossref": "https://dx.doi.org/10.1109/18.661502",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/18.661502"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6240\u20136249, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Elsayed_et+al_2018_a",
            "entry": "Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep networks for classification. arXiv preprint arXiv:1803.05598, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05598"
        },
        {
            "id": "Engstrom_et+al_2017_a",
            "entry": "Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02779"
        },
        {
            "id": "Garg_et+al_2002_a",
            "entry": "Ashutosh Garg, Sariel Har-Peled, and Dan Roth. On generalization bounds, projection profile, and margin distribution. In Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002, pp. 171\u2013178, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garg%2C%20Ashutosh%20Har-Peled%2C%20Sariel%20Roth%2C%20Dan%20On%20generalization%20bounds%2C%20projection%20profile%2C%20and%20margin%20distribution%202002-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garg%2C%20Ashutosh%20Har-Peled%2C%20Sariel%20Roth%2C%20Dan%20On%20generalization%20bounds%2C%20projection%20profile%2C%20and%20margin%20distribution%202002-07"
        },
        {
            "id": "Glantz_et+al_1990_a",
            "entry": "Stanton A Glantz, Bryan K Slinker, and Torsten B Neilands. Primer of applied regression and analysis of variance, volume 309. McGraw-Hill New York, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glantz%2C%20Stanton%20A.%20Slinker%2C%20Bryan%20K.%20Neilands%2C%20Torsten%20B.%20Primer%20of%20applied%20regression%20and%20analysis%20of%20variance%2C%20volume%20309%201990"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Kawaguchi_et+al_2017_a",
            "entry": "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "Langford_2002_a",
            "entry": "John Langford and John Shawe-Taylor. Pac-bayes margins. In Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS\u201902, pp. 439\u2013446, Cambridge, MA, USA, 2002. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Shawe-Taylor%2C%20John%20Pac-bayes%20margins%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Shawe-Taylor%2C%20John%20Pac-bayes%20margins%202002"
        },
        {
            "id": "Liao_et+al_2018_a",
            "entry": "Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising linear relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.09659"
        },
        {
            "id": "Lin_et+al_2013_a",
            "entry": "Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.4400"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In ICML, pp. 507\u2013516, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Lv_et+al_2019_a",
            "entry": "Shen-Huan Lv, Lu Wang, and Zhi-Hua Zhou. Optimal margin distribution network, 2019. URL https://openreview.net/forum?id=HygcvsAcFX.",
            "url": "https://openreview.net/forum?id=HygcvsAcFX"
        },
        {
            "id": "Mcgill_et+al_1978_a",
            "entry": "Robert McGill, John W Tukey, and Wayne A Larsen. Variations of box plots. The American Statistician, 32(1):12\u201316, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McGill%2C%20Robert%20Tukey%2C%20John%20W.%20Larsen%2C%20Wayne%20A.%20Variations%20of%20box%20plots%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McGill%2C%20Robert%20Tukey%2C%20John%20W.%20Larsen%2C%20Wayne%20A.%20Variations%20of%20box%20plots%201978"
        },
        {
            "id": "Neyshabur_et+al_0000_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947\u20135956, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02697"
        },
        {
            "id": "Poggio_et+al_2017_a",
            "entry": "Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00173"
        },
        {
            "id": "Reyzin_2006_a",
            "entry": "Lev Reyzin and Robert E Schapire. How boosting the margin can also boost classifier complexity. In Proceedings of the 23rd international conference on Machine learning, pp. 753\u2013760. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lev%20Reyzin%20and%20Robert%20E%20Schapire%20How%20boosting%20the%20margin%20can%20also%20boost%20classifier%20complexity%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20753760%20ACM%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lev%20Reyzin%20and%20Robert%20E%20Schapire%20How%20boosting%20the%20margin%20can%20also%20boost%20classifier%20complexity%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20753760%20ACM%202006"
        },
        {
            "id": "Schapire_et+al_1998_a",
            "entry": "Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651\u20131686, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schapire%2C%20Robert%20E.%20Freund%2C%20Yoav%20Bartlett%2C%20Peter%20Lee%2C%20Wee%20Sun%20Boosting%20the%20margin%3A%20A%20new%20explanation%20for%20the%20effectiveness%20of%20voting%20methods%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schapire%2C%20Robert%20E.%20Freund%2C%20Yoav%20Bartlett%2C%20Peter%20Lee%2C%20Wee%20Sun%20Boosting%20the%20margin%3A%20A%20new%20explanation%20for%20the%20effectiveness%20of%20voting%20methods%201998"
        },
        {
            "id": "Sokolic_et+al_2016_a",
            "entry": "Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin deep neural networks. CoRR, abs/1605.08254, 2016. URL http://arxiv.org/abs/1605.08254.",
            "url": "http://arxiv.org/abs/1605.08254",
            "arxiv_url": "https://arxiv.org/pdf/1605.08254"
        },
        {
            "id": "Sun_et+al_2015_a",
            "entry": "Shizhao Sun, Wei Chen, Liwei Wang, and Tie-Yan Liu. Large margin deep neural networks: Theory and algorithms. CoRR, abs/1506.05232, 2015. URL http://arxiv.org/abs/1506.05232.",
            "url": "http://arxiv.org/abs/1506.05232",
            "arxiv_url": "https://arxiv.org/pdf/1506.05232"
        },
        {
            "id": "Vapnik_1995_a",
            "entry": "Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA, 1995. ISBN 0-387-94559-8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20N.%20The%20Nature%20of%20Statistical%20Learning%20Theory%201995"
        },
        {
            "id": "Wu_2018_a",
            "entry": "Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494, 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 4063\u20134071, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/zhang17h.html. Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution clustering, 2018. URL https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16895.",
            "url": "http://proceedings.mlr.press/v70/zhang17h.html",
            "arxiv_url": "https://arxiv.org/pdf/1803.08494"
        }
    ]
}
