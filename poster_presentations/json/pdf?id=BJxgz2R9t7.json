{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TO SOLVE CIRCUIT-SAT: AN UNSUPERVISED DIFFERENTIABLE APPROACH",
        "author": "Saeed Amizadeh, Sergiy Matusevych, Markus Weimer Microsoft Redmond, WA 98052 {saamizad,sergiym,Markus.Weimer}@microsoft.com",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJxgz2R9t7"
        },
        "abstract": "Recent efforts to combine Representation Learning with Formal Methods, commonly known as Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Conjunctive Normal Form",
            "url": "https://en.wikipedia.org/wiki/Conjunctive_Normal_Form"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "network model",
            "url": "https://en.wikipedia.org/wiki/network_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "formal methods",
            "url": "https://en.wikipedia.org/wiki/formal_methods"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "Directed Acyclic Graph",
            "url": "https://en.wikipedia.org/wiki/Directed_Acyclic_Graph"
        },
        {
            "term": "computer science",
            "url": "https://en.wikipedia.org/wiki/computer_science"
        }
    ],
    "abbreviations": {
        "DAG": "Directed Acyclic Graph",
        "GGS-NN": "Gated Graph Sequence Neural Networks",
        "DAG-RNN": "DAG Recurrent Neural Networks",
        "GRU": "Gated Recurrent Unit",
        "CNF": "Conjunctive Normal Form"
    },
    "highlights": [
        "Recent advances in neural network models for discrete structures have given rise to a new field in Representation Learning known as the Neuro-Symbolic methods",
        "One of the most exciting outcomes of this marriage is the emergence of neural models for learning how to solve the classical combinatorial optimization problems in Computer Science",
        "We proposed a neural framework for efficiently learning a Circuit-SAT solver",
        "Our methodology relies on two fundamental contributions: (1) a rich Directed Acyclic Graph-embedding architecture that implements the sequential propagation mechanism on Directed Acyclic Graph-structured data and is capable of learning useful representations for the input circuits, and (2) an efficient training procedure that trains the DAGembedding architecture directly toward solving the SAT problem without requiring SAT/UNSAT labels in general",
        "The proposed embedding architecture is able to harness structural information in the input Directed Acyclic Graph distribution and as a result solve the test SAT cases in a fewer number of iterations compared to the baseline",
        "We argued that not only does direct training give us superior out-of-sample generalization, but it is essential for the problem domains where we cannot enforce the strict training regime where SAT and UNSAT cases come in pairs with almost identical structures, as proposed by <a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\">Selsam et al (2018</a>)"
    ],
    "key_statements": [
        "Recent advances in neural network models for discrete structures have given rise to a new field in Representation Learning known as the Neuro-Symbolic methods",
        "One of the most exciting outcomes of this marriage is the emergence of neural models for learning how to solve the classical combinatorial optimization problems in Computer Science",
        "We propose a neural Circuit-SAT solver framework that effectively belongs to the second class above; that is, it learns the entire solution structure from scratch",
        "(c) We propose a training strategy for our architecture that is end-to-end differentiable, yet similar to Reinforcement Learning techniques, it directly trains our model toward solving the SAT problem with an Explore-Exploit mechanism",
        "We extend the single layer Directed Acyclic Graph-RNN model for Directed Acyclic Graph-structured data <a class=\"ref-link\" id=\"cBaldi_2003_a\" href=\"#rBaldi_2003_a\">Baldi & Pollastri (2003</a>); <a class=\"ref-link\" id=\"cShuai_et+al_2016_a\" href=\"#rShuai_et+al_2016_a\">Shuai et al (2016</a>) to the more general deep version with Gated Recurrent Units, where each layer processes the input Directed Acyclic Graph either in the forward or the backward direction",
        "We propose a deep learning framework for the Circuit-SAT problem, but in contrast to NeuroSAT, our model is directly trained toward finding SAT solutions without requiring to see them in the training sample.\n3 Directed Acyclic Graph EMBEDDING",
        "We introduce the reversed layers that are similar to the regular forward layers except that the input Directed Acyclic Graph is processed in the reversed order",
        "Instead for our method, we propose to use pre-processing methods to convert the input Conjunctive Normal Form into circuit that has the potential of injecting structural information into the circuit structure",
        "We proposed a neural framework for efficiently learning a Circuit-SAT solver",
        "Our methodology relies on two fundamental contributions: (1) a rich Directed Acyclic Graph-embedding architecture that implements the sequential propagation mechanism on Directed Acyclic Graph-structured data and is capable of learning useful representations for the input circuits, and (2) an efficient training procedure that trains the DAGembedding architecture directly toward solving the SAT problem without requiring SAT/UNSAT labels in general",
        "The proposed embedding architecture is able to harness structural information in the input Directed Acyclic Graph distribution and as a result solve the test SAT cases in a fewer number of iterations compared to the baseline",
        "We argued that not only does direct training give us superior out-of-sample generalization, but it is essential for the problem domains where we cannot enforce the strict training regime where SAT and UNSAT cases come in pairs with almost identical structures, as proposed by <a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\">Selsam et al (2018</a>)"
    ],
    "summary": [
        "Recent advances in neural network models for discrete structures have given rise to a new field in Representation Learning known as the Neuro-Symbolic methods.",
        "As an alternative method for training, more recently <a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\"><a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\">Selsam et al (2018</a></a>) have proposed using the latent representations learned for the binary classification of the Satisfiability (SAT) problem to produce a neural SAT solver model.",
        "We propose a deep learning framework for the Circuit-SAT problem, but in contrast to NeuroSAT, our model is directly trained toward finding SAT solutions without requiring to see them in the training sample.",
        "Using the DG-DAGRNN framework, we learn a neural functional F\u03b8 on the space of circuits \u03bcG such that given an input circuit, it would directly generate a satisfying assignment for the circuit if it is SAT.",
        "In the beginning of the training we start with a high temperature value to let the model explore all paths in the input circuits for finding a SAT solution.",
        "To our surprise, the same NeuroSAT model that generated the out-of-sample results on k-SAT datasets in Figure 2, could not solve any of the SAT graph k-coloring problems in Dataset-1 and Dataset-2, even after 128 propagation iterations.",
        "In order to avoid learning superficial classification features, NeuroSAT restricts its training to a strict regime of SAT-UNSAT pairs, where the two examples in a pair only differ in negation of one literal.",
        "Our methodology relies on two fundamental contributions: (1) a rich DAG-embedding architecture that implements the sequential propagation mechanism on DAG-structured data and is capable of learning useful representations for the input circuits, and (2) an efficient training procedure that trains the DAGembedding architecture directly toward solving the SAT problem without requiring SAT/UNSAT labels in general.",
        "The proposed embedding architecture is able to harness structural information in the input DAG distribution and as a result solve the test SAT cases in a fewer number of iterations compared to the baseline.",
        "Our direct training procedure as opposed to the indirect, classification-based method in NeuroSAT enables our model to generalize better to out-of-sample test cases, as demonstrated by the experiments.",
        "We argued that not only does direct training give us superior out-of-sample generalization, but it is essential for the problem domains where we cannot enforce the strict training regime where SAT and UNSAT cases come in pairs with almost identical structures, as proposed by <a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\"><a class=\"ref-link\" id=\"cSelsam_et+al_2018_a\" href=\"#rSelsam_et+al_2018_a\">Selsam et al (2018</a></a>)."
    ],
    "headline": "We propose a neural framework that can learn to solve the Circuit Satisfiability problem",
    "reference_links": [
        {
            "id": "Andrews_2002_a",
            "entry": "Peter B Andrews. An introduction to mathematical logic and type theory, volume 27. Springer Science & Business Media, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrews%2C%20Peter%20B.%20An%20introduction%20to%20mathematical%20logic%20and%20type%20theory%2C%20volume%2027%202002"
        },
        {
            "id": "Arabshahi_et+al_2018_a",
            "entry": "Forough Arabshahi, Sameer Singh, and Animashree Anandkumar. Combining symbolic and function evaluation expressions in neural programs. arXiv preprint arXiv:1801.04342, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04342"
        },
        {
            "id": "Arulkumaran_et+al_2017_a",
            "entry": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05866"
        },
        {
            "id": "Baldi_2003_a",
            "entry": "Pierre Baldi and Gianluca Pollastri. The principled design of large-scale recursive neural network architectures\u2013dag-rnns and the protein structure prediction problem. Journal of Machine Learning Research, 4(Sep):575\u2013602, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Pollastri%2C%20Gianluca%20The%20principled%20design%20of%20large-scale%20recursive%20neural%20network%20architectures%E2%80%93dag-rnns%20and%20the%20protein%20structure%20prediction%20problem%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Pollastri%2C%20Gianluca%20The%20principled%20design%20of%20large-scale%20recursive%20neural%20network%20architectures%E2%80%93dag-rnns%20and%20the%20protein%20structure%20prediction%20problem%202003"
        },
        {
            "id": "Bello_et+al_2016_a",
            "entry": "Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09940"
        },
        {
            "id": "Biere_2008_a",
            "entry": "Armin Biere. Picosat essentials. Journal on Satisfiability, Boolean Modeling and Computation, 4: 75\u201397, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biere%2C%20Armin%20Picosat%20essentials.%20Journal%20on%20Satisfiability%2C%20Boolean%20Modeling%20and%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biere%2C%20Armin%20Picosat%20essentials.%20Journal%20on%20Satisfiability%2C%20Boolean%20Modeling%20and%202008"
        },
        {
            "id": "Biere_et+al_2009_a",
            "entry": "Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh. Conflict-driven clause learning sat solvers. Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, pp. 131\u2013153, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biere%2C%20Armin%20Heule%2C%20Marijn%20van%20Maaren%2C%20Hans%20Walsh%2C%20Toby%20Conflict-driven%20clause%20learning%20sat%20solvers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biere%2C%20Armin%20Heule%2C%20Marijn%20van%20Maaren%2C%20Hans%20Walsh%2C%20Toby%20Conflict-driven%20clause%20learning%20sat%20solvers%202009"
        },
        {
            "id": "Bronstein_et+al_2017_a",
            "entry": "Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bronstein%2C%20Michael%20M.%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Szlam%2C%20Arthur%20Geometric%20deep%20learning%3A%20going%20beyond%20euclidean%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bronstein%2C%20Michael%20M.%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Szlam%2C%20Arthur%20Geometric%20deep%20learning%3A%20going%20beyond%20euclidean%20data%202017"
        },
        {
            "id": "Bruna_et+al_2013_a",
            "entry": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6203"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Defferrard_et+al_2016_a",
            "entry": "Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844\u20133852, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016"
        },
        {
            "id": "Evans_et+al_2018_a",
            "entry": "Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? arXiv preprint arXiv:1802.08535, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08535"
        },
        {
            "id": "Fu_2007_a",
            "entry": "Zhaohui Fu and Sharad Malik. Extracting logic circuit structure from conjunctive normal form descriptions. In VLSI Design, 2007. Held jointly with 6th International Conference on Embedded Systems., 20th International Conference on, pp. 37\u201342. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhaohui%20Fu%20and%20Sharad%20Malik%20Extracting%20logic%20circuit%20structure%20from%20conjunctive%20normal%20form%20descriptions%20In%20VLSI%20Design%202007%20Held%20jointly%20with%206th%20International%20Conference%20on%20Embedded%20Systems%2020th%20International%20Conference%20on%20pp%203742%20IEEE%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhaohui%20Fu%20and%20Sharad%20Malik%20Extracting%20logic%20circuit%20structure%20from%20conjunctive%20normal%20form%20descriptions%20In%20VLSI%20Design%202007%20Held%20jointly%20with%206th%20International%20Conference%20on%20Embedded%20Systems%2020th%20International%20Conference%20on%20pp%203742%20IEEE%202007"
        },
        {
            "id": "Hu_et+al_2016_a",
            "entry": "Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. arXiv preprint arXiv:1603.06318, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.06318"
        },
        {
            "id": "Jain_2009_a",
            "entry": "Himanshu Jain and Edmund M Clarke. Efficient sat solving for non-clausal formulas using dpll, graphs, and watched cuts. In Proceedings of the 46th Annual Design Automation Conference, pp. 563\u2013568. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Himanshu%20Clarke%2C%20Edmund%20M.%20Efficient%20sat%20solving%20for%20non-clausal%20formulas%20using%20dpll%2C%20graphs%2C%20and%20watched%20cuts%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Himanshu%20Clarke%2C%20Edmund%20M.%20Efficient%20sat%20solving%20for%20non-clausal%20formulas%20using%20dpll%2C%20graphs%2C%20and%20watched%20cuts%202009"
        },
        {
            "id": "Khalil_et+al_2017_a",
            "entry": "Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6351\u20136361, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khalil%2C%20Elias%20Dai%2C%20Hanjun%20Zhang%2C%20Yuyu%20Dilkina%2C%20Bistra%20Learning%20combinatorial%20optimization%20algorithms%20over%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khalil%2C%20Elias%20Dai%2C%20Hanjun%20Zhang%2C%20Yuyu%20Dilkina%2C%20Bistra%20Learning%20combinatorial%20optimization%20algorithms%20over%20graphs%202017"
        },
        {
            "id": "Kondor_et+al_2018_a",
            "entry": "Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02144"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05493"
        },
        {
            "id": "Monti_et+al_2017_a",
            "entry": "Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proc. CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Monti%2C%20Federico%20Boscaini%2C%20Davide%20Masci%2C%20Jonathan%20Rodola%2C%20Emanuele%20Geometric%20deep%20learning%20on%20graphs%20and%20manifolds%20using%20mixture%20model%20cnns%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Monti%2C%20Federico%20Boscaini%2C%20Davide%20Masci%2C%20Jonathan%20Rodola%2C%20Emanuele%20Geometric%20deep%20learning%20on%20graphs%20and%20manifolds%20using%20mixture%20model%20cnns%202017"
        },
        {
            "id": "Moskewicz_et+al_2001_a",
            "entry": "Matthew W Moskewicz, Conor F Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient sat solver. In Proceedings of the 38th annual Design Automation Conference, pp. 530\u2013535. ACM, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moskewicz%2C%20Matthew%20W.%20Madigan%2C%20Conor%20F.%20Zhao%2C%20Ying%20Zhang%2C%20Lintao%20Chaff%3A%20Engineering%20an%20efficient%20sat%20solver%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moskewicz%2C%20Matthew%20W.%20Madigan%2C%20Conor%20F.%20Zhao%2C%20Ying%20Zhang%2C%20Lintao%20Chaff%3A%20Engineering%20an%20efficient%20sat%20solver%202001"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6026"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Selsam_et+al_2018_a",
            "entry": "Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03685"
        },
        {
            "id": "Shuai_et+al_2016_a",
            "entry": "Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang. Dag-recurrent neural networks for scene labeling. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3620\u20133629, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shuai%2C%20Bing%20Zuo%2C%20Zhen%20Wang%2C%20Bing%20Wang%2C%20Gang%20Dag-recurrent%20neural%20networks%20for%20scene%20labeling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shuai%2C%20Bing%20Zuo%2C%20Zhen%20Wang%2C%20Bing%20Wang%2C%20Gang%20Dag-recurrent%20neural%20networks%20for%20scene%20labeling%202016"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.00075"
        },
        {
            "id": "Velev_2007_a",
            "entry": "Miroslav N Velev. Exploiting hierarchy and structure to efficiently solve graph coloring as sat. In Proceedings of the 2007 IEEE/ACM international conference on Computer-aided design, pp. 135\u2013142. IEEE Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Velev%2C%20Miroslav%20N.%20Exploiting%20hierarchy%20and%20structure%20to%20efficiently%20solve%20graph%20coloring%20as%20sat%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Velev%2C%20Miroslav%20N.%20Exploiting%20hierarchy%20and%20structure%20to%20efficiently%20solve%20graph%20coloring%20as%20sat%202007"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692\u20132700, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Fortunato%2C%20Meire%20Jaitly%2C%20Navdeep%20Pointer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Fortunato%2C%20Meire%20Jaitly%2C%20Navdeep%20Pointer%20networks%202015"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. arXiv preprint arXiv:1711.11157, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11157"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp. 3394\u20133404, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033943404%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033943404%202017"
        }
    ]
}
