{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NEURAL PROBABILISTIC MOTOR PRIMITIVES FOR",
        "author": "HUMANOID CONTROL",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJl6TjRcY7"
        },
        "abstract": "We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Additionally, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and the resulting movements are relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video summarizing our results."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "differential dynamic programming",
            "url": "https://en.wikipedia.org/wiki/differential_dynamic_programming"
        },
        {
            "term": "motion capture",
            "url": "https://en.wikipedia.org/wiki/motion_capture"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "robotics",
            "url": "https://en.wikipedia.org/wiki/robotics"
        }
    ],
    "abbreviations": {
        "LFPC": "linearfeedback policy cloning",
        "ELBO": "evidence lower bound"
    },
    "highlights": [
        "A broad challenge in machine learning for control and robotics is to produce policies capable of general, flexible, and adaptive behavior of complex, physical bodies",
        "CORE RESULTS: COMPRESSING THOUSANDS OF EXPERTS Having validated that single skills can be transferred, we consider how well we can compress behaviors of the 2707 experts in our training set into the neural probabilistic motor primitive architecture",
        "The trained neural probabilistic motor primitive system can execute behaviors conditioned on an open-loop noisy latent variable trajectory, implying that the decoder has learned to stabilize the body during latent-conditioned behavior",
        "We emphasize a few points about these results to impact their importance: (1) Using a pretrained neural probabilistic motor primitives module, new controllers can be trained effectively from scratch on sparse reward tasks, (2) the resulting movements are visually rather humanlike without additional constraints implying that the learned embedding space is well structured, and (3) the module enables fairly comprehensive and smooth coverage for the purposes of physics-based control",
        "Using either a variant of behavioral cloning or linear feedback policy cloning we can train the neural probabilistic motor primitive sytem to perform robust one-shotimitation, and with the latter we can use relatively restricted data consisting of only single rollouts from each expert"
    ],
    "key_statements": [
        "A broad challenge in machine learning for control and robotics is to produce policies capable of general, flexible, and adaptive behavior of complex, physical bodies",
        "Recent progress in deep reinforcement learning has raised hopes that such behaviors can be learned end-to-end with minimal manual intervention",
        "As a more efficient alternative we consider a second solution that operates by comprehensively transferring the functional properties of an expert to a student policy by matching the local noise-feedback properties along one or a small number of representative expert reference trajectories. We call this specific proposal linear feedback policy cloning (LFPC), and we demonstrate that it is competitive with behavioral cloning from many more rollouts in our setting.\n1.1",
        "Motor primitives refer to the reusable embedding space learned from many related behaviors and the associated context-modulable policy capable of generating sensory-feedback-stabilized motor behavior when executed in an environment",
        "We describe the Neural Probabilistic Motor Primitive architecture and objective (Sec. 2.2)",
        "CORE RESULTS: COMPRESSING THOUSANDS OF EXPERTS Having validated that single skills can be transferred, we consider how well we can compress behaviors of the 2707 experts in our training set into the neural probabilistic motor primitive architecture",
        "The trained neural probabilistic motor primitive system can execute behaviors conditioned on an open-loop noisy latent variable trajectory, implying that the decoder has learned to stabilize the body during latent-conditioned behavior",
        "We emphasize a few points about these results to impact their importance: (1) Using a pretrained neural probabilistic motor primitives module, new controllers can be trained effectively from scratch on sparse reward tasks, (2) the resulting movements are visually rather humanlike without additional constraints implying that the learned embedding space is well structured, and (3) the module enables fairly comprehensive and smooth coverage for the purposes of physics-based control",
        "Using either a variant of behavioral cloning or linear feedback policy cloning we can train the neural probabilistic motor primitive sytem to perform robust one-shotimitation, and with the latter we can use relatively restricted data consisting of only single rollouts from each expert"
    ],
    "summary": [
        "A broad challenge in machine learning for control and robotics is to produce policies capable of general, flexible, and adaptive behavior of complex, physical bodies.",
        "Motor primitives refer to the reusable embedding space learned from many related behaviors and the associated context-modulable policy capable of generating sensory-feedback-stabilized motor behavior when executed in an environment.",
        "Our goal is to obtain a motor primitive module that can flexibly and robustly deploy, sequence, and interpolate a diverse set of skills from a large database of reference trajectories without any manual alignment or other processing of the raw experts.",
        "Behavioral cloning can refer to optimization of this objective, where \u03c1E is replaced with an empirical distribution of a set of state-action pairs S.",
        "Linear-feedback policy cloning (LFPC) The second approach, which we refer to as linearfeedback policy cloning (LFPC), logs the action-state Jacobian as well as the expert action along a single nominal trajectory.",
        "The approach attempts to match the mean action as well as the Jacobian at the set of relevant behavioral states, here sampled along the nominal trajectory.",
        "3.2 CORE RESULTS: COMPRESSING THOUSANDS OF EXPERTS Having validated that single skills can be transferred, we consider how well we can compress behaviors of the 2707 experts in our training set into the neural probabilistic motor primitive architecture.",
        "The trained neural probabilistic motor primitive system can execute behaviors conditioned on an open-loop noisy latent variable trajectory, implying that the decoder has learned to stabilize the body during latent-conditioned behavior.",
        "Other exploratory probes of the module suggest that it is possible in certain cases to obtain seamless transitioning between behaviors by concatenating latent-variable trajectories and running the policy conditioned on this sequence.",
        "We emphasize a few points about these results to impact their importance: (1) Using a pretrained neural probabilistic motor primitives module, new controllers can be trained effectively from scratch on sparse reward tasks, (2) the resulting movements are visually rather humanlike without additional constraints implying that the learned embedding space is well structured, and (3) the module enables fairly comprehensive and smooth coverage for the purposes of physics-based control.",
        "Using either a variant of behavioral cloning or linear feedback policy cloning we can train the neural probabilistic motor primitive sytem to perform robust one-shotimitation, and with the latter we can use relatively restricted data consisting of only single rollouts from each expert.",
        "While LFPC did not work quite as well in the full-scale model as cloning from noisy rollouts, we consider it remarkable that it is possible in our setting to transfer expert behavior using a single rollout."
    ],
    "headline": "We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids",
    "reference_links": [
        {
            "id": "Alemi_et+al_2017_a",
            "entry": "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017"
        },
        {
            "id": "Athans_1971_a",
            "entry": "Michael Athans. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. IEEE transactions on automatic control, 16(6):529\u2013552, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athans%2C%20Michael%20The%20role%20and%20use%20of%20the%20stochastic%20linear-quadratic-gaussian%20problem%20in%20control%20system%20design%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athans%2C%20Michael%20The%20role%20and%20use%20of%20the%20stochastic%20linear-quadratic-gaussian%20problem%20in%20control%20system%20design%201971"
        },
        {
            "id": "Bansal_et+al_2018_a",
            "entry": "Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bansal%2C%20Trapit%20Pachocki%2C%20Jakub%20Sidor%2C%20Szymon%20Sutskever%2C%20Ilya%20Emergent%20complexity%20via%20multi-agent%20competition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bansal%2C%20Trapit%20Pachocki%2C%20Jakub%20Sidor%2C%20Szymon%20Sutskever%2C%20Ilya%20Emergent%20complexity%20via%20multi-agent%20competition%202018"
        },
        {
            "id": "Bizzi_et+al_2008_a",
            "entry": "E Bizzi, VCK Cheung, A d\u2019Avella, P Saltiel, and Matthew Tresch. Combining modules for movement. Brain research reviews, 57(1):125\u2013133, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bizzi%2C%20E.%20Cheung%2C%20V.C.K.%20d%E2%80%99Avella%2C%20A.%20Saltiel%2C%20P.%20Combining%20modules%20for%20movement%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bizzi%2C%20E.%20Cheung%2C%20V.C.K.%20d%E2%80%99Avella%2C%20A.%20Saltiel%2C%20P.%20Combining%20modules%20for%20movement%202008"
        },
        {
            "id": "Chentanez_et+al_2018_a",
            "entry": "Nuttapong Chentanez, Matthias Muller, Miles Macklin, Viktor Makoviychuk, and Stefan Jeschke. Physics-based motion capture imitation with deep reinforcement learning. In Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games, pp. 1. ACM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20Nuttapong%20Muller%2C%20Matthias%20Macklin%2C%20Miles%20Makoviychuk%2C%20Viktor%20Physics-based%20motion%20capture%20imitation%20with%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20Nuttapong%20Muller%2C%20Matthias%20Macklin%2C%20Miles%20Makoviychuk%2C%20Viktor%20Physics-based%20motion%20capture%20imitation%20with%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Czarnecki_et+al_2017_a",
            "entry": "Wojciech M Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan Pascanu. Sobolev training for neural networks. In Advances in Neural Information Processing Systems, pp. 4281\u20134290, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Czarnecki%2C%20Wojciech%20M.%20Osindero%2C%20Simon%20Jaderberg%2C%20Max%20Swirszcz%2C%20Grzegorz%20Sobolev%20training%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Czarnecki%2C%20Wojciech%20M.%20Osindero%2C%20Simon%20Jaderberg%2C%20Max%20Swirszcz%2C%20Grzegorz%20Sobolev%20training%20for%20neural%20networks%202017"
        },
        {
            "id": "Ding_et+al_2015_a",
            "entry": "Kai Ding, Libin Liu, Michiel Van de Panne, and KangKang Yin. Learning reduced-order feedback policies for motion skills. In Proceedings of the 14th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pp. 83\u201392. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Kai%20Liu%2C%20Libin%20de%20Panne%2C%20Michiel%20Van%20Yin%2C%20KangKang%20Learning%20reduced-order%20feedback%20policies%20for%20motion%20skills%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Kai%20Liu%2C%20Libin%20de%20Panne%2C%20Michiel%20Van%20Yin%2C%20KangKang%20Learning%20reduced-order%20feedback%20policies%20for%20motion%20skills%202015"
        },
        {
            "id": "Duan_et+al_2017_a",
            "entry": "Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in neural information processing systems, pp. 1087\u20131098, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017"
        },
        {
            "id": "Furlanello_et+al_2018_a",
            "entry": "Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. arXiv preprint arXiv:1805.04770, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.04770"
        },
        {
            "id": "Graziano_2006_a",
            "entry": "Michael Graziano. The organization of behavioral repertoire in motor cortex. Annu. Rev. Neurosci., 29:105\u2013134, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graziano%2C%20Michael%20The%20organization%20of%20behavioral%20repertoire%20in%20motor%20cortex%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graziano%2C%20Michael%20The%20organization%20of%20behavioral%20repertoire%20in%20motor%20cortex%202006"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Heess_et+al_2017_a",
            "entry": "Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Jacobson_1970_a",
            "entry": "David H. Jacobson and David Q. Mayne. Differential Dynamic Programming. Elsevier, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobson%2C%20David%20H.%20Mayne%2C%20David%20Q.%20Differential%20Dynamic%20Programming%201970"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Conference on Learning Representations, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202013"
        },
        {
            "id": "Laskey_et+al_2017_a",
            "entry": "Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg. Dart: Noise injection for robust imitation learning. In Conference on Robot Learning, pp. 143\u2013156, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laskey%2C%20Michael%20Lee%2C%20Jonathan%20Fox%2C%20Roy%20Dragan%2C%20Anca%20Dart%3A%20Noise%20injection%20for%20robust%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laskey%2C%20Michael%20Lee%2C%20Jonathan%20Fox%2C%20Roy%20Dragan%2C%20Anca%20Dart%3A%20Noise%20injection%20for%20robust%20imitation%20learning%202017"
        },
        {
            "id": "Liu_2018_a",
            "entry": "Libin Liu and Jessica Hodgins. Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning. ACM Transactions on Graphics (TOG), 37(4):142, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20basketball%20dribbling%20skills%20using%20trajectory%20optimization%20and%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20basketball%20dribbling%20skills%20using%20trajectory%20optimization%20and%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Liu_et+al_2010_a",
            "entry": "Libin Liu, KangKang Yin, Michiel van de Panne, Tianjia Shao, and Weiwei Xu. Sampling-based contact-rich motion control. In ACM Transactions on Graphics (TOG), volume 29, pp. 128. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Shao%2C%20Tianjia%20Sampling-based%20contact-rich%20motion%20control%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Shao%2C%20Tianjia%20Sampling-based%20contact-rich%20motion%20control%202010"
        },
        {
            "id": "Mayne_1966_a",
            "entry": "David Mayne. A second-order gradient method for determining optimal trajectories of non-linear discrete-time systems. International Journal of Control, 3(1):85\u201395, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mayne%2C%20David%20A%20second-order%20gradient%20method%20for%20determining%20optimal%20trajectories%20of%20non-linear%20discrete-time%20systems%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mayne%2C%20David%20A%20second-order%20gradient%20method%20for%20determining%20optimal%20trajectories%20of%20non-linear%20discrete-time%20systems%201966"
        },
        {
            "id": "Meier_2016_a",
            "entry": "Franziska Meier and Stefan Schaal. A probabilistic representation for dynamic movement primitives. arXiv preprint arXiv:1612.05932, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.05932"
        },
        {
            "id": "Merel_et+al_2017_a",
            "entry": "Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02201"
        },
        {
            "id": "Merel_et+al_2018_a",
            "entry": "Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas Heess, and Greg Wayne. Hierarchical visuomotor control of humanoids. arXiv preprint arXiv:1811.09656, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.09656"
        },
        {
            "id": "Mordatch_et+al_2015_a",
            "entry": "Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic, and Emanuel V Todorov. Interactive control of diverse complex characters with neural networks. In Advances in Neural Information Processing Systems, pp. 3132\u20133140, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mordatch%2C%20Igor%20Lowrey%2C%20Kendall%20Andrew%2C%20Galen%20Popovic%2C%20Zoran%20Interactive%20control%20of%20diverse%20complex%20characters%20with%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mordatch%2C%20Igor%20Lowrey%2C%20Kendall%20Andrew%2C%20Galen%20Popovic%2C%20Zoran%20Interactive%20control%20of%20diverse%20complex%20characters%20with%20neural%20networks%202015"
        },
        {
            "id": "Morimoto_2003_a",
            "entry": "Jun Morimoto and Christopher G Atkeson. Minimax differential dynamic programming: An application to robust biped walking. In Advances in neural information processing systems, pp. 1563\u20131570, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morimoto%2C%20Jun%20Atkeson%2C%20Christopher%20G.%20Minimax%20differential%20dynamic%20programming%3A%20An%20application%20to%20robust%20biped%20walking%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morimoto%2C%20Jun%20Atkeson%2C%20Christopher%20G.%20Minimax%20differential%20dynamic%20programming%3A%20An%20application%20to%20robust%20biped%20walking%202003"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Neumann_et+al_2014_a",
            "entry": "Gerhard Neumann, Christian Daniel, Alexandros Paraschos, Andras Kupcsik, and Jan Peters. Learning modular policies for robotics. Frontiers in computational neuroscience, 8:62, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20Gerhard%20Daniel%2C%20Christian%20Paraschos%2C%20Alexandros%20Kupcsik%2C%20Andras%20Learning%20modular%20policies%20for%20robotics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20Gerhard%20Daniel%2C%20Christian%20Paraschos%2C%20Alexandros%20Kupcsik%2C%20Andras%20Learning%20modular%20policies%20for%20robotics%202014"
        },
        {
            "id": "Paraschos_et+al_2013_a",
            "entry": "Alexandros Paraschos, Christian Daniel, Jan R Peters, and Gerhard Neumann. Probabilistic movement primitives. In Advances in neural information processing systems, pp. 2616\u20132624, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paraschos%2C%20Alexandros%20Daniel%2C%20Christian%20Peters%2C%20Jan%20R.%20Neumann%2C%20Gerhard%20Probabilistic%20movement%20primitives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paraschos%2C%20Alexandros%20Daniel%2C%20Christian%20Peters%2C%20Jan%20R.%20Neumann%2C%20Gerhard%20Probabilistic%20movement%20primitives%202013"
        },
        {
            "id": "Parisotto_et+al_2015_a",
            "entry": "Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06342"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (TOG), 36(4):41, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Peng_et+al_2018_a",
            "entry": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML\u201914, pp. II\u20131278\u2013 II\u20131286. JMLR.org, 2014. URL http://dl.acm.org/citation.cfm?id=3044805.3045035.",
            "url": "http://dl.acm.org/citation.cfm?id=3044805.3045035",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Rusu_et+al_2015_a",
            "entry": "Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06295"
        },
        {
            "id": "Schaal_et+al_2003_a",
            "entry": "Stefan Schaal, Jan Peters, Jun Nakanishi, and Auke Ijspeert. Learning movement primitives. In International Symposium on Robotics Research, (ISRR), 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaal%2C%20Stefan%20Peters%2C%20Jan%20Nakanishi%2C%20Jun%20Ijspeert%2C%20Auke%20Learning%20movement%20primitives%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaal%2C%20Stefan%20Peters%2C%20Jan%20Nakanishi%2C%20Jun%20Ijspeert%2C%20Auke%20Learning%20movement%20primitives%202003"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Srinivas_2018_a",
            "entry": "Suraj Srinivas and Francois Fleuret. Knowledge transfer with jacobian matching. CoRR, abs/1803.00443, 2018. URL http://arxiv.org/abs/1803.00443.",
            "url": "http://arxiv.org/abs/1803.00443",
            "arxiv_url": "https://arxiv.org/pdf/1803.00443"
        },
        {
            "id": "Tassa_et+al_2012_a",
            "entry": "Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 4906\u20134913. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tassa%2C%20Yuval%20Erez%2C%20Tom%20Todorov%2C%20Emanuel%20Synthesis%20and%20stabilization%20of%20complex%20behaviors%20through%20online%20trajectory%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tassa%2C%20Yuval%20Erez%2C%20Tom%20Todorov%2C%20Emanuel%20Synthesis%20and%20stabilization%20of%20complex%20behaviors%20through%20online%20trajectory%20optimization%202012"
        },
        {
            "id": "Tassa_et+al_1175_a",
            "entry": "Yuval Tassa, Nicolas Mansard, and Emo Todorov. Control-limited differential dynamic programming. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pp. 1168\u2013 1175. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tassa%2C%20Yuval%20Mansard%2C%20Nicolas%20Todorov%2C%20Emo%20Control-limited%20differential%20dynamic%20programming%201175",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tassa%2C%20Yuval%20Mansard%2C%20Nicolas%20Todorov%2C%20Emo%20Control-limited%20differential%20dynamic%20programming%201175"
        },
        {
            "id": "Tedrake_2009_a",
            "entry": "Russ Tedrake. LQR-trees: Feedback motion planning on sparse randomized trees. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tedrake%2C%20Russ%20LQR-trees%3A%20Feedback%20motion%20planning%20on%20sparse%20randomized%20trees%202009"
        },
        {
            "id": "Teh_et+al_2017_a",
            "entry": "Yee Whye Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4499\u20134509, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Yee%20Whye%20Bapst%2C%20Victor%20Czarnecki%2C%20Wojciech%20M.%20Quan%2C%20John%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Yee%20Whye%20Bapst%2C%20Victor%20Czarnecki%2C%20Wojciech%20M.%20Quan%2C%20John%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017"
        },
        {
            "id": "Todorov_2003_a",
            "entry": "Emanuel Todorov and Zoubin Ghahramani. Unsupervised learning of sensory-motor primitives. In Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE, volume 2, pp. 1750\u20131753. IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Ghahramani%2C%20Zoubin%20Unsupervised%20learning%20of%20sensory-motor%20primitives.%20In%20Engineering%20in%20Medicine%20and%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Ghahramani%2C%20Zoubin%20Unsupervised%20learning%20of%20sensory-motor%20primitives.%20In%20Engineering%20in%20Medicine%20and%202003"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Vincent_et+al_2008_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, pp. 5320\u20135329, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017"
        }
    ]
}
