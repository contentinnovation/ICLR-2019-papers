{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EFFICIENT MULTI-OBJECTIVE NEURAL ARCHITECTURE SEARCH VIA LAMARCKIAN EVOLUTION",
        "author": "Thomas Elsken Bosch Center for Artificial Intelligence and University of Freiburg Thomas.Elsken@de.bosch.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByME42AqK7"
        },
        "abstract": "Neural Architecture Search aims at automatically finding neural network architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for, e.g., image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates child networks that are warm started with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks."
    },
    "keywords": [
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "subgraphs",
            "url": "https://en.wikipedia.org/wiki/Subgraph"
        },
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "morphism",
            "url": "https://en.wikipedia.org/wiki/morphism"
        },
        {
            "term": "resource consumption",
            "url": "https://en.wikipedia.org/wiki/resource_consumption"
        },
        {
            "term": "lamarckian inheritance",
            "url": "https://en.wikipedia.org/wiki/lamarckian_inheritance"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "supergraph",
            "url": "https://en.wikipedia.org/wiki/supergraph"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "evolutionary algorithm",
            "url": "https://en.wikipedia.org/wiki/evolutionary_algorithm"
        }
    ],
    "abbreviations": {
        "NAS": "neural architecture search",
        "RL": "reinforcement learning",
        "T N": "to another neural network",
        "ANM": "approximate network morphism",
        "WRNs": "Wide Residual Networks",
        "ERC": "European Research Council"
    },
    "highlights": [
        "Deep learning has enabled remarkable progress on a variety of perceptual tasks, such as image recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>)",
        "This mechanism can be interpreted as Lamarckian inheritance in the context of evolutionary algorithms, where Lamarckism refers to a mechanism which allows passing skills acquired during an individual\u2019s lifetime, on to children by means of inheritance",
        "Since network morphisms are limited to solely increasing a network\u2019s size, we introduce approximate network morphisms (Section 3.2) to allow shrinking networks, which is essential in the context of multi-objective search",
        "Our work extends this approach by introducing approximate network morphisms, making the use of such operators suitable for multi-objective optimization",
        "We have proposed LEMONADE, a multi-objective evolutionary algorithm for architecture search",
        "LEMONADE exploits the fact that evaluating several objectives, such as the performance of a neural network, is orders of magnitude more expensive than evaluating, e.g., a model\u2019s number of parameters"
    ],
    "key_statements": [
        "Deep learning has enabled remarkable progress on a variety of perceptual tasks, such as image recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>)",
        "To overcome the need for thousands of GPU days (Zoph & Le, 2017; Zoph et al, 2018; <a class=\"ref-link\" id=\"cReal_et+al_2018_a\" href=\"#rReal_et+al_2018_a\">Real et al, 2018</a>), we make use of operators acting on the space of neural network architectures that preserve the function a network represents, dubbed network morphisms (<a class=\"ref-link\" id=\"cChen_et+al_2015_a\" href=\"#rChen_et+al_2015_a\">Chen et al, 2015</a>; Wei et al, 2016), obviating training from scratch and thereby substantially reducing the required training time per network",
        "This mechanism can be interpreted as Lamarckian inheritance in the context of evolutionary algorithms, where Lamarckism refers to a mechanism which allows passing skills acquired during an individual\u2019s lifetime, on to children by means of inheritance",
        "Since network morphisms are limited to solely increasing a network\u2019s size, we introduce approximate network morphisms (Section 3.2) to allow shrinking networks, which is essential in the context of multi-objective search",
        "The proposed Lamarckian inheritance mechanism could in principle be combined with any evolutionary algorithm for architecture search, or any other method using localized changes in architecture space.\n2",
        "We propose a Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture DEsign, dubbed LEMONADE, Section 4, which is suited for the joint optimization of several objectives, such as predictive performance, inference time, or number of parameters",
        "Within only 5 days on 16 GPUs, LEMONADE discovers architectures that are competitive in terms of predictive performance and resource consumption with hand-designed networks, such as MobileNet V2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a>), as well as architectures that were automatically designed using 40x greater resources (Zoph et al, 2018) and other multi-objective methods (<a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\">Dong et al, 2018</a>).\n2 BACKGROUND AND RELATED WORK",
        "It was recently proposed to frame neural architecture search as a reinforcement learning (RL) problem, where the reward of the reinforcement learning agent is based on the validation performance of the trained architecture (<a class=\"ref-link\" id=\"cBaker_et+al_2017_a\" href=\"#rBaker_et+al_2017_a\">Baker et al, 2017a</a>; Zoph & Le, 2017; Zhong et al, 2018; Pham et al, 2018)",
        "A general limitation of one-shot neural architecture search is that the supergraph defined a-priori restricts the search space to its subgraphs",
        "Approaches which require that the entire supergraph resides in GPU memory during architecture search will be restricted to relatively small supergraphs",
        "It is not obvious how one-shot models could be employed for multi-objective optimization as all subgraphs of the one-shot models are of roughly the same size and it is not clear if weight sharing would work for very different-sized architectures",
        "Our work extends this approach by introducing approximate network morphisms, making the use of such operators suitable for multi-objective optimization",
        "We describe the operators used in LEMONADE and how they can be formulated as a network morphism",
        "We have proposed LEMONADE, a multi-objective evolutionary algorithm for architecture search",
        "LEMONADE exploits the fact that evaluating several objectives, such as the performance of a neural network, is orders of magnitude more expensive than evaluating, e.g., a model\u2019s number of parameters"
    ],
    "summary": [
        "Deep learning has enabled remarkable progress on a variety of perceptual tasks, such as image recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>).",
        "2. We propose a Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture DEsign, dubbed LEMONADE, Section 4, which is suited for the joint optimization of several objectives, such as predictive performance, inference time, or number of parameters.",
        "We evaluate LEMONADE for up to five objectives on two different search spaces for image classification: (i) non-modularized architectures and cells that are used as repeatable building blocks within an architecture (Zoph et al, 2018; Zhong et al, 2018) and allow transfer to other data sets.",
        "Within only 5 days on 16 GPUs, LEMONADE discovers architectures that are competitive in terms of predictive performance and resource consumption with hand-designed networks, such as MobileNet V2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a>), as well as architectures that were automatically designed using 40x greater resources (Zoph et al, 2018) and other multi-objective methods (<a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\"><a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\">Dong et al, 2018</a></a>).",
        "Our work extends this approach by introducing approximate network morphisms, making the use of such operators suitable for multi-objective optimization.",
        "In contrast to all mentioned work, LEMONADE (i) does not require a complex macro architecture but rather can start from trivial initial networks, can handle arbitrary search spaces, does not require to define hard constraints or weights on objectives a-priori.",
        "When considering the performance on CIFAR-10 versus the number of parameters or multiply-add operations, LEMONADE is on par with NASNets and MobileNets V2 for resource-intensive models while it outperforms them in the area of very efficient models.",
        "We compare LEMONADE to the state of the art single-objective methods by Zoph et al (2018) (NASNet), Pham et al (2018) (ENAS) and Cai et al (2018b) (PLNT), as well as with the multi-objective method by <a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\">Dong et al (2018</a>) (DPP-Net).",
        "For larger models, LEMONADE is competitive to methods that require significantly more computational resources (Zoph et al, 2018) or start their search with non-trivial architectures (<a class=\"ref-link\" id=\"cCai_et+al_2018_b\" href=\"#rCai_et+al_2018_b\">Cai et al, 2018b</a>; <a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\"><a class=\"ref-link\" id=\"cDong_et+al_2018_a\" href=\"#rDong_et+al_2018_a\">Dong et al, 2018</a></a>).",
        "The cell found by LEMONADE achieved a top-1 error of 28.3% and a top-5 error of 9.6%; this is slightly worse than published results for, e.g., NASNet (26% and 8.4%, respectively) but still competitive, especially seeing that, we used an off-the-shelf training pipeline, on a single GPU, and did not alter any hyperparameters.",
        "LEMONADE exploits the fact that evaluating several objectives, such as the performance of a neural network, is orders of magnitude more expensive than evaluating, e.g., a model\u2019s number of parameters.",
        "This work has partly been supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme under grant no. 716721"
    ],
    "headline": "We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Baker_et+al_2017_a",
            "entry": "Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. In International Conference on Learning Representations, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017"
        },
        {
            "id": "Baker_et+al_2017_b",
            "entry": "Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating Neural Architecture Search using Performance Prediction. In arXiv:1705.10823 [cs], 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10823"
        },
        {
            "id": "Bender_et+al_2018_a",
            "entry": "Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M. Ritchie, and Nick Weston. SMASH: one-shot model architecture search through hypernetworks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20SMASH%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks.%20arXiv%20p%202017"
        },
        {
            "id": "Cai_et+al_2018_a",
            "entry": "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. In Association for the Advancement of Artificial Intelligence, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018"
        },
        {
            "id": "Cai_et+al_2018_b",
            "entry": "Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-Level Network Transformation for Efficient Architecture Search. In International Conference on Machine Learning, June 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Han%20Yang%2C%20Jiacheng%20Zhang%2C%20Weinan%20Han%2C%20Song%20Path-Level%20Network%20Transformation%20for%20Efficient%20Architecture%20Search%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Han%20Yang%2C%20Jiacheng%20Zhang%2C%20Weinan%20Han%2C%20Song%20Path-Level%20Network%20Transformation%20for%20Efficient%20Architecture%20Search%202018-06"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Net2net%3A%20Accelerating%20learning%20via%20knowledge%20transfer.%20arXiv%20p%202015"
        },
        {
            "id": "Cheng_et+al_2018_a",
            "entry": "Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for deep neural networks: The principles, progress, and challenges. IEEE Signal Process. Mag., 35(1): 126\u2013136, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Yu%20Wang%2C%20Duo%20Zhou%2C%20Pan%20Zhang%2C%20Tao%20Model%20compression%20and%20acceleration%20for%20deep%20neural%20networks%3A%20The%20principles%2C%20progress%2C%20and%20challenges%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Yu%20Wang%2C%20Duo%20Zhou%2C%20Pan%20Zhang%2C%20Tao%20Model%20compression%20and%20acceleration%20for%20deep%20neural%20networks%3A%20The%20principles%2C%20progress%2C%20and%20challenges%202018"
        },
        {
            "id": "Chrabaszcz_et+al_2017_a",
            "entry": "Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs/1707.08819, 2017. URL http://arxiv.org/abs/1707.08819.",
            "url": "http://arxiv.org/abs/1707.08819",
            "arxiv_url": "https://arxiv.org/pdf/1707.08819"
        },
        {
            "id": "Deb_2001_a",
            "entry": "Kalyanmoy Deb and Deb Kalyanmoy. Multi-Objective Optimization Using Evolutionary Algorithms. John Wiley & Sons, Inc., New York, NY, USA, 2001. ISBN 047187339X.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deb%2C%20Kalyanmoy%20Kalyanmoy%2C%20Deb%20Multi-Objective%20Optimization%20Using%20Evolutionary%20Algorithms%202001"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint, abs/1708.04552, 2017. URL http://arxiv.org/abs/1708.04552.",
            "url": "http://arxiv.org/abs/1708.04552",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Domhan_et+al_2015_a",
            "entry": "T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Domhan%2C%20T.%20Springenberg%2C%20J.T.%20Hutter%2C%20F.%20Speeding%20up%20automatic%20hyperparameter%20optimization%20of%20deep%20neural%20networks%20by%20extrapolation%20of%20learning%20curves%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Domhan%2C%20T.%20Springenberg%2C%20J.T.%20Hutter%2C%20F.%20Speeding%20up%20automatic%20hyperparameter%20optimization%20of%20deep%20neural%20networks%20by%20extrapolation%20of%20learning%20curves%202015"
        },
        {
            "id": "Dong_et+al_2018_a",
            "entry": "Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. In European Conference on Computer Vision, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Jin-Dong%20Cheng%2C%20An-Chieh%20Juan%2C%20Da-Cheng%20Wei%2C%20Wei%20Dpp-net%3A%20Device-aware%20progressive%20search%20for%20pareto-optimal%20neural%20architectures%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Jin-Dong%20Cheng%2C%20An-Chieh%20Juan%2C%20Da-Cheng%20Wei%2C%20Wei%20Dpp-net%3A%20Device-aware%20progressive%20search%20for%20pareto-optimal%20neural%20architectures%202018"
        },
        {
            "id": "Elsken_et+al_2017_a",
            "entry": "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Simple And Efficient Architecture Search for Convolutional Neural Networks. In NIPS Workshop on Meta-Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elsken%2C%20Thomas%20Metzen%2C%20Jan%20Hendrik%20Hutter%2C%20Frank%20Simple%20And%20Efficient%20Architecture%20Search%20for%20Convolutional%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elsken%2C%20Thomas%20Metzen%2C%20Jan%20Hendrik%20Hutter%2C%20Frank%20Simple%20And%20Efficient%20Architecture%20Search%20for%20Convolutional%20Neural%20Networks%202017"
        },
        {
            "id": "Elsken_et+al_2018_a",
            "entry": "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. AutoML: Methods, Systems, Challenges, 2018. URL https://www.automl.org/book/.",
            "url": "https://www.automl.org/book/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elsken%2C%20Thomas%20Metzen%2C%20Jan%20Hendrik%20Hutter%2C%20Frank%20Neural%20architecture%20search%3A%20A%20survey%202018"
        },
        {
            "id": "Gastaldi_2017_a",
            "entry": "Xavier Gastaldi. Shake-shake regularization. ICLR 2017 Workshop, 2017. Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xavier%20Gastaldi%20Shakeshake%20regularization%20ICLR%202017%20Workshop%202017%20Song%20Han%20Huizi%20Mao%20and%20William%20J%20Dally%20Deep%20compression%20Compressing%20deep%20neural%20networks%20with%20pruning%20trained%20quantization%20and%20huffman%20coding%20In%20International%20Conference%20on%20Learning%20Representations%202016%20Kaiming%20He%20Xiangyu%20Zhang%20Shaoqing%20Ren%20and%20Jian%20Sun%20Deep%20Residual%20Learning%20for%20Image%20Recognition%20In%20CVPR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xavier%20Gastaldi%20Shakeshake%20regularization%20ICLR%202017%20Workshop%202017%20Song%20Han%20Huizi%20Mao%20and%20William%20J%20Dally%20Deep%20compression%20Compressing%20deep%20neural%20networks%20with%20pruning%20trained%20quantization%20and%20huffman%20coding%20In%20International%20Conference%20on%20Learning%20Representations%202016%20Kaiming%20He%20Xiangyu%20Zhang%20Shaoqing%20Ren%20and%20Jian%20Sun%20Deep%20Residual%20Learning%20for%20Image%20Recognition%20In%20CVPR%202016"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%202012"
        },
        {
            "id": "Hinton_et+al_1503_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint, abs/1503.02531, 2015. URL https://arxiv.org/abs/1503.02531.",
            "url": "https://arxiv.org/abs/1503.02531",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. In arXiv:1704.04861 [cs], April 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Huang_et+al_2016_a",
            "entry": "Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Condensenet: An efficient densenet using learned group convolutions. arXiv preprint, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Shichen%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Condensenet%3A%20An%20efficient%20densenet%20using%20learned%20group%20convolutions.%20arXiv%20p%202017"
        },
        {
            "id": "Huang_et+al_2017_b",
            "entry": "Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely Connected Convolutional Networks. In CVPR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%20Huang%20Zhuang%20Liu%20and%20Kilian%20Q%20Weinberger%20Densely%20Connected%20Convolutional%20Networks%20In%20CVPR%202017b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%20Huang%20Zhuang%20Liu%20and%20Kilian%20Q%20Weinberger%20Densely%20Connected%20Convolutional%20Networks%20In%20CVPR%202017b"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5mb model size. arXiv:1602.07360 [cs], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. CoRR, abs/1711.09846, 2017. URL http://arxiv.org/abs/1711.09846.",
            "url": "http://arxiv.org/abs/1711.09846",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Y.-H. Kim, B. Reddy, S. Yun, and Ch. Seo. NEMO: Neuro-evolution with multiobjective optimization of deep neural network for speed and accuracy. In ICML\u201917 AutoML Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.-H.%20Reddy%2C%20B.%20Yun%2C%20S.%20Seo%2C%20Ch%20NEMO%3A%20Neuro-evolution%20with%20multiobjective%20optimization%20of%20deep%20neural%20network%20for%20speed%20and%20accuracy%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.-H.%20Reddy%2C%20B.%20Yun%2C%20S.%20Seo%2C%20Ch%20NEMO%3A%20Neuro-evolution%20with%20multiobjective%20optimization%20of%20deep%20neural%20network%20for%20speed%20and%20accuracy%202017"
        },
        {
            "id": "Klein_et+al_2017_a",
            "entry": "A. Klein, S. Falkner, J. T. Springenberg, and F. Hutter. Learning curve prediction with Bayesian neural networks. In International Conference on Learning Representations (ICLR) 2017 Conference Track, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klein%2C%20A.%20Falkner%2C%20S.%20Springenberg%2C%20J.T.%20Hutter%2C%20F.%20Learning%20curve%20prediction%20with%20Bayesian%20neural%20networks%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klein%2C%20A.%20Falkner%2C%20S.%20Springenberg%2C%20J.T.%20Hutter%2C%20F.%20Learning%20curve%20prediction%20with%20Bayesian%20neural%20networks%202017-04"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive Neural Architecture Search. In arXiv:1712.00559 [cs, stat], December 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical Representations for Efficient Architecture Search. In ICLR, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20Representations%20for%20Efficient%20Architecture%20Search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20Representations%20for%20Efficient%20Architecture%20Search%202018"
        },
        {
            "id": "Liu_et+al_0000_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In arXiv:1806.09055, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09055"
        },
        {
            "id": "Loshchilov_2017_a",
            "entry": "I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR) 2017 Conference Track, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20I.%20Hutter%2C%20F.%20Sgdr%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20I.%20Hutter%2C%20F.%20Sgdr%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017-04"
        },
        {
            "id": "Miettinen_0000_a",
            "entry": "Kaisa Miettinen. Nonlinear Multiobjective Optimization. Springer Science & Business Media, 1999. Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miettinen%2C%20Kaisa%20Nonlinear%20Multiobjective%20Optimization"
        },
        {
            "id": "Raju_et+al_2017_a",
            "entry": "Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving Deep Neural Networks. In arXiv:1703.00548 [cs], March 2017. Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In International Conference on Machine Learning, 2018. Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc V. Le, and Alex Kurakin. Large-scale evolution of image classifiers. ICML, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00548"
        },
        {
            "id": "Real_et+al_2018_a",
            "entry": "Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized Evolution for Image Classifier Architecture Search. In arXiv:1802.01548 [cs], February 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "Sandler_et+al_2018_a",
            "entry": "Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. CoRR, abs/1801.04381, 2018. URL http://arxiv.org/abs/1801.04381.",
            "url": "http://arxiv.org/abs/1801.04381",
            "arxiv_url": "https://arxiv.org/pdf/1801.04381"
        },
        {
            "id": "Saxena_2016_a",
            "entry": "Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. arXiv preprint, 2016. Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shreyas%20Saxena%20and%20Jakob%20Verbeek%20Convolutional%20neural%20fabrics%20arXiv%20preprint%202016%20Kenneth%20O%20Stanley%20and%20Risto%20Miikkulainen%20Evolving%20neural%20networks%20through%20augmenting%20topologies"
        },
        {
            "id": "with_2015_a",
            "entry": "with wi = (wi, A, b). The network morphism equation (3) then holds for A = 1, b = 0. This morphism can be used to add a fully-connected or convolutional layer, as these layers are simply linear mappings. Chen et al. (2015) dubbed this morphism \u201dNet2DeeperNet\u201d. Alternatively to the above replacement, one could also choose",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=with%20wi%20%20wi%20A%20b%20The%20network%20morphism%20equation%203%20then%20holds%20for%20A%20%201%20b%20%200%20This%20morphism%20can%20be%20used%20to%20add%20a%20fullyconnected%20or%20convolutional%20layer%20as%20these%20layers%20are%20simply%20linear%20mappings%20Chen%20et%20al%202015%20dubbed%20this%20morphism%20Net2DeeperNet%20Alternatively%20to%20the%20above%20replacement%20one%20could%20also%20choose",
            "oa_query": "https://api.scholarcy.com/oa_version?query=with%20wi%20%20wi%20A%20b%20The%20network%20morphism%20equation%203%20then%20holds%20for%20A%20%201%20b%20%200%20This%20morphism%20can%20be%20used%20to%20add%20a%20fullyconnected%20or%20convolutional%20layer%20as%20these%20layers%20are%20simply%20linear%20mappings%20Chen%20et%20al%202015%20dubbed%20this%20morphism%20Net2DeeperNet%20Alternatively%20to%20the%20above%20replacement%20one%20could%20also%20choose"
        },
        {
            "id": "with_2015_b",
            "entry": "with an arbitrary function hwh (x). The new parameters are wi = (wi, wh, A). Again, Equation (3) can trivially be satisfied by setting A = 0. We can think of two modifications of a neural network that can be expressed by this morphism: firstly, a layer can be widened (i.e., increasing the number of units in a fully connected layer or the number of channels in a CNN - the Net2WiderNet transformation of Chen et al. (2015)). Let h(x) be the layer to be widened. For example, we can then set h = h to simply double the width. Secondly, skip-connections by concatenation as used by Huang et al. (2016) can also be expressed. If h(x) itself is a sequence of layers, h(x) = hn(x) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 h0(x), then one could choose h(x) = x to realize a skip from h0 to the layer subsequent to hn.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=with%20an%20arbitrary%20function%20hwh%20x%20The%20new%20parameters%20are%20wi%20%20wi%20wh%20A%20Again%20Equation%203%20can%20trivially%20be%20satisfied%20by%20setting%20A%20%200%20We%20can%20think%20of%20two%20modifications%20of%20a%20neural%20network%20that%20can%20be%20expressed%20by%20this%20morphism%20firstly%20a%20layer%20can%20be%20widened%20ie%20increasing%20the%20number%20of%20units%20in%20a%20fully%20connected%20layer%20or%20the%20number%20of%20channels%20in%20a%20CNN%20%20the%20Net2WiderNet%20transformation%20of%20Chen%20et%20al%202015%20Let%20hx%20be%20the%20layer%20to%20be%20widened%20For%20example%20we%20can%20then%20set%20h%20%20h%20to%20simply%20double%20the%20width%20Secondly%20skipconnections%20by%20concatenation%20as%20used%20by%20Huang%20et%20al%202016%20can%20also%20be%20expressed%20If%20hx%20itself%20is%20a%20sequence%20of%20layers%20hx%20%20hnx%20%20%20%20%20%20h0x%20then%20one%20could%20choose%20hx%20%20x%20to%20realize%20a%20skip%20from%20h0%20to%20the%20layer%20subsequent%20to%20hn",
            "oa_query": "https://api.scholarcy.com/oa_version?query=with%20an%20arbitrary%20function%20hwh%20x%20The%20new%20parameters%20are%20wi%20%20wi%20wh%20A%20Again%20Equation%203%20can%20trivially%20be%20satisfied%20by%20setting%20A%20%200%20We%20can%20think%20of%20two%20modifications%20of%20a%20neural%20network%20that%20can%20be%20expressed%20by%20this%20morphism%20firstly%20a%20layer%20can%20be%20widened%20ie%20increasing%20the%20number%20of%20units%20in%20a%20fully%20connected%20layer%20or%20the%20number%20of%20channels%20in%20a%20CNN%20%20the%20Net2WiderNet%20transformation%20of%20Chen%20et%20al%202015%20Let%20hx%20be%20the%20layer%20to%20be%20widened%20For%20example%20we%20can%20then%20set%20h%20%20h%20to%20simply%20double%20the%20width%20Secondly%20skipconnections%20by%20concatenation%20as%20used%20by%20Huang%20et%20al%202016%20can%20also%20be%20expressed%20If%20hx%20itself%20is%20a%20sequence%20of%20layers%20hx%20%20hnx%20%20%20%20%20%20h0x%20then%20one%20could%20choose%20hx%20%20x%20to%20realize%20a%20skip%20from%20h0%20to%20the%20layer%20subsequent%20to%20hn"
        },
        {
            "id": "(2016)_0000_a",
            "entry": "(2016) use a special case of this operator to deal with non-linear, non-idempotent activation functions.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=use%20a%20special%20case%20of%20this%20operator%20to%20deal%20with%20nonlinear%20nonidempotent%20activation%20functions"
        },
        {
            "id": "Another_2016_a",
            "entry": "Another example would be the insertion of an additive skip connection, which were proposed by He et al. (2016) to then one could simplify training: If Niwi itself is choose h(x) = x to realize a skip a sequence from",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Another%20example%20would%20be%20the%20insertion%20of%20an%20additive%20skip%20connection%20which%20were%20proposed%20by%20He%20et%20al%202016%20to%20then%20one%20could%20simplify%20training%20If%20Niwi%20itself%20is%20choose%20hx%20%20x%20to%20realize%20a%20skip%20a%20sequence%20from",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Another%20example%20would%20be%20the%20insertion%20of%20an%20additive%20skip%20connection%20which%20were%20proposed%20by%20He%20et%20al%202016%20to%20then%20one%20could%20simplify%20training%20If%20Niwi%20itself%20is%20choose%20hx%20%20x%20to%20realize%20a%20skip%20a%20sequence%20from"
        }
    ]
}
