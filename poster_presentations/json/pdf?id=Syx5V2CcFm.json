{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNIVERSAL STAGEWISE LEARNING FOR NONCONVEX PROBLEMS WITH CONVERGENCE ON AVERAGED SOLUTIONS",
        "author": "Zaiyi Chen \u2217 University of Science and Technology of China",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Syx5V2CcFm"
        },
        "abstract": "Although stochastic gradient descent (SGD) method and its variants (e.g., stochastic momentum methods, ADAGRAD) are algorithms of choice for solving nonconvex problems (especially deep learning), big gaps still remain between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of ADAGRAD could improve non-adaptive step size of SGD is still missing for non-convex optimization. This paper aims to address these questions and fill the gap between theory and practice. We propose a universal stagewise optimization framework for a broad family of non-smooth nonconvex problems with the following key features: (i) at each stage any suitable stochastic convex optimization algorithms (e.g., SGD or ADAGRAD) that return an averaged solution can be employed for minimizing a regularized convex problem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution is returned as the final solution. Our theoretical results of stagewise ADAGRAD exhibit its adaptive convergence, therefore shed insights on its faster convergence than stagewise SGD for problems with slowly growing cumulative stochastic gradients. To the best of our knowledge, this is the first work for addressing the unresolved issues of existing theories mentioned earlier. Besides theoretical contributions, our empirical studies show that our stagewise SGD and ADAGRAD improve the generalization performance of existing variants/implementations of SGD and ADAGRAD."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convex functions",
            "url": "https://en.wikipedia.org/wiki/convex_functions"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "SHB": "stochastic heavy-ball method",
        "SNAG": "stochastic Nesterov\u2019s accelerated gradient method",
        "SA": "stochastic algorithm",
        "ADMM": "alternating direction methods of multipliers",
        "SM": "stochastic momentum"
    },
    "highlights": [
        "Non-convex optimization has recently received increasing attention due to its popularity in emerging machine learning tasks, for learning deep neural networks",
        "The evaluation is done based on the current averaged solution, and for other baselines the evaluation is done based on the current solution",
        "We have several observations. (i) The proposed stagewise algorithms perform much better in terms of testing error than the existing theoretical versions reported in the literature",
        "This indicates the proposed stagewise step size scheme is better than iteratively decreasing step size scheme. The proposed stagewise algorithms achieve similar and sometimes even better testing error than the heuristic approaches with a fixed frequency decay scheme used in practice",
        "We have proposed a universal stagewise learning framework for solving stochastic nonconvex optimization problems, which employs well-known tricks in practice that have not been well analyzed theoretically",
        "We provided theoretical convergence results for the proposed algorithms for non-smooth non-convex optimization problems"
    ],
    "key_statements": [
        "Non-convex optimization has recently received increasing attention due to its popularity in emerging machine learning tasks, for learning deep neural networks",
        "One issue that has been largely ignored in existing theoretical analysis is that the employed algorithms in practice usually differ from their plain versions that are well understood in theory",
        "We propose a universal stagewise optimization framework for solving a family of non-convex problems, i.e., weakly convex problems, which is broader than smooth non-convex problems and includes some non-smooth non-convex problems",
        "We prove the convergence of their stagewise versions for an averaged solution that is randomly selected from all stagewise averaged solutions",
        "To justify a heuristic approach that returns the last averaged solution in stagewise learning, we present and analyze a non-uniform sampling strategy over stagewise averaged solutions with sampling probabilities increasing as the stage number",
        "The key differences from (<a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\"><a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\">Xu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cDavis_2017_a\" href=\"#rDavis_2017_a\"><a class=\"ref-link\" id=\"cDavis_2017_a\" href=\"#rDavis_2017_a\">Davis & Grimmer, 2017</a></a>) are that (i) we focus on non-convex problems instead of convex problems considered in (<a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\"><a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\">Xu et al, 2017</a></a>); we analyze a non-uniform sampling strategy with sampling probabilities increasing as the stage number to justify a heuristic approach that uses the last averaged solution for prediction, unlike the uniform sampling used in (<a class=\"ref-link\" id=\"cDavis_2017_a\" href=\"#rDavis_2017_a\"><a class=\"ref-link\" id=\"cDavis_2017_a\" href=\"#rDavis_2017_a\">Davis & Grimmer, 2017</a></a>); we present a unified algorithmic framework and convergence analysis, which enable one to employ any suitable stochastic convex optimization algorithms at each stage",
        "For each baseline algorithms of stochastic gradient descent, S\u221aHB, stochastic Nesterov\u2019s accelerated gradient method, we implement two versions - a theory version with iteratively decreasing size \u03b70/ t suggested by previous theories and a heuristic approach with fixed frequency decay scheme used in practice, using and to indicate them",
        "The evaluation is done based on the current averaged solution, and for other baselines the evaluation is done based on the current solution",
        "Due to the limit of space, we only show the results of testing error on CIFAR-10 and CIFAR-100 for the setting without regularization in Figure 1",
        "We have several observations. (i) The proposed stagewise algorithms perform much better in terms of testing error than the existing theoretical versions reported in the literature",
        "This indicates the proposed stagewise step size scheme is better than iteratively decreasing step size scheme. The proposed stagewise algorithms achieve similar and sometimes even better testing error than the heuristic approaches with a fixed frequency decay scheme used in practice",
        "The heuristic approaches usually give smaller training error. This seems to indicate that the proposed algorithms are less vulnerable to the overfitting",
        "We have proposed a universal stagewise learning framework for solving stochastic nonconvex optimization problems, which employs well-known tricks in practice that have not been well analyzed theoretically",
        "We provided theoretical convergence results for the proposed algorithms for non-smooth non-convex optimization problems"
    ],
    "summary": [
        "Non-convex optimization has recently received increasing attention due to its popularity in emerging machine learning tasks, for learning deep neural networks.",
        "Any suitable stochastic convex optimization algorithms (e.g., SGD, ADAGRAD) with a constant step size parameter can be employed for optimizing a regularized convex problem with a number of iterations, which usually return an averaged solution.",
        "Regarding the convergence results, for stagewise SGD, SHB, SNAG, we establish the same order of iteration complexity for finding a nearly stationary point as the existing theories of their nonstagewise variants.",
        "SGD for unconstrained smooth non-convex problems was first analyzed by <a class=\"ref-link\" id=\"cGhadimi_2013_a\" href=\"#rGhadimi_2013_a\">Ghadimi & Lan (2013</a>), who established an O(1/ 4) iteration complexity for finding an -stationary point x in expectation satisfying E[ \u2207f (x) ] \u2264 , where f (\u00b7) denotes the objective function.",
        "These studies have established an iteration complexity of O(1/ 4) for different variants of ADAGRAD for finding an -stationary solution of a stochastic non-convex optimization problem, none of them can exhibit the potential adaptive advantage of ADAGRAD over SGD as in the convex case.",
        "These studies suffer from the following shortcomings: (i) they all assume smoothness of the objective function, while we consider non-smooth and non-convex problems; their convergence is provided on a solution with minimum magnitude of gradient that is expensive to compute, though their results imply a convergence on a random solution selected from all iterates with decreasing sampling probabilities.",
        "One of the main contributions of the present work is to develop a variant of ADAGRAD with adaptive convergence to data for stochastic non-convex optimization.",
        "To contrast with the results in (<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>), we will consider the same unified stochastic momentum methods that subsume SHB, SNAG and SGD as special cases when \u03a9 = Rd. The updates are presented in Algorithm 4 in the Appendix.",
        "For each baseline algorithms of SGD, S\u221aHB, SNAG, we implement two versions - a theory version with iteratively decreasing size \u03b70/ t suggested by previous theories and a heuristic approach with fixed frequency decay scheme used in practice, using and to indicate them.",
        "We have proposed a universal stagewise learning framework for solving stochastic nonconvex optimization problems, which employs well-known tricks in practice that have not been well analyzed theoretically.",
        "We provided theoretical convergence results for the proposed algorithms for non-smooth non-convex optimization problems.",
        "We established an adaptive convergence of a stochastic algorithm using data adaptive coordinate-wise step size of ADAGRAD, and exhibited its faster convergence than non-adaptive stepsize when the growth of cumulative stochastic gradients is slow similar to that in the convex case.",
        "We will consider the empirical studies on the large-scale ImageNet data set"
    ],
    "headline": "This paper aims to address these questions and fill the gap between theory and practice",
    "reference_links": [
        {
            "id": "Harp_et+al_2015_a",
            "entry": "Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Allen-Zhu_2017_a",
            "entry": "Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex parameter. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 89\u201397, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Natasha%3A%20Faster%20non-convex%20stochastic%20optimization%20via%20strongly%20non-convex%20parameter%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Natasha%3A%20Faster%20non-convex%20stochastic%20optimization%20via%20strongly%20non-convex%20parameter%202017"
        },
        {
            "id": "Bottou_2010_a",
            "entry": "Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of International Conference on Computational Statistics (COMPSTAT), pp. 177\u2013187, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20Leon%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20Leon%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010"
        },
        {
            "id": "Carmon_et+al_2016_a",
            "entry": "Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex optimization. CoRR, abs/1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "Chen_et+al_0000_a",
            "entry": "Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. CoRR, abs/1808.02941, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02941"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. Sadagrad: Strongly adaptive stochastic gradient methods. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Zaiyi%20Xu%2C%20Yi%20Chen%2C%20Enhong%20Yang%2C%20Tianbao%20Sadagrad%3A%20Strongly%20adaptive%20stochastic%20gradient%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Zaiyi%20Xu%2C%20Yi%20Chen%2C%20Enhong%20Yang%2C%20Tianbao%20Sadagrad%3A%20Strongly%20adaptive%20stochastic%20gradient%20methods%202018"
        },
        {
            "id": "Davis_0000_a",
            "entry": "Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. CoRR, abs/1803.06523, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06523"
        },
        {
            "id": "Davis_0000_b",
            "entry": "Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate o(k\u22121/4) on weakly convex functions. CoRR, /abs/1802.02988, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02988"
        },
        {
            "id": "Davis_2017_a",
            "entry": "Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. arXiv preprint arXiv:1707.03505, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03505"
        },
        {
            "id": "Dean_et+al_2012_a",
            "entry": "Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS, pp. 1223\u20131231, USA, 2012. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20S.%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20S.%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012"
        },
        {
            "id": "Drusvyatskiy_2018_a",
            "entry": "D. Drusvyatskiy and C. Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. Mathematical Programming, Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drusvyatskiy%2C%20D.%20Paquette%2C%20C.%20Efficiency%20of%20minimizing%20compositions%20of%20convex%20functions%20and%20smooth%20maps%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drusvyatskiy%2C%20D.%20Paquette%2C%20C.%20Efficiency%20of%20minimizing%20compositions%20of%20convex%20functions%20and%20smooth%20maps%202018-07"
        },
        {
            "id": "Drusvyatskiy_2016_a",
            "entry": "Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. arXiv:1602.06661, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.06661"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-%20and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-%20and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "Ghadimi_2016_a",
            "entry": "Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Math. Program., 156(1-2):59\u201399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of the 33nd International Conference on Machine Learning (ICML), pp. 1225\u20131234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, (ICML), pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20ICML%20pp%20448456%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20ICML%20pp%20448456%202015"
        },
        {
            "id": "Jia_et+al_2014_a",
            "entry": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.5093"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 1106\u20131114, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lan_2018_a",
            "entry": "Guanghui Lan and Yu Yang. Accelerated stochastic algorithms for nonconvex finite-sum and multiblock optimization. CoRR, abs/1805.05411, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.05411"
        },
        {
            "id": "Li_2018_a",
            "entry": "Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. CoRR, abs/1805.08114, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08114"
        },
        {
            "id": "Loshchilov_2017_a",
            "entry": "Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. In ICLR, volume abs/1608.03983, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20stochastic%20gradient%20descent%20with%20restarts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20stochastic%20gradient%20descent%20with%20restarts%202017"
        },
        {
            "id": "Mukkamala_2017_a",
            "entry": "Mahesh Chandra Mukkamala and Matthias Hein. Variants of RMSProp and Adagrad with logarithmic regret bounds. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2545\u20132553, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mukkamala%2C%20Mahesh%20Chandra%20Hein%2C%20Matthias%20Variants%20of%20RMSProp%20and%20Adagrad%20with%20logarithmic%20regret%20bounds%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mukkamala%2C%20Mahesh%20Chandra%20Hein%2C%20Matthias%20Variants%20of%20RMSProp%20and%20Adagrad%20with%20logarithmic%20regret%20bounds%202017-08"
        },
        {
            "id": "Ouyang_et+al_2013_a",
            "entry": "Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction method of multipliers. In International Conference on Machine Learning, pp. 80\u201388, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ouyang%2C%20Hua%20He%2C%20Niao%20Tran%2C%20Long%20Gray%2C%20Alexander%20Stochastic%20alternating%20direction%20method%20of%20multipliers%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ouyang%2C%20Hua%20He%2C%20Niao%20Tran%2C%20Long%20Gray%2C%20Alexander%20Stochastic%20alternating%20direction%20method%20of%20multipliers%202013"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Ren_et+al_2018_a",
            "entry": "Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. arXiv preprint arXiv:1803.09050, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09050"
        },
        {
            "id": "Rockafellar_1976_a",
            "entry": "R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14:877\u2013898, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.Tyrrell%20Monotone%20operators%20and%20the%20proximal%20point%20algorithm%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rockafellar%2C%20R.Tyrrell%20Monotone%20operators%20and%20the%20proximal%20point%20algorithm%201976"
        },
        {
            "id": "Rockafellar_1970_a",
            "entry": "R.T. Rockafellar. Convex Analysis. Princeton mathematical series. Princeton University Press, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.T.%20Convex%20Analysis.%20Princeton%20mathematical%20series%201970"
        },
        {
            "id": "Sutskever_et+al_2013_a",
            "entry": "Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML), pp. 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20E.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20E.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "Suzuki_2013_a",
            "entry": "Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction multiplier method. In International Conference on Machine Learning, pp. 392\u2013400, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suzuki%2C%20Taiji%20Dual%20averaging%20and%20proximal%20gradient%20descent%20for%20online%20alternating%20direction%20multiplier%20method%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suzuki%2C%20Taiji%20Dual%20averaging%20and%20proximal%20gradient%20descent%20for%20online%20alternating%20direction%20multiplier%20method%202013"
        },
        {
            "id": "Ward_et+al_2018_a",
            "entry": "Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. CoRR, abs/1806.01811, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01811"
        },
        {
            "id": "Wilson_et+al_2017_a",
            "entry": "Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In NIPS, pp. 4151\u20134161, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20NIPS%20pp%2041514161%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20NIPS%20pp%2041514161%202017"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies faster global convergence. In ICML, pp. 3821 \u2013 3830, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. volume abs/1604.03257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Lin%2C%20Qihang%20Li%2C%20Zhe%20Unified%20convergence%20analysis%20of%20stochastic%20momentum%20methods%20for%20convex%20and%20non-convex%20optimization%202016"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Martin%20Zinkevich%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%20In",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Martin%20Zinkevich%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%20In"
        },
        {
            "id": "I_2003_a",
            "entry": "ICML, pp. 928\u2013936, 2003. Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural networks. CoRR, abs/1808.03408, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.03408"
        },
        {
            "id": "In_1,_b",
            "entry": "In this section, we present more experimental results. Comparison of training and testing error in the two settings (w/o regularization) on the two data sets are plotted in Figure 2, 3, 4, 5. We also report the final testing error (after running 80k iterations) of different algorithms in the two settings on the two datasets in Table 1. For parameter tuning, the initial step sizes of all algorithms are tuned in {0.1, 0.3, 0.5, 0.7, 0.9}. The value of \u03b3 of stagewise algorithms is tuned in {1, 10, 100, 500, 1000, 1500, 2000, 3000}. The initial value T0 for stagewise SGD, SHB, SNAG is tuned in {10, 100, 1k, 5k, 6k, 7k, 10k, 20k}, and that for stagewise ADAGRAD is tuned in {1, 10, 15, 20, 25, 50, 100}.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20section%20we%20present%20more%20experimental%20results%20Comparison%20of%20training%20and%20testing%20error%20in%20the%20two%20settings%20wo%20regularization%20on%20the%20two%20data%20sets%20are%20plotted%20in%20Figure%202%203%204%205%20We%20also%20report%20the%20final%20testing%20error%20after%20running%2080k%20iterations%20of%20different%20algorithms%20in%20the%20two%20settings%20on%20the%20two%20datasets%20in%20Table%201%20For%20parameter%20tuning%20the%20initial%20step%20sizes%20of%20all%20algorithms%20are%20tuned%20in%2001%2003%2005%2007%2009%20The%20value%20of%20%CE%B3%20of%20stagewise%20algorithms%20is%20tuned%20in%201%2010%20100%20500%201000%201500%202000%203000%20The%20initial%20value%20T0%20for%20stagewise%20SGD%20SHB%20SNAG%20is%20tuned%20in%2010%20100%201k%205k%206k%207k%2010k%2020k%20and%20that%20for%20stagewise%20ADAGRAD%20is%20tuned%20in%201%2010%2015%2020%2025%2050%20100",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20this%20section%20we%20present%20more%20experimental%20results%20Comparison%20of%20training%20and%20testing%20error%20in%20the%20two%20settings%20wo%20regularization%20on%20the%20two%20data%20sets%20are%20plotted%20in%20Figure%202%203%204%205%20We%20also%20report%20the%20final%20testing%20error%20after%20running%2080k%20iterations%20of%20different%20algorithms%20in%20the%20two%20settings%20on%20the%20two%20datasets%20in%20Table%201%20For%20parameter%20tuning%20the%20initial%20step%20sizes%20of%20all%20algorithms%20are%20tuned%20in%2001%2003%2005%2007%2009%20The%20value%20of%20%CE%B3%20of%20stagewise%20algorithms%20is%20tuned%20in%201%2010%20100%20500%201000%201500%202000%203000%20The%20initial%20value%20T0%20for%20stagewise%20SGD%20SHB%20SNAG%20is%20tuned%20in%2010%20100%201k%205k%206k%207k%2010k%2020k%20and%20that%20for%20stagewise%20ADAGRAD%20is%20tuned%20in%201%2010%2015%2020%2025%2050%20100"
        }
    ]
}
