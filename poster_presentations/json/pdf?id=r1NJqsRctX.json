{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AUXILIARY VARIATIONAL MCMC",
        "author": "Raza. Habib Department of Computer Science University College London raza.habib@cs.ucl.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1NJqsRctX"
        },
        "abstract": "We introduce Auxiliary Variational MCMC, a novel framework for learning MCMC kernels that combines recent advances in variational inference with insights drawn from traditional auxiliary variable MCMC methods such as Hamiltonian Monte Carlo. Our framework exploits low dimensional structure in the target distribution in order to learn a more efficient MCMC sampler. The resulting sampler is able to suppress random walk behaviour and mix between modes efficiently, without the need to compute gradients of the target distribution. We test our sampler on a number of challenging distributions, where the underlying structure is known, and on the task of posterior sampling in Bayesian logistic regression. Code to reproduce all experiments is available at https://github.com/AVMCMC."
    },
    "keywords": [
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "markov chain monte carlo",
            "url": "https://en.wikipedia.org/wiki/markov_chain_monte_carlo"
        },
        {
            "term": "effective Sample Size",
            "url": "https://en.wikipedia.org/wiki/effective_Sample_Size"
        },
        {
            "term": "Hamiltonian Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo"
        }
    ],
    "abbreviations": {
        "MCMC": "Markov Chain Monte Carlo",
        "VI": "Variational Inference",
        "HMC": "Hamiltonian Monte Carlo",
        "AVS": "Auxiliary Variational Sampler",
        "RWM": "random walk Metropolis",
        "ESS": "effective Sample Size"
    },
    "highlights": [
        "Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) are well-established approaches to approximating expectations under the complex distributions p that frequently arise in machine learning and statistics (Wainwright & Jordan, 2008; <a class=\"ref-link\" id=\"cBrooks_et+al_2011_a\" href=\"#rBrooks_et+al_2011_a\">Brooks et al, 2011</a>)",
        "In practice there are two factors that are of primary concern to an end-user: How easy is the sampler to implement and tune? and how computationally efficient is the resulting sampler? We evaluate our performance by comparison to random walk Metropolis, Hamiltonian Monte Carlo and two recently proposed neural adaptive samplers: A-NICE-MC (<a class=\"ref-link\" id=\"cSong_et+al_2017_a\" href=\"#rSong_et+al_2017_a\">Song et al, 2017</a>) and L2HMC (<a class=\"ref-link\" id=\"cLevy_et+al_2018_a\" href=\"#rLevy_et+al_2018_a\">Levy et al, 2018</a>), discussed further in section 4",
        "We introduced a novel framework for combining Markov Chain Monte Carlo and Variational Inference that makes use of the auxiliary variational method to capture low-dimensional latent structure",
        "We have explored a particular black-box instance of this framework and demonstrated that it can be used to create a fast mixing sampler without the need to take gradients of the target distribution",
        "The method is competitive with other recent geometry-learning approaches and opens up additional avenues for exploring how to combine the best of variational inference and sampling"
    ],
    "key_statements": [
        "Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) are well-established approaches to approximating expectations under the complex distributions p that frequently arise in machine learning and statistics (Wainwright & Jordan, 2008; <a class=\"ref-link\" id=\"cBrooks_et+al_2011_a\" href=\"#rBrooks_et+al_2011_a\">Brooks et al, 2011</a>)",
        "Variational Inference is usually fast and cheap but often places strong restrictions on the class of approximating distributions leading to irreducible bias",
        "We prove in supplement (7) that the following forms a valid Markov Chain Monte Carlo sampling step: 1",
        "We show how the mixture proposal can be naturally combined with the auxiliary variational method.\n2.2",
        "To avoid the above identity mapping issue and encourage the sampler to take large steps, we introduce an additional random perturbation in the auxiliary a-space",
        "We show how Auxiliary Variational Sampler is both able to recover the latent structure and exploit it for the purposes of sampling",
        "In practice there are two factors that are of primary concern to an end-user: How easy is the sampler to implement and tune? and how computationally efficient is the resulting sampler? We evaluate our performance by comparison to random walk Metropolis, Hamiltonian Monte Carlo and two recently proposed neural adaptive samplers: A-NICE-MC (<a class=\"ref-link\" id=\"cSong_et+al_2017_a\" href=\"#rSong_et+al_2017_a\">Song et al, 2017</a>) and L2HMC (<a class=\"ref-link\" id=\"cLevy_et+al_2018_a\" href=\"#rLevy_et+al_2018_a\">Levy et al, 2018</a>), discussed further in section 4",
        "The results demonstrate that Auxiliary Markov Chain Monte Carlo can be used to construct a practical sampler that is able to exploit low dimensional structure and mix between modes",
        "Auxiliary Variational Sampler offers a straightforward method to interpolate between pure Variational Inference and Markov Chain Monte Carlo",
        "We found qualitatively that Auxiliary Variational Sampler and A-NICE-MC were more straightforwards to tune with good performance from many hyper-parameter configurations",
        "We introduced a novel framework for combining Markov Chain Monte Carlo and Variational Inference that makes use of the auxiliary variational method to capture low-dimensional latent structure",
        "We have explored a particular black-box instance of this framework and demonstrated that it can be used to create a fast mixing sampler without the need to take gradients of the target distribution",
        "The method is competitive with other recent geometry-learning approaches and opens up additional avenues for exploring how to combine the best of variational inference and sampling"
    ],
    "summary": [
        "Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) are well-established approaches to approximating expectations under the complex distributions p that frequently arise in machine learning and statistics (Wainwright & Jordan, 2008; <a class=\"ref-link\" id=\"cBrooks_et+al_2011_a\" href=\"#rBrooks_et+al_2011_a\"><a class=\"ref-link\" id=\"cBrooks_et+al_2011_a\" href=\"#rBrooks_et+al_2011_a\">Brooks et al, 2011</a></a>).",
        "The key idea behind Auxiliary Variational MCMC is to exploit structure present in the target distribution p(x) by first fitting a parameterized variational approximation in an augmented space.",
        "A key point of fitting an auxiliary variational distribution is that, by constraining a to have dimension much lower than x, we can exploit this low dimensional manifold to form a more efficient proposal distribution, staying close to the manifold of significant probability.",
        "We perform the random perturbation in the low-dimensional auxiliary space and our variational distribution q\u2217(x |a ) ensures that this move remains within the manifold of high density.",
        "Much like HMC, we are able to exploit the addition of auxiliary variables to encourage large moves of high probability in the target space but unlike HMC we do so by explicitly modelling the structure of the target distribution and do not require gradients of the target density.",
        "To demonstrate the benefit of fitting an auxiliary variational approximation we first test our sampler on a number of distributions with known low-dimensional structure.",
        "To train our AVS sampler, we fit an auxiliary variational approximation with a 1 dimensional continuous mixture.",
        "Looking at figure (3), we can see that the variational distribution has successfully captured the latent structure, automatically learning to map points of high probability in x onto distinct auxiliary variables.",
        "Used with the mixture proposal (10) this results in an effective sampler that is able to take much larger step sizes than random walk Metropolis (RWM) as shown in figure (2).",
        "Posterior sampling in Bayesian models The low-dimensional distributions above demonstrate the ability of our sampler to learn and exploit latent structure.",
        "The results demonstrate that Auxiliary MCMC can be used to construct a practical sampler that is able to exploit low dimensional structure and mix between modes.",
        "Their use of a variational distribution aids the convergence of MCMC, their method still struggles to mix efficiently even in relatively low dimensional problems.",
        "By introducing low dimensional auxiliary variables, we are able both to fit a more accurate approximating distribution and to leverage the learned low dimensional structure to take very large steps in the target space, even hopping between modes.",
        "We introduced a novel framework for combining MCMC and Variational Inference that makes use of the auxiliary variational method to capture low-dimensional latent structure.",
        "The method is competitive with other recent geometry-learning approaches and opens up additional avenues for exploring how to combine the best of variational inference and sampling"
    ],
    "headline": "We introduce Auxiliary Variational Markov Chain Monte Carlo, a novel framework for learning Markov Chain Monte Carlo kernels that combines recent advances in variational inference with insights drawn from traditional auxiliary variable Markov Chain Monte Carlo methods such as Hamiltonian Monte Carlo",
    "reference_links": [
        {
            "id": "Abadi_2015_a",
            "entry": "M. Abadi and A. Agarwal. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Agakov_2004_a",
            "entry": "F. V. Agakov and D. Barber. An auxiliary variational method. International Conference on Neural Information Processing, pp. 561\u2013566, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agakov%2C%20F.V.%20Barber%2C%20D.%20An%20auxiliary%20variational%20method%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agakov%2C%20F.V.%20Barber%2C%20D.%20An%20auxiliary%20variational%20method%202004"
        },
        {
            "id": "Andrieu_2008_a",
            "entry": "C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statistics and computing, pp. 343\u2013373, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrieu%2C%20C.%20Thoms%2C%20J.%20A%20tutorial%20on%20adaptive%20MCMC%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrieu%2C%20C.%20Thoms%2C%20J.%20A%20tutorial%20on%20adaptive%20MCMC%202008"
        },
        {
            "id": "Brooks_et+al_2011_a",
            "entry": "S. Brooks, A. Gelman, G. Jones, and M. Xiao-Li. Handbook of Markov chain Monte Carlo. CRC press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brooks%2C%20S.%20Gelman%2C%20A.%20Jones%2C%20G.%20Xiao-Li%2C%20M.%20Handbook%20of%20Markov%20chain%20Monte%20Carlo%202011"
        },
        {
            "id": "Freitas_et+al_2001_a",
            "entry": "N. De Freitas, P. H\u00f8jen-S\u00f8rensen, M I. Jordan, and S. Russell. Variational MCMC. Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, pp. 120\u2013127, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freitas%2C%20N.De%20H%C3%B8jen-S%C3%B8rensen%2C%20P.%20Jordan%2C%20M.I.%20Russell%2C%20S.%20Variational%20MCMC%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freitas%2C%20N.De%20H%C3%B8jen-S%C3%B8rensen%2C%20P.%20Jordan%2C%20M.I.%20Russell%2C%20S.%20Variational%20MCMC%202001"
        },
        {
            "id": "Dinh_et+al_2015_a",
            "entry": "L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear independent components estimation. International Conference in Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20L.%20Krueger%2C%20D.%20Bengio%2C%20Y.%20NICE%3A%20Non-linear%20independent%20components%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20L.%20Krueger%2C%20D.%20Bengio%2C%20Y.%20NICE%3A%20Non-linear%20independent%20components%20estimation%202015"
        },
        {
            "id": "Duane_et+al_1987_a",
            "entry": "S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Duane%20A%20D%20Kennedy%20B%20J%20Pendleton%20and%20D%20Roweth%20Hybrid%20Monte%20Carlo%20Physics%20Letters%20B%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Duane%20A%20D%20Kennedy%20B%20J%20Pendleton%20and%20D%20Roweth%20Hybrid%20Monte%20Carlo%20Physics%20Letters%20B%201987"
        },
        {
            "id": "Gamerman_2006_a",
            "entry": "D. Gamerman and H. F. Lopes. Markov Chain Monte Carlo. Chapman & Hall, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gamerman%2C%20D.%20Lopes%2C%20H.F.%20Markov%20Chain%20Monte%20Carlo%202006"
        },
        {
            "id": "Gelman_1992_a",
            "entry": "A. Gelman and D. B. Rubin. Inference from iterative simulation using multiple sequences. Statistical Science, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gelman%2C%20A.%20Rubin%2C%20D.B.%20Inference%20from%20iterative%20simulation%20using%20multiple%20sequences%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gelman%2C%20A.%20Rubin%2C%20D.B.%20Inference%20from%20iterative%20simulation%20using%20multiple%20sequences%201992"
        },
        {
            "id": "Girolami_2011_a",
            "entry": "Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. 123\u2013214, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girolami%2C%20Mark%20Calderhead%2C%20Ben%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girolami%2C%20Mark%20Calderhead%2C%20Ben%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and M. Welling. Auto-encoding variational Bayes. International Conference for Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Kleijnen_1996_a",
            "entry": "J. P. C. Kleijnen and R. Y. Rubinstein. Optimization and sensitivity analysis of computer simulation models by the score function method. European Journal of Operational Research, (3):413\u2013427, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleijnen%2C%20J.P.C.%20Rubinstein%2C%20R.Y.%20Optimization%20and%20sensitivity%20analysis%20of%20computer%20simulation%20models%20by%20the%20score%20function%20method%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleijnen%2C%20J.P.C.%20Rubinstein%2C%20R.Y.%20Optimization%20and%20sensitivity%20analysis%20of%20computer%20simulation%20models%20by%20the%20score%20function%20method%201996"
        },
        {
            "id": "Levy_et+al_2018_a",
            "entry": "D. Levy, M. D. Hoffman, and J. Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20D.%20Hoffman%2C%20M.D.%20Sohl-Dickstein%2C%20J.%20Generalizing%20Hamiltonian%20Monte%20Carlo%20with%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20D.%20Hoffman%2C%20M.D.%20Sohl-Dickstein%2C%20J.%20Generalizing%20Hamiltonian%20Monte%20Carlo%20with%20neural%20networks%202018"
        },
        {
            "id": "Minka_2005_a",
            "entry": "T. Minka. Divergence measures and message passing. Technical report, Microsoft Research Ltd, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.%20Divergence%20measures%20and%20message%20passing%202005"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. Artificial Intelligence and Statistics, pp. 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Ranganath_et+al_2016_a",
            "entry": "R. Ranganath, D. Tran, and D. Blei. Hierarchical variational models. In International Conference on Machine Learning, pp. 324\u2013333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models%202016"
        },
        {
            "id": "Roberts_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 G. O. Roberts and J. S. Rosenthal. Examples of adaptive MCMC. Journal of Computational and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roberts%2C%20G.O.%20Rosenthal%2C%20J.S.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roberts%2C%20G.O.%20Rosenthal%2C%20J.S.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Graphical_2009_a",
            "entry": "Graphical Statistics, 18(2):349\u2013367, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graphical%20Statistics%20182349367%202009"
        },
        {
            "id": "Salimans_et+al_2015_a",
            "entry": "T. Salimans, D. Kingma, and M. Welling. Markov chain Monte Carlo and variational inference: Bridging the gap. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1218\u20131226, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Kingma%2C%20D.%20Welling%2C%20M.%20Markov%20chain%20Monte%20Carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Kingma%2C%20D.%20Welling%2C%20M.%20Markov%20chain%20Monte%20Carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015"
        },
        {
            "id": "Song_et+al_2017_a",
            "entry": "J. Song, S. Zhao, and S. Ermon. A-NICE-MC: Adversarial training for MCMC. Advances in Neural Information Processing Systems 30, pp. 5140\u20135150, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20J.%20Zhao%2C%20S.%20Ermon%2C%20S.%20A-NICE-MC%3A%20Adversarial%20training%20for%20MCMC%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20J.%20Zhao%2C%20S.%20Ermon%2C%20S.%20A-NICE-MC%3A%20Adversarial%20training%20for%20MCMC%202017"
        },
        {
            "id": "H_2015_a",
            "entry": "H. Strathmann, D. Sejdinovic, S. Livingstone, Z. Szab\u00f3, and A. Gretton. Gradient-free Hamiltonian Monte Carlo with efficient kernel exponential families. Advances in Neural Information Processing Systems, 2015. M. B. Thompson. A comparison of methods for computing autocorrelation time. arXiv preprint arXiv:1011.0175, 2010. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, pp. 1\u2013305, 2008.",
            "arxiv_url": "https://arxiv.org/pdf/1011.0175"
        }
    ]
}
