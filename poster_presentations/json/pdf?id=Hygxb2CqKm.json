{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "STABLE RECURRENT MODELS",
        "author": "John Miller & Moritz Hardt University of California, Berkeley {miller john,hardt}@berkeley.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hygxb2CqKm"
        },
        "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models."
    },
    "keywords": [
        {
            "term": "sequence learning",
            "url": "https://en.wikipedia.org/wiki/sequence_learning"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "dynamical system",
            "url": "https://en.wikipedia.org/wiki/dynamical_system"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "linear dynamical system",
            "url": "https://en.wikipedia.org/wiki/linear_dynamical_system"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "abbreviations": {
        "LSTM": "Long Short-Term Memory",
        "RNN": "recurrent neural network",
        "ATIS": "Airline Travel Information Systems"
    },
    "highlights": [
        "Recurrent neural networks are a popular modeling choice for solving sequence learning problems arising in domains such as speech recognition and natural language processing",
        "Recurrent models trained in practice do not satisfy stability in an obvious manner, suggesting that perhaps training happens in a chaotic regime",
        "We present a generic definition of stable recurrent models in terms of non-linear dynamical systems and show how to ensure stability of several commonly used models",
        "We prove stable recurrent models can be well-approximated by feed-forward networks for the purposes of both inference and training by gradient descent",
        "Their experiments are complimentary to ours; we find recurrent models can often be replaced with stable recurrent models, which we show are equivalent to feed-forward networks"
    ],
    "key_statements": [
        "Recurrent neural networks are a popular modeling choice for solving sequence learning problems arising in domains such as speech recognition and natural language processing",
        "Recurrent models trained in practice do not satisfy stability in an obvious manner, suggesting that perhaps training happens in a chaotic regime",
        "We present a generic definition of stable recurrent models in terms of non-linear dynamical systems and show how to ensure stability of several commonly used models",
        "We prove stable recurrent models can be well-approximated by feed-forward networks for the purposes of both inference and training by gradient descent",
        "If feed-forward models cannot match the performance of recurrent models, sequence learning in practice is in the unstable regime.\n3.1",
        "We show gradient descent for stable recurrent models finds essentially the same solutions as gradient descent for truncated models",
        "We show stable recurrent models can achieve solid performance on several benchmark sequence tasks",
        "We show unstable recurrent models can often be made stable without a loss in performance",
        "In the Long Short-Term Memory case, after each gradient update, we normalize each row of the weight matrices to satisfy the sufficient conditions for stability given in Section 2.2",
        "Using the data-dependent measure, in Figure 2(a), we show the iterated character-level Long Short-Term Memory, \u03c6rLSTM, is stable for r \u2248 80 iterations",
        "We showed nominally unstable models often satisfy a data-dependent notion of stability",
        "For both word and character-level language models, we find both unstable recurrent neural network and Long Short-Term Memory exhibit vanishing gradients",
        "In Figures 4(a) and 4(b), we show the performance of both Long Short-Term Memory and recurrent neural network for various values of the truncation parameter k on word-level language modeling and polyphonic music modeling",
        "Their approximation result is the same as our inference result for linear dynamical systems, and we extend this result to the non-linear setting",
        "Their experiments are complimentary to ours; we find recurrent models can often be replaced with stable recurrent models, which we show are equivalent to feed-forward networks"
    ],
    "summary": [
        "Recurrent neural networks are a popular modeling choice for solving sequence learning problems arising in domains such as speech recognition and natural language processing.",
        "2. We prove, under the stability assumption, feed-forward networks can approximate recurrent networks for purposes of both inference and training by gradient descent.",
        "We prove stable recurrent models can be well-approximated by feed-forward networks for the purposes of both inference and training by gradient descent.",
        "If feed-forward models cannot match the performance of recurrent models, sequence learning in practice is in the unstable regime.",
        "We show if k \u2248 O), after N steps of gradient descent, the difference in the weights between the recurrent and truncated models is at most \u03b5.",
        "In the LSTM case, after each gradient update, we normalize each row of the weight matrices to satisfy the sufficient conditions for stability given in Section 2.2.",
        "On word and character level language modeling, both stable and unstable RNNs achieve comparable results to (Bai et al, 2018).",
        "If we measure stability in a data-dependent fashion, the unstable LSTM language models are stable, indicating this gap is illusory.",
        "If we instead consider a weaker, data-dependent notion of stability, the word and character-level LSTM models are stable.",
        "Stable models necessarily have vanishing gradients, and this ingredient is a key ingredient in the proof of our training-time approximation result.",
        "For both word and character-level language models, we find both unstable RNNs and LSTMs exhibit vanishing gradients.",
        "In Figures 4(a) and 4(b), we show the performance of both LSTMs and RNNs for various values of the truncation parameter k on word-level language modeling and polyphonic music modeling.",
        "In either case, diminishing returns to performance for large values of k means truncation and feed-forward approximation is possible even for these unstable models.",
        "Proposition (4) in Section 3 ensures the distance between the weight matrices wrecurr \u2212 wtrunc grows slowly as training progresses, and this rate decreases as k becomes large.",
        "These unitary models have not yet seen widespread use, and our work shows much of the sequence learning in practice, even with nominally unstable models, occurs in the stable regime.",
        "Laurent & von Brecht (2017) introduce a non-chaotic recurrent architecture and demonstrate it can perform as well more complex models like LSTMs. Bai et al (2018) conduct a detailed evaluation of recurrent and convolutional, feed-forward models on a variety of sequence modeling tasks."
    ],
    "headline": "We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent",
    "reference_links": [
        {
            "id": "Allan_2005_a",
            "entry": "Moray Allan and Christopher Williams. Harmonising chorales by probabilistic inference. In Advances in Neural Information Processing Systems (NeurIPS), pp. 25\u201332, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allan%2C%20Moray%20Williams%2C%20Christopher%20Harmonising%20chorales%20by%20probabilistic%20inference%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allan%2C%20Moray%20Williams%2C%20Christopher%20Harmonising%20chorales%20by%20probabilistic%20inference%202005"
        },
        {
            "id": "Arjovsky_et+al_2016_a",
            "entry": "Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning (ICML), pp. 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Shaojie_2018_a",
            "entry": "Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01271"
        },
        {
            "id": "Bertsekas_1999_a",
            "entry": "Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Nonlinear%20Programming%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Nonlinear%20Programming%201999"
        },
        {
            "id": "Dauphin_et+al_2017_a",
            "entry": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning (ICML), pp. 933\u2013941, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning (ICML), pp. 1243\u20131252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning (ICML), pp. 1225\u2013 1234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "Hardt_et+al_2018_a",
            "entry": "Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. The Journal of Machine Learning Research, 19(1):1025\u20131068, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Recht%2C%20Benjamin%20Gradient%20descent%20learns%20linear%20dynamical%20systems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Recht%2C%20Benjamin%20Gradient%20descent%20learns%20linear%20dynamical%20systems%202018"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jin_et+al_1994_a",
            "entry": "Liang Jin, Peter N Nikiforuk, and Madan M Gupta. Absolute stability conditions for discrete-time recurrent neural networks. IEEE Transactions on Neural Networks, 5(6):954\u2013964, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Liang%20Nikiforuk%2C%20Peter%20N.%20Gupta%2C%20Madan%20M.%20Absolute%20stability%20conditions%20for%20discrete-time%20recurrent%20neural%20networks%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Liang%20Nikiforuk%2C%20Peter%20N.%20Gupta%2C%20Madan%20M.%20Absolute%20stability%20conditions%20for%20discrete-time%20recurrent%20neural%20networks%201994"
        },
        {
            "id": "Jing_et+al_2017_a",
            "entry": "Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin Soljacic. Tunable efficient unitary neural networks (EUNN) and their application to rnns. In International Conference on Machine Learning (ICML), pp. 1733\u20131741, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20Li%20Shen%2C%20Yichen%20Dubcek%2C%20Tena%20Peurifoy%2C%20John%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20rnns%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20Li%20Shen%2C%20Yichen%20Dubcek%2C%20Tena%20Peurifoy%2C%20John%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20rnns%202017"
        },
        {
            "id": "Jose_et+al_2018_a",
            "entry": "Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. In International Conference on Machine Learning (ICML), pp. 2380\u20132389, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jose%2C%20Cijo%20Cisse%2C%20Moustpaha%20Fleuret%2C%20Francois%20Kronecker%20recurrent%20units%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jose%2C%20Cijo%20Cisse%2C%20Moustpaha%20Fleuret%2C%20Francois%20Kronecker%20recurrent%20units%202018"
        },
        {
            "id": "Kusupati_et+al_2018_a",
            "entry": "Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma. Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9031\u20139042, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kusupati%2C%20Aditya%20Singh%2C%20Manish%20Bhatia%2C%20Kush%20Kumar%2C%20Ashish%20Fastgrnn%3A%20A%20fast%2C%20accurate%2C%20stable%20and%20tiny%20kilobyte%20sized%20gated%20recurrent%20neural%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusupati%2C%20Aditya%20Singh%2C%20Manish%20Bhatia%2C%20Kush%20Kumar%2C%20Ashish%20Fastgrnn%3A%20A%20fast%2C%20accurate%2C%20stable%20and%20tiny%20kilobyte%20sized%20gated%20recurrent%20neural%20network%202018"
        },
        {
            "id": "Laurent_2017_a",
            "entry": "Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Thomas%20von%20Brecht%2C%20James%20A%20recurrent%20neural%20network%20without%20chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Thomas%20von%20Brecht%2C%20James%20A%20recurrent%20neural%20network%20without%20chaos%202017"
        },
        {
            "id": "Marcus_et+al_1993_a",
            "entry": "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993"
        },
        {
            "id": "Merity_et+al_2017_a",
            "entry": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Models. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20Optimizing%20LSTM%20Language%20Models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20Optimizing%20LSTM%20Language%20Models%202018"
        },
        {
            "id": "Mesnil_et+al_2015_a",
            "entry": "Gregoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. Using recurrent neural networks for slot filling in spoken language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):530\u2013539, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mesnil%2C%20Gregoire%20Dauphin%2C%20Yann%20Yao%2C%20Kaisheng%20Bengio%2C%20Yoshua%20Using%20recurrent%20neural%20networks%20for%20slot%20filling%20in%20spoken%20language%20understanding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mesnil%2C%20Gregoire%20Dauphin%2C%20Yann%20Yao%2C%20Kaisheng%20Bengio%2C%20Yoshua%20Using%20recurrent%20neural%20networks%20for%20slot%20filling%20in%20spoken%20language%20understanding%202015"
        },
        {
            "id": "Mhammedi_et+al_2017_a",
            "entry": "Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In International Conference on Machine Learning (ICML), pp. 2401\u20132409, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhammedi%2C%20Zakaria%20Hellicar%2C%20Andrew%20Rahman%2C%20Ashfaqur%20Bailey%2C%20James%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhammedi%2C%20Zakaria%20Hellicar%2C%20Andrew%20Rahman%2C%20Ashfaqur%20Bailey%2C%20James%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017"
        },
        {
            "id": "Oymak_2018_a",
            "entry": "Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. arXiv preprint arXiv:1809.03019, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.03019"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning (ICML), pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Price_1990_a",
            "entry": "Patti J Price. Evaluation of spoken language systems: The atis domain. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Price%2C%20Patti%20J.%20Evaluation%20of%20spoken%20language%20systems%3A%20The%20atis%20domain%201990-06-24",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Price%2C%20Patti%20J.%20Evaluation%20of%20spoken%20language%20systems%3A%20The%20atis%20domain%201990-06-24"
        },
        {
            "id": "Sedghi_2016_a",
            "entry": "Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral methods. CoRR, abs/1603.00954, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.00954"
        },
        {
            "id": "Tu_et+al_2017_a",
            "entry": "Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust control from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04791"
        },
        {
            "id": "Oord_et+al_2016_a",
            "entry": "Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NeurIPS%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NeurIPS%20pp%2060006010%202017"
        },
        {
            "id": "Vorontsov_et+al_2017_a",
            "entry": "Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. In International Conference on Machine Learning (ICML), pp. 3570\u20133578, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vorontsov%2C%20Eugene%20Trabelsi%2C%20Chiheb%20Kadoury%2C%20Samuel%20Pal%2C%20Chris%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vorontsov%2C%20Eugene%20Trabelsi%2C%20Chiheb%20Kadoury%2C%20Samuel%20Pal%2C%20Chris%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017"
        },
        {
            "id": "Wisdom_et+al_2016_a",
            "entry": "Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4880\u20134888, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient SVD parameterization. In International Conference on Machine Learning (ICML), pp. 5806\u2013 5814, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jiong%20Lei%2C%20Qi%20Dhillon%2C%20Inderjit%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jiong%20Lei%2C%20Qi%20Dhillon%2C%20Inderjit%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018"
        }
    ]
}
