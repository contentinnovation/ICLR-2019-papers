{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNSUPERVISED LEARNING VIA META-LEARNING",
        "author": "Kyle Hsu, University of Toronto kyle.hsu@mail.utoronto.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1My6sR9tX"
        },
        "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods."
    },
    "keywords": [
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "use case",
            "url": "https://en.wikipedia.org/wiki/use_case"
        }
    ],
    "abbreviations": {
        "MAML": "model agnostic meta-learning"
    },
    "highlights": [
        "Unsupervised learning is a fundamental, unsolved problem (<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>) and has seen promising results in domains such as image recognition (<a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\"><a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\">Le et al, 2013</a></a>) and natural language understanding (<a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\"><a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\">Ramachandran et al, 2017</a></a>)",
        "To begin addressing these problems, we propose an unsupervised meta-learning method: one which aims to learn a learning procedure, without supervision, that is useful for solving a wide range of new, human-specified tasks",
        "CACTUs-model agnostic meta-learning consistently yields a learning procedure that results in more successful downstream task performance than all other unsupervised methods, including those that learn on top of the embedding that generated meta-training tasks for model agnostic meta-learning",
        "We find the same result for CACTUs-ProtoNets for 1-shot downstream tasks",
        "CACTUs can be seen as a facilitating interface between an embedding learning method and a meta-learning algorithm",
        "CACTUs-based meta-learning may outperform supervised meta-learning when the latter is trained on a misaligned task distribution"
    ],
    "key_statements": [
        "Unsupervised learning is a fundamental, unsolved problem (<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>) and has seen promising results in domains such as image recognition (<a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\"><a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\">Le et al, 2013</a></a>) and natural language understanding (<a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\"><a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\">Ramachandran et al, 2017</a></a>)",
        "If a central goal of unsupervised learning is to learn useful representations, can we derive an unsupervised learning objective that explicitly takes into account how the representation will be used?",
        "To begin addressing these problems, we propose an unsupervised meta-learning method: one which aims to learn a learning procedure, without supervision, that is useful for solving a wide range of new, human-specified tasks",
        "Even though our unsupervised meta-learning algorithm trains for one-shot generalization, one instantiation of our approach performs well not only on few-shot learning, but when learning downstream tasks with up to 50 training examples per class",
        "Our problem setting is similar to that considered by <a class=\"ref-link\" id=\"cGupta_et+al_2018_a\" href=\"#rGupta_et+al_2018_a\">Gupta et al (2018</a>), but we develop an approach that is suitable for supervised downstream tasks, rather than reinforcement learning problems, and demonstrate our algorithm on problems with high-dimensional visual observations",
        "Can unsupervised meta-learning yield a good prior for a variety of task types? In other words, can unsupervised meta-learning yield a good representation for tasks that assess the ability to distinguish between features on different scales, or tasks with various amounts of supervision signal? To investigate this, we evaluate our procedure on tasks assessing recognition of character identity, object identity, and facial attributes",
        "How does the performance of our unsupervised meta-learning method compare to supervised meta-learning with a human-specified, near-optimal task distribution derived from a labeled dataset? To investigate this, we use labeled versions of the meta-training datasets to run model agnostic meta-learning and ProtoNets as supervised meta-learning algorithms (Oracle-model agnostic meta-learning, Oracle-ProtoNets)",
        "We run unsupervised pre-training on the unlabeled meta-training split and report performance on downstream tasks dictated by the labeled data of the meta-testing split, generated using the procedure from prior work recounted in Section 2.2",
        "CACTUs-model agnostic meta-learning consistently yields a learning procedure that results in more successful downstream task performance than all other unsupervised methods, including those that learn on top of the embedding that generated meta-training tasks for model agnostic meta-learning",
        "We find the same result for CACTUs-ProtoNets for 1-shot downstream tasks",
        "We find that using multiple partitions for CACTUs-model agnostic meta-learning, while beneficial, is not strictly necessary",
        "CACTUs can be seen as a facilitating interface between an embedding learning method and a meta-learning algorithm",
        "With its evaluation-agnostic task generation, CACTUs-based meta-learning trades off performance in specific use-cases for broad applicability and the ability to train on unlabeled data",
        "CACTUs-based meta-learning may outperform supervised meta-learning when the latter is trained on a misaligned task distribution"
    ],
    "summary": [
        "Unsupervised learning is a fundamental, unsolved problem (<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>) and has seen promising results in domains such as image recognition (<a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\"><a class=\"ref-link\" id=\"cLe_et+al_2013_a\" href=\"#rLe_et+al_2013_a\">Le et al, 2013</a></a>) and natural language understanding (<a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\"><a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\">Ramachandran et al, 2017</a></a>).",
        "How does the performance of our unsupervised meta-learning method compare to supervised meta-learning with a human-specified, near-optimal task distribution derived from a labeled dataset?",
        "To investigate the limitations of our method, we consider an easier version of our problem statement where the data distributions at meta-training and meta-test time perfectly overlap, i.e. the images share a common set of underlying labels (Appendix D).",
        "We run unsupervised pre-training on the unlabeled meta-training split and report performance on downstream tasks dictated by the labeled data of the meta-testing split, generated using the procedure from prior work recounted in Section 2.2.",
        "CACTUs-MAML consistently yields a learning procedure that results in more successful downstream task performance than all other unsupervised methods, including those that learn on top of the embedding that generated meta-training tasks for MAML.",
        "As noted by <a class=\"ref-link\" id=\"cSnell_et+al_2017_a\" href=\"#rSnell_et+al_2017_a\">Snell et al (2017</a>), ProtoNets perform best when meta-training shot and meta-testing shot are matched; this characteristic prevents ProtoNets from improving upon ACAI for 20-way 5-shot Omniglot and upon DeepCluster for 50-shot miniImageNet. We attribute the success of CACTUs-based meta-learning over the embedding-based methods to two factors: its practice in distinguishing between many distinct sets of clusters from modest amounts of signal, and the underlying classes of the meta-testing split data being out-of-distribution.",
        "The MNIST results (Table 7 in Appendix F) suggest that when the meta-training and meta-testing data distributions have perfect overlap and the embedding is well-suited enough that embedding cluster matching can already achieve high performance, CACTUs-MAML yields only a small benefit, as we would expect.",
        "Meta-learning on random tasks or tasks derived from pixel-space clustering (Table 3) results in a prior that is much less useful than any other considered algorithm, including a random network initialization; practicing badly is worse than not practicing at all.",
        "With its evaluation-agnostic task generation, CACTUs-based meta-learning trades off performance in specific use-cases for broad applicability and the ability to train on unlabeled data.",
        "An important direction for future work is to find examples of datasets and human-designed tasks for which CACTUs-based meta-learning results in ineffective downstream learning."
    ],
    "headline": "We develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data",
    "reference_links": [
        {
            "id": "Bengio_et+al_1991_a",
            "entry": "Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. In International Joint Conference on Neural Networks (IJCNN), 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Bengio%2C%20Samy%20Cloutier%2C%20Jocelyn%20Learning%20a%20synaptic%20learning%20rule%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Bengio%2C%20Samy%20Cloutier%2C%20Jocelyn%20Learning%20a%20synaptic%20learning%20rule%201991"
        },
        {
            "id": "Bengio_et+al_2007_a",
            "entry": "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems (NIPS), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Berthelot_et+al_2018_a",
            "entry": "David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving interpolation in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.07543"
        },
        {
            "id": "Bojanowski_2017_a",
            "entry": "Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Joulin%2C%20Armand%20Unsupervised%20learning%20by%20predicting%20noise%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Joulin%2C%20Armand%20Unsupervised%20learning%20by%20predicting%20noise%202017"
        },
        {
            "id": "Caron_et+al_2018_a",
            "entry": "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caron%2C%20Mathilde%20Bojanowski%2C%20Piotr%20Joulin%2C%20Armand%20Douze%2C%20Matthijs%20Deep%20clustering%20for%20unsupervised%20learning%20of%20visual%20features%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caron%2C%20Mathilde%20Bojanowski%2C%20Piotr%20Joulin%2C%20Armand%20Douze%2C%20Matthijs%20Deep%20clustering%20for%20unsupervised%20learning%20of%20visual%20features%202018"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Cheung_et+al_2015_a",
            "entry": "Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden factors of variation in deep networks. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheung%2C%20Brian%20Livezey%2C%20Jesse%20A.%20Bansal%2C%20Arjun%20K.%20Olshausen%2C%20Bruno%20A.%20Discovering%20hidden%20factors%20of%20variation%20in%20deep%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheung%2C%20Brian%20Livezey%2C%20Jesse%20A.%20Bansal%2C%20Arjun%20K.%20Olshausen%2C%20Bruno%20A.%20Discovering%20hidden%20factors%20of%20variation%20in%20deep%20networks%202015"
        },
        {
            "id": "Coates_2012_a",
            "entry": "Adam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural Networks: Tricks of the Trade. Springer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Y.%20Learning%20feature%20representations%20with%20k-means%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Y.%20Learning%20feature%20representations%20with%20k-means%202012"
        },
        {
            "id": "Dai_2015_a",
            "entry": "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015"
        },
        {
            "id": "Denton_2017_a",
            "entry": "Emily L Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from video. In Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20L.%20Birodkar%2C%20Vighnesh%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20L.%20Birodkar%2C%20Vighnesh%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "Donahue_et+al_2017_a",
            "entry": "Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Krahenbuhl%2C%20Philipp%20Darrell%2C%20Trevor%20Adversarial%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Krahenbuhl%2C%20Philipp%20Darrell%2C%20Trevor%20Adversarial%20feature%20learning%202017"
        },
        {
            "id": "Dumoulin_et+al_2017_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20Vincent%20Belghazi%2C%20Ishmael%20Poole%2C%20Ben%20Mastropietro%2C%20Olivier%20Adversarially%20learned%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20Vincent%20Belghazi%2C%20Ishmael%20Poole%2C%20Ben%20Mastropietro%2C%20Olivier%20Adversarially%20learned%20inference%202017"
        },
        {
            "id": "Erhan_et+al_2010_a",
            "entry": "Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research (JMLR), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Manzagol%2C%20Pierre-Antoine%20Why%20does%20unsupervised%20pre-training%20help%20deep%20learning%3F%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Manzagol%2C%20Pierre-Antoine%20Why%20does%20unsupervised%20pre-training%20help%20deep%20learning%3F%202010"
        },
        {
            "id": "Finn_2018_a",
            "entry": "Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Meta-learning%20and%20universality%3A%20Deep%20representations%20and%20gradient%20descent%20can%20approximate%20any%20learning%20algorithm%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Meta-learning%20and%20universality%3A%20Deep%20representations%20and%20gradient%20descent%20can%20approximate%20any%20learning%20algorithm%202018"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Finn_et+al_2018_b",
            "entry": "Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Xu%2C%20Kelvin%20Levine%2C%20Sergey%20Probabilistic%20model-agnostic%20meta-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Xu%2C%20Kelvin%20Levine%2C%20Sergey%20Probabilistic%20model-agnostic%20meta-learning%202018"
        },
        {
            "id": "Garg_2017_a",
            "entry": "Vikas K Garg and Adam Kalai. Supervising unsupervised learning. arXiv preprint arXiv:1709.05262, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.05262"
        },
        {
            "id": "Gupta_et+al_2018_a",
            "entry": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised metalearning for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04640"
        },
        {
            "id": "Hastie_et+al_2009_a",
            "entry": "Trevor Hastie, Robert Tibshirani, and Jerome Friedman. Unsupervised learning. In The Elements of Statistical Learning. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Friedman%2C%20Jerome%20Unsupervised%20learning.%20In%20The%20Elements%20of%20Statistical%20Learning%202009"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. \u03b2-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20%CE%B2-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20%CE%B2-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017"
        },
        {
            "id": "Hinton_et+al_2006_a",
            "entry": "Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "Howard_2018_a",
            "entry": "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Association for Computational Linguistics (ACL), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Howard%2C%20Jeremy%20Ruder%2C%20Sebastian%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Howard%2C%20Jeremy%20Ruder%2C%20Sebastian%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, and Jiwon Kim. Auto-Meta: Automated gradient based meta learner search. arXiv preprint arXiv:1806.06927, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06927"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Kingma_et+al_2014_b",
            "entry": "Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "Krahenbuhl_et+al_2016_a",
            "entry": "Philipp Krahenbuhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations of convolutional neural networks. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krahenbuhl%2C%20Philipp%20Doersch%2C%20Carl%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Data-dependent%20initializations%20of%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krahenbuhl%2C%20Philipp%20Doersch%2C%20Carl%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Data-dependent%20initializations%20of%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Le_et+al_2013_a",
            "entry": "Quoc V Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Gregory S. Corrado, Kai Chen, Jeffrey Dean, and Andrew Y Ng. Building high-level features using large scale unsupervised learning. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Monga%2C%20Rajat%20Devin%2C%20Matthieu%20and%20Andrew%20Y%20Ng.%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Monga%2C%20Rajat%20Devin%2C%20Matthieu%20and%20Andrew%20Y%20Ng.%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202013"
        },
        {
            "id": "Mathieu_et+al_2016_a",
            "entry": "Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. Disentangling factors of variation in deep representation using adversarial training. In Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20Michael%20F.%20Zhao%2C%20Junbo%20Jake%20Zhao%2C%20Junbo%20Ramesh%2C%20Aditya%20Disentangling%20factors%20of%20variation%20in%20deep%20representation%20using%20adversarial%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20Michael%20F.%20Zhao%2C%20Junbo%20Jake%20Zhao%2C%20Junbo%20Ramesh%2C%20Aditya%20Disentangling%20factors%20of%20variation%20in%20deep%20representation%20using%20adversarial%20training%202016"
        },
        {
            "id": "Metz_et+al_2018_a",
            "entry": "Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Learning to learn without labels. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metz%2C%20Luke%20Maheswaranathan%2C%20Niru%20Cheung%2C%20Brian%20Sohl-Dickstein%2C%20Jascha%20Learning%20to%20learn%20without%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metz%2C%20Luke%20Maheswaranathan%2C%20Niru%20Cheung%2C%20Brian%20Sohl-Dickstein%2C%20Jascha%20Learning%20to%20learn%20without%20labels%202018"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munkhdalai%2C%20Tsendsuren%20Yu%2C%20Hong%20Meta%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munkhdalai%2C%20Tsendsuren%20Yu%2C%20Hong%20Meta%20networks%202017"
        },
        {
            "id": "Naik_1992_a",
            "entry": "Devang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In International Joint Conference on Neural Networks (IJCNN), 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naik%2C%20Devang%20K.%20Mammone%2C%20R.J.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naik%2C%20Devang%20K.%20Mammone%2C%20R.J.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992"
        },
        {
            "id": "Oliver_et+al_2018_a",
            "entry": "Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliver%2C%20Avital%20Odena%2C%20Augustus%20Raffel%2C%20Colin%20Cubuk%2C%20Ekin%20D.%20Realistic%20evaluation%20of%20deep%20semi-supervised%20learning%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oliver%2C%20Avital%20Odena%2C%20Augustus%20Raffel%2C%20Colin%20Cubuk%2C%20Ekin%20D.%20Realistic%20evaluation%20of%20deep%20semi-supervised%20learning%20algorithms%202018"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Radford_et+al_2018_a",
            "entry": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Preprint, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Narasimhan%2C%20Karthik%20Salimans%2C%20Tim%20Sutskever%2C%20Ilya%20Improving%20language%20understanding%20by%20generative%20pre-training%202018"
        },
        {
            "id": "Ramachandran_et+al_2017_a",
            "entry": "Prajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to sequence learning. In Empirical Methods in Natural Language Processing (EMNLP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramachandran%2C%20Prajit%20Liu%2C%20Peter%20J.%20Le%2C%20Quoc%20V.%20Unsupervised%20pretraining%20for%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramachandran%2C%20Prajit%20Liu%2C%20Peter%20J.%20Le%2C%20Quoc%20V.%20Unsupervised%20pretraining%20for%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "Ranzato_et+al_2006_a",
            "entry": "Marc\u2019Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In Neural Information Processing Systems (NIPS), 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20learning%20of%20sparse%20representations%20with%20an%20energy-based%20model%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20learning%20of%20sparse%20representations%20with%20an%20energy-based%20model%202006"
        },
        {
            "id": "Rasmus_et+al_2015_a",
            "entry": "Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Reed_et+al_2014_a",
            "entry": "Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20Sohn%2C%20Kihyuk%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20Sohn%2C%20Kihyuk%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "Jurgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Institut fur Informatik, Technische Universitat Munchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Evolutionary%20principles%20in%20self-referential%20learning%201987"
        },
        {
            "id": "Shelhamer_et+al_2017_a",
            "entry": "Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Selfsupervision for reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shelhamer%2C%20Evan%20Mahmoudieh%2C%20Parsa%20Argus%2C%20Max%20Darrell%2C%20Trevor%20Loss%20is%20its%20own%20reward%3A%20Selfsupervision%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shelhamer%2C%20Evan%20Mahmoudieh%2C%20Parsa%20Argus%2C%20Max%20Darrell%2C%20Trevor%20Loss%20is%20its%20own%20reward%3A%20Selfsupervision%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. In Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20S.%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20S.%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Vincent_et+al_2008_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning (ICML), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "Vincent_et+al_2010_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research (JMLR), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Timothy%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Timothy%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Yu_et+al_2010_a",
            "entry": "Dong Yu, Li Deng, and George Dahl. Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Dong%20Deng%2C%20Li%20Dahl%2C%20George%20Roles%20of%20pre-training%20and%20fine-tuning%20in%20context-dependent%20DBN-HMMs%20for%20real-world%20speech%20recognition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Dong%20Deng%2C%20Li%20Dahl%2C%20George%20Roles%20of%20pre-training%20and%20fine-tuning%20in%20context-dependent%20DBN-HMMs%20for%20real-world%20speech%20recognition%202010"
        },
        {
            "id": "Published_2011_a",
            "entry": "Published as a conference paper at ICLR 2019 Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In Computer Vision and Pattern Recognition (CVPR), 2017. Xiaojin Zhu. Semi-supervised learning. In Encyclopedia of Machine Learning. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Richard%20Zhang%20Phillip%20Isola%20and%20Alexei%20A%20Efros%20Splitbrain%20autoencoders%20Unsupervised%20learning%20by%20crosschannel%20prediction%20In%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202017%20Xiaojin%20Zhu%20Semisupervised%20learning%20In%20Encyclopedia%20of%20Machine%20Learning%20Springer%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Richard%20Zhang%20Phillip%20Isola%20and%20Alexei%20A%20Efros%20Splitbrain%20autoencoders%20Unsupervised%20learning%20by%20crosschannel%20prediction%20In%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202017%20Xiaojin%20Zhu%20Semisupervised%20learning%20In%20Encyclopedia%20of%20Machine%20Learning%20Springer%202011"
        }
    ]
}
