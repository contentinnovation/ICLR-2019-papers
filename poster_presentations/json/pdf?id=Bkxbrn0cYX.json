{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SELFLESS SEQUENTIAL LEARNING",
        "author": "Rahaf Aljundi KU Leuven ESAT-PSI, Belgium rahaf.aljundi@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bkxbrn0cYX"
        },
        "abstract": "Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets."
    },
    "keywords": [
        {
            "term": "lateral inhibition",
            "url": "https://en.wikipedia.org/wiki/lateral_inhibition"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "catastrophic interference",
            "url": "https://en.wikipedia.org/wiki/catastrophic_interference"
        },
        {
            "term": "episodic memory",
            "url": "https://en.wikipedia.org/wiki/episodic_memory"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        }
    ],
    "abbreviations": {
        "LLL": "lifelong learning",
        "SNI": "SNID without neuron importance",
        "SLNI": "SLNID without neuron importance"
    },
    "highlights": [
        "Sequential learning, referred to as continual, incremental, or lifelong learning (LLL), studies the problem of learning a sequence of tasks, one at a time, without access to the training data of previous or future tasks",
        "SLNID can be applied to different regularization based lifelong learning approaches, and we show experiments with MAS (<a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al, 2017</a>) and EWC (<a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al, 2016</a>)",
        "We examine the use of activation functions that are inspired by lateral inhibition in biological neurons that could be advantageous in sequential learning",
        "The locality in SLNID improves the performance in the Cifar sequence, which suggests that a richer representation is needed and multiple active neurons should be tolerated.\n4.5",
        "In this paper we study the problem of sequential learning using a network with fixed capacity \u2013 a prerequisite for a scalable and computationally efficient solution",
        "A key insight of our approach is that in the context of sequential learning, sparsity should be imposed at the level of the representation rather than at the level of the network parameters"
    ],
    "key_statements": [
        "Sequential learning, referred to as continual, incremental, or lifelong learning (LLL), studies the problem of learning a sequence of tasks, one at a time, without access to the training data of previous or future tasks",
        "Some methods exploit an additional episodic memory to store a small amount of previous tasks data to regularize future task learning (e.g. Lopez-Paz et al (2017))",
        "An important component of SLNID is to discount inhibition from/to neurons which have high neuron importance \u2013 a new concept that we introduce in analogy to parameter importance (<a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al, 2016</a>; Zenke et al, 2017; <a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al, 2017</a>)",
        "We propose a novel regularizer, SLNID, which is inspired by lateral inhibition in the brain",
        "SLNID can be applied to different regularization based lifelong learning approaches, and we show experiments with MAS (<a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al, 2017</a>) and EWC (<a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al, 2016</a>)",
        "We study possible regularization techniques that could lead to less interference between the different tasks in a sequential learning scenario either by enforcing sparsity or decorrelation",
        "We examine the use of activation functions that are inspired by lateral inhibition in biological neurons that could be advantageous in sequential learning",
        "In all the different tasks, the representational regularizers show a superior performance to the other studied techniques",
        "The locality in SLNID improves the performance in the Cifar sequence, which suggests that a richer representation is needed and multiple active neurons should be tolerated.\n4.5",
        "We considered the standard task based scenario as in (Li & Hoiem, 2016; Zenke et al, 2017; <a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al, 2017</a>; Serr\u00e0 et al, 2018), where at each time step we receive a task along with its training data and a new classification layer is initiated for the new task, if needed",
        "In this paper we study the problem of sequential learning using a network with fixed capacity \u2013 a prerequisite for a scalable and computationally efficient solution",
        "A key insight of our approach is that in the context of sequential learning, sparsity should be imposed at the level of the representation rather than at the level of the network parameters"
    ],
    "summary": [
        "Sequential learning, referred to as continual, incremental, or lifelong learning (LLL), studies the problem of learning a sequence of tasks, one at a time, without access to the training data of previous or future tasks.",
        "A sparse and decorrelated representation would lead to a powerful representation and at the same time enough free neurons that can be changed without interference with the neural activations learned for the previous tasks.",
        "When combined with a state-of-the-art important parameters preservation method (<a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al, 2017</a>; <a class=\"ref-link\" id=\"cKirkpatrick_et+al_2016_a\" href=\"#rKirkpatrick_et+al_2016_a\">Kirkpatrick et al, 2016</a>), our proposed regularizer leads to sparse and decorrelated representations which improves the lifelong learning performance.",
        "We study possible regularization techniques that could lead to less interference between the different tasks in a sequential learning scenario either by enforcing sparsity or decorrelation.",
        "Results: Figure 2 presents the test accuracy on each task at the end of the sequence, achieved by the different regularizers and activation functions on the network with hidden layer of size 128.",
        "This seems to hint at the effectiveness of our approach SLNID in sequence, hidden layer=128, Top: percentage of unused parameters in the 1st layer using different \u03bbSLNID; Bottom: histogram of neural activations on the first task.",
        "L1-Rep and DeCov continue to improve over the non regularized case No-Reg. These results confirm our proposal on the importance of sparsity and decorrelation in sequential learning.",
        "We have shown that our proposed regularizer SLNID exhibits stable and superior performance on the different tested networks when using MAS as importance weight preservation method.",
        "To prove the effectiveness of our regularizer regardless of the used importance weight based method, we have tested SLNID on the 5 tasks permuted MNIST sequence in combination with Elastic Weight Consolidation (EWC,Kirkpatrick et al (2016)) and obtained a boost in the average performance at the end of the learned sequence equal to 3.1% on the network with hidden layer size 128 and a boost of 2.8% with hidden layer size 64.",
        "The locality in SLNID improves the performance in the Cifar sequence, which suggests that a richer representation is needed and multiple active neurons should be tolerated.",
        "SLNI without MAS improves the individual models performance (72.14% compared to 69.20%), it fails to improve the overall performance at the end of the sequence (63.54% compared to 65.15%), as important neurons are not excluded from the penalty and they are changed or inhibited leading to tasks interference and performance deterioration.",
        "To compare our proposed approach with the different state-of-the-art sequential learning methods, we use a sequence of 8 different object recognition tasks, introduced in <a class=\"ref-link\" id=\"cAljundi_et+al_2017_a\" href=\"#rAljundi_et+al_2017_a\">Aljundi et al (2017</a>).",
        "Acknowledgment: The first author\u2019s PhD is funded by an FWO scholarship"
    ],
    "headline": "To achieve Selfless Sequential Learning we study different regularization strategies and activation functions",
    "reference_links": [
        {
            "id": "Aghasi_et+al_2017_a",
            "entry": "Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. In Advances in Neural Information Processing Systems, pp. 3180\u20133189, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aghasi%2C%20Alireza%20Abdi%2C%20Afshin%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20Convex%20pruning%20of%20deep%20neural%20networks%20with%20performance%20guarantee%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aghasi%2C%20Alireza%20Abdi%2C%20Afshin%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20Convex%20pruning%20of%20deep%20neural%20networks%20with%20performance%20guarantee%202017"
        },
        {
            "id": "Aljundi_et+al_2016_a",
            "entry": "Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aljundi%2C%20Rahaf%20Chakravarty%2C%20Punarjay%20Tuytelaars%2C%20Tinne%20Expert%20gate%3A%20Lifelong%20learning%20with%20a%20network%20of%20experts%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aljundi%2C%20Rahaf%20Chakravarty%2C%20Punarjay%20Tuytelaars%2C%20Tinne%20Expert%20gate%3A%20Lifelong%20learning%20with%20a%20network%20of%20experts%202016"
        },
        {
            "id": "Aljundi_et+al_2017_a",
            "entry": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. arXiv preprint arXiv:1711.09601, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09601"
        },
        {
            "id": "Bengio_2009_a",
            "entry": "Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1): 1\u2013127, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Learning%20deep%20architectures%20for%20ai%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Learning%20deep%20architectures%20for%20ai%202009"
        },
        {
            "id": "Chaudhry_et+al_2018_a",
            "entry": "Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.10112"
        },
        {
            "id": "Cogswell_et+al_2015_a",
            "entry": "Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06068"
        },
        {
            "id": "De_et+al_2009_a",
            "entry": "T. E. de Campos, B. R. Babu, and M. Varma. Character recognition in natural images. In Proceedings of the International Conference on Computer Vision Theory and Applications, Lisbon, Portugal, February 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Campos%2C%20T.E.%20Babu%2C%20B.R.%20Varma%2C%20M.%20Character%20recognition%20in%20natural%20images%202009-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Campos%2C%20T.E.%20Babu%2C%20B.R.%20Varma%2C%20M.%20Character%20recognition%20in%20natural%20images%202009-02"
        },
        {
            "id": "CAL_2012_a",
            "entry": "CAL Visual Object Classes Challenge 2012 (VOC2012) Results.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CAL%20Visual%20Object%20Classes%20Challenge%202012%20VOC2012%20Results",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CAL%20Visual%20Object%20Classes%20Challenge%202012%20VOC2012%20Results"
        },
        {
            "id": "_0000_a",
            "entry": "http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.",
            "url": "http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "French_1992_a",
            "entry": "Robert M French. Semi-distributed representations and catastrophic forgetting in connectionist networks. Connection Science, 4(3-4):365\u2013377, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20Robert%20M.%20Semi-distributed%20representations%20and%20catastrophic%20forgetting%20in%20connectionist%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20Robert%20M.%20Semi-distributed%20representations%20and%20catastrophic%20forgetting%20in%20connectionist%20networks%201992"
        },
        {
            "id": "French_1994_a",
            "entry": "Robert M French. Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference. network, 1111:00001, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20Robert%20M.%20Dynamically%20constraining%20connectionist%20networks%20to%20produce%20distributed%2C%20orthogonal%20representations%20to%20reduce%20catastrophic%20interference%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20Robert%20M.%20Dynamically%20constraining%20connectionist%20networks%20to%20produce%20distributed%2C%20orthogonal%20representations%20to%20reduce%20catastrophic%20interference%201994"
        },
        {
            "id": "French_1999_a",
            "entry": "Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128\u2013135, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20Robert%20M.%20Catastrophic%20forgetting%20in%20connectionist%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20Robert%20M.%20Catastrophic%20forgetting%20in%20connectionist%20networks%201999"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 315\u2013323, Fort Lauderdale, FL, USA, 11\u201313 Apr 2011. PMLR. URL http://proceedings.mlr.press/v15/glorot11a.html.",
            "url": "http://proceedings.mlr.press/v15/glorot11a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%202011-04-11"
        },
        {
            "id": "Goodfellow_et+al_0000_a",
            "entry": "Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013a.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6211"
        },
        {
            "id": "Goodfellow_et+al_2013_a",
            "entry": "Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013b.",
            "arxiv_url": "https://arxiv.org/pdf/1302.4389"
        },
        {
            "id": "Hebb_2002_a",
            "entry": "DO Hebb. The organization of behavior. 1949. New York Wiely, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DO%20Hebb%20The%20organization%20of%20behavior%201949%20New%20York%20Wiely%202002"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. arXiv preprint arXiv:1612.00796, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00796"
        },
        {
            "id": "Krause_et+al_2013_a",
            "entry": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 554\u2013561, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Jonathan%20Stark%2C%20Michael%20Deng%2C%20Jia%20Fei-Fei%2C%20Li%203d%20object%20representations%20for%20fine-grained%20categorization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krause%2C%20Jonathan%20Stark%2C%20Michael%20Deng%2C%20Jia%20Fei-Fei%2C%20Li%203d%20object%20representations%20for%20fine-grained%20categorization%202013"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Krogh_1992_a",
            "entry": "Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950\u2013957, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "Kruschke_1992_a",
            "entry": "John K Kruschke. Alcove: an exemplar-based connectionist model of category learning. Psychological review, 99(1):22, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kruschke%2C%20John%20K.%20Alcove%3A%20an%20exemplar-based%20connectionist%20model%20of%20category%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kruschke%2C%20John%20K.%20Alcove%3A%20an%20exemplar-based%20connectionist%20model%20of%20category%20learning%201992"
        },
        {
            "id": "Kruschke_1993_a",
            "entry": "John K Kruschke. Human category learning: Implications for backpropagation models. Connection Science, 5 (1):3\u201336, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kruschke%2C%20John%20K.%20Human%20category%20learning%3A%20Implications%20for%20backpropagation%20models.%20Connection%20Science%2C%205%201993"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Sang-Woo Lee, Jin-Hwa Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. arXiv preprint arXiv:1703.08475, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08475"
        },
        {
            "id": "Lennie_2003_a",
            "entry": "Peter Lennie. The cost of cortical computation. Current biology, 13(6):493\u2013497, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lennie%2C%20Peter%20The%20cost%20of%20cortical%20computation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lennie%2C%20Peter%20The%20cost%20of%20cortical%20computation%202003"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806\u2013814, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your networks: Better weight consolidation and less catastrophic forgetting. arXiv preprint arXiv:1802.02950, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02950"
        },
        {
            "id": "Lopez-Paz_2017_a",
            "entry": "David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6470\u20136479, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017"
        },
        {
            "id": "Louizos_et+al_2017_a",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pp. 3290\u20133300, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Maji_et+al_2013_a",
            "entry": "S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maji%2C%20S.%20Kannala%2C%20J.%20Rahtu%2C%20E.%20Blaschko%2C%20M.%20Fine-grained%20visual%20classification%20of%20aircraft%202013"
        },
        {
            "id": "Mallya_2017_a",
            "entry": "Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. arXiv preprint arXiv:1711.05769, 1(2):3, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05769"
        },
        {
            "id": "Netzer_et+al_2011_a",
            "entry": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Nilsback_2008_a",
            "entry": "M-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12"
        },
        {
            "id": "Quattoni_2009_a",
            "entry": "Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 413\u2013420. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quattoni%2C%20Ariadna%20Torralba%2C%20Antonio%20Recognizing%20indoor%20scenes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quattoni%2C%20Ariadna%20Torralba%2C%20Antonio%20Recognizing%20indoor%20scenes%202009"
        },
        {
            "id": "Rodr_et+al_2016_a",
            "entry": "Pau Rodr\u00edguez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing cnns with locally constrained decorrelations. arXiv preprint arXiv:1611.01967, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01967"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "Serr_et+al_2018_a",
            "entry": "Joan Serr\u00e0, D\u00eddac Sur\u00eds, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01423"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Sloman_1992_a",
            "entry": "Steven A Sloman and David E Rumelhart. Reducing interference in distributed memories through episodic gating. Essays in honor of WK Estes, 1:227\u2013248, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sloman%2C%20Steven%20A.%20Rumelhart%2C%20David%20E.%20Reducing%20interference%20in%20distributed%20memories%20through%20episodic%20gating%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sloman%2C%20Steven%20A.%20Rumelhart%2C%20David%20E.%20Reducing%20interference%20in%20distributed%20memories%20through%20episodic%20gating%201992"
        },
        {
            "id": "Rupesh_2013_a",
            "entry": "Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and J\u00fcrgen Schmidhuber. Compete to compute. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2310\u20132318. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/5059-compete-to-compute.pdf.",
            "url": "http://papers.nips.cc/paper/5059-compete-to-compute.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rupesh%20K%20Srivastava%20Jonathan%20Masci%20Sohrob%20Kazerounian%20Faustino%20Gomez%20and%20J%C3%BCrgen%20Schmidhuber%20Compete%20to%20compute%20In%20C%20J%20C%20Burges%20L%20Bottou%20M%20Welling%20Z%20Ghahramani%20and%20K%20Q%20Weinberger%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2026%20pp%2023102318%20Curran%20Associates%20Inc%202013%20URL%20httppapersnipsccpaper5059competetocomputepdf"
        },
        {
            "id": "Sun_et+al_2016_a",
            "entry": "Yi Sun, Xiaogang Wang, and Xiaoou Tang. Sparsifying neural network connections for face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4856\u20134864, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Yi%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Sparsifying%20neural%20network%20connections%20for%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Yi%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Sparsifying%20neural%20network%20connections%20for%20face%20recognition%202016"
        },
        {
            "id": "Thrun_1995_a",
            "entry": "Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous systems, 15(1-2): 25\u201346, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Lifelong%20robot%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Lifelong%20robot%20learning%201995"
        },
        {
            "id": "Triki_et+al_2017_a",
            "entry": "Amal Rannen Triki, Rahaf Aljundi, Mathew B Blaschko, and Tinne Tuytelaars. Encoder based lifelong learning. arXiv preprint arXiv:1704.01920, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01920"
        },
        {
            "id": "Welinder_et+al_2010_a",
            "entry": "P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welinder%2C%20P.%20Branson%2C%20S.%20Mita%2C%20T.%20Wah%2C%20C.%20Caltech-UCSD%20Birds%20200%202010"
        },
        {
            "id": "Xiong_et+al_2016_a",
            "entry": "Wei Xiong, Bo Du, Lefei Zhang, Ruimin Hu, and Dacheng Tao. Regularizing deep convolutional neural networks with a structured decorrelation constraint. In ICDM, pp. 519\u2013528, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiong%2C%20Wei%20Du%2C%20Bo%20Zhang%2C%20Lefei%20Hu%2C%20Ruimin%20Regularizing%20deep%20convolutional%20neural%20networks%20with%20a%20structured%20decorrelation%20constraint%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiong%2C%20Wei%20Du%2C%20Bo%20Zhang%2C%20Lefei%20Hu%2C%20Ruimin%20Regularizing%20deep%20convolutional%20neural%20networks%20with%20a%20structured%20decorrelation%20constraint%202016"
        },
        {
            "id": "Xue_et+al_2013_a",
            "entry": "Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, pp. 2365\u20132369, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xue%2C%20Jian%20Li%2C%20Jinyu%20Gong%2C%20Yifan%20Restructuring%20of%20deep%20neural%20network%20acoustic%20models%20with%20singular%20value%20decomposition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xue%2C%20Jian%20Li%2C%20Jinyu%20Gong%2C%20Yifan%20Restructuring%20of%20deep%20neural%20network%20acoustic%20models%20with%20singular%20value%20decomposition%202013"
        },
        {
            "id": "Yao_2015_a",
            "entry": "Leon Yao and John Miller. Tiny imagenet classification with convolutional neural networks. CS 231N, 2015. Yuguo Yu, Michele Migliore, Michael L Hines, and Gordon M Shepherd. Sparse coding and lateral inhibition arising from balanced and unbalanced dendrodendritic excitation and inhibition. Journal of Neuroscience, 34 (41):13701\u201313713, 2014. Friedemann Zenke, Ben Poole, and Surya Ganguli. Improved multitask learning through synaptic intelligence. In Proceedings of the International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Leon%20Miller%2C%20John%20Tiny%20imagenet%20classification%20with%20convolutional%20neural%20networks.%20CS%20231N%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Leon%20Miller%2C%20John%20Tiny%20imagenet%20classification%20with%20convolutional%20neural%20networks.%20CS%20231N%202015"
        }
    ]
}
