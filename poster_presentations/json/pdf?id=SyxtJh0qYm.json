{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "VARIATIONAL AUTOENCODER WITH ARBITRARY CONDITIONING",
        "author": "Oleg Ivanov Samsung AI Center Moscow Moscow, Russia tigvarts@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyxtJh0qYm"
        },
        "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \u201cone shot\u201d. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples."
    },
    "keywords": [
        {
            "term": "probabilistic model",
            "url": "https://en.wikipedia.org/wiki/probabilistic_model"
        },
        {
            "term": "conditional distribution",
            "url": "https://en.wikipedia.org/wiki/conditional_distribution"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "peak signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/peak_signal-to-noise_ratio"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "VAEAC": "Variational Autoencoder with Arbitrary Conditioning",
        "PSNR": "peak signal-to-noise ratio",
        "PFC": "proportion of falsely classified"
    },
    "highlights": [
        "A number of generative probabilistic models based on neural networks have been proposed",
        "We propose a Variational Autoencoder with Arbitrary Conditioning (VAEAC) model",
        "The generative process in variational autoencoder is as follows: first, a latent variable z is generated from the prior distribution p(z), and the data x is generated from the generative distribution p\u03b8(x|z), where \u03b8 are the generative model\u2019s parameters",
        "We show inpainitngs generated by Variational Autoencoder with Arbitrary Conditioning and compare our model with models from papers <a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al (2016</a>), <a class=\"ref-link\" id=\"cYeh_et+al_2017_a\" href=\"#rYeh_et+al_2017_a\">Yeh et al (2017</a>) and <a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al (2017</a>) in terms of peak signal-tonoise ratio (PSNR) of obtained inpaintings on CelebA dataset (<a class=\"ref-link\" id=\"cLiu_et+al_2015_a\" href=\"#rLiu_et+al_2015_a\">Liu et al, 2015</a>)",
        "We propose neural network based probabilistic model for distribution conditioning learning with Gaussian latent variables"
    ],
    "key_statements": [
        "A number of generative probabilistic models based on neural networks have been proposed",
        "We consider the problem of learning all conditional distributions of the form p, where U is the set of all features and I is its arbitrary subset",
        "We propose a Variational Autoencoder with Arbitrary Conditioning (VAEAC) model",
        "The experimental evaluation shows that the proposed model successfully samples from the conditional distributions",
        "We demonstrate that model can generate diverse and realistic image inpaintings on MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>), Omniglot (<a class=\"ref-link\" id=\"cLake_et+al_2015_a\" href=\"#rLake_et+al_2015_a\">Lake et al, 2015</a>) and CelebA (<a class=\"ref-link\" id=\"cLiu_et+al_2015_a\" href=\"#rLiu_et+al_2015_a\">Liu et al, 2015</a>) datasets, and works even better than the current state of the art inpainting techniques in terms of peak signal to noise ratio (PSNR)",
        "In section 3 we briefly describe variational autoencoders and conditional variational autoencoders",
        "In section 5 we evaluate Variational Autoencoder with Arbitrary Conditioning",
        "Universal Marginalizer (<a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\">Douglas et al, 2017</a>) is a model based on a feed-forward neural network which approximates marginals of unobserved features conditioned on observable values",
        "In contrast to Variational Autoencoder with Arbitrary Conditioning, GAIN doesnt use unobserved data during training, which makes it easier to apply to the missing features imputation problem",
        "The comparison of Variational Autoencoder with Arbitrary Conditioning and GAIN on the missing feature imputation problem is given in section 5.1 and appendix D.2",
        "The generative process in variational autoencoder is as follows: first, a latent variable z is generated from the prior distribution p(z), and the data x is generated from the generative distribution p\u03b8(x|z), where \u03b8 are the generative model\u2019s parameters",
        "In the first set of experiments we evaluate Variational Autoencoder with Arbitrary Conditioning missing features imputation performance using various UCI datasets (<a class=\"ref-link\" id=\"cLichman_2013_a\" href=\"#rLichman_2013_a\">Lichman, 2013</a>)",
        "We show inpainitngs generated by Variational Autoencoder with Arbitrary Conditioning and compare our model with models from papers <a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al (2016</a>), <a class=\"ref-link\" id=\"cYeh_et+al_2017_a\" href=\"#rYeh_et+al_2017_a\">Yeh et al (2017</a>) and <a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al (2017</a>) in terms of peak signal-tonoise ratio (PSNR) of obtained inpaintings on CelebA dataset (<a class=\"ref-link\" id=\"cLiu_et+al_2015_a\" href=\"#rLiu_et+al_2015_a\">Liu et al, 2015</a>)",
        "We evaluate the quality of imputations produced by Variational Autoencoder with Arbitrary Conditioning",
        "After that we impute missing features using MICE (Buuren & Groothuis-Oudshoorn, 2010), MissForest (<a class=\"ref-link\" id=\"cStekhoven_2011_a\" href=\"#rStekhoven_2011_a\">Stekhoven & Buhlmann, 2011</a>), GAIN (<a class=\"ref-link\" id=\"cYoon_et+al_2018_a\" href=\"#rYoon_et+al_2018_a\">Yoon et al, 2018</a>) and Variational Autoencoder with Arbitrary Conditioning trained on the observed data",
        "In table 1 we report NRMSE (i.e. RMSE normalized by the standard deviation of each feature and averaged over all features) of imputations for continuous datasets and proportion of falsely classified (PFC) for categorical ones",
        "Even if peak signal-to-noise ratio does not reflect completely the visual quality of images and tends to encourage blurry VAE samples instead of realistic GANs samples, the results show that Variational Autoencoder with Arbitrary Conditioning is able to solve inpainting problem comparably to the state of the art methods",
        "Universal Marginalizer (<a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\">Douglas et al, 2017</a>) (UM) is a model which uses a single neural network to estimate the marginal distributions over the unobserved features",
        "An example of such distribution and more illustrations for comparison of Variational Autoencoder with Arbitrary Conditioning and UM are available in appendix D.5",
        "We propose neural network based probabilistic model for distribution conditioning learning with Gaussian latent variables"
    ],
    "summary": [
        "A number of generative probabilistic models based on neural networks have been proposed.",
        "We consider the problem of learning all conditional distributions of the form p, where U is the set of all features and I is its arbitrary subset.",
        "We propose a Variational Autoencoder with Arbitrary Conditioning (VAEAC) model.",
        "The experimental evaluation shows that the proposed model successfully samples from the conditional distributions.",
        "Universal Marginalizer (<a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\"><a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\">Douglas et al, 2017</a></a>) is a model based on a feed-forward neural network which approximates marginals of unobserved features conditioned on observable values.",
        "In contrast to VAEAC, GAIN doesnt use unobserved data during training, which makes it easier to apply to the missing features imputation problem.",
        "In inpainting setting GAIN cannot learn the conditional distribution over MNIST digits given one horizontal line of the image while VAEAC can.",
        "The comparison of VAEAC and GAIN on the missing feature imputation problem is given in section 5.1 and appendix D.2.",
        "We can say that VAEAC is a similar model which uses prior network to find a proper latents instead of solving the optimization problem.",
        "Variational lower bound for CVAE can be derived to VAE by conditioning all considered distributions on y: LCV AE(x, y; \u03b8, \u03c8, \u03c6) = Eq\u03c6(z|x,y) log p\u03b8(x|z, y) \u2212 DKL p\u03c8(z|y)) \u2264 log p\u03b8,\u03c8(x|y) (3)",
        "This process induces the following model distribution over unobserved features: p\u03c8,\u03b8 = Ez\u223cp\u03c8(z|x1\u2212b,b)p\u03b8",
        "In the first set of experiments we evaluate VAEAC missing features imputation performance using various UCI datasets (<a class=\"ref-link\" id=\"cLichman_2013_a\" href=\"#rLichman_2013_a\">Lichman, 2013</a>).",
        "After that we impute missing features using MICE (Buuren & Groothuis-Oudshoorn, 2010), MissForest (<a class=\"ref-link\" id=\"cStekhoven_2011_a\" href=\"#rStekhoven_2011_a\">Stekhoven & Buhlmann, 2011</a>), GAIN (<a class=\"ref-link\" id=\"cYoon_et+al_2018_a\" href=\"#rYoon_et+al_2018_a\">Yoon et al, 2018</a>) and VAEAC trained on the observed data.",
        "As can be seen from the tables 1 and 2, VAEAC can learn joint data distribution and use it for missing feature imputation.",
        "Even if PSNR does not reflect completely the visual quality of images and tends to encourage blurry VAE samples instead of realistic GANs samples, the results show that VAEAC is able to solve inpainting problem comparably to the state of the art methods.",
        "Universal Marginalizer (<a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\"><a class=\"ref-link\" id=\"cDouglas_et+al_2017_a\" href=\"#rDouglas_et+al_2017_a\">Douglas et al, 2017</a></a>) (UM) is a model which uses a single neural network to estimate the marginal distributions over the unobserved features.",
        "We propose neural network based probabilistic model for distribution conditioning learning with Gaussian latent variables.",
        "The experimental results show that the model is competitive with state of the art methods for both missing features imputation and image inpainting problems."
    ],
    "headline": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and sample the remaining features in \u201cone shot\u201d",
    "reference_links": [
        {
            "id": "Bertalmio_et+al_2000_a",
            "entry": "Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH \u201900, pp. 417\u2013424, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co. ISBN 158113-208-5.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertalmio%2C%20Marcelo%20Sapiro%2C%20Guillermo%20Caselles%2C%20Vincent%20Ballester%2C%20Coloma%20Image%20inpainting%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertalmio%2C%20Marcelo%20Sapiro%2C%20Guillermo%20Caselles%2C%20Vincent%20Ballester%2C%20Coloma%20Image%20inpainting%202000"
        },
        {
            "id": "Bordes_et+al_2017_a",
            "entry": "Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordes%2C%20Florian%20Honari%2C%20Sina%20Vincent%2C%20Pascal%20Learning%20to%20generate%20samples%20from%20noise%20through%20infusion%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordes%2C%20Florian%20Honari%2C%20Sina%20Vincent%2C%20Pascal%20Learning%20to%20generate%20samples%20from%20noise%20through%20infusion%20training%202017"
        },
        {
            "id": "Burda_et+al_2015_a",
            "entry": "Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "Van_2010_a",
            "entry": "S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations in r. Journal of statistical software, pp. 1\u201368, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Buuren%2C%20S.%20Groothuis-Oudshoorn%2C%20Karin%20mice%3A%20Multivariate%20imputation%20by%20chained%20equations%20in%20r%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Buuren%2C%20S.%20Groothuis-Oudshoorn%2C%20Karin%20mice%3A%20Multivariate%20imputation%20by%20chained%20equations%20in%20r%202010"
        },
        {
            "id": "Douglas_et+al_2017_a",
            "entry": "Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, and Saurabh Johri. A universal marginalizer for amortized inference in generative models. arXiv preprint arXiv:1711.00695, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00695"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 881\u2013889, Lille, France, 07\u201309 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/germain15.html.",
            "url": "http://proceedings.mlr.press/v37/germain15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Made%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015-07"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Anirudh Goyal Alias Parth Goyal, Nan Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems, pp. 4392\u20134402, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Anirudh%20Goyal%20Alias%20Parth%20Ke%2C%20Nan%20Ganguli%2C%20Surya%20Bengio%2C%20Yoshua%20Variational%20walkback%3A%20Learning%20a%20transition%20operator%20as%20a%20stochastic%20recurrent%20net%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Anirudh%20Goyal%20Alias%20Parth%20Ke%2C%20Nan%20Ganguli%2C%20Surya%20Bengio%2C%20Yoshua%20Variational%20walkback%3A%20Learning%20a%20transition%20operator%20as%20a%20stochastic%20recurrent%20net%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Cortes%2C%20Corinna%20Burges%2C%20Christopher%20J.C.%20The%20mnist%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang. Generative face completion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yijun%20Liu%2C%20Sifei%20Yang%2C%20Jimei%20Yang%2C%20Ming-Hsuan%20Generative%20face%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yijun%20Liu%2C%20Sifei%20Yang%2C%20Jimei%20Yang%2C%20Ming-Hsuan%20Generative%20face%20completion%202017"
        },
        {
            "id": "Lichman_2013_a",
            "entry": "M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.",
            "url": "http://archive.ics.uci.edu/ml"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12"
        },
        {
            "id": "Makhzani_et+al_2016_a",
            "entry": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders. In International Conference on Learning Representations, 2016. URL http://arxiv.org/abs/1511.05644.",
            "url": "http://arxiv.org/abs/1511.05644",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "Mao_et+al_2016_a",
            "entry": "Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional encoderdecoder networks with symmetric skip connections. In Advances in neural information processing systems, pp. 2802\u20132810, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xiaojiao%20Shen%2C%20Chunhua%20Yang%2C%20Yu-Bin%20Image%20restoration%20using%20very%20deep%20convolutional%20encoderdecoder%20networks%20with%20symmetric%20skip%20connections%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xiaojiao%20Shen%2C%20Chunhua%20Yang%2C%20Yu-Bin%20Image%20restoration%20using%20very%20deep%20convolutional%20encoderdecoder%20networks%20with%20symmetric%20skip%20connections%202016"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Rezende_et+al_0000_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278\u2013 1286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models"
        },
        {
            "id": "Sohl-Dickstein_et+al_2015_a",
            "entry": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/sohl-dickstein15.html.",
            "url": "http://proceedings.mlr.press/v37/sohl-dickstein15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohl-Dickstein%2C%20Jascha%20Weiss%2C%20Eric%20Maheswaranathan%2C%20Niru%20Ganguli%2C%20Surya%20Deep%20unsupervised%20learning%20using%20nonequilibrium%20thermodynamics%202015-07"
        },
        {
            "id": "Sohn_et+al_2015_a",
            "entry": "Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3483\u20133491. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015"
        },
        {
            "id": "S_et+al_2016_a",
            "entry": "Casper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Ladder variational autoencoders. In Advances in neural information processing systems, pp. 3738\u20133746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%B8nderby%2C%20Casper%20Kaae%20Raiko%2C%20Tapani%20Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Ladder%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%B8nderby%2C%20Casper%20Kaae%20Raiko%2C%20Tapani%20Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Ladder%20variational%20autoencoders%202016"
        },
        {
            "id": "Stekhoven_2011_a",
            "entry": "Daniel J Stekhoven and Peter Buhlmann. Missforest - non-parametric missing value imputation for mixedtype data. Bioinformatics, 28(1):112\u2013118, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stekhoven%2C%20Daniel%20J.%20Buhlmann%2C%20Peter%20Missforest%20-%20non-parametric%20missing%20value%20imputation%20for%20mixedtype%20data%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stekhoven%2C%20Daniel%20J.%20Buhlmann%2C%20Peter%20Missforest%20-%20non-parametric%20missing%20value%20imputation%20for%20mixedtype%20data%202011"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li. High-resolution image inpainting using multiscale neural patch synthesis. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4076\u20134084, July 2017. doi: 10.1109/CVPR.2017.434.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2017.434",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2017.434"
        },
        {
            "id": "Yeh_et+al_2016_a",
            "entry": "Raymond Yeh, Chen Chen, Teck-Yian Lim, Mark Hasegawa-Johnson, and Minh N. Do. Semantic image inpainting with perceptual and contextual losses. CoRR, abs/1607.07539, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.07539"
        },
        {
            "id": "Yeh_et+al_2017_a",
            "entry": "Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G Schwing, Mark Hasegawa-Johnson, and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "Yoon_et+al_2018_a",
            "entry": "Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: Missing data imputation using generative adversarial nets. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5689\u20135698, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/yoon18a.html.",
            "url": "http://proceedings.mlr.press/v80/yoon18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoon%2C%20Jinsung%20Jordon%2C%20James%20van%20der%20Schaar%2C%20Mihaela%20GAIN%3A%20Missing%20data%20imputation%20using%20generative%20adversarial%20nets%202018-07"
        }
    ]
}
