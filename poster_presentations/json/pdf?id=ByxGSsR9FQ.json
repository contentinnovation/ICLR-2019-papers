{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "L2-NONEXPANSIVE NEURAL NETWORKS",
        "author": "Haifeng Qian & Mark N. Wegman IBM Research Yorktown Heights, NY 10598, USA qianhaifeng,wegman@us.ibm.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByxGSsR9FQ"
        },
        "abstract": "This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L2-bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties."
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        }
    ],
    "abbreviations": {
        "L2NNNs": "L2-nonexpansive neural networks"
    },
    "highlights": [
        "Artificial neural networks are often ill-conditioned systems in that a small change in the inputs can cause significant changes in the outputs (<a class=\"ref-link\" id=\"cSzegedy_et+al_2014_a\" href=\"#rSzegedy_et+al_2014_a\">Szegedy et al, 2014</a>). This results in poor robustness and vulnerability under adversarial attacks which has been reported on a variety of networks including image classification (<a class=\"ref-link\" id=\"cCarlini_2017_a\" href=\"#rCarlini_2017_a\">Carlini & Wagner, 2017a</a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>), speech recognition (<a class=\"ref-link\" id=\"cKreuk_et+al_2018_a\" href=\"#rKreuk_et+al_2018_a\">Kreuk et al, 2018</a>; <a class=\"ref-link\" id=\"cAlzantot_et+al_2018_a\" href=\"#rAlzantot_et+al_2018_a\">Alzantot et al, 2018</a>; <a class=\"ref-link\" id=\"cCarlini_2018_a\" href=\"#rCarlini_2018_a\">Carlini & Wagner, 2018</a>), image captioning (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>) and natural language processing (<a class=\"ref-link\" id=\"cGao_et+al_2018_a\" href=\"#rGao_et+al_2018_a\">Gao et al, 2018</a>; <a class=\"ref-link\" id=\"cEbrahimi_et+al_2017_a\" href=\"#rEbrahimi_et+al_2017_a\">Ebrahimi et al, 2017</a>)",
        "The reason is that L2-nonexpansive neural networks classifiers achieve their defense by creating a confidence gap between the largest logit and the rest, and that half of this gap is a lower bound of L2-norm of distortion to the input data in order to change the classification",
        "The work of <a class=\"ref-link\" id=\"cHein_2017_a\" href=\"#rHein_2017_a\">Hein & Andriushchenko (2017</a>) makes an important point regarding guarantees provided by local Lipschitz constants, which helps explain many observations in our results, including why adversarial training on L2-nonexpansive neural networks leads to lasting robustness gains",
        "In this work we have presented L2-nonexpansive neural networks which are well-conditioned systems by construction",
        "Their properties are studied through experiments and benefits demonstrated, including that our MNIST and CIFAR-10 classifiers exceed the state of the art in robustness against white-box adversarial attacks, that they are robust against partially random training labels, and that they output confidence gaps which are strongly correlated with robustness and generalization"
    ],
    "key_statements": [
        "Artificial neural networks are often ill-conditioned systems in that a small change in the inputs can cause significant changes in the outputs (<a class=\"ref-link\" id=\"cSzegedy_et+al_2014_a\" href=\"#rSzegedy_et+al_2014_a\">Szegedy et al, 2014</a>). This results in poor robustness and vulnerability under adversarial attacks which has been reported on a variety of networks including image classification (<a class=\"ref-link\" id=\"cCarlini_2017_a\" href=\"#rCarlini_2017_a\">Carlini & Wagner, 2017a</a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>), speech recognition (<a class=\"ref-link\" id=\"cKreuk_et+al_2018_a\" href=\"#rKreuk_et+al_2018_a\">Kreuk et al, 2018</a>; <a class=\"ref-link\" id=\"cAlzantot_et+al_2018_a\" href=\"#rAlzantot_et+al_2018_a\">Alzantot et al, 2018</a>; <a class=\"ref-link\" id=\"cCarlini_2018_a\" href=\"#rCarlini_2018_a\">Carlini & Wagner, 2018</a>), image captioning (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>) and natural language processing (<a class=\"ref-link\" id=\"cGao_et+al_2018_a\" href=\"#rGao_et+al_2018_a\">Gao et al, 2018</a>; <a class=\"ref-link\" id=\"cEbrahimi_et+al_2017_a\" href=\"#rEbrahimi_et+al_2017_a\">Ebrahimi et al, 2017</a>)",
        "The reason is that L2-nonexpansive neural networks classifiers achieve their defense by creating a confidence gap between the largest logit and the rest, and that half of this gap is a lower bound of L2-norm of distortion to the input data in order to change the classification",
        "The work of <a class=\"ref-link\" id=\"cHein_2017_a\" href=\"#rHein_2017_a\">Hein & Andriushchenko (2017</a>) makes an important point regarding guarantees provided by local Lipschitz constants, which helps explain many observations in our results, including why adversarial training on L2-nonexpansive neural networks leads to lasting robustness gains",
        "In this work we have presented L2-nonexpansive neural networks which are well-conditioned systems by construction",
        "Their properties are studied through experiments and benefits demonstrated, including that our MNIST and CIFAR-10 classifiers exceed the state of the art in robustness against white-box adversarial attacks, that they are robust against partially random training labels, and that they output confidence gaps which are strongly correlated with robustness and generalization"
    ],
    "summary": [
        "Artificial neural networks are often ill-conditioned systems in that a small change in the inputs can cause significant changes in the outputs (<a class=\"ref-link\" id=\"cSzegedy_et+al_2014_a\" href=\"#rSzegedy_et+al_2014_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2014_a\" href=\"#rSzegedy_et+al_2014_a\">Szegedy et al, 2014</a></a>).",
        "The reason is that L2NNN classifiers achieve their defense by creating a confidence gap between the largest logit and the rest, and that half of this gap is a lower bound of L2-norm of distortion to the input data in order to change the classification.",
        "The first one is without weight regularization of Section 2.1 and it becomes an ordinary network which has little defense against adversarial attacks; its large average confidence gap is meaningless.",
        "For the second one we remove the third loss term (6) and for the third one we replace norm-pooling with regular max-pooling, both resulting in smaller average confidence gap and less defense against attacks.",
        "Parseval networks (<a class=\"ref-link\" id=\"cCisse_et+al_2017_a\" href=\"#rCisse_et+al_2017_a\">Cisse et al, 2017</a>) can be viewed as models without Lc term, norm-pooling or two-sided ReLU, and with a more restrictive scheme for weight matrix regularization.",
        "If for a data point the L2NNN confidence gap is substantially below average, the classification is delegated to the alternative classifier, and this way we can recover nominal accuracy at a moderate cost of robustness.",
        "To illustrate why L2NNNs generalize better than ordinary networks from noisy data, we show in Table 6 trade-off points between accuracy and confidence gap on the 50%-scrambled training set.",
        "In Table 6, as we adjust the loss function to favor larger average gap, the L2NNNs are forced to make more and more mistakes on the training set.",
        "The practical implication is that after an L2NNN model is trained, one can measure its average confidence gap to know whether and how much it has learned to generalize rather than to memorize the training data.",
        "We use non-standard techniques, e.g. two-sided ReLU, to modify various network components to maximize confidence gaps while keeping the network nonexpansive, and we propose a new loss function for the same purpose.",
        "The work of <a class=\"ref-link\" id=\"cHein_2017_a\" href=\"#rHein_2017_a\">Hein & Andriushchenko (2017</a>) makes an important point regarding guarantees provided by local Lipschitz constants, which helps explain many observations in our results, including why adversarial training on L2NNNs leads to lasting robustness gains.",
        "Their properties are studied through experiments and benefits demonstrated, including that our MNIST and CIFAR-10 classifiers exceed the state of the art in robustness against white-box adversarial attacks, that they are robust against partially random training labels, and that they output confidence gaps which are strongly correlated with robustness and generalization.",
        "There are a number of future directions, for example, other applications of L2NNN, L2NNN-friendly neural network architectures, and the relation between L2NNNs and interpretability"
    ],
    "headline": "This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers",
    "reference_links": [
        {
            "id": "Akhtar_2018_a",
            "entry": "Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. arXiv preprint arXiv:1801.00553, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00553"
        },
        {
            "id": "Alzantot_et+al_2018_a",
            "entry": "Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. Did you hear that? Adversarial examples against automatic speech recognition. arXiv preprint arXiv:1801.00554, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00554"
        },
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Boureau_et+al_2010_a",
            "entry": "Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual recognition. In International Conference on Machine Learning, pp. 111\u2013118, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boureau%2C%20Y.-Lan%20Ponce%2C%20Jean%20LeCun%2C%20Yann%20A%20theoretical%20analysis%20of%20feature%20pooling%20in%20visual%20recognition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boureau%2C%20Y.-Lan%20Ponce%2C%20Jean%20LeCun%2C%20Yann%20A%20theoretical%20analysis%20of%20feature%20pooling%20in%20visual%20recognition%202010"
        },
        {
            "id": "Carlini_0000_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Proceedings of the IEEE Symposium on Security and Privacy, pp. 39\u201357, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3\u201314. ACM, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "Carlini_2018_a",
            "entry": "Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-totext. arXiv preprint arXiv:1801.01944, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01944"
        },
        {
            "id": "Carlini_et+al_2017_b",
            "entry": "Nicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Provably minimally-distorted adversarial examples. arXiv preprint arXiv:1709.10207, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10207"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Show-and-fool: Crafting adversarial examples for neural image captioning. arXiv preprint arXiv:1712.02051, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02051"
        },
        {
            "id": "Cisse_et+al_2017_a",
            "entry": "Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854\u2013863, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "Drucker_1992_a",
            "entry": "Harris Drucker and Yann Le Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991\u2013997, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drucker%2C%20Harris%20Cun%2C%20Yann%20Le%20Improving%20generalization%20performance%20using%20double%20backpropagation%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drucker%2C%20Harris%20Cun%2C%20Yann%20Le%20Improving%20generalization%20performance%20using%20double%20backpropagation%201992"
        },
        {
            "id": "Ebrahimi_et+al_2017_a",
            "entry": "Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for nlp. arXiv preprint arXiv:1712.06751, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06751"
        },
        {
            "id": "Gao_et+al_2018_a",
            "entry": "Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. arXiv preprint arXiv:1801.04354, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04354"
        },
        {
            "id": "Gilmer_et+al_2018_a",
            "entry": "Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Haber_2017_a",
            "entry": "Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Stable%20architectures%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Stable%20architectures%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Hein_2017_a",
            "entry": "Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems, pp. 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "Kawaguchi_et+al_2017_a",
            "entry": "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "J_2017_a",
            "entry": "J. Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "Kreuk_et+al_2018_a",
            "entry": "Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fooling end-to-end speaker verification by adversarial examples. arXiv preprint arXiv:1801.03339, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03339"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Meng_2017_a",
            "entry": "Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "Andrew_2017_a",
            "entry": "Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. arXiv preprint arXiv:1711.09404, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09404"
        },
        {
            "id": "Shang_et+al_2016_a",
            "entry": "Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, pp. 2217\u20132225, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shang%2C%20Wenling%20Sohn%2C%20Kihyuk%20Almeida%2C%20Diogo%20Lee%2C%20Honglak%20Understanding%20and%20improving%20convolutional%20neural%20networks%20via%20concatenated%20rectified%20linear%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shang%2C%20Wenling%20Sohn%2C%20Kihyuk%20Almeida%2C%20Diogo%20Lee%2C%20Honglak%20Understanding%20and%20improving%20convolutional%20neural%20networks%20via%20concatenated%20rectified%20linear%20units%202016"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2014_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "Tramer_et+al_2017_a",
            "entry": "Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07204"
        },
        {
            "id": "Tsipras_et+al_2019_a",
            "entry": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsipras%2C%20Dimitris%20Santurkar%2C%20Shibani%20Engstrom%2C%20Logan%20Turner%2C%20Alexander%20and%20Aleksander%20Madry.%20Robustness%20may%20be%20at%20odds%20with%20accuracy%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsipras%2C%20Dimitris%20Santurkar%2C%20Shibani%20Engstrom%2C%20Logan%20Turner%2C%20Alexander%20and%20Aleksander%20Madry.%20Robustness%20may%20be%20at%20odds%20with%20accuracy%202019"
        },
        {
            "id": "Wong_et+al_2018_a",
            "entry": "Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial defenses. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20Eric%20Schmidt%2C%20Frank%20Metzen%2C%20Jan%20Hendrik%20Kolter%2C%20J.Zico%20Scaling%20provable%20adversarial%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20Eric%20Schmidt%2C%20Frank%20Metzen%2C%20Jan%20Hendrik%20Kolter%2C%20J.Zico%20Scaling%20provable%20adversarial%20defenses%202018"
        },
        {
            "id": "Xu_2012_a",
            "entry": "Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391\u2013423, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012"
        },
        {
            "id": "Yoshida_2017_a",
            "entry": "Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10941"
        },
        {
            "id": "Zantedeschi_et+al_2017_a",
            "entry": "Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adversarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 39\u201349. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "ResNet-like_2017_b",
            "entry": "ResNet-like reconvergence is referred to as aggregation layers in Cisse et al. (2017) and a different formula was used: y = \u03b1 \u00b7 x1 + (1 \u2212 \u03b1) \u00b7 f (x2)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ResNetlike%20reconvergence%20is%20referred%20to%20as%20aggregation%20layers%20in%20Cisse%20et%20al%202017%20and%20a%20different%20formula%20was%20used%20y%20%20%CE%B1%20%20x1%20%201%20%20%CE%B1%20%20f%20x2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ResNetlike%20reconvergence%20is%20referred%20to%20as%20aggregation%20layers%20in%20Cisse%20et%20al%202017%20and%20a%20different%20formula%20was%20used%20y%20%20%CE%B1%20%20x1%20%201%20%20%CE%B1%20%20f%20x2"
        },
        {
            "id": "where_2017_c",
            "entry": "where \u03b1 \u2208 [0, 1] is a trainable parameter. Because splitting is not modified in Cisse et al. (2017), their scheme may seem approximately equivalent to ours if a common t parameter is used for (9) and (10). However, there is a substantial difference: in many ResNet blocks, f (x2) is a subset of rather than all o\u221af the output channels of convolution layers, and our scheme does not apply the shrinking factor of 1 \u2212 t2 on channels that are not part of f (x2) and therefore better preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of Cisse et al. (2017) must apply the shrinking factor of 1 \u2212 \u03b1 on all outputs of convolution layers, regardless of whether a channel is part of the aggregation or not. To state the difference in more general terms, our scheme enables splitting and reconvergence at arbitrary levels of granularity and multiplies shrinking factors to only the necessary components. We can also have a different t per channel or even per entry.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=where%20%CE%B1%20%200%201%20is%20a%20trainable%20parameter%20Because%20splitting%20is%20not%20modified%20in%20Cisse%20et%20al%202017%20their%20scheme%20may%20seem%20approximately%20equivalent%20to%20ours%20if%20a%20common%20t%20parameter%20is%20used%20for%209%20and%2010%20However%20there%20is%20a%20substantial%20difference%20in%20many%20ResNet%20blocks%20f%20x2%20is%20a%20subset%20of%20rather%20than%20all%20of%20the%20output%20channels%20of%20convolution%20layers%20and%20our%20scheme%20does%20not%20apply%20the%20shrinking%20factor%20of%201%20%20t2%20on%20channels%20that%20are%20not%20part%20of%20f%20x2%20and%20therefore%20better%20preserve%20distances%20In%20contrast%20because%20splitting%20is%20not%20modified%20at%20reconvergence%20the%20scheme%20of%20Cisse%20et%20al%202017%20must%20apply%20the%20shrinking%20factor%20of%201%20%20%CE%B1%20on%20all%20outputs%20of%20convolution%20layers%20regardless%20of%20whether%20a%20channel%20is%20part%20of%20the%20aggregation%20or%20not%20To%20state%20the%20difference%20in%20more%20general%20terms%20our%20scheme%20enables%20splitting%20and%20reconvergence%20at%20arbitrary%20levels%20of%20granularity%20and%20multiplies%20shrinking%20factors%20to%20only%20the%20necessary%20components%20We%20can%20also%20have%20a%20different%20t%20per%20channel%20or%20even%20per%20entry",
            "oa_query": "https://api.scholarcy.com/oa_version?query=where%20%CE%B1%20%200%201%20is%20a%20trainable%20parameter%20Because%20splitting%20is%20not%20modified%20in%20Cisse%20et%20al%202017%20their%20scheme%20may%20seem%20approximately%20equivalent%20to%20ours%20if%20a%20common%20t%20parameter%20is%20used%20for%209%20and%2010%20However%20there%20is%20a%20substantial%20difference%20in%20many%20ResNet%20blocks%20f%20x2%20is%20a%20subset%20of%20rather%20than%20all%20of%20the%20output%20channels%20of%20convolution%20layers%20and%20our%20scheme%20does%20not%20apply%20the%20shrinking%20factor%20of%201%20%20t2%20on%20channels%20that%20are%20not%20part%20of%20f%20x2%20and%20therefore%20better%20preserve%20distances%20In%20contrast%20because%20splitting%20is%20not%20modified%20at%20reconvergence%20the%20scheme%20of%20Cisse%20et%20al%202017%20must%20apply%20the%20shrinking%20factor%20of%201%20%20%CE%B1%20on%20all%20outputs%20of%20convolution%20layers%20regardless%20of%20whether%20a%20channel%20is%20part%20of%20the%20aggregation%20or%20not%20To%20state%20the%20difference%20in%20more%20general%20terms%20our%20scheme%20enables%20splitting%20and%20reconvergence%20at%20arbitrary%20levels%20of%20granularity%20and%20multiplies%20shrinking%20factors%20to%20only%20the%20necessary%20components%20We%20can%20also%20have%20a%20different%20t%20per%20channel%20or%20even%20per%20entry"
        },
        {
            "id": "Be_2017_a",
            "entry": "To be fair, the scheme of Cisse et al. (2017) has an advantage of being nonexpansive with respect to any Lp-norm. However, for L2-norm, it is inferior to ours in preserving distances and maximizing confidence gaps.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=be%20fair%2C%20To%20the%20scheme%20of%20Cisse%20has%20an%20advantage%20of%20being%20nonexpansive%20with%20respect%20to%20any%20Lp-norm.%20However%2C%20for%20L2-norm%2C%20it%20is%20inferior%20to%20ours%20in%20preserving%20distances%20and%20maximizing%20confidence%20gaps%202017"
        }
    ]
}
