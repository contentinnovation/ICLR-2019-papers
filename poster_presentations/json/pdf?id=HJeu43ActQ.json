{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NOODL: PROVABLE ONLINE DICTIONARY LEARNING",
        "author": "AND SPARSE CODING",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJeu43ActQ"
        },
        "abstract": "We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL, is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via experimental evaluation of the proposed algorithm with the current state-of-the-art techniques."
    },
    "keywords": [
        {
            "term": "International Symposium on Information Theory",
            "url": "https://en.wikipedia.org/wiki/International_Symposium_on_Information_Theory"
        },
        {
            "term": "sparse coding",
            "url": "https://en.wikipedia.org/wiki/sparse_coding"
        },
        {
            "term": "dictionary learning",
            "url": "https://en.wikipedia.org/wiki/dictionary_learning"
        },
        {
            "term": "International Conference on Machine Learning",
            "url": "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning"
        },
        {
            "term": "Neural Information Processing Systems",
            "url": "https://en.wikipedia.org/wiki/Neural_Information_Processing_Systems"
        },
        {
            "term": "sparse representation",
            "url": "https://en.wikipedia.org/wiki/sparse_representation"
        },
        {
            "term": "linear combination",
            "url": "https://en.wikipedia.org/wiki/linear_combination"
        }
    ],
    "abbreviations": {
        "DL": "dictionary learning",
        "MOD": "method of optimal directions",
        "IHT": "iterative hard-thresholding",
        "HT": "hard thresholding",
        "COLT": "Conference on Learning Theory",
        "ISIT": "International Symposium on Information Theory",
        "ICML": "International Conference on Machine Learning",
        "EUSIPCO": "European Signal Processing Conference",
        "CVPR": "Computer Vision and Pattern Recognition",
        "NIPS": "Neural Information Processing Systems",
        "FISTA": "Fast Iterative Shrinkage-Thresholding Algorithm"
    },
    "highlights": [
        "Sparse models avoid overfitting by favoring simple yet highly expressive representations",
        "In addition to deriving conditions on the parameters to preserve the correct signed-support, we analyze the recursive iterative hard-thresholding update step, and decompose the noise term into a component that depends on the error in the dictionary, and the other that depends on the initial coefficient estimate",
        "We present a neural implementation of our algorithm in Fig. 1, which showcases the applicability of NOODL in large-scale distributed learning tasks, motivated from the implementations described in (<a class=\"ref-link\" id=\"cOlshausen_1997_a\" href=\"#rOlshausen_1997_a\">Olshausen and Field, 1997</a>) and (<a class=\"ref-link\" id=\"cArora_et+al_2015_a\" href=\"#rArora_et+al_2015_a\">Arora et al, 2015</a>)",
        "We present NOODL, to the best of our knowledge, the first neurally plausible provable online algorithm for exact recovery of both factors of the dictionary learning (DL) model",
        "We show that once initialized appropriately, the sequence of estimates produced by NOODL converge linearly to the true dictionary and coefficients without incurring any bias in the estimation",
        "We present some additional results to highlight the features of NOODL"
    ],
    "key_statements": [
        "Sparse models avoid overfitting by favoring simple yet highly expressive representations",
        "We present a simple online dictionary learning algorithm motivated from the following regularized least squares-based problem, where S(\u00b7) is a nonlinear function that promotes sparsity",
        "We develop an iterative hard thresholding (IHT)-based update step (<a class=\"ref-link\" id=\"cHaupt_2006_a\" href=\"#rHaupt_2006_a\">Haupt and Nowak, 2006</a>; <a class=\"ref-link\" id=\"cBlumensath_2009_a\" href=\"#rBlumensath_2009_a\">Blumensath and Davies, 2009</a>), and show that \u2013 given an appropriate initial estimate of the dictionary and a mini-batch of p data samples at each iteration t of the online algorithm \u2013",
        "We present a prototype neural implementation of NOODL",
        "We show that for NOODL, \u03b6t = 0, which facilitates linear convergence to A\u2217 without incurring any bias",
        "In addition to deriving conditions on the parameters to preserve the correct signed-support, we analyze the recursive iterative hard-thresholding update step, and decompose the noise term into a component that depends on the error in the dictionary, and the other that depends on the initial coefficient estimate",
        "We present a neural implementation of our algorithm in Fig. 1, which showcases the applicability of NOODL in large-scale distributed learning tasks, motivated from the implementations described in (<a class=\"ref-link\" id=\"cOlshausen_1997_a\" href=\"#rOlshausen_1997_a\">Olshausen and Field, 1997</a>) and (<a class=\"ref-link\" id=\"cArora_et+al_2015_a\" href=\"#rArora_et+al_2015_a\">Arora et al, 2015</a>)",
        "In panels (a-ii), (b-ii), (c-ii) and (d-ii) we show the corresponding performance of NOODL in terms of the error in the overall fit ( Y \u2212 AX F/ Y F), and the error in the coefficients and the dictionary, in terms of relative Frobenius error metric discussed above",
        "We present NOODL, to the best of our knowledge, the first neurally plausible provable online algorithm for exact recovery of both factors of the dictionary learning (DL) model",
        "We show that once initialized appropriately, the sequence of estimates produced by NOODL converge linearly to the true dictionary and coefficients without incurring any bias in the estimation",
        "To get a handle on the coefficients, in Step II.A, we derive an upper-bound on the error incurred by each non-zero element of the estimated coefficient vector, i.e., |xi \u2212 xi\u2217| for i \u2208 S for a general coefficient vector x\u2217, and show that this error only depends on t given enough iterative hard-thresholding iterations R as per the chosen decay parameter \u03b4R",
        "Step I.B: The iterative iterative hard-thresholding-type updates preserve the correct signed support\u2013 we show that the iterative hard-thresholding-type coefficient update step (4) preserves the correct signed-support for an appropriate choice of step-size parameter \u03b7x(r) and threshold \u03c4 (r)",
        "We present some additional results to highlight the features of NOODL",
        "We analyze the computation time per iteration via two metrics. First of these is the average computation time taken per iteration by accounting for the average time take per Lasso update, and the second is the average time taken per iteration to scan over all (10) values of the regularization parameter .\n2We use the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) (<a class=\"ref-link\" id=\"cBeck_2009_a\" href=\"#rBeck_2009_a\">Beck and Teboulle, 2009</a>), which is among the most efficient algorithms for solving the 1-regularized problems"
    ],
    "summary": [
        "Sparse models avoid overfitting by favoring simple yet highly expressive representations.",
        "We develop an iterative hard thresholding (IHT)-based update step (<a class=\"ref-link\" id=\"cHaupt_2006_a\" href=\"#rHaupt_2006_a\">Haupt and Nowak, 2006</a>; <a class=\"ref-link\" id=\"cBlumensath_2009_a\" href=\"#rBlumensath_2009_a\">Blumensath and Davies, 2009</a>), and show that \u2013 given an appropriate initial estimate of the dictionary and a mini-batch of p data samples at each iteration t of the online algorithm \u2013",
        "In addition to deriving conditions on the parameters to preserve the correct signed-support, we analyze the recursive IHT update step, and decompose the noise term into a component that depends on the error in the dictionary, and the other that depends on the initial coefficient estimate.",
        "NOODL alternates between: (a) an iterative hard thresholding (IHT)-based step for coefficient recovery, and (b) a gradient descent-based update for the dictionary, resulting in a simple and scalable algorithm, suitable for large-scale distributed implementations.",
        "To get a handle on the coefficients, in Step II.A, we derive an upper-bound on the error incurred by each non-zero element of the estimated coefficient vector, i.e., |xi \u2212 xi\u2217| for i \u2208 S for a general coefficient vector x\u2217, and show that this error only depends on t given enough IHT iterations R as per the chosen decay parameter \u03b4R.",
        "Given R = log(n), with probability at least (1 \u2212 \u03b4a) for some small constant \u03b4a, the coefficient estimate xi(t) at t-th iteration has the correct signed-support and satisfies (x \u2212 xi\u2217)2 = O(k(1 \u2212 \u03c9)t/2 Ai(0) \u2212 A\u2217i ), for all i \u2208 supp(x\u2217).",
        "Suppose A(t) log(m)) is twith probability at least (1 \u2212 \u03b4\u03b2(t) \u2212 \u03b4T(t)), each iterate of the IHT-based coefficient update step shown in (4) has the correct signed-support, if for a constant c1(r)( t, \u03bc, k, n) = \u03a9(k2/n), the step size is chosen as \u03b7x(r) \u2264 c1(r) , and the threshold \u03c4 (r) is chosen as \u03c4 (r)",
        "1. This result allows us to show that if the column-wise error in the dictionary decreases at each iteration t, the corresponding estimates of the coefficients improve.",
        "Step IV.B: The \u201ccloseness\u201d property is maintained after the updates made using the empirical gradient estimate: the following lemma shows that the updated dictionary A(t+1)",
        "Expression for the coefficient estimate at the end of R-th IHT iteration Expression for the expected gradient vector Concentration of the empirical gradient vector Empirical gradient vector is correlated with the descent direction Concentration of the empirical gradient matrix A(t+1) maintains closeness",
        "The analysis of this inherently non-convex problem impacts other matrix and tensor factorization tasks arising in signal processing, collaborative filtering, and machine learning"
    ],
    "headline": "We develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately",
    "reference_links": [
        {
            "id": "Agarwal_et+al_2014_a",
            "entry": "AGARWAL, A., ANANDKUMAR, A., JAIN, P., NETRAPALLI, P. and TANDON, R. (2014). Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory (COLT).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AGARWAL%2C%20A.%20ANANDKUMAR%2C%20A.%20JAIN%2C%20P.%20NETRAPALLI%2C%20P.%20Learning%20sparsely%20used%20overcomplete%20dictionaries%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AGARWAL%2C%20A.%20ANANDKUMAR%2C%20A.%20JAIN%2C%20P.%20NETRAPALLI%2C%20P.%20Learning%20sparsely%20used%20overcomplete%20dictionaries%202014"
        },
        {
            "id": "Aharon_et+al_2006_a",
            "entry": "AHARON, M., ELAD, M. and BRUCKSTEIN, A. (2006). k-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54 4311\u20134322.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AHARON%2C%20M.%20ELAD%2C%20M.%20BRUCKSTEIN%2C%20A.%20k-svd%3A%20An%20algorithm%20for%20designing%20overcomplete%20dictionaries%20for%20sparse%20representation%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AHARON%2C%20M.%20ELAD%2C%20M.%20BRUCKSTEIN%2C%20A.%20k-svd%3A%20An%20algorithm%20for%20designing%20overcomplete%20dictionaries%20for%20sparse%20representation%202006"
        },
        {
            "id": "Arora_et+al_2015_a",
            "entry": "ARORA, S., GE, R., MA, T. and MOITRA, A. (2015). Simple, efficient, and neural algorithms for sparse coding. In Conference on Learning Theory (COLT).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ARORA%2C%20S.%20GE%2C%20R.%20MA%2C%20T.%20MOITRA%2C%20A.%20Simple%2C%20efficient%2C%20and%20neural%20algorithms%20for%20sparse%20coding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ARORA%2C%20S.%20GE%2C%20R.%20MA%2C%20T.%20MOITRA%2C%20A.%20Simple%2C%20efficient%2C%20and%20neural%20algorithms%20for%20sparse%20coding%202015"
        },
        {
            "id": "Arora_et+al_2014_a",
            "entry": "ARORA, S., GE, R. and MOITRA, A. (2014). New algorithms for learning incoherent and overcomplete dictionaries. In Conference on Learning Theory (COLT).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ARORA%2C%20S.%20GE%2C%20R.%20MOITRA%2C%20A.%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ARORA%2C%20S.%20GE%2C%20R.%20MOITRA%2C%20A.%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014"
        },
        {
            "id": "Barak_et+al_2015_a",
            "entry": "BARAK, B., KELNER, J. A. and STEURER, D. (2015). Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the 47th annual ACM symposium on Theory of Computing. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BARAK%2C%20B.%20KELNER%2C%20J.A.%20STEURER%2C%20D.%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BARAK%2C%20B.%20KELNER%2C%20J.A.%20STEURER%2C%20D.%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015"
        },
        {
            "id": "Beck_2009_a",
            "entry": "BECK, A. and TEBOULLE, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2 183\u2013202.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BECK%2C%20A.%20TEBOULLE%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BECK%2C%20A.%20TEBOULLE%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "Blumensath_2009_a",
            "entry": "BLUMENSATH, T. and DAVIES, M. E. (2009). Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27 265\u2013274.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BLUMENSATH%2C%20T.%20DAVIES%2C%20M.E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BLUMENSATH%2C%20T.%20DAVIES%2C%20M.E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009"
        },
        {
            "id": "Cande_et+al_2007_a",
            "entry": "CANDE S, E. and ROMBERG, J. (2007). Sparsity and incoherence in compressive sampling. Inverse Problems, 23 969.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CANDE%2C%20S.%20E.%20ROMBERG%2C%20J.%20Sparsity%20and%20incoherence%20in%20compressive%20sampling%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CANDE%2C%20S.%20E.%20ROMBERG%2C%20J.%20Sparsity%20and%20incoherence%20in%20compressive%20sampling%202007"
        },
        {
            "id": "Cande_et+al_2015_a",
            "entry": "CANDE S, E. J., LI, X. and SOLTANOLKOTABI, M. (2015). Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61 1985\u20132007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CANDE%2C%20S.%20J.%2C%20E.%20LI%2C%20X.%20SOLTANOLKOTABI%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CANDE%2C%20S.%20J.%2C%20E.%20LI%2C%20X.%20SOLTANOLKOTABI%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015"
        },
        {
            "id": "Chen_et+al_1998_a",
            "entry": "CHEN, S. S., DONOHO, D. L. and SAUNDERS, M. A. (1998). Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20 33\u201361.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CHEN%2C%20S.S.%20DONOHO%2C%20D.L.%20SAUNDERS%2C%20M.A.%20Atomic%20decomposition%20by%20basis%20pursuit%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CHEN%2C%20S.S.%20DONOHO%2C%20D.L.%20SAUNDERS%2C%20M.A.%20Atomic%20decomposition%20by%20basis%20pursuit%201998"
        },
        {
            "id": "Chen_2015_a",
            "entry": "CHEN, Y. and WAINWRIGHT, M. J. (2015). Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. CoRR, abs/1509.03025.",
            "arxiv_url": "https://arxiv.org/pdf/1509.03025"
        },
        {
            "id": "Donoho_et+al_2006_a",
            "entry": "DONOHO, D., ELAD, M. and TEMLYAKOV, V. N. (2006). Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52 6\u201318.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DONOHO%2C%20D.%20ELAD%2C%20M.%20TEMLYAKOV%2C%20V.N.%20Stable%20recovery%20of%20sparse%20overcomplete%20representations%20in%20the%20presence%20of%20noise%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DONOHO%2C%20D.%20ELAD%2C%20M.%20TEMLYAKOV%2C%20V.N.%20Stable%20recovery%20of%20sparse%20overcomplete%20representations%20in%20the%20presence%20of%20noise%202006"
        },
        {
            "id": "Donoho_2001_a",
            "entry": "DONOHO, D. L. and HUO, X. (2001). Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47 2845\u20132862.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DONOHO%2C%20D.L.%20HUO%2C%20X.%20Uncertainty%20principles%20and%20ideal%20atomic%20decomposition%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DONOHO%2C%20D.L.%20HUO%2C%20X.%20Uncertainty%20principles%20and%20ideal%20atomic%20decomposition%202001"
        },
        {
            "id": "Elad_2010_a",
            "entry": "ELAD, M. (2010). Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. 1st ed. Springer Publishing Company, Incorporated.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ELAD%2C%20M.%20Sparse%20and%20Redundant%20Representations%3A%20From%20Theory%20to%20Applications%20in%20Signal%20and%20Image%20Processing%202010"
        },
        {
            "id": "Elad_2006_a",
            "entry": "ELAD, M. and AHARON, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image Processing, 15 3736\u20133745.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ELAD%2C%20M.%20AHARON%2C%20M.%20Image%20denoising%20via%20sparse%20and%20redundant%20representations%20over%20learned%20dictionaries%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ELAD%2C%20M.%20AHARON%2C%20M.%20Image%20denoising%20via%20sparse%20and%20redundant%20representations%20over%20learned%20dictionaries%202006"
        },
        {
            "id": "Engan_et+al_1999_a",
            "entry": "ENGAN, K., AASE, S. O. and HUSOY, J. H. (1999). Method of optimal directions for frame design. In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ENGAN%2C%20K.%20AASE%2C%20S.O.%20HUSOY%2C%20J.H.%20Method%20of%20optimal%20directions%20for%20frame%20design%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ENGAN%2C%20K.%20AASE%2C%20S.O.%20HUSOY%2C%20J.H.%20Method%20of%20optimal%20directions%20for%20frame%20design%201999"
        },
        {
            "id": "Fuller_2009_a",
            "entry": "FULLER, W. A. (2009). Measurement error models, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=FULLER%2C%20W.A.%20Measurement%20error%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=FULLER%2C%20W.A.%20Measurement%20error%202009"
        },
        {
            "id": "305",
            "entry": "305. John Wiley & Sons. GENG, Q. and WRIGHT, J. (2014). On the local correctness of 1-minimization for dictionary learning. In 2014 IEEE International Symposium on Information Theory (ISIT). IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiley%2C%20John%20GENG%2C%20Sons%20Q.%20WRIGHT%2C%20J.%20On%20the%20local%20correctness%20of%201-minimization%20for%20dictionary%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiley%2C%20John%20GENG%2C%20Sons%20Q.%20WRIGHT%2C%20J.%20On%20the%20local%20correctness%20of%201-minimization%20for%20dictionary%20learning%202014"
        },
        {
            "id": "Gershgorin_1931_a",
            "entry": "GERSHGORIN, S. A. (1931). Uber die abgrenzung der eigenwerte einer matrix 749\u2013754.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GERSHGORIN%2C%20S.A.%20Uber%20die%20abgrenzung%20der%20eigenwerte%20einer%20matrix%20749%E2%80%93754%201931"
        },
        {
            "id": "Gregor_2010_a",
            "entry": "GREGOR, K. and LECUN, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML). Omnipress.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GREGOR%2C%20K.%20LECUN%2C%20Y.%20Learning%20fast%20approximations%20of%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GREGOR%2C%20K.%20LECUN%2C%20Y.%20Learning%20fast%20approximations%20of%20sparse%20coding%202010"
        },
        {
            "id": "Gribonval_2010_a",
            "entry": "GRIBONVAL, R. and SCHNASS, K. (2010). Dictionary identification and sparse matrix-factorization via 1 -minimization. IEEE Transactions on Information Theory, 56 3523\u20133539.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GRIBONVAL%2C%20R.%20SCHNASS%2C%20K.%20Dictionary%20identification%20and%20sparse%20matrix-factorization%20via%201%20-minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GRIBONVAL%2C%20R.%20SCHNASS%2C%20K.%20Dictionary%20identification%20and%20sparse%20matrix-factorization%20via%201%20-minimization%202010"
        },
        {
            "id": "Hanson_1971_a",
            "entry": "HANSON, D. and WRIGHT, F. T. (1971). A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42 1079\u20131083.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HANSON%2C%20D.%20WRIGHT%2C%20F.T.%20A%20bound%20on%20tail%20probabilities%20for%20quadratic%20forms%20in%20independent%20random%20variables%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HANSON%2C%20D.%20WRIGHT%2C%20F.T.%20A%20bound%20on%20tail%20probabilities%20for%20quadratic%20forms%20in%20independent%20random%20variables%201971"
        },
        {
            "id": "Haupt_2006_a",
            "entry": "HAUPT, J. and NOWAK, R. (2006). Signal reconstruction from noisy random projections. IEEE Transactions on Information Theory, 52 4036\u20134048.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HAUPT%2C%20J.%20NOWAK%2C%20R.%20Signal%20reconstruction%20from%20noisy%20random%20projections%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HAUPT%2C%20J.%20NOWAK%2C%20R.%20Signal%20reconstruction%20from%20noisy%20random%20projections%202006"
        },
        {
            "id": "Jenatton_et+al_2012_a",
            "entry": "JENATTON, R., GRIBONVAL, R. and BACH, F. (2012). Local stability and robustness of sparse dictionary learning in the presence of noise. Research report. https://hal.inria.fr/hal-00737152",
            "url": "https://hal.inria.fr/hal-00737152"
        },
        {
            "id": "Jung_et+al_2014_a",
            "entry": "JUNG, A., ELDAR, Y. and GO RTZ, N. (2014). Performance limits of dictionary learning for sparse coding. In Proceedings of the European Signal Processing Conference (EUSIPCO),. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JUNG%2C%20A.%20ELDAR%2C%20Y.%20RTZ%2C%20G.O.%20N.%20Performance%20limits%20of%20dictionary%20learning%20for%20sparse%20coding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=JUNG%2C%20A.%20ELDAR%2C%20Y.%20RTZ%2C%20G.O.%20N.%20Performance%20limits%20of%20dictionary%20learning%20for%20sparse%20coding%202014"
        },
        {
            "id": "Jung_et+al_2016_a",
            "entry": "JUNG, A., ELDAR, Y. C. and GRTZ, N. (2016). On the minimax risk of dictionary learning. IEEE Transactions on Information Theory, 62 1501\u20131515.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JUNG%2C%20A.%20ELDAR%2C%20Y.C.%20GRTZ%2C%20N.%20On%20the%20minimax%20risk%20of%20dictionary%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=JUNG%2C%20A.%20ELDAR%2C%20Y.C.%20GRTZ%2C%20N.%20On%20the%20minimax%20risk%20of%20dictionary%20learning%202016"
        },
        {
            "id": "Kreutz-Delgado_et+al_2003_a",
            "entry": "KREUTZ-DELGADO, K., MURRAY, J. F., RAO, B. D., ENGAN, K., LEE, T. and SEJNOWSKI, T. J. (2003). Dictionary learning algorithms for sparse representation. Neural Computation, 15 349\u2013396.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KREUTZ-DELGADO%2C%20K.%20MURRAY%2C%20J.F.%20RAO%2C%20B.D.%20ENGAN%2C%20K.%20Dictionary%20learning%20algorithms%20for%20sparse%20representation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KREUTZ-DELGADO%2C%20K.%20MURRAY%2C%20J.F.%20RAO%2C%20B.D.%20ENGAN%2C%20K.%20Dictionary%20learning%20algorithms%20for%20sparse%20representation%202003"
        },
        {
            "id": "Lee_et+al_2007_a",
            "entry": "LEE, H., BATTLE, A., RAINA, R. and NG, A. Y. (2007). Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems (NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LEE%2C%20H.%20BATTLE%2C%20A.%20RAINA%2C%20R.%20NG%2C%20A.Y.%20Efficient%20sparse%20coding%20algorithms%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LEE%2C%20H.%20BATTLE%2C%20A.%20RAINA%2C%20R.%20NG%2C%20A.Y.%20Efficient%20sparse%20coding%20algorithms%202007"
        },
        {
            "id": "Lewicki_2000_a",
            "entry": "LEWICKI, M. S. and SEJNOWSKI, T. J. (2000). Learning overcomplete representations. Neural Computation, 12 337\u2013365.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LEWICKI%2C%20M.S.%20SEJNOWSKI%2C%20T.J.%20Learning%20overcomplete%20representations%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LEWICKI%2C%20M.S.%20SEJNOWSKI%2C%20T.J.%20Learning%20overcomplete%20representations%202000"
        },
        {
            "id": "X_et+al_2016_a",
            "entry": "LI, X., WANG, Z., LU, J., ARORA, R., HAUPT, J., LIU, H. and ZHAO, T. (2016a). Symmetry, saddle points, and global geometry of nonconvex matrix factorization. arXiv preprint arXiv:1612.09296.",
            "arxiv_url": "https://arxiv.org/pdf/1612.09296"
        },
        {
            "id": "X_et+al_2016_a",
            "entry": "LI, X., ZHAO, T., ARORA, R., LIU, H. and HAUPT, J. (2016b). Stochastic variance reduced optimization for nonconvex sparse learning. In International Conference on Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LI%2C%20X.%20ZHAO%2C%20T.%20ARORA%2C%20R.%20LIU%2C%20H.%20Stochastic%20variance%20reduced%20optimization%20for%20nonconvex%20sparse%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LI%2C%20X.%20ZHAO%2C%20T.%20ARORA%2C%20R.%20LIU%2C%20H.%20Stochastic%20variance%20reduced%20optimization%20for%20nonconvex%20sparse%20learning%202016"
        },
        {
            "id": "Mairal_et+al_2009_a",
            "entry": "MAIRAL, J., BACH, F., PONCE, J. and SAPIRO, G. (2009). Online dictionary learning for sparse coding. In Proceedings of the International Conference on Machine Learning (ICML). ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MAIRAL%2C%20J.%20BACH%2C%20F.%20PONCE%2C%20J.%20SAPIRO%2C%20G.%20Online%20dictionary%20learning%20for%20sparse%20coding%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MAIRAL%2C%20J.%20BACH%2C%20F.%20PONCE%2C%20J.%20SAPIRO%2C%20G.%20Online%20dictionary%20learning%20for%20sparse%20coding%202009"
        },
        {
            "id": "Mallat_1993_a",
            "entry": "MALLAT, S. G. and ZHANG, Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41 3397\u20133415.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MALLAT%2C%20S.G.%20ZHANG%2C%20Z.%20Matching%20pursuits%20with%20time-frequency%20dictionaries%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MALLAT%2C%20S.G.%20ZHANG%2C%20Z.%20Matching%20pursuits%20with%20time-frequency%20dictionaries%201993"
        },
        {
            "id": "Olshausen_1997_a",
            "entry": "OLSHAUSEN, B. A. and FIELD, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research, 37 3311\u20133325.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=OLSHAUSEN%2C%20B.A.%20FIELD%2C%20D.J.%20Sparse%20coding%20with%20an%20overcomplete%20basis%20set%3A%20A%20strategy%20employed%20by%20v1%3F%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=OLSHAUSEN%2C%20B.A.%20FIELD%2C%20D.J.%20Sparse%20coding%20with%20an%20overcomplete%20basis%20set%3A%20A%20strategy%20employed%20by%20v1%3F%201997"
        },
        {
            "id": "Pearson_1901_a",
            "entry": "PEARSON, K. (1901). On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2 559\u2013572. https://doi.org/10.1080/14786440109462720",
            "crossref": "https://dx.doi.org/10.1080/14786440109462720",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/14786440109462720"
        },
        {
            "id": "Rambhatla_2013_a",
            "entry": "RAMBHATLA, S. and HAUPT, J. (2013). Semi-blind source separation via sparse representations and online dictionary learning. In 2013 Asilomar Conference on Signals, Systems and Computers,. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RAMBHATLA%2C%20S.%20HAUPT%2C%20J.%20Semi-blind%20source%20separation%20via%20sparse%20representations%20and%20online%20dictionary%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RAMBHATLA%2C%20S.%20HAUPT%2C%20J.%20Semi-blind%20source%20separation%20via%20sparse%20representations%20and%20online%20dictionary%20learning%202013"
        },
        {
            "id": "Rambhatla_et+al_2016_a",
            "entry": "RAMBHATLA, S., LI, X. and HAUPT, J. (2016). A dictionary based generalization of robust PCA. In IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RAMBHATLA%2C%20S.%20LI%2C%20X.%20HAUPT%2C%20J.%20A%20dictionary%20based%20generalization%20of%20robust%20PCA%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RAMBHATLA%2C%20S.%20LI%2C%20X.%20HAUPT%2C%20J.%20A%20dictionary%20based%20generalization%20of%20robust%20PCA%202016"
        },
        {
            "id": "Rambhatla_et+al_2017_a",
            "entry": "RAMBHATLA, S., LI, X. and HAUPT, J. (2017). Target-based hyperspectral demixing via generalized robust PCA. In 51st Asilomar Conference on Signals, Systems, and Computers. IEEE. https://doi.org/10.1109/ACSSC.2017.8335372",
            "crossref": "https://dx.doi.org/10.1109/ACSSC.2017.8335372",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ACSSC.2017.8335372"
        },
        {
            "id": "Rambhatla_et+al_2019_a",
            "entry": "RAMBHATLA, S., LI, X., REN, J. and HAUPT, J. (2019a). A dictionary-based generalization of robust PCA part I: Study of theoretical properties. abs/1902.08304. https://arxiv.org/abs/1902.08304",
            "url": "https://arxiv.org/abs/1902.08304",
            "arxiv_url": "https://arxiv.org/pdf/1902.08304"
        },
        {
            "id": "Rambhatla_et+al_2019_b",
            "entry": "RAMBHATLA, S., LI, X., REN, J. and HAUPT, J. (2019b). A dictionary-based generalization of robust PCA part II: Applications to hyperspectral demixing. abs/1902.10238. https://arxiv.org/abs/1902.10238",
            "url": "https://arxiv.org/abs/1902.10238",
            "arxiv_url": "https://arxiv.org/pdf/1902.10238"
        },
        {
            "id": "Ramirez_et+al_2010_a",
            "entry": "RAMIREZ, I., SPRECHMANN, P. and SAPIRO, G. (2010). Classification and clustering via dictionary learning with structured incoherence and shared features. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RAMIREZ%2C%20I.%20SPRECHMANN%2C%20P.%20SAPIRO%2C%20G.%20Classification%20and%20clustering%20via%20dictionary%20learning%20with%20structured%20incoherence%20and%20shared%20features%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RAMIREZ%2C%20I.%20SPRECHMANN%2C%20P.%20SAPIRO%2C%20G.%20Classification%20and%20clustering%20via%20dictionary%20learning%20with%20structured%20incoherence%20and%20shared%20features%202010"
        },
        {
            "id": "Ranzato_et+al_2008_a",
            "entry": "RANZATO, M., BOUREAU, Y. and LECUN, Y. (2008). Sparse feature learning for deep belief networks. In Advances in Neural Information Processing Systems (NIPS). 1185\u20131192.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RANZATO%2C%20M.%20BOUREAU%2C%20Y.%20LECUN%2C%20Y.%20Sparse%20feature%20learning%20for%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RANZATO%2C%20M.%20BOUREAU%2C%20Y.%20LECUN%2C%20Y.%20Sparse%20feature%20learning%20for%20deep%20belief%20networks%202008"
        },
        {
            "id": "Rudelson_2013_a",
            "entry": "RUDELSON, M. and VERSHYNIN, R. (2013). Hanson-wright inequality and sub-gaussian concentration. Electronic Communications in Probability, 18.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RUDELSON%2C%20M.%20VERSHYNIN%2C%20R.%20Hanson-wright%20inequality%20and%20sub-gaussian%20concentration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RUDELSON%2C%20M.%20VERSHYNIN%2C%20R.%20Hanson-wright%20inequality%20and%20sub-gaussian%20concentration%202013"
        },
        {
            "id": "Spielman_et+al_2012_a",
            "entry": "SPIELMAN, D. A., WANG, H. and WRIGHT, J. (2012). Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory (COLT).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SPIELMAN%2C%20D.A.%20WANG%2C%20H.%20WRIGHT%2C%20J.%20Exact%20recovery%20of%20sparsely-used%20dictionaries%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SPIELMAN%2C%20D.A.%20WANG%2C%20H.%20WRIGHT%2C%20J.%20Exact%20recovery%20of%20sparsely-used%20dictionaries%202012"
        },
        {
            "id": "Tibshirani_1996_a",
            "entry": "TIBSHIRANI, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) 267\u2013288.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TIBSHIRANI%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TIBSHIRANI%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "Tropp_2015_a",
            "entry": "TROPP, J. (2015). An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8 1\u2013230.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TROPP%2C%20J.%20An%20introduction%20to%20matrix%20concentration%20inequalities%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TROPP%2C%20J.%20An%20introduction%20to%20matrix%20concentration%20inequalities%202015"
        },
        {
            "id": "Wainwright_2009_a",
            "entry": "WAINWRIGHT, M. J. (2009). Sharp thresholds for high-dimensional and noisy sparsity recovery using 1-constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55 2183\u20132202.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=WAINWRIGHT%2C%20M.J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using%201-constrained%20quadratic%20programming%20%28lasso%29%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=WAINWRIGHT%2C%20M.J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using%201-constrained%20quadratic%20programming%20%28lasso%29%202009"
        },
        {
            "id": "Yuan_et+al_2016_a",
            "entry": "YUAN, X., LI, P. and ZHANG, T. (2016). Exact recovery of hard thresholding pursuit. In Advances in Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=YUAN%2C%20X.%20LI%2C%20P.%20ZHANG%2C%20T.%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=YUAN%2C%20X.%20LI%2C%20P.%20ZHANG%2C%20T.%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016"
        },
        {
            "id": "1",
            "entry": "1. This result allows us to show that if the column-wise error in the dictionary decreases at each iteration t, then the corresponding estimates of the coefficients also improve.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20result%20allows%20us%20to%20show%20that%20if%20the%20columnwise%20error%20in%20the%20dictionary%20decreases%20at%20each%20iteration%20t%20then%20the%20corresponding%20estimates%20of%20the%20coefficients%20also%20improve"
        },
        {
            "id": "2",
            "entry": "2. Similarly, expanding E[Wj Wj], and using the fact that E[(y \u2212 A(t)x)sign(x) ] E[(y \u2212 A(t)x)sign(x) ] is positive semi-definite. Now, using Claim 8 and the fact that entries of E[(sign(x(j))sign(x(j)) ] are qi on the diagonal and zero elsewhere, where qi = O(k/m), E[Wj Wj]",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Similarly%20expanding%20E%5BWj%20Wj%5D%2C%20and%20using%20the%20fact%20that%20E%5B%28y%20%E2%88%92%20A%28t%29x%29sign%28x%29%20%22%5D%22%20%23%20%22%5D%22%20%23%20%22%5D%22",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Similarly%20expanding%20E%5BWj%20Wj%5D%2C%20and%20using%20the%20fact%20that%20E%5B%28y%20%E2%88%92%20A%28t%29x%29sign%28x%29%20%22%5D%22%20%23%20%22%5D%22%20%23%20%22%5D%22"
        },
        {
            "id": "2",
            "entry": "2. Proof of Claim 10. Let Fx\u2217 be the event that sign(x\u2217) = sign(x), and let 1Fx\u2217 denote the indicator function corresponding to this event. As we show in Lemma 2, this event occurs with probability at least (1 \u2212 \u03b4\u03b2(t) \u2212 \u03b4T(t)), therefore, E[(y \u2212 A(t)x)(y \u2212 A(t)x) ]",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20of%20Claim%2010%20Let%20Fx%20be%20the%20event%20that%20signx%20%20signx%20and%20let%201Fx%20denote%20the%20indicator%20function%20corresponding%20to%20this%20event%20As%20we%20show%20in%20Lemma%202%20this%20event%20occurs%20with%20probability%20at%20least%201%20%20%CE%B4%CE%B2t%20%20%CE%B4Tt%20therefore%20Ey%20%20Atxy%20%20Atx",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20of%20Claim%2010%20Let%20Fx%20be%20the%20event%20that%20signx%20%20signx%20and%20let%201Fx%20denote%20the%20indicator%20function%20corresponding%20to%20this%20event%20As%20we%20show%20in%20Lemma%202%20this%20event%20occurs%20with%20probability%20at%20least%201%20%20%CE%B4%CE%B2t%20%20%CE%B4Tt%20therefore%20Ey%20%20Atxy%20%20Atx"
        },
        {
            "id": "2",
            "entry": "2. Next, we consider \u26634:=",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Next%20we%20consider%20%E2%99%A3%204%3A%20%3D"
        },
        {
            "id": "2",
            "entry": "2. Therefore, we have the following for \u2663 in (37)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Therefore%20we%20have%20the%20following%20for%20%20in%2037"
        },
        {
            "id": "2",
            "entry": "2. Similarly, \u2665 in (37) is also bounded as \u2660. Next, we consider \u2666 in (37). In this case, letting",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Similarly%20%E2%99%A5%20in%20%2837%29%20is%20also%20bounded%20as%20%E2%99%A0.%20Next%2C%20we%20consider%20%E2%99%A6",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Similarly%20%E2%99%A5%20in%20%2837%29%20is%20also%20bounded%20as%20%E2%99%A0.%20Next%2C%20we%20consider%20%E2%99%A6"
        },
        {
            "id": "2",
            "entry": "2. Combining all the results for \u2663, \u2660, \u2665 and \u2666, we have,",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combining%20all%20the%20results%20for%20%20%20%20and%20%20we%20have"
        },
        {
            "id": "1",
            "entry": "1. If p = nO(1) then Z(j) \u2264 O(L) holds for each j with probability at least(1 \u2212 \u03c1) and, 2. E[Z1 Z \u2265\u03a9(L)] = n\u2212\u03c9(1). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z.%20If%20p%20%3D%20nO%281%29%20then%20Z%28j%29%20%E2%89%A4%20O%28L%29%20holds%20for%20each%20j%20with%20probability%20at%20least%281%20%E2%88%92%20%CF%81%29",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z.%20If%20p%20%3D%20nO%281%29%20then%20Z%28j%29%20%E2%89%A4%20O%28L%29%20holds%20for%20each%20j%20with%20probability%20at%20least%281%20%E2%88%92%20%CF%81%29"
        }
    ]
}
