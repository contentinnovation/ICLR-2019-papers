{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AN ANALYTIC THEORY OF GENERALIZATION DYNAMICS AND TRANSFER LEARNING IN DEEP LINEAR NET-",
        "author": "Andrew K. Lampinen Department of Psychology Stanford University lampinen@stanford.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryfMLoCqtQ"
        },
        "abstract": "Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "knowledge transfer",
            "url": "https://en.wikipedia.org/wiki/knowledge_transfer"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "abbreviations": {
        "SVD": "singular value decomposition",
        "SNRs": "signal to noise ratios",
        "TA": "training aligned",
        "A+s2": "A(1+s2 ) s2"
    },
    "highlights": [
        "Many deep learning practitioners closely monitor both training and test errors, hoping to achieve both a small training error and a small generalization error, or gap between testing and training errors",
        "Our analytic theory of generalization dynamics in deep linear networks reveals that many puzzling aspects of generalization in deep learning already arise in the simple linear setting, where the puzzles can be understood analytically",
        "Deep linear networks learn more important structure in data first, leading to generalization errors that depend on task structure much more than network size",
        "Our theory explains why deep linear networks learn randomized data more slowly than structured data, and provides a non-gradient based learning method that out-performs gradient descent learning in the linear case",
        "We provide an analytic theory of how knowledge is transferred from one task to another, demonstrating that the degree of alignment of input features important for each task, but not how one must respond to these features, is critical for facilitating knowledge transfer",
        "We hope our work will motivate and enable: (1) the search for tighter upper bounds on generalization error that take into account task structure; (2) the design of non gradient based training algorithms that outperform gradient-based learning; and (3) the theory-driven selection of auxiliary tasks that maximize knowledge transfer"
    ],
    "key_statements": [
        "Many deep learning practitioners closely monitor both training and test errors, hoping to achieve both a small training error and a small generalization error, or gap between testing and training errors",
        "Training is usually stopped early, before overfitting sets in and increases the test error. This procedure often results in large networks that generalize well on structured tasks, raising an important generalization puzzle (<a class=\"ref-link\" id=\"cZhang_et+al_2016_a\" href=\"#rZhang_et+al_2016_a\">Zhang et al, 2016</a>): many existing theories that upper bound generalization error (<a class=\"ref-link\" id=\"cBartlett_2002_a\" href=\"#rBartlett_2002_a\">Bartlett & Mendelson, 2002</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_a\" href=\"#rNeyshabur_et+al_2015_a\">Neyshabur et al, 2015</a>; <a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite & Roy, 2017</a>; <a class=\"ref-link\" id=\"cGolowich_et+al_2017_a\" href=\"#rGolowich_et+al_2017_a\">Golowich et al, 2017</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a>; <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al, 2017</a>; <a class=\"ref-link\" id=\"cArora_et+al_2018_a\" href=\"#rArora_et+al_2018_a\">Arora et al, 2018</a>, e.g) in terms of various measures of network complexity yield very loose bounds",
        "In the absence of any such tight and computable theory of deep network generalization error, we develop an analytic theory of generalization error for deep linear networks",
        "In particular we develop an analytic theory for both the training and test error of a deep linear network as a function of training time, number of training examples, network architecture, initialization, and task structure and signal to noise ratios",
        "We provide an analytic theory for how much knowledge is transferred between pairs of tasks, and we find that it displays a sensitive but computable dependence on the relationship between pairs of tasks, in particular, their signal to noise ratios and feature space alignments",
        "As the student learns, its training and test error dynamics depends on the alignment of the time-evolving student singular modes {s\u03b1, u\u03b1, v\u03b1} with the fixed training data {s\u03b1, u\u03b1, v\u03b1} and teacher {s\u03b1, u\u03b1, v\u03b1} singular modes respectively.\n3 SINGLE TASK GENERALIZATION DYNAMICS: THEORY AND EXPERIMENT",
        "In Appendix F we demonstrate these phenomena are qualitatively recapitulated in nonlinear networks, which suggests that our theory may give insight into how to choose auxiliary tasks",
        "Our analytic theory of generalization dynamics in deep linear networks reveals that many puzzling aspects of generalization in deep learning already arise in the simple linear setting, where the puzzles can be understood analytically",
        "Deep linear networks learn more important structure in data first, leading to generalization errors that depend on task structure much more than network size",
        "Our theory explains why deep linear networks learn randomized data more slowly than structured data, and provides a non-gradient based learning method that out-performs gradient descent learning in the linear case",
        "We provide an analytic theory of how knowledge is transferred from one task to another, demonstrating that the degree of alignment of input features important for each task, but not how one must respond to these features, is critical for facilitating knowledge transfer",
        "We hope our work will motivate and enable: (1) the search for tighter upper bounds on generalization error that take into account task structure; (2) the design of non gradient based training algorithms that outperform gradient-based learning; and (3) the theory-driven selection of auxiliary tasks that maximize knowledge transfer"
    ],
    "summary": [
        "Many deep learning practitioners closely monitor both training and test errors, hoping to achieve both a small training error and a small generalization error, or gap between testing and training errors.",
        "As the student learns, its training and test error dynamics depends on the alignment of the time-evolving student singular modes {s\u03b1, u\u03b1, v\u03b1} with the fixed training data {s\u03b1, u\u03b1, v\u03b1} and teacher {s\u03b1, u\u03b1, v\u03b1} singular modes respectively.",
        "We derive and numerically test analytic formulas for both the training and test errors of a student network as it learns from training data generated from a teacher network.",
        "First Rin contains those top N2 \u2212 N 2 training data singular values that do not correspond to the N 2 singular values of the teacher but will be learned by a rank N2 student.",
        "Together (14) and (15) constitute a complete theory of generalization dynamics in terms of the structure of the data distribution, the architectural complexity of the student, and the training time t.",
        "Strong singular values associated with large SNR teacher modes are learned and both \u03b5train and \u03b5test drop.",
        "As time progresses, the singular mode detection wave penetrates the MP sea, and the student picks up noise structure in the data, so \u03b5train drops but \u03b5test rises, indicating the onset of overfitting.",
        "For structured data generated by a low rank teacher with singular values s\u03b1, the diagonal output variance is given by \u03c3r2",
        "The randomized data will lead to slower initial training error drops relative to the structured data (Fig. 6B) since the singular mode detection wave encounters the first signal singular values in structured data earlier than it encounters the edge of the stretched MP sea in randomized data.",
        "The optimal generalization error with a rank 1 teacher is very related to the alignment of the top training data singular vectors with the teacher singular vectors, and it decreases as this alignment increases.",
        "Two student networks can learn from the two teacher networks separately, each achieving optimal early stopping test errors \u03b5oApt and \u03b5oBpt. Alternatively, one could construct a composite teacher that concatenates the hidden and output units, but shares the same input units (Fig. 7).",
        "Deep linear networks learn more important structure in data first, leading to generalization errors that depend on task structure much more than network size.",
        "We hope our work will motivate and enable: (1) the search for tighter upper bounds on generalization error that take into account task structure; (2) the design of non gradient based training algorithms that outperform gradient-based learning; and (3) the theory-driven selection of auxiliary tasks that maximize knowledge transfer."
    ],
    "headline": "We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks",
    "reference_links": [
        {
            "id": "Advani_2017_a",
            "entry": "Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv, pp. 1\u201332, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Advani%2C%20Madhu%20S.%20Saxe%2C%20Andrew%20M.%20High-dimensional%20dynamics%20of%20generalization%20error%20in%20neural%20networks.%20arXiv%202017"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint, pp. 1\u201339, 2018. URL http://arxiv.org/abs/1802.05296.",
            "url": "http://arxiv.org/abs/1802.05296",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Arpit_et+al_2017_a",
            "entry": "Devansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A Closer Look at Memorization in Deep Networks. arXiv preprint, 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1706.05394.",
            "url": "http://arxiv.org/abs/1706.05394",
            "arxiv_url": "https://arxiv.org/pdf/1706.05394"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint, pp. 1\u201324, 2017. URL http://arxiv.org/abs/1706.08498.",
            "url": "http://arxiv.org/abs/1706.08498",
            "arxiv_url": "https://arxiv.org/pdf/1706.08498"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. Journal of Machine Learning Research, 3:463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20Complexities%3A%20Risk%20Bounds%20and%20Structural%20Results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20Complexities%3A%20Risk%20Bounds%20and%20Structural%20Results%202002"
        },
        {
            "id": "Benaych-Georges_2012_a",
            "entry": "Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120\u2013135, 2012. ISSN 0047259X. doi: 10.1016/j.jmva.2012.04.019.",
            "crossref": "https://dx.doi.org/10.1016/j.jmva.2012.04.019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.jmva.2012.04.019"
        },
        {
            "id": "Dong_et+al_2015_a",
            "entry": "Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-Task Learning for Multiple Language Translation. Acl, pp. 1723\u20131732, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Daxiang%20Wu%2C%20Hua%20He%2C%20Wei%20Yu%2C%20Dianhai%20Multi-Task%20Learning%20for%20Multiple%20Language%20Translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Daxiang%20Wu%2C%20Hua%20He%2C%20Wei%20Yu%2C%20Dianhai%20Multi-Task%20Learning%20for%20Multiple%20Language%20Translation%202015"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. arXiv preprint, 2017. URL http://arxiv.org/abs/1703.11008.",
            "url": "http://arxiv.org/abs/1703.11008",
            "arxiv_url": "https://arxiv.org/pdf/1703.11008"
        },
        {
            "id": "Golowich_et+al_2017_a",
            "entry": "Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-Independent Sample Complexity of Neural Networks. arXiv preprint, (1):1\u201326, 2017. URL http://arxiv.org/abs/1712.06541.",
            "url": "http://arxiv.org/abs/1712.06541",
            "arxiv_url": "https://arxiv.org/pdf/1712.06541"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I.J. Goodfellow, J Pouget-Abadie, and Mehdi Mirza. Generative Adversarial Networks. arXiv preprint, pp. 1\u20139, 2014. ISSN 10495258. doi: 10.1001/jamainternmed.2016.8245.",
            "crossref": "https://dx.doi.org/10.1001/jamainternmed.2016.8245"
        },
        {
            "id": "Hansen_et+al_2017_a",
            "entry": "Steven S. Hansen, Andrew Lampinen, Gaurav Suri, and James L. McClelland. Building on prior knowledge without building it in. Behavioral and Brain Sciences, 40, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20Steven%20S.%20Lampinen%2C%20Andrew%20Suri%2C%20Gaurav%20McClelland%2C%20James%20L.%20Building%20on%20prior%20knowledge%20without%20building%20it%20in%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20Steven%20S.%20Lampinen%2C%20Andrew%20Suri%2C%20Gaurav%20McClelland%2C%20James%20L.%20Building%20on%20prior%20knowledge%20without%20building%20it%20in%202017"
        },
        {
            "id": "Lampinen_et+al_2017_a",
            "entry": "Andrew Lampinen, Shaw Hsu, and James L Mcclelland. Analogies Emerge from Learning Dyamics in Neural Networks. Proceedings of the 39th Annual Conference of the Cognitive Science Society, pp. 2512\u20132517, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampinen%2C%20Andrew%20Hsu%2C%20Shaw%20Mcclelland%2C%20James%20L.%20Analogies%20Emerge%20from%20Learning%20Dyamics%20in%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampinen%2C%20Andrew%20Hsu%2C%20Shaw%20Mcclelland%2C%20James%20L.%20Analogies%20Emerge%20from%20Learning%20Dyamics%20in%20Neural%20Networks%202017"
        },
        {
            "id": "Luong_et+al_2016_a",
            "entry": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. Iclr, pp. 1\u20139, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Minh-Thang%20Le%2C%20Quoc%20V.%20Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Multi-task%20Sequence%20to%20Sequence%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Minh-Thang%20Le%2C%20Quoc%20V.%20Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Multi-task%20Sequence%20to%20Sequence%20Learning%202016"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. Conference on Learning Theory (COLT), pp. 1376\u20131401, 2015. ISSN 15337928. URL http://arxiv.org/abs/1503.00036.",
            "url": "http://arxiv.org/abs/1503.00036",
            "arxiv_url": "https://arxiv.org/pdf/1503.00036"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. arXiv preprint, (2017):1\u20139, 2017. URL http://arxiv.org/abs/1707.09564.",
            "url": "http://arxiv.org/abs/1707.09564",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Pennington_et+al_2017_a",
            "entry": "Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in Neural Information Processing Systems 30, (Nips):1\u201311, 2017. URL http://arxiv.org/abs/1711.04735.",
            "url": "http://arxiv.org/abs/1711.04735",
            "arxiv_url": "https://arxiv.org/pdf/1711.04735"
        },
        {
            "id": "Pennington_et+al_2018_a",
            "entry": "Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Universality in Deep Networks. In AISTATS 2018, 2018. URL http://arxiv.org/abs/1802.09979.",
            "url": "http://arxiv.org/abs/1802.09979",
            "arxiv_url": "https://arxiv.org/pdf/1802.09979"
        },
        {
            "id": "Rusu_et+al_2015_a",
            "entry": "Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. arXiv, pp. 1\u201312, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Advances in Neural Information Processing Systems, pp. 1\u20139, 2013a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20M.%20McClelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20M.%20McClelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202013"
        },
        {
            "id": "Saxe_et+al_2013_b",
            "entry": "Andrew M Saxe, James L Mcclelland, and Surya Ganguli. Learning hierarchical category structure in deep neural networks. Proceedings of the 35th annual meeting of the Cognitive Science Society, pp. 1271\u20131276, 2013b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20M.%20Mcclelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Learning%20hierarchical%20category%20structure%20in%20deep%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20M.%20Mcclelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Learning%20hierarchical%20category%20structure%20in%20deep%20neural%20networks%202013"
        },
        {
            "id": "Schoenholz_et+al_2016_a",
            "entry": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. In International Conference on Learning Representations (ICLR),, number 2016, pp. 1\u201318, 2016. URL http://arxiv.org/abs/1611.01232.",
            "url": "http://arxiv.org/abs/1611.01232",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, and Koray Kavukcuoglu. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7585):484\u2013489, 2016. ISSN 0028-0836. doi: 10.1038/nature16961. URL http://dx.doi.org/10.1038/nature16961.",
            "crossref": "https://dx.doi.org/10.1038/nature16961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature16961"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint, 2016. ISSN 10414347. doi: 10.1109/TKDE.2015.2507132. URL http://arxiv.org/abs/1611.03530.",
            "crossref": "https://dx.doi.org/10.1109/TKDE.2015.2507132",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "In_2013_a",
            "entry": "In the main text, we described the dynamics of how a single-hidden-layer network converges toward the training data singular modes {s\u03b1, u\u03b1, v\u03b1}, which were originally derived in Saxe et al. (2013a).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20the%20main%20text%20we%20described%20the%20dynamics%20of%20how%20a%20singlehiddenlayer%20network%20converges%20toward%20the%20training%20data%20singular%20modes%20s%CE%B1%20u%CE%B1%20v%CE%B1%20which%20were%20originally%20derived%20in%20Saxe%20et%20al%202013a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20the%20main%20text%20we%20described%20the%20dynamics%20of%20how%20a%20singlehiddenlayer%20network%20converges%20toward%20the%20training%20data%20singular%20modes%20s%CE%B1%20u%CE%B1%20v%CE%B1%20which%20were%20originally%20derived%20in%20Saxe%20et%20al%202013a"
        }
    ]
}
