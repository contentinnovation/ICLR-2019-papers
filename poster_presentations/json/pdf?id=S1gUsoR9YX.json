{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MULTILINGUAL NEURAL MACHINE TRANSLATION WITH KNOWLEDGE DISTILLATION",
        "author": "Xu Tan, Yi Ren,\u2217 Di He, Tao Qin, Zhou Zhao, & Tie-Yan Liu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1gUsoR9YX"
        },
        "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models."
    },
    "keywords": [
        {
            "term": "language diversity",
            "url": "https://en.wikipedia.org/wiki/language_diversity"
        },
        {
            "term": "NAACL",
            "url": "https://en.wikipedia.org/wiki/NAACL"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "NMT": "Neural Machine Translation",
        "BPE": "Byte Pair Encoding"
    },
    "highlights": [
        "Neural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al, 2015; <a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\"><a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\">Luong et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\"><a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\"><a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\">Song et al, 2018</a></a>; <a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\"><a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\"><a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\">Guo et al, 2018</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\"><a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\">He et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\"><a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\">Gong et al, 2018</a></a>), including advanced model structures (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>) and human parity achievements (<a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\">Hassan et al, 2018</a></a>)",
        "\u2217Authors contribute to this work. 1https://www.ethnologue.com/browse of higher accuracy than the multilingual model in conventional model training, we propose to transfer the knowledge from individual models to the multilingual model with knowledge distillation, which has been studied for model compression and knowledge transfer and well matches our setting of multilingual translation",
        "It usually starts by training a big/deep teacher model, and train a small/shallow student model to mimic the behaviors of the teacher model, such as its hidden representation (Yim et al, 2017; <a class=\"ref-link\" id=\"cRomero_et+al_2014_a\" href=\"#rRomero_et+al_2014_a\">Romero et al, 2014</a>), its output probabilities (<a class=\"ref-link\" id=\"cHinton_et+al_2015_a\" href=\"#rHinton_et+al_2015_a\">Hinton et al, 2015</a>; <a class=\"ref-link\" id=\"cFreitag_et+al_2017_a\" href=\"#rFreitag_et+al_2017_a\">Freitag et al, 2017</a>) or directly training on the sentences generated by the teacher model in neural machine translation (<a class=\"ref-link\" id=\"cKim_2016_a\" href=\"#rKim_2016_a\">Kim & Rush, 2016a</a>)",
        "We propose a new method based on knowledge distillation for multilingual translation to eliminate the accuracy gap between the multilingual model and individual models",
        "It is observed that when there are dozens of language pairs, multilingual Neural Machine Translation usually achieves inferior accuracy compared with its counterpart which trains an individual model for each language pair",
        "We have proposed a distillation-based approach to boost the accuracy of multilingual Neural Machine Translation, which is usually of lower accuracy than the individual models in previous works"
    ],
    "key_statements": [
        "Neural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al, 2015; <a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\"><a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\">Luong et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\"><a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\"><a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\">Song et al, 2018</a></a>; <a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\"><a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\"><a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\">Guo et al, 2018</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\"><a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\">He et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\"><a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\">Gong et al, 2018</a></a>), including advanced model structures (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>) and human parity achievements (<a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\">Hassan et al, 2018</a></a>)",
        "When handling more language pairs, the translation accuracy of multilingual model is usually inferior to individual models, due to language diversity",
        "\u2217Authors contribute to this work. 1https://www.ethnologue.com/browse of higher accuracy than the multilingual model in conventional model training, we propose to transfer the knowledge from individual models to the multilingual model with knowledge distillation, which has been studied for model compression and knowledge transfer and well matches our setting of multilingual translation",
        "It usually starts by training a big/deep teacher model, and train a small/shallow student model to mimic the behaviors of the teacher model, such as its hidden representation (Yim et al, 2017; <a class=\"ref-link\" id=\"cRomero_et+al_2014_a\" href=\"#rRomero_et+al_2014_a\">Romero et al, 2014</a>), its output probabilities (<a class=\"ref-link\" id=\"cHinton_et+al_2015_a\" href=\"#rHinton_et+al_2015_a\">Hinton et al, 2015</a>; <a class=\"ref-link\" id=\"cFreitag_et+al_2017_a\" href=\"#rFreitag_et+al_2017_a\">Freitag et al, 2017</a>) or directly training on the sentences generated by the teacher model in neural machine translation (<a class=\"ref-link\" id=\"cKim_2016_a\" href=\"#rKim_2016_a\">Kim & Rush, 2016a</a>)",
        "We propose a new method based on knowledge distillation for multilingual translation to eliminate the accuracy gap between the multilingual model and individual models",
        "It is observed that when there are dozens of language pairs, multilingual Neural Machine Translation usually achieves inferior accuracy compared with its counterpart which trains an individual model for each language pair",
        "In this work we propose the multilingual distillation framework to boost the accuracy of multilingual Neural Machine Translation, so as to match or even surpass the accuracy of individual models.\n2.3",
        "When the accuracy of multilingual model surpasses the individual model for the accuracy threshold \u03c4 on a certain language pair, we remove the distillation loss and just train the model with original negative log-likelihood loss for this pair",
        "It can be seen that the multi-baseline model performs worse than the individual models on 5 out of 6 languages, while in contrast, our method performs better on all the 6 languages",
        "Our method improves the accuracy of some languages with more than 2 BLEU scores over individual models",
        "We have proposed a distillation-based approach to boost the accuracy of multilingual Neural Machine Translation, which is usually of lower accuracy than the individual models in previous works",
        "Experiments on three translation datasets with up to 44 languages demonstrate the multilingual model based on our proposed method can nearly match or even outperform the individual models, with just 1/N model parameters (N is up to 44 in our experiments)"
    ],
    "summary": [
        "Neural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al, 2015; <a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\"><a class=\"ref-link\" id=\"cLuong_et+al_2015_b\" href=\"#rLuong_et+al_2015_b\">Luong et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\"><a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\"><a class=\"ref-link\" id=\"cSong_et+al_2018_a\" href=\"#rSong_et+al_2018_a\">Song et al, 2018</a></a>; <a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\"><a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\"><a class=\"ref-link\" id=\"cGuo_et+al_2018_a\" href=\"#rGuo_et+al_2018_a\">Guo et al, 2018</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\"><a class=\"ref-link\" id=\"cHe_et+al_2018_a\" href=\"#rHe_et+al_2018_a\">He et al, 2018</a></a>; <a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\"><a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\">Gong et al, 2018</a></a>), including advanced model structures (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a></a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>) and human parity achievements (<a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHassan_et+al_2018_a\" href=\"#rHassan_et+al_2018_a\">Hassan et al, 2018</a></a>).",
        "When handling more language pairs, the translation accuracy of multilingual model is usually inferior to individual models, due to language diversity.",
        "It is challenging to train a multilingual translation model supporting dozens of language pairs while achieving comparable accuracy as individual models.",
        "We propose a new method based on knowledge distillation for multilingual translation to eliminate the accuracy gap between the multilingual model and individual models.",
        "After some iterations of training, the multilingual model may get higher translation accuracy than the individual models on some language pairs.",
        "We remove the distillation loss and keep training the multilingual model on these languages pairs with the original log-likelihood loss of the ground-truth translation.",
        "Our proposed method boosts the translation accuracy of the baseline multilingual model and achieve similar accuracy as individual models for most language pairs.",
        "The multilingual model with only 1/44 parameters can match or surpass the accuracy of individual models on the Ted talk datasets.",
        "It is observed that when there are dozens of language pairs, multilingual NMT usually achieves inferior accuracy compared with its counterpart which trains an individual model for each language pair.",
        "1: Input: Training corpus {Dl}lL=1 and pretrained individual models {\u03b8Il }lL=1 for L language pairs, learning rate \u03b7, total training steps T , distillation check step Tcheck, threshold \u03c4 of distillation accuracy.",
        "When the accuracy of multilingual model surpasses the individual model for the accuracy threshold \u03c4 on a certain language pair, we remove the distillation loss and just train the model with original negative log-likelihood loss for this pair.",
        "Due to the large number of languages and space limitations, we just show the BLEU score improvements of our method over individual models and the multi-baseline for each language in Table 5, and leave the detailed experiment results to Appendix (Section 3).",
        "We conduct experiments on IWSLT dataset with varying K, and just show the BLEU scores on the validation set of De-En translation due to space limitation, as illustrated in Table 7.",
        "We have proposed a distillation-based approach to boost the accuracy of multilingual NMT, which is usually of lower accuracy than the individual models in previous works.",
        "Experiments on three translation datasets with up to 44 languages demonstrate the multilingual model based on our proposed method can nearly match or even outperform the individual models, with just 1/N model parameters (N is up to 44 in our experiments).",
        "We will apply our method to larger datasets and more languages pairs, to study the upper limit of our proposed method"
    ],
    "headline": "We propose a distillation-based approach to boost the accuracy of multilingual machine translation",
    "reference_links": [
        {
            "id": "Anil_et+al_2018_a",
            "entry": "Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03235"
        },
        {
            "id": "Bahdanau_et+al_0000_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR 2015, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate"
        },
        {
            "id": "Bucilu_et+al_2006_a",
            "entry": "Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535\u2013541. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilu%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bucilu%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006"
        },
        {
            "id": "Chaudhari_et+al_2016_a",
            "entry": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01838"
        },
        {
            "id": "Dong_et+al_2015_a",
            "entry": "Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-task learning for multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 1723\u20131732, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Daxiang%20Wu%2C%20Hua%20He%2C%20Wei%20Yu%2C%20Dianhai%20Multi-task%20learning%20for%20multiple%20language%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Daxiang%20Wu%2C%20Hua%20He%2C%20Wei%20Yu%2C%20Dianhai%20Multi-task%20learning%20for%20multiple%20language%20translation%202015"
        },
        {
            "id": "Firat_et+al_2016_a",
            "entry": "Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pp. 866\u2013875, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orhan%20Firat%20Kyunghyun%20Cho%20and%20Yoshua%20Bengio%20Multiway%20multilingual%20neural%20machine%20translation%20with%20a%20shared%20attention%20mechanism%20In%20NAACL%20HLT%202016%20The%202016%20Conference%20of%20the%20North%20American%20Chapter%20of%20the%20Association%20for%20Computational%20Linguistics%20Human%20Language%20Technologies%20San%20Diego%20California%20USA%20June%201217%202016%20pp%20866875%202016"
        },
        {
            "id": "Freitag_et+al_2017_a",
            "entry": "Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran. Ensemble distillation for neural machine translation. arXiv preprint arXiv:1702.01802, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.01802"
        },
        {
            "id": "Furlanello_et+al_2018_a",
            "entry": "Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. arXiv preprint arXiv:1805.04770, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.04770"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1243\u20131252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017-08-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017-08-06"
        },
        {
            "id": "Gong_et+al_2018_a",
            "entry": "Chengyue Gong, Xu Tan, Di He, and Tao Qin. Sentence-wise smooth regularization for sequence to sequence learning. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Chengyue%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Sentence-wise%20smooth%20regularization%20for%20sequence%20to%20sequence%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Chengyue%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Sentence-wise%20smooth%20regularization%20for%20sequence%20to%20sequence%20learning%202018"
        },
        {
            "id": "Gu_et+al_2018_a",
            "entry": "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. Universal neural machine translation for extremely low resource languages. In NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 344\u2013354, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiatao%20Gu%20Hany%20Hassan%20Jacob%20Devlin%20and%20Victor%20O%20K%20Li%20Universal%20neural%20machine%20translation%20for%20extremely%20low%20resource%20languages%20In%20NAACLHLT%202018%20New%20Orleans%20Louisiana%20USA%20June%2016%202018%20Volume%201%20Long%20Papers%20pp%20344354%202018a"
        },
        {
            "id": "Gu_et+al_0000_a",
            "entry": "Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for lowresource neural machine translation. arXiv preprint arXiv:1808.08437, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1808.08437"
        },
        {
            "id": "Guo_et+al_2018_a",
            "entry": "Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neural machine translation with enhanced decoder input. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Junliang%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Non-autoregressive%20neural%20machine%20translation%20with%20enhanced%20decoder%20input%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Junliang%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Non-autoregressive%20neural%20machine%20translation%20with%20enhanced%20decoder%20input%202018"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. Toward multilingual neural machine translation with universal encoder and decoder. CoRR, abs/1611.04798, 2016. URL http://arxiv.org/abs/1611.04798.",
            "url": "http://arxiv.org/abs/1611.04798",
            "arxiv_url": "https://arxiv.org/pdf/1611.04798"
        },
        {
            "id": "Hassan_et+al_2018_a",
            "entry": "Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. Achieving human parity on automatic chinese to english news translation. CoRR, abs/1803.05567, 2018. URL http://arxiv.org/abs/1803.05567.",
            "url": "http://arxiv.org/abs/1803.05567",
            "arxiv_url": "https://arxiv.org/pdf/1803.05567"
        },
        {
            "id": "He_et+al_2018_a",
            "entry": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise coordination between encoder and decoder for neural machine translation. In NIPS, pp. 7955\u20137965, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Tianyu%20Tan%2C%20Xu%20Xia%2C%20Yingce%20He%2C%20Di%20Layer-wise%20coordination%20between%20encoder%20and%20decoder%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Tianyu%20Tan%2C%20Xu%20Xia%2C%20Yingce%20He%2C%20Di%20Layer-wise%20coordination%20between%20encoder%20and%20decoder%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Johnson_et+al_2017_a",
            "entry": "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viegas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. TACL, 5:339\u2013351, 2017. URL https://transacl.org/ojs/index.php/tacl/article/view/1081.",
            "url": "https://transacl.org/ojs/index.php/tacl/article/view/1081",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Melvin%20Schuster%2C%20Mike%20Le%2C%20Quoc%20V.%20Krikun%2C%20Maxim%20Google%E2%80%99s%20multilingual%20neural%20machine%20translation%20system%3A%20Enabling%20zero-shot%20translation%202017"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Kim_2016_a",
            "entry": "Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 1317\u20131327, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Rush%2C%20Alexander%20M.%20Sequence-level%20knowledge%20distillation%202016-11-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Rush%2C%20Alexander%20M.%20Sequence-level%20knowledge%20distillation%202016-11-01"
        },
        {
            "id": "Kim_0000_a",
            "entry": "Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07947"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lan_et+al_2018_a",
            "entry": "Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation by on-the-fly native ensemble. arXiv preprint arXiv:1806.04606, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04606"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from noisy labels with distillation. In ICCV, pp. 1928\u20131936, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuncheng%20Yang%2C%20Jianchao%20Song%2C%20Yale%20Cao%2C%20Liangliang%20Learning%20from%20noisy%20labels%20with%20distillation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuncheng%20Yang%2C%20Jianchao%20Song%2C%20Yale%20Cao%2C%20Liangliang%20Learning%20from%20noisy%20labels%20with%20distillation%202017"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhardwaj, Shaonan Zhang, and Jason Sun. A neural interlingua for multilingual machine translation. CoRR, abs/1804.08198, 2018. URL http://arxiv.org/abs/1804.08198.",
            "url": "http://arxiv.org/abs/1804.08198",
            "arxiv_url": "https://arxiv.org/pdf/1804.08198"
        },
        {
            "id": "Luong_et+al_2015_a",
            "entry": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. CoRR, abs/1511.06114, 2015a. URL http://arxiv.org/abs/1511.06114.",
            "url": "http://arxiv.org/abs/1511.06114",
            "arxiv_url": "https://arxiv.org/pdf/1511.06114"
        },
        {
            "id": "Luong_et+al_2015_b",
            "entry": "Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1412\u2013 1421, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015-09-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015-09-17"
        },
        {
            "id": "Neubig_2018_a",
            "entry": "Graham Neubig and Junjie Hu. Rapid adaptation of neural machine translation to new languages. arXiv preprint arXiv:1808.04189, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04189"
        },
        {
            "id": "Papineni_et+al_2002_a",
            "entry": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pp. 311\u2013318, 2002. URL http://www.aclweb.org/anthology/P02-1040.pdf.",
            "url": "http://www.aclweb.org/anthology/P02-1040.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002-07-06"
        },
        {
            "id": "Romero_et+al_2014_a",
            "entry": "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6550"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016. URL http://aclweb.org/anthology/P/P16/P16-1162.pdf.",
            "url": "http://aclweb.org/anthology/P/P16/P16-1162.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016-08-07"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Yanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan Liu. Dense information flow for neural machine translation. In NAACL, volume 1, pp. 1294\u20131303, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Yanyao%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Dense%20information%20flow%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Yanyao%20Tan%2C%20Xu%20He%2C%20Di%20Qin%2C%20Tao%20Dense%20information%20flow%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Song_et+al_2018_a",
            "entry": "Kaitao Song, Xu Tan, Di He, Jianfeng Lu, Tao Qin, and Tie-Yan Liu. Double path networks for sequence to sequence learning. In COLING, pp. 3064\u20133074, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Kaitao%20Tan%2C%20Xu%20He%2C%20Di%20Lu%2C%20Jianfeng%20Double%20path%20networks%20for%20sequence%20to%20sequence%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Kaitao%20Tan%2C%20Xu%20He%2C%20Di%20Lu%2C%20Jianfeng%20Double%20path%20networks%20for%20sequence%20to%20sequence%20learning%202018"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014-12-08"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017%2049%20December%202017%20Long%20Beach%20CA%20USA%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017%2049%20December%202017%20Long%20Beach%20CA%20USA%20pp%2060006010%202017"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Beyond error propagation in neural machine translation: Characteristics of language also matter. In EMNLP, pp. 3602\u20133611, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Lijun%20Tan%2C%20Xu%20He%2C%20Di%20Tian%2C%20Fei%20Beyond%20error%20propagation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Lijun%20Tan%2C%20Xu%20He%2C%20Di%20Tian%2C%20Fei%20Beyond%20error%20propagation%202018"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.",
            "url": "http://arxiv.org/abs/1609.08144",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Yang_et+al_2018_a",
            "entry": "Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan Yuille. Knowledge distillation in generations: More tolerant teachers educate better students. arXiv preprint arXiv:1805.05551, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.05551"
        },
        {
            "id": "Ye_et+al_2018_a",
            "entry": "Qi Ye, Sachan Devendra, Felix Matthieu, Padmanabhan Sarguna, and Neubig Graham. When and why are pre-trained word embeddings useful for neural machine translation. In HLT-NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Qi%20Devendra%2C%20Sachan%20Matthieu%2C%20Felix%20Sarguna%2C%20Padmanabhan%20When%20and%20why%20are%20pre-trained%20word%20embeddings%20useful%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Qi%20Devendra%2C%20Sachan%20Matthieu%2C%20Felix%20Sarguna%2C%20Padmanabhan%20When%20and%20why%20are%20pre-trained%20word%20embeddings%20useful%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Yim_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017. Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. arXiv preprint arXiv:1706.00384, 6, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00384"
        }
    ]
}
