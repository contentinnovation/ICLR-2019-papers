{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADAPTIVITY OF DEEP RELU NETWORK FOR LEARNING IN BESOV AND MIXED SMOOTH BESOV SPACES: OPTIMAL RATE AND CURSE OF DIMENSIONALITY",
        "author": "Taiji Suzuki The University of Tokyo, Tokyo, Japan Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan Japan Digital Design, Tokyo, Japan taiji@mist.i.u-tokyo.ac.jp",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1ebTsActm"
        },
        "abstract": "Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Throughout the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any nonadaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor."
    },
    "keywords": [
        {
            "term": "Mathematics",
            "url": "https://en.wikipedia.org/wiki/Mathematics"
        },
        {
            "term": "periodic functions",
            "url": "https://en.wikipedia.org/wiki/periodic_functions"
        },
        {
            "term": "target function",
            "url": "https://en.wikipedia.org/wiki/target_function"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "besov space",
            "url": "https://en.wikipedia.org/wiki/besov_space"
        },
        {
            "term": "artificial neural network",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "expressive power",
            "url": "https://en.wikipedia.org/wiki/expressive_power"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Deep learning has shown great success in several applications such as computer vision and natural language processing",
        "By doing so, (i) we show that deep learning1 achieves the minimax optimal rate on the Besov space and notably it outperforms any linear estimator such as the kernel ridge regression, and we show that deep learning can avoid the curse of dimensionality on the mixed smooth Besov space and achieves the minimax optimal rate",
        "<a class=\"ref-link\" id=\"cWilliamson_1992_a\" href=\"#rWilliamson_1992_a\">Williamson & Bartlett (1992</a>) derived a spline approximation error bound for an element in a Besov space when d = 1, but the derived bound is only O(N \u2212s+(1/p\u22121/r)+ ) which is the rate of non-adaptive methods described below, and approximation by a ReLU activation network is not discussed",
        "This paper investigated the learning ability of deep ReLU neural network when the target function is in a Besov space or a mixed smooth Besov space",
        "Based on the analysis for the Besov space, it was shown that deep learning using the ReLU activation can achieve the minimax optimal rate and outperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape of the target function",
        "The analysis for the mixed smooth Besov space showed that deep learning can adaptively avoid the curse of dimensionality"
    ],
    "key_statements": [
        "Deep learning has shown great success in several applications such as computer vision and natural language processing",
        "By doing so, (i) we show that deep learning1 achieves the minimax optimal rate on the Besov space and notably it outperforms any linear estimator such as the kernel ridge regression, and we show that deep learning can avoid the curse of dimensionality on the mixed smooth Besov space and achieves the minimax optimal rate",
        "We evaluate how well the functions in the Besov and m-Besov spaces can be approximated by neural networks with the ReLU activation",
        "We show how the neural network can approximate a function in the Besov space which is useful to derive the generalization error of deep learning",
        "We can translate several B-spline approximation results into those of deep neural network approximation. Combining this lemma and the B-spline interpolant representations of functions in Besov spaces (<a class=\"ref-link\" id=\"cDevore_1988_a\" href=\"#rDevore_1988_a\">DeVore & Popov, 1988</a>; <a class=\"ref-link\" id=\"cDevore_et+al_1993_a\" href=\"#rDevore_et+al_1993_a\">DeVore et al, 1993</a>; <a class=\"ref-link\" id=\"cDung_2011_b\" href=\"#rDung_2011_b\">Dung, 2011b</a>), we obtain the optimal approximation error bound for deep neural networks",
        "<a class=\"ref-link\" id=\"cWilliamson_1992_a\" href=\"#rWilliamson_1992_a\">Williamson & Bartlett (1992</a>) derived a spline approximation error bound for an element in a Besov space when d = 1, but the derived bound is only O(N \u2212s+(1/p\u22121/r)+ ) which is the rate of non-adaptive methods described below, and approximation by a ReLU activation network is not discussed",
        "Minimax optimal rate for estimating a function in the m-Besov space Here, we show the minimax optimality of the obtained bound as follows",
        "This paper investigated the learning ability of deep ReLU neural network when the target function is in a Besov space or a mixed smooth Besov space",
        "Based on the analysis for the Besov space, it was shown that deep learning using the ReLU activation can achieve the minimax optimal rate and outperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape of the target function",
        "The analysis for the mixed smooth Besov space showed that deep learning can adaptively avoid the curse of dimensionality"
    ],
    "summary": [
        "Deep learning has shown great success in several applications such as computer vision and natural language processing.",
        "We give generalization error bounds of deep ReLU networks for a Besov space and its variant with mixed smoothness, which includes the Holder space, the Sobolev space, and the function class with total variation as special cases.",
        "To investigate the effect of dimensionality, we analyze approximation and estimation problems in so-called the mixed smooth Besov space by ReLU neural network.",
        "We show how the neural network can approximate a function in the Besov space which is useful to derive the generalization error of deep learning.",
        "Combining this lemma and the B-spline interpolant representations of functions in Besov spaces (<a class=\"ref-link\" id=\"cDevore_1988_a\" href=\"#rDevore_1988_a\">DeVore & Popov, 1988</a>; <a class=\"ref-link\" id=\"cDevore_et+al_1993_a\" href=\"#rDevore_et+al_1993_a\">DeVore et al, 1993</a>; <a class=\"ref-link\" id=\"cDung_2011_b\" href=\"#rDung_2011_b\">Dung, 2011b</a>), we obtain the optimal approximation error bound for deep neural networks.",
        "Let U (H) be the unit ball of a quasi-Banach space H, and for a set F of functions, define the worst case approximation error as",
        "<a class=\"ref-link\" id=\"cWilliamson_1992_a\" href=\"#rWilliamson_1992_a\">Williamson & Bartlett (1992</a>) derived a spline approximation error bound for an element in a Besov space when d = 1, but the derived bound is only O(N \u2212s+(1/p\u22121/r)+ ) which is the rate of non-adaptive methods described below, and approximation by a ReLU activation network is not discussed.",
        "The theorem gives the approximation error bound to approximate functions in the m-Besov spaces by deep neural network models.",
        "The approximation error given in Theorem 1 achieves the optimal linear width ((N \u22121 logd\u22121(N ))s) for several parameter settings of p, q, s.",
        "We provide the estimation error rate of deep learning to estimate functions in Besov spaces.",
        "We provide the estimation error rate to estimate functions in mixed smooth Besov spaces.",
        "The risk bound (Theorem 3) indicates that the curse of dimensionality can be eased by assuming the mixed smoothness compared with the ordinary Besov space (n\u2212",
        "Minimax optimal rate for estimating a function in the m-Besov space Here, we show the minimax optimality of the obtained bound as follows.",
        "This paper investigated the learning ability of deep ReLU neural network when the target function is in a Besov space or a mixed smooth Besov space.",
        "Based on the analysis for the Besov space, it was shown that deep learning using the ReLU activation can achieve the minimax optimal rate and outperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape of the target function.",
        "The analysis for the mixed smooth Besov space showed that deep learning can adaptively avoid the curse of dimensionality.",
        "From more high level view point, these favorable property is reduced to its high feature extraction ability"
    ],
    "headline": "To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness",
    "reference_links": [
        {
            "id": "Adams_2003_a",
            "entry": "R. Adams and J. Fournier. Sobolev Spaces. Pure and Applied Mathematics. Elsevier Science, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adams%2C%20R.%20Fournier%2C%20J.%20Sobolev%20Spaces.%20Pure%20and%20Applied%20Mathematics%202003"
        },
        {
            "id": "Barron_1991_a",
            "entry": "A. R. Barron. Approximation and estimation bounds for artificial neural networks. In Proceedings of the 4th Annual Workshop on Computational Learning Theory, pp. 243\u2013249, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201991"
        },
        {
            "id": "Barron_1993_a",
            "entry": "A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930\u2013945, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993"
        },
        {
            "id": "Barron_1994_a",
            "entry": "A. R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115\u2013133, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201994"
        },
        {
            "id": "Bianchini_2014_a",
            "entry": "M. Bianchini and F. Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning Systems, 25(8):1553\u20131565, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bianchini%2C%20M.%20Scarselli%2C%20F.%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bianchini%2C%20M.%20Scarselli%2C%20F.%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014"
        },
        {
            "id": "Bolcskei_et+al_2017_a",
            "entry": "H. Bolcskei, P. Grohs, G. Kutyniok, and P. Petersen. Optimal approximation with sparsely connected deep neural networks. arXiv preprint arXiv:1705.01714, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.01714"
        },
        {
            "id": "Chui_et+al_1994_a",
            "entry": "C. Chui, X. Li, and H. Mhaskar. Neural networks for localized approximation. Mathematics of Computation, 63(208):607\u2013623, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chui%2C%20C.%20Li%2C%20X.%20Mhaskar%2C%20H.%20Neural%20networks%20for%20localized%20approximation%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chui%2C%20C.%20Li%2C%20X.%20Mhaskar%2C%20H.%20Neural%20networks%20for%20localized%20approximation%201994"
        },
        {
            "id": "Cohen_et+al_2001_a",
            "entry": "A. Cohen, W. Dahmen, I. Daubechies, and R. A. DeVore. Tree approximation and optimal encoding. Applied and Computational Harmonic Analysis, 11(2):192\u2013226, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20A.%20Dahmen%2C%20W.%20Daubechies%2C%20I.%20DeVore%2C%20R.A.%20Tree%20approximation%20and%20optimal%20encoding%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20A.%20Dahmen%2C%20W.%20Daubechies%2C%20I.%20DeVore%2C%20R.A.%20Tree%20approximation%20and%20optimal%20encoding%202001"
        },
        {
            "id": "Cohen_2016_a",
            "entry": "N. Cohen and A. Shashua. Convolutional rectifier networks as generalized tensor decompositions. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 955\u2013963, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20N.%20Shashua%2C%20A.%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20N.%20Shashua%2C%20A.%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016"
        },
        {
            "id": "Cohen_et+al_2016_b",
            "entry": "N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis. In Proceedings of the 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 698\u2013728, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016"
        },
        {
            "id": "Cybenko_1989_a",
            "entry": "G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989"
        },
        {
            "id": "Devore_1998_a",
            "entry": "R. A. DeVore. Nonlinear approximation. Acta Numerica, 7:51\u2013150, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DeVore%2C%20R.A.%20Nonlinear%20approximation%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DeVore%2C%20R.A.%20Nonlinear%20approximation%201998"
        },
        {
            "id": "Devore_1988_a",
            "entry": "R. A. DeVore and V. A. Popov. Interpolation of Besov spaces. Transactions of the American Mathematical Society, 305(1):397\u2013414, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DeVore%2C%20R.A.%20Popov%2C%20V.A.%20Interpolation%20of%20Besov%20spaces%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DeVore%2C%20R.A.%20Popov%2C%20V.A.%20Interpolation%20of%20Besov%20spaces%201988"
        },
        {
            "id": "Devore_et+al_1993_a",
            "entry": "R. A. DeVore, G. Kyriazis, D. Leviatan, and V. M. Tikhomirov. Wavelet compression and nonlinearn-widths. Advances in Computational Mathematics, 1(2):197\u2013214, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DeVore%2C%20R.A.%20Kyriazis%2C%20G.%20Leviatan%2C%20D.%20Tikhomirov%2C%20V.M.%20Wavelet%20compression%20and%20nonlinearn-widths%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DeVore%2C%20R.A.%20Kyriazis%2C%20G.%20Leviatan%2C%20D.%20Tikhomirov%2C%20V.M.%20Wavelet%20compression%20and%20nonlinearn-widths%201993"
        },
        {
            "id": "Donoho_1998_a",
            "entry": "D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of Statistics, 26(3):879\u2013921, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.L.%20Johnstone%2C%20I.M.%20Minimax%20estimation%20via%20wavelet%20shrinkage%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.L.%20Johnstone%2C%20I.M.%20Minimax%20estimation%20via%20wavelet%20shrinkage%201998"
        },
        {
            "id": "Donoho_et+al_1996_a",
            "entry": "D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet thresholding. The Annals of Statistics, 24(2):508\u2013539, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.L.%20Johnstone%2C%20I.M.%20Kerkyacharian%2C%20G.%20Picard%2C%20D.%20Density%20estimation%20by%20wavelet%20thresholding%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.L.%20Johnstone%2C%20I.M.%20Kerkyacharian%2C%20G.%20Picard%2C%20D.%20Density%20estimation%20by%20wavelet%20thresholding%201996"
        },
        {
            "id": "Dung_1990_a",
            "entry": "D. Dung. On recovery and one-sided approximation of periodic functions of several variables. Doklady Akademii Nauk SSSR, 313:787\u2013790, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%2C%20D.%20On%20recovery%20and%20one-sided%20approximation%20of%20periodic%20functions%20of%20several%20variables%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dung%2C%20D.%20On%20recovery%20and%20one-sided%20approximation%20of%20periodic%20functions%20of%20several%20variables%201990"
        },
        {
            "id": "Dung_1991_a",
            "entry": "D. Dung. On optimal recovery of multivariate periodic functions. In ICM-90 Satellite Conference Proceedings, pp. 96\u2013105, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%2C%20D.%20On%20optimal%20recovery%20of%20multivariate%20periodic%20functions%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dung%2C%20D.%20On%20optimal%20recovery%20of%20multivariate%20periodic%20functions%201991"
        },
        {
            "id": "Dung_1992_a",
            "entry": "D. Dung. Optimal recovery of functions of a certain mixed smoothness. Vietnam Journal of Mathematics, 20(2):18\u201332, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%2C%20D.%20Optimal%20recovery%20of%20functions%20of%20a%20certain%20mixed%20smoothness%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dung%2C%20D.%20Optimal%20recovery%20of%20functions%20of%20a%20certain%20mixed%20smoothness%201992"
        },
        {
            "id": "Dung_2011_a",
            "entry": "D. Dung. B-spline quasi-interpolant representations and sampling recovery of functions with mixed smoothness. Journal of Complexity, 27(6):541\u2013567, 2011a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%2C%20D.%20B-spline%20quasi-interpolant%20representations%20and%20sampling%20recovery%20of%20functions%20with%20mixed%20smoothness%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dung%2C%20D.%20B-spline%20quasi-interpolant%20representations%20and%20sampling%20recovery%20of%20functions%20with%20mixed%20smoothness%202011"
        },
        {
            "id": "Dung_2011_b",
            "entry": "D. Dung. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34(1): 1\u201341, 2011b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%2C%20D.%20Optimal%20adaptive%20sampling%20recovery%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dung%2C%20D.%20Optimal%20adaptive%20sampling%20recovery%202011"
        },
        {
            "id": "Dung_et+al_2016_a",
            "entry": "D. Dung, V. N. Temlyakov, and T. Ullrich. Hyperbolic cross approximation. arXiv preprint arXiv:1601.03978, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.03978"
        },
        {
            "id": "Galeev_1996_a",
            "entry": "E. M. Galeev. Linear widths of Holder-Nikol\u2019skii classes of periodic functions of several variables. Matematicheskie Zametki, 59(2):189\u2013199, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galeev%2C%20E.M.%20Linear%20widths%20of%20Holder-Nikol%E2%80%99skii%20classes%20of%20periodic%20functions%20of%20several%20variables%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Galeev%2C%20E.M.%20Linear%20widths%20of%20Holder-Nikol%E2%80%99skii%20classes%20of%20periodic%20functions%20of%20several%20variables%201996"
        },
        {
            "id": "Nickl_2015_a",
            "entry": "E. Gineand R. Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickl%2C%20E.Gineand%20R.%20Mathematical%20Foundations%20of%20Infinite-Dimensional%20Statistical%20Models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickl%2C%20E.Gineand%20R.%20Mathematical%20Foundations%20of%20Infinite-Dimensional%20Statistical%20Models%202015"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 315\u2013323, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011"
        },
        {
            "id": "Hornik_1991_a",
            "entry": "K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2): 251\u2013257, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20K.%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20K.%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991"
        },
        {
            "id": "Imaizumi_2018_a",
            "entry": "M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. arXiv preprint arXiv:1802.04474, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04474"
        },
        {
            "id": "Kanagawa_et+al_2016_a",
            "entry": "H. Kanagawa, T. Suzuki, H. Kobayashi, N. Shimizu, and Y. Tagami. Gaussian process nonparametric tensor estimator and its minimax optimality. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1632\u20131641, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanagawa%2C%20H.%20Suzuki%2C%20T.%20Kobayashi%2C%20H.%20Shimizu%2C%20N.%20Gaussian%20process%20nonparametric%20tensor%20estimator%20and%20its%20minimax%20optimality%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kanagawa%2C%20H.%20Suzuki%2C%20T.%20Kobayashi%2C%20H.%20Shimizu%2C%20N.%20Gaussian%20process%20nonparametric%20tensor%20estimator%20and%20its%20minimax%20optimality%202016"
        },
        {
            "id": "Kerkyacharian_1992_a",
            "entry": "G. Kerkyacharian and D. Picard. Density estimation in Besov spaces. Statistics & Probability Letters, 13:15\u201324, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kerkyacharian%2C%20G.%20Picard%2C%20D.%20Density%20estimation%20in%20Besov%20spaces%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kerkyacharian%2C%20G.%20Picard%2C%20D.%20Density%20estimation%20in%20Besov%20spaces%201992"
        },
        {
            "id": "Klusowski_2016_a",
            "entry": "J. M. Klusowski and A. R. Barron. Risk bounds for high-dimensional ridge function combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.01434"
        },
        {
            "id": "Liang_2016_a",
            "entry": "S. Liang and R. Srikant. Why deep neural networks for function approximation? arXiv preprint arXiv:1610.04161, 2016. ICLR2017.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04161"
        },
        {
            "id": "Mallat_1999_a",
            "entry": "S. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mallat%2C%20S.%20A%20Wavelet%20Tour%20of%20Signal%20Processing%201999"
        },
        {
            "id": "Meier_et+al_2009_a",
            "entry": "L. Meier, S. van de Geer, and P. Buhlmann. High-dimensional additive modeling. The Annals of Statistics, 37(6B):3779\u20133821, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meier%2C%20L.%20van%20de%20Geer%2C%20S.%20Buhlmann%2C%20P.%20High-dimensional%20additive%20modeling%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meier%2C%20L.%20van%20de%20Geer%2C%20S.%20Buhlmann%2C%20P.%20High-dimensional%20additive%20modeling%202009"
        },
        {
            "id": "Mhaskar_1996_a",
            "entry": "H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural Computation, 8(1):164\u2013177, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996"
        },
        {
            "id": "Mhaskar_1992_a",
            "entry": "H. N. Mhaskar and C. A. Micchelli. Approximation by superposition of sigmoidal and radial basis functions. Advances in Applied mathematics, 13(3):350\u2013373, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Micchelli%2C%20C.A.%20Approximation%20by%20superposition%20of%20sigmoidal%20and%20radial%20basis%20functions%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Micchelli%2C%20C.A.%20Approximation%20by%20superposition%20of%20sigmoidal%20and%20radial%20basis%20functions%201992"
        },
        {
            "id": "Mhaskar_1993_a",
            "entry": "H. N. Mhaskar. Approximation properties of a multilayered feedforward artificial neural network. Advances in Computational Mathematics, 1(1):61\u201380, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Approximation%20properties%20of%20a%20multilayered%20feedforward%20artificial%20neural%20network%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Approximation%20properties%20of%20a%20multilayered%20feedforward%20artificial%20neural%20network%201993"
        },
        {
            "id": "Montanelli_2017_a",
            "entry": "H. Montanelli and Q. Du. Deep relu networks lessen the curse of dimensionality. arXiv preprint arXiv:1712.08688, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08688"
        },
        {
            "id": "Montufar_et+al_2014_a",
            "entry": "G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems, pp. 2924\u20132932. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Bengio%2C%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Bengio%2C%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "Myronyuk_2016_a",
            "entry": "V. Myronyuk. Kolmogorov widths of the anisotropic Besov classes of periodic functions of many variables. Ukrainian Mathematical Journal, 68(5), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Myronyuk%2C%20V.%20Kolmogorov%20widths%20of%20the%20anisotropic%20Besov%20classes%20of%20periodic%20functions%20of%20many%20variables%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Myronyuk%2C%20V.%20Kolmogorov%20widths%20of%20the%20anisotropic%20Besov%20classes%20of%20periodic%20functions%20of%20many%20variables%202016"
        },
        {
            "id": "Nair_2010_a",
            "entry": "V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning, pp. 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Neumann_2000_a",
            "entry": "M. H. Neumann. Multivariate wavelet thresholding in anisotropic function spaces. Statistica Sinica, 10(2):399\u2013431, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20M.H.%20Multivariate%20wavelet%20thresholding%20in%20anisotropic%20function%20spaces%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20M.H.%20Multivariate%20wavelet%20thresholding%20in%20anisotropic%20function%20spaces%202000"
        },
        {
            "id": "Peetre_1976_a",
            "entry": "J. Peetre and D. U. M. Dept. New Thoughts on Besov Spaces. Duke University mathematics series. Mathematics Dept., Duke University, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peetre%2C%20J.%20Dept%2C%20D.U.M.%20New%20Thoughts%20on%20Besov%20Spaces.%20Duke%20University%20mathematics%20series%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peetre%2C%20J.%20Dept%2C%20D.U.M.%20New%20Thoughts%20on%20Besov%20Spaces.%20Duke%20University%20mathematics%20series%201976"
        },
        {
            "id": "Petersen_2017_a",
            "entry": "P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. arXiv preprint arXiv:1709.05289, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.05289"
        },
        {
            "id": "Petrushev_1998_a",
            "entry": "P. P. Petrushev. Approximation by ridge functions and neural networks. SIAM Journal on Mathematical Analysis, 30(1):155\u2013189, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrushev%2C%20P.P.%20Approximation%20by%20ridge%20functions%20and%20neural%20networks%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petrushev%2C%20P.P.%20Approximation%20by%20ridge%20functions%20and%20neural%20networks%201998"
        },
        {
            "id": "Pinkus_1999_a",
            "entry": "A. Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica, 8:143\u2013195, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinkus%2C%20A.%20Approximation%20theory%20of%20the%20mlp%20model%20in%20neural%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinkus%2C%20A.%20Approximation%20theory%20of%20the%20mlp%20model%20in%20neural%20networks%201999"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in Neural Information Processing Systems, pp. 3360\u20133368. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20B.%20Lahiri%2C%20S.%20Raghu%2C%20M.%20Sohl-Dickstein%2C%20J.%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20B.%20Lahiri%2C%20S.%20Raghu%2C%20M.%20Sohl-Dickstein%2C%20J.%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016"
        },
        {
            "id": "Raskutti_et+al_2012_a",
            "entry": "G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. The Journal of Machine Learning Research, 13(1):389\u2013 427, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20G.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Minimax-optimal%20rates%20for%20sparse%20additive%20models%20over%20kernel%20classes%20via%20convex%20programming%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20G.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Minimax-optimal%20rates%20for%20sparse%20additive%20models%20over%20kernel%20classes%20via%20convex%20programming%202012"
        },
        {
            "id": "Romanyuk_2001_a",
            "entry": "A. S. Romanyuk. Linear widths of the Besov classes of periodic functions of many variables. II. Ukrainian Mathematical Journal, 53(6):965\u2013977, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romanyuk%2C%20A.S.%20Linear%20widths%20of%20the%20Besov%20classes%20of%20periodic%20functions%20of%20many%20variables%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romanyuk%2C%20A.S.%20Linear%20widths%20of%20the%20Besov%20classes%20of%20periodic%20functions%20of%20many%20variables%202001"
        },
        {
            "id": "Romanyuk_2009_a",
            "entry": "A. S. Romanyuk. Bilinear approximations and Kolmogorov widths of periodic Besov classes. Theory of Operators, Differential Equations, and the Theory of Functions, 6(1):222\u2013236, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romanyuk%2C%20A.S.%20Bilinear%20approximations%20and%20Kolmogorov%20widths%20of%20periodic%20Besov%20classes.%20Theory%20of%20Operators%2C%20Differential%20Equations%2C%20and%20the%20Theory%20of%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romanyuk%2C%20A.S.%20Bilinear%20approximations%20and%20Kolmogorov%20widths%20of%20periodic%20Besov%20classes.%20Theory%20of%20Operators%2C%20Differential%20Equations%2C%20and%20the%20Theory%20of%202009"
        },
        {
            "id": "Schmeisser_1987_a",
            "entry": "H.-J. Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness properties. Analysis Mathematica, 13(2):153\u2013168, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmeisser%2C%20H.-J.%20An%20unconditional%20basis%20in%20periodic%20spaces%20with%20dominating%20mixed%20smoothness%20properties%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmeisser%2C%20H.-J.%20An%20unconditional%20basis%20in%20periodic%20spaces%20with%20dominating%20mixed%20smoothness%20properties%201987"
        },
        {
            "id": "Schmidt-Hieber_2018_a",
            "entry": "J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv preprint arXiv:1708.06633(v3), 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06633"
        },
        {
            "id": "Sickel_2009_a",
            "entry": "W. Sickel and T. Ullrich. Tensor products of Sobolev\u2013Besov spaces and applications to approximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748\u2013786, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sickel%2C%20W.%20Ullrich%2C%20T.%20Tensor%20products%20of%20Sobolev%E2%80%93Besov%20spaces%20and%20applications%20to%20approximation%20from%20the%20hyperbolic%20cross%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sickel%2C%20W.%20Ullrich%2C%20T.%20Tensor%20products%20of%20Sobolev%E2%80%93Besov%20spaces%20and%20applications%20to%20approximation%20from%20the%20hyperbolic%20cross%202009"
        },
        {
            "id": "Sickel_2011_a",
            "entry": "W. Sickel and T. Ullrich. Spline interpolation on sparse grids. Applicable Analysis, 90(3-4):337\u2013 383, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sickel%2C%20W.%20Ullrich%2C%20T.%20Spline%20interpolation%20on%20sparse%20grids%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sickel%2C%20W.%20Ullrich%2C%20T.%20Spline%20interpolation%20on%20sparse%20grids%202011"
        },
        {
            "id": "Signoretto_et+al_2010_a",
            "entry": "M. Signoretto, L. D. Lathauwer, and J. Suykens. Nuclear norms for tensors and their use for convex multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Signoretto%2C%20M.%20Lathauwer%2C%20L.D.%20Suykens%2C%20J.%20Nuclear%20norms%20for%20tensors%20and%20their%20use%20for%20convex%20multilinear%20estimation%202010"
        },
        {
            "id": "Smolyak_1963_a",
            "entry": "S. Smolyak. Quadrature and interpolation formulas for tensor products of certain classes of functions. Doklady Akademii Nauk SSSR, 148:1042\u20131045, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolyak%2C%20S.%20Quadrature%20and%20interpolation%20formulas%20for%20tensor%20products%20of%20certain%20classes%20of%20functions%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolyak%2C%20S.%20Quadrature%20and%20interpolation%20formulas%20for%20tensor%20products%20of%20certain%20classes%20of%20functions%201963"
        },
        {
            "id": "Sonoda_2017_a",
            "entry": "S. Sonoda and N. Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233\u2013268, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sonoda%2C%20S.%20Murata%2C%20N.%20Neural%20network%20with%20unbounded%20activation%20functions%20is%20universal%20approximator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sonoda%2C%20S.%20Murata%2C%20N.%20Neural%20network%20with%20unbounded%20activation%20functions%20is%20universal%20approximator%202017"
        },
        {
            "id": "Suzuki_et+al_2016_a",
            "entry": "T. Suzuki, H. Kanagawa, H. Kobayashi, N. Shimizu, and Y. Tagami. Minimax optimal alternating minimization for kernel nonparametric tensor learning. In Advances In Neural Information Processing Systems, pp. 3783\u20133791, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suzuki%2C%20T.%20Kanagawa%2C%20H.%20Kobayashi%2C%20H.%20Shimizu%2C%20N.%20Minimax%20optimal%20alternating%20minimization%20for%20kernel%20nonparametric%20tensor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suzuki%2C%20T.%20Kanagawa%2C%20H.%20Kobayashi%2C%20H.%20Shimizu%2C%20N.%20Minimax%20optimal%20alternating%20minimization%20for%20kernel%20nonparametric%20tensor%20learning%202016"
        },
        {
            "id": "Temlyakov_1982_a",
            "entry": "V. Temlyakov. Approximation of periodic functions of several variables with bounded mixed difference. Mathematics of the USSR-Sbornik, 41(1):53\u201366, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Temlyakov%2C%20V.%20Approximation%20of%20periodic%20functions%20of%20several%20variables%20with%20bounded%20mixed%20difference%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Temlyakov%2C%20V.%20Approximation%20of%20periodic%20functions%20of%20several%20variables%20with%20bounded%20mixed%20difference%201982"
        },
        {
            "id": "Temlyakov_1993_a",
            "entry": "V. Temlyakov. Approximation of Periodic Functions. Nova Science Publishers, 1993a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Temlyakov%2C%20V.%20Approximation%20of%20Periodic%20Functions%201993"
        },
        {
            "id": "Temlyakov_1993_b",
            "entry": "V. Temlyakov. On approximate recovery of functions with bounded mixed derivative. Journal of Complexity, 9:41\u201359, 1993b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Temlyakov%2C%20V.%20On%20approximate%20recovery%20of%20functions%20with%20bounded%20mixed%20derivative%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Temlyakov%2C%20V.%20On%20approximate%20recovery%20of%20functions%20with%20bounded%20mixed%20derivative%201993"
        },
        {
            "id": "Tikhomirov_1960_a",
            "entry": "V. M. Tikhomirov. Diameters of sets in function spaces and the theory of best approximations. Uspekhi Matematicheskikh Nauk, 15(3):81\u2013120, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tikhomirov%2C%20V.M.%20Diameters%20of%20sets%20in%20function%20spaces%20and%20the%20theory%20of%20best%20approximations%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tikhomirov%2C%20V.M.%20Diameters%20of%20sets%20in%20function%20spaces%20and%20the%20theory%20of%20best%20approximations%201960"
        },
        {
            "id": "Triebel_1983_a",
            "entry": "H. Triebel. Theory of Function Spaces. Monographs in Mathematics. Birkhauser Verlag, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Triebel%2C%20H.%20Theory%20of%20Function%20Spaces.%20Monographs%20in%20Mathematics%201983"
        },
        {
            "id": "Van_1996_a",
            "entry": "A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Vaart%2C%20A.W.%20Wellner%2C%20J.A.%20Weak%20Convergence%20and%20Empirical%20Processes%3A%20With%20Applications%20to%20Statistics%201996"
        },
        {
            "id": "Vybaral_2008_a",
            "entry": "J. Vybaral. Widths of embeddings in function spaces. Journal of Complexity, 24:545\u2013570, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vybaral%2C%20J.%20Widths%20of%20embeddings%20in%20function%20spaces%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vybaral%2C%20J.%20Widths%20of%20embeddings%20in%20function%20spaces%202008"
        },
        {
            "id": "Williamson_1992_a",
            "entry": "R. C. Williamson and P. L. Bartlett. Splines, rational functions and neural networks. In Advances in Neural Information Processing Systems, pp. 1040\u20131047, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williamson%2C%20R.C.%20Bartlett%2C%20P.L.%20Splines%2C%20rational%20functions%20and%20neural%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williamson%2C%20R.C.%20Bartlett%2C%20P.L.%20Splines%2C%20rational%20functions%20and%20neural%20networks%201992"
        },
        {
            "id": "Yang_1999_a",
            "entry": "Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564\u20131599, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Barron%2C%20A.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Y.%20Barron%2C%20A.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999"
        },
        {
            "id": "Yarotsky_2017_a",
            "entry": "D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        },
        {
            "id": "Zhang_et+al_2002_a",
            "entry": "S. Zhang, M.-Y. Wong, and Z. Zheng. Wavelet threshold estimation of a regression function with random design. Journal of Multivariate Analysis, 80(2):256\u2013284, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20S.%20Wong%2C%20M.-Y.%20Zheng%2C%20Z.%20Wavelet%20threshold%20estimation%20of%20a%20regression%20function%20with%20random%20design%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20S.%20Wong%2C%20M.-Y.%20Zheng%2C%20Z.%20Wavelet%20threshold%20estimation%20of%20a%20regression%20function%20with%20random%20design%202002"
        },
        {
            "id": "Mhaskar_1992_b",
            "entry": "Mhaskar & Micchelli (1992) for example). Thus, if we can make an approximation of \u03b7(x)m, then by taking a summation of those basis, we obtain an approximate of Nm(x). It is shown by Yarotsky (2017); Schmidt-Hieber (2018) that, for D \u2208 N and any > 0, there exists a neural network \u03c6mult \u2208 \u03a6(L, W, S, B) with L = log2 3D + 5 log2(D), W = 6d, S = LW 2 and B = 1 such that",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%20%20Micchelli%201992%20for%20example%20Thus%20if%20we%20can%20make%20an%20approximation%20of%20%CE%B7xm%20then%20by%20taking%20a%20summation%20of%20those%20basis%20we%20obtain%20an%20approximate%20of%20Nmx%20It%20is%20shown%20by%20Yarotsky%202017%20SchmidtHieber%202018%20that%20for%20D%20%20N%20and%20any%20%200%20there%20exists%20a%20neural%20network%20%CF%86mult%20%20%CE%A6L%20W%20S%20B%20with%20L%20%20log2%203D%20%205%20log2D%20W%20%206d%20S%20%20LW%202%20and%20B%20%201%20such%20that",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%20%20Micchelli%201992%20for%20example%20Thus%20if%20we%20can%20make%20an%20approximation%20of%20%CE%B7xm%20then%20by%20taking%20a%20summation%20of%20those%20basis%20we%20obtain%20an%20approximate%20of%20Nmx%20It%20is%20shown%20by%20Yarotsky%202017%20SchmidtHieber%202018%20that%20for%20D%20%20N%20and%20any%20%200%20there%20exists%20a%20neural%20network%20%CF%86mult%20%20%CE%A6L%20W%20S%20B%20with%20L%20%20log2%203D%20%205%20log2D%20W%20%206d%20S%20%20LW%202%20and%20B%20%201%20such%20that"
        },
        {
            "id": "0",
            "entry": "0. Moreover, for any Proof of Lemma 2. DeVore & Popov (1988) constructed a linear bounded operator Pk having the following form: Pk(f )(x) =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreover%20for%20any%20Proof%20of%20Lemma%202.%20DeVore%20%26%20Popov%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreover%20for%20any%20Proof%20of%20Lemma%202.%20DeVore%20%26%20Popov%201988"
        },
        {
            "id": "Based_2011_a",
            "entry": "Based on this decomposition, Dung (2011b) proposed an optimal adaptive recovery method such that the approximator has the form (10) under the conditions for K, K\u2217, nk given in the statement and satisfies the approximation accuracy (9). This can be proven by applying the proof of Theorem 3.1 in",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Based%20on%20this%20decomposition%20Dung%202011b%20proposed%20an%20optimal%20adaptive%20recovery%20method%20such%20that%20the%20approximator%20has%20the%20form%2010%20under%20the%20conditions%20for%20K%20K%20nk%20given%20in%20the%20statement%20and%20satisfies%20the%20approximation%20accuracy%209%20This%20can%20be%20proven%20by%20applying%20the%20proof%20of%20Theorem%2031%20in",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Based%20on%20this%20decomposition%20Dung%202011b%20proposed%20an%20optimal%20adaptive%20recovery%20method%20such%20that%20the%20approximator%20has%20the%20form%2010%20under%20the%20conditions%20for%20K%20K%20nk%20given%20in%20the%20statement%20and%20satisfies%20the%20approximation%20accuracy%209%20This%20can%20be%20proven%20by%20applying%20the%20proof%20of%20Theorem%2031%20in"
        },
        {
            "id": "Dung_0000_a",
            "entry": "Dung (2011b) to the decomposition (13) instead of Eq. (3.8) of that paper. See also Theorem 5.4 of",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%202011b%20to%20the%20decomposition%2013%20instead%20of%20Eq%2038%20of%20that%20paper%20See%20also%20Theorem%2054%20of"
        },
        {
            "id": "Dung_2011_c",
            "entry": "Dung (2011b). Moreover, the equivalence (14) gives the norm bound of the coefficient (\u03b1k,j).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dung%20Moreover%2C%20the%20equivalence%20%2814%29%20gives%20the%20norm%20bound%20of%20the%20coefficient%202011"
        },
        {
            "id": "Here_1963_a",
            "entry": "Here, we give technical details behind the approximation bound. The analysis utilizes the so called sparse grid technique Smolyak (1963) which has been developed in the function approximation theory field.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Here%20we%20give%20technical%20details%20behind%20the%20approximation%20bound.%20The%20analysis%20utilizes%20the%20so%20called%20sparse%20grid%20technique%20Smolyak%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Here%20we%20give%20technical%20details%20behind%20the%20approximation%20bound.%20The%20analysis%20utilizes%20the%20so%20called%20sparse%20grid%20technique%20Smolyak%201963"
        },
        {
            "id": "Several_2016_a",
            "entry": "Several aspects of the m-Besov space such as the optimal N -term approximation error and Kolmogorov widths have been extensively studied in the literature (see a comprehensive survey (Dung et al., 2016)). An analogous result is already given by Dung (2011a) in which s > 1/p is assumed and a linear interpolation method is investigated. However, our result only requires s > (1/p \u2212 1/q)+. This difference comes from a point that our analysis allows nonlinear adaptive interpolation instead of (linear) non-adaptive sampling considered in Dung (2011a). Because of this, our bound is better than the optimal rate of linear methods (Galeev, 1996; Romanyuk, 2001) and non-adaptive methods (Dung, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b) especially in the regime of p < r (Dung (1992) also deals with adaptive methods but does not cover p < r for adaptive method). See Proposition 2 for comparison.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Several%20aspects%20of%20the%20mBesov%20space%20such%20as%20the%20optimal%20N%20term%20approximation%20error%20and%20Kolmogorov%20widths%20have%20been%20extensively%20studied%20in%20the%20literature%20see%20a%20comprehensive%20survey%20Dung%20et%20al%202016%20An%20analogous%20result%20is%20already%20given%20by%20Dung%202011a%20in%20which%20s%20%201p%20is%20assumed%20and%20a%20linear%20interpolation%20method%20is%20investigated%20However%20our%20result%20only%20requires%20s%20%201p%20%201q%20This%20difference%20comes%20from%20a%20point%20that%20our%20analysis%20allows%20nonlinear%20adaptive%20interpolation%20instead%20of%20linear%20nonadaptive%20sampling%20considered%20in%20Dung%202011a%20Because%20of%20this%20our%20bound%20is%20better%20than%20the%20optimal%20rate%20of%20linear%20methods%20Galeev%201996%20Romanyuk%202001%20and%20nonadaptive%20methods%20Dung%201990%201991%201992%20Temlyakov%201982%201993ab%20especially%20in%20the%20regime%20of%20p%20%20r%20Dung%201992%20also%20deals%20with%20adaptive%20methods%20but%20does%20not%20cover%20p%20%20r%20for%20adaptive%20method%20See%20Proposition%202%20for%20comparison",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Several%20aspects%20of%20the%20mBesov%20space%20such%20as%20the%20optimal%20N%20term%20approximation%20error%20and%20Kolmogorov%20widths%20have%20been%20extensively%20studied%20in%20the%20literature%20see%20a%20comprehensive%20survey%20Dung%20et%20al%202016%20An%20analogous%20result%20is%20already%20given%20by%20Dung%202011a%20in%20which%20s%20%201p%20is%20assumed%20and%20a%20linear%20interpolation%20method%20is%20investigated%20However%20our%20result%20only%20requires%20s%20%201p%20%201q%20This%20difference%20comes%20from%20a%20point%20that%20our%20analysis%20allows%20nonlinear%20adaptive%20interpolation%20instead%20of%20linear%20nonadaptive%20sampling%20considered%20in%20Dung%202011a%20Because%20of%20this%20our%20bound%20is%20better%20than%20the%20optimal%20rate%20of%20linear%20methods%20Galeev%201996%20Romanyuk%202001%20and%20nonadaptive%20methods%20Dung%201990%201991%201992%20Temlyakov%201982%201993ab%20especially%20in%20the%20regime%20of%20p%20%20r%20Dung%201992%20also%20deals%20with%20adaptive%20methods%20but%20does%20not%20cover%20p%20%20r%20for%20adaptive%20method%20See%20Proposition%202%20for%20comparison"
        },
        {
            "id": "On_2011_b",
            "entry": "On the other hand, following the same line of Theorem 2.1 (ii) of Dung (2011a), we also obtain the opposite inequality f MBps,q (ak) b\u03b1q (Lp) (pk)k b\u03b1q (Lp) (note that the analogous inequality to Lemma 2.3 of Dung (2011a) also holds in our setting by replacing qs with ps and \u03c9re(f, 2\u2212k)p by wre,p).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=On%20the%20other%20hand%20following%20the%20same%20line%20of%20Theorem%2021%20ii%20of%20Dung%202011a%20we%20also%20obtain%20the%20opposite%20inequality%20f%20MBpsq%20ak%20b%CE%B1q%20Lp%20pkk%20b%CE%B1q%20Lp%20note%20that%20the%20analogous%20inequality%20to%20Lemma%2023%20of%20Dung%202011a%20also%20holds%20in%20our%20setting%20by%20replacing%20qs%20with%20ps%20and%20%CF%89ref%202kp%20by%20wrep",
            "oa_query": "https://api.scholarcy.com/oa_version?query=On%20the%20other%20hand%20following%20the%20same%20line%20of%20Theorem%2021%20ii%20of%20Dung%202011a%20we%20also%20obtain%20the%20opposite%20inequality%20f%20MBpsq%20ak%20b%CE%B1q%20Lp%20pkk%20b%CE%B1q%20Lp%20note%20that%20the%20analogous%20inequality%20to%20Lemma%2023%20of%20Dung%202011a%20also%20holds%20in%20our%20setting%20by%20replacing%20qs%20with%20ps%20and%20%CF%89ref%202kp%20by%20wrep"
        },
        {
            "id": "Therefore,_2011_c",
            "entry": "Therefore, f \u2208 MBp\u03b1,q if and only if (pk)k\u2208Zd+ given by Eq. (15) satisfies (pk)k bq\u03b1(Lp) < \u221e and f can be decomposed into f = k\u2208Zd+ pk where convergence is in MBp\u03b1,q. Moreover, it holds that f MBp\u03b1,q (pk)k. bq\u03b1(Lp) This can be shown by Theorem 2.1 of Dung (2011a). Moreover, by the quasi-norm equivalence pk p 2\u2212 k 1/p( j\u2208Jm d (k) |\u03b1k,j |p)1/p, we also have (\u03b1k,j )k,j mb\u03b1p,q f. MBp\u03b1,q",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Therefore%20f%20%20MBp%CE%B1q%20if%20and%20only%20if%20pkkZd%20given%20by%20Eq%2015%20satisfies%20pkk%20bq%CE%B1Lp%20%20%20and%20f%20can%20be%20decomposed%20into%20f%20%20kZd%20pk%20where%20convergence%20is%20in%20MBp%CE%B1q%20Moreover%20it%20holds%20that%20f%20MBp%CE%B1q%20pkk%20bq%CE%B1Lp%20This%20can%20be%20shown%20by%20Theorem%2021%20of%20Dung%202011a%20Moreover%20by%20the%20quasinorm%20equivalence%20pk%20p%202%20k%201p%20jJm%20d%20k%20%CE%B1kj%20p1p%20we%20also%20have%20%CE%B1kj%20kj%20mb%CE%B1pq%20f%20MBp%CE%B1q",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Therefore%20f%20%20MBp%CE%B1q%20if%20and%20only%20if%20pkkZd%20given%20by%20Eq%2015%20satisfies%20pkk%20bq%CE%B1Lp%20%20%20and%20f%20can%20be%20decomposed%20into%20f%20%20kZd%20pk%20where%20convergence%20is%20in%20MBp%CE%B1q%20Moreover%20it%20holds%20that%20f%20MBp%CE%B1q%20pkk%20bq%CE%B1Lp%20This%20can%20be%20shown%20by%20Theorem%2021%20of%20Dung%202011a%20Moreover%20by%20the%20quasinorm%20equivalence%20pk%20p%202%20k%201p%20jJm%20d%20k%20%CE%B1kj%20p1p%20we%20also%20have%20%CE%B1kj%20kj%20mb%CE%B1pq%20f%20MBp%CE%B1q"
        },
        {
            "id": "If_2011_d",
            "entry": "If p \u2265 r, the assertion can be shown in the same manner as Theorem 3.1 of Dung (2011a).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20p%20%20r%20the%20assertion%20can%20be%20shown%20in%20the%20same%20manner%20as%20Theorem%2031%20of%20Dung%202011a"
        },
        {
            "id": "N_)_b",
            "entry": "N = log2(K). K\u2217 = K(1 + 1/ ), and nk = 2K\u2212 ( k 1\u2212K) for k \u2208 Z+d with K + 1 \u2264 k 1 \u2264 K\u2217. Then, by Lemma 5.3 of Dung (2011a), we have that f \u2212 RK (f )",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%20%20log2K%20K%20%20K1%20%201%20%20and%20nk%20%202K%20%20k%201K%20for%20k%20%20Zd%20with%20K%20%201%20%20k%201%20%20K%20Then%20by%20Lemma%2053%20of%20Dung%202011a%20we%20have%20that%20f%20%20RK%20f"
        }
    ]
}
