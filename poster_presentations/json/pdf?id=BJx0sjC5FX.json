{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
        "author": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, Lucia Specia",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJx0sjC5FX",
            "doi": "10.18653/v1/s17-2001"
        },
        "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
        "abstract": "Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; <a class=\"ref-link\" id=\"cSmolensky_1990_a\" href=\"#rSmolensky_1990_a\">Smolensky, 1990</a>), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structuresensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations."
    },
    "keywords": [
        {
            "term": "mean squared errors",
            "url": "https://en.wikipedia.org/wiki/Mean_Squared_Error"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "tensor product",
            "url": "https://en.wikipedia.org/wiki/tensor_product"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent neural networks",
        "TPRs": "tensorproduct representations",
        "TPDNs": "Tensor Product Decomposition Networks",
        "TPDN": "tensor-product decomposition network",
        "SNLI": "Stanford Natural Language Inference",
        "SST": "Stanford Sentiment Treebank",
        "MSEs": "mean squared errors",
        "STS-B": "Semantic Textual Similarity Benchmark"
    },
    "highlights": [
        "Compositional symbolic representations are widely held to be necessary for intelligence (<a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\"><a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\">Newell, 1980</a></a>; <a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\"><a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\">Fodor & Pylyshyn, 1988</a></a>), in the domain of language (<a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\"><a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\">Montague, 1974</a></a>)",
        "Will the Tensor Product Decomposition Networks\u2019s success with digit-sequence autoencoders extend to models trained on naturally occurring data? We explore this question using sentence representations from four models: InferSent (<a class=\"ref-link\" id=\"cConneau_et+al_2017_a\" href=\"#rConneau_et+al_2017_a\">Conneau et al, 2017</a>), a BiLSTM trained on the Stanford Natural Language Inference (SNLI) corpus (<a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>); Skip-thought (<a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\">Kiros et al, 2015</a>), an LSTM trained to predict the sentence before or after a given sentence; the Stanford sentiment model (SST) (<a class=\"ref-link\" id=\"cSocher_et+al_2013_a\" href=\"#rSocher_et+al_2013_a\">Socher et al, 2013</a>), a tree-based recursive neural tensor network trained to predict movie review sentiment; and SPINN (<a class=\"ref-link\" id=\"cBowman_et+al_2016_a\" href=\"#rBowman_et+al_2016_a\">Bowman et al, 2016</a>), a tree-based Recurrent neural networks trained on Stanford Natural Language Inference",
        "What kind of internal representations could allow simple sequence-to-sequence models to perform the remarkable feats they do, including tasks previously thought to require compositional, symbolic representations? Our experiments show that, in heavily structure-sensitive tasks, sequence-to-sequence models learn representations that are extremely well approximated by tensorproduct representations (TPRs), distributed embeddings of symbol structures that enable powerful symbolic computation to be performed with neural operations (<a class=\"ref-link\" id=\"cSmolensky_2012_a\" href=\"#rSmolensky_2012_a\">Smolensky, 2012</a>)",
        "We demonstrated this by approximating learned representations via tensorproduct representations using the proposed tensor-product decomposition network (TPDN)",
        "Tensor Product Decomposition Networks applied to mainstream sentence-embedding models reveal that unstructured bag-of-words models provide a respectable approximation; this experiment provides evidence for a moderate degree of structuresensitivity",
        "A limitation of the current Tensor Product Decomposition Networks architecture is that it requires a hypothesis about the representations to be selected in advance"
    ],
    "key_statements": [
        "Compositional symbolic representations are widely held to be necessary for intelligence (<a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\"><a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\">Newell, 1980</a></a>; <a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\"><a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\">Fodor & Pylyshyn, 1988</a></a>), in the domain of language (<a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\"><a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\">Montague, 1974</a></a>)",
        "This hypothesis is supported by the spatial relationships between such vector representations, which have been argued to display geometric regularities that parallel plausible symbolic structures of the elements being represented (<a class=\"ref-link\" id=\"cMikolov_et+al_2013_a\" href=\"#rMikolov_et+al_2013_a\">Mikolov et al 2013</a>; see Figure 1)",
        "We introduce the Tensor Product Decomposition Network (TPDN) which takes a set of continuous vector representations to be analyzed and learns filler and role embeddings that best predict those vectors, given a particular hypothesis for the relevant set of roles",
        "For the representations learned by these autoencoders, Tensor Product Decomposition Networks find excellent approximations that are tensorproduct representations",
        "We show that standard Recurrent neural networks can induce compositional representations that are remarkably well approximated by tensorproduct representations and that the nature of these representations depends, in intrepretable ways, on the architecture and training task",
        "Evaluation: We evaluate how well a given sequence-to-sequence network can be approximated by a tensorproduct representations with a particular role scheme as follows",
        "Accuracy was lower (0.834) for the bidirectional architecture; this might mean that the hidden size of 60 becomes too small when divided into two 30-dimensional halves, one half for each direction",
        "Quality of Tensor Product Decomposition Networks approximation: For each of the six role schemes, we fitted a Tensor Product Decomposition Networks to the vectors generated by the trained encoder, and evaluated it using substitution accuracy (Section 2.1)",
        "Left-to-right roles are fairly successful, and right-to-left roles are decidedly unsuccessful. This asymmetry suggests that the unidirectional network",
        "Will the Tensor Product Decomposition Networks\u2019s success with digit-sequence autoencoders extend to models trained on naturally occurring data? We explore this question using sentence representations from four models: InferSent (<a class=\"ref-link\" id=\"cConneau_et+al_2017_a\" href=\"#rConneau_et+al_2017_a\">Conneau et al, 2017</a>), a BiLSTM trained on the Stanford Natural Language Inference (SNLI) corpus (<a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>); Skip-thought (<a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\">Kiros et al, 2015</a>), an LSTM trained to predict the sentence before or after a given sentence; the Stanford sentiment model (SST) (<a class=\"ref-link\" id=\"cSocher_et+al_2013_a\" href=\"#rSocher_et+al_2013_a\">Socher et al, 2013</a>), a tree-based recursive neural tensor network trained to predict movie review sentiment; and SPINN (<a class=\"ref-link\" id=\"cBowman_et+al_2016_a\" href=\"#rBowman_et+al_2016_a\">Bowman et al, 2016</a>), a tree-based Recurrent neural networks trained on Stanford Natural Language Inference",
        "InferSent is better approximated with structural roles than with bag-of-words roles, but all structural role schemes perform",
        "Though work on novel architectures often focuses on the encoder, this finding suggests that focusing on the decoder may be more fruitful for getting neural networks to learn specific types of representations",
        "What kind of internal representations could allow simple sequence-to-sequence models to perform the remarkable feats they do, including tasks previously thought to require compositional, symbolic representations? Our experiments show that, in heavily structure-sensitive tasks, sequence-to-sequence models learn representations that are extremely well approximated by tensorproduct representations (TPRs), distributed embeddings of symbol structures that enable powerful symbolic computation to be performed with neural operations (<a class=\"ref-link\" id=\"cSmolensky_2012_a\" href=\"#rSmolensky_2012_a\">Smolensky, 2012</a>)",
        "We demonstrated this by approximating learned representations via tensorproduct representations using the proposed tensor-product decomposition network (TPDN)",
        "Variations in architecture and task were shown to induce different types and degrees of structure-sensitivity in representations, with the decoder playing a greater role than the encoder in determining the structure of the learned representation",
        "Tensor Product Decomposition Networks applied to mainstream sentence-embedding models reveal that unstructured bag-of-words models provide a respectable approximation; this experiment provides evidence for a moderate degree of structuresensitivity",
        "The presence of structure-sensitivity is corroborated by targeted analogy tests motivated by the linearity of tensorproduct representations",
        "A limitation of the current Tensor Product Decomposition Networks architecture is that it requires a hypothesis about the representations to be selected in advance",
        "A fruitful future research direction would be to automatically explore hypotheses about the nature of the tensorproduct representations encoded by a network"
    ],
    "summary": [
        "Compositional symbolic representations are widely held to be necessary for intelligence (<a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\"><a class=\"ref-link\" id=\"cNewell_1980_a\" href=\"#rNewell_1980_a\">Newell, 1980</a></a>; <a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\"><a class=\"ref-link\" id=\"cFodor_1988_a\" href=\"#rFodor_1988_a\">Fodor & Pylyshyn, 1988</a></a>), in the domain of language (<a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\"><a class=\"ref-link\" id=\"cMontague_1974_a\" href=\"#rMontague_1974_a\">Montague, 1974</a></a>).",
        "For the representations learned by these autoencoders, TPDNs find excellent approximations that are TPRs. In Section 3, we turn to sentence-embedding models from the contemporary literature.",
        "Combined with our finding that standard sentence encoders do not seem to learn robust representations of structure, these findings suggest that more structured architectures or more structure-dependent training tasks could improve the compositional capabilities of existing models.",
        "The Tensor Product Decomposition Network (TPDN), depicted in Figure 2c, learns a TPR that best approximates an existing set of vector encodings.",
        "Hypothesis: We hypothesize that RNN autoencoders will learn to use role representations that parallel their architectures: left-to-right roles for a unidirectional network, bidirectional roles for a bidirectional network, and tree-position roles for a tree-based network.",
        "Quality of TPDN approximation: For each of the six role schemes, we fitted a TPDN to the vectors generated by the trained encoder, and evaluated it using substitution accuracy (Section 2.1).",
        "The other possible explanations mentioned above\u2212namely, the possibilities that the models use alternate role schemes that we did not test or that they use some structural encoding other than tensor product representation\u2212still remain.",
        "The previous section suggested that all sentence models surveyed did not robustly encode structure and could even be approximated fairly well with a bag of words.",
        "Though work on novel architectures often focuses on the encoder, this finding suggests that focusing on the decoder may be more fruitful for getting neural networks to learn specific types of representations.",
        "This result might explain why the sentence embedding models do not seem to robustly encode structure: perhaps the training tasks for these models do not heavily rely on sentence structure (e.g. <a class=\"ref-link\" id=\"cParikh_et+al_2016_a\" href=\"#rParikh_et+al_2016_a\">Parikh et al (2016</a>) achieved high accuracy on SNLI using a model that ignores word order), such that the models learn to ignore structural information, as was the case with models trained on sorting.",
        "In heavily structure-sensitive tasks, sequence-to-sequence models learn representations that are extremely well approximated by tensorproduct representations (TPRs), distributed embeddings of symbol structures that enable powerful symbolic computation to be performed with neural operations (<a class=\"ref-link\" id=\"cSmolensky_2012_a\" href=\"#rSmolensky_2012_a\">Smolensky, 2012</a>).",
        "We demonstrated this by approximating learned representations via TPRs using the proposed tensor-product decomposition network (TPDN).",
        "Variations in architecture and task were shown to induce different types and degrees of structure-sensitivity in representations, with the decoder playing a greater role than the encoder in determining the structure of the learned representation.",
        "A fruitful future research direction would be to automatically explore hypotheses about the nature of the TPR encoded by a network"
    ],
    "headline": "We introduce Tensor Product Decomposition Networks, which use tensorproduct representations to approximate existing vector representations",
    "reference_links": [
        {
            "id": "Anandkumar_et+al_2014_a",
            "entry": "Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773\u20132832, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014"
        },
        {
            "id": "Battaglia_et+al_2018_a",
            "entry": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01261"
        },
        {
            "id": "Belinkov_et+al_2017_a",
            "entry": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 861\u2013872, Vancouver, Canada, 2017. Association for Computational Linguistics. URL http://aclweb.org/anthology/P17-1080.",
            "url": "http://aclweb.org/anthology/P17-1080",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belinkov%2C%20Yonatan%20Durrani%2C%20Nadir%20Dalvi%2C%20Fahim%20Sajjad%2C%20Hassan%20What%20do%20neural%20machine%20translation%20models%20learn%20about%20morphology%3F%202017"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632\u2013642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. URL http://aclweb.org/anthology/D15-1075.",
            "url": "http://aclweb.org/anthology/D15-1075",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20Samuel%20R.%20Angeli%2C%20Gabor%20Potts%2C%20Christopher%20Manning%2C%20Christopher%20D.%20A%20large%20annotated%20corpus%20for%20learning%20natural%20language%20inference%202015-09"
        },
        {
            "id": "Bowman_et+al_2016_a",
            "entry": "Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. A fast unified model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1466\u20131477. Association for Computational Linguistics, 2016. URL http://aclweb.org/anthology/P16-1139.",
            "url": "http://aclweb.org/anthology/P16-1139",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20Samuel%20R.%20Gauthier%2C%20Jon%20Rastogi%2C%20Abhinav%20Gupta%2C%20Raghav%20A%20fast%20unified%20model%20for%20parsing%20and%20sentence%20understanding%202016"
        },
        {
            "id": "Cer_et+al_2017_a",
            "entry": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1\u2013 14. Association for Computational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL http://www.aclweb.org/anthology/S17-2001.",
            "crossref": "https://dx.doi.org/10.18653/v1/S17-2001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/S17-2001"
        },
        {
            "id": "Chen_et+al_1945_a",
            "entry": "Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. Improved neural machine translation with a syntax-aware encoder and decoder. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1936\u2013 1945. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1177. URL http://www.aclweb.org/anthology/P17-1177.",
            "crossref": "https://dx.doi.org/10.18653/v1/P17-1177",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/P17-1177"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. arXiv preprint arXiv:1802.03691, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03691"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Conneau_2018_a",
            "entry": "Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. In International Conference on Language Resources and Evaluation, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Senteval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Senteval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018"
        },
        {
            "id": "Conneau_et+al_2017_a",
            "entry": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u0131c Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670\u2013680. Association for Computational Linguistics, 2017. URL http://aclweb.org/anthology/D17-1070.",
            "url": "http://aclweb.org/anthology/D17-1070",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017"
        },
        {
            "id": "Dasgupta_et+al_2018_a",
            "entry": "Ishita Dasgupta, Demi Guo, Andreas Stuhlmuller, Samuel J. Gershman, and Noah D. Goodman. Evaluating compositionality in sentence embeddings. In Proceedings of the 40th Annual Conference of the Cognitive Science Society, 2018. URL https://arxiv.org/abs/1802.04302.",
            "url": "https://arxiv.org/abs/1802.04302",
            "arxiv_url": "https://arxiv.org/pdf/1802.04302"
        },
        {
            "id": "Dolan_et+al_2004_a",
            "entry": "Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004. URL http://www.aclweb.org/anthology/C04-1051.",
            "url": "http://www.aclweb.org/anthology/C04-1051",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dolan%2C%20Bill%20Quirk%2C%20Chris%20Brockett%2C%20Chris%20Unsupervised%20construction%20of%20large%20paraphrase%20corpora%3A%20Exploiting%20massively%20parallel%20news%20sources%202004"
        },
        {
            "id": "Ettinger_et+al_2018_a",
            "entry": "Allyson Ettinger, Ahmed Elgohary, Colin Phillips, and Philip Resnik. Assessing composition in sentence vector representations. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 1790\u20131801. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/C18-1152.",
            "url": "http://aclweb.org/anthology/C18-1152",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ettinger%2C%20Allyson%20Elgohary%2C%20Ahmed%20Phillips%2C%20Colin%20Resnik%2C%20Philip%20Assessing%20composition%20in%20sentence%20vector%20representations%202018"
        },
        {
            "id": "Fischer-Baum_et+al_2010_a",
            "entry": "Simon Fischer-Baum, Michael McCloskey, and Brenda Rapp. Representation of letter position in spelling: Evidence from acquired dysgraphia. Cognition, 115(3):466\u2013490, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fischer-Baum%2C%20Simon%20McCloskey%2C%20Michael%20Rapp%2C%20Brenda%20Representation%20of%20letter%20position%20in%20spelling%3A%20Evidence%20from%20acquired%20dysgraphia%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fischer-Baum%2C%20Simon%20McCloskey%2C%20Michael%20Rapp%2C%20Brenda%20Representation%20of%20letter%20position%20in%20spelling%3A%20Evidence%20from%20acquired%20dysgraphia%202010"
        },
        {
            "id": "Fodor_1988_a",
            "entry": "Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. Tensor product generation networks for deep nlp modeling. In Proceedings of NAACL, 2018. URL https://arxiv.org/abs/1709.09118.",
            "url": "https://arxiv.org/abs/1709.09118",
            "arxiv_url": "https://arxiv.org/pdf/1709.09118"
        },
        {
            "id": "Kadar_et+al_2017_a",
            "entry": "Akos Kadar, Grzegorz Chrupa\u0142a, and Afra Alishahi. Representation of linguistic form and function in recurrent neural networks. Computational Linguistics, 43(4):761\u2013780, 2017. URL http://aclweb.org/anthology/J17-4003.",
            "url": "http://aclweb.org/anthology/J17-4003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kadar%2C%20Akos%20Chrupa%C5%82a%2C%20Grzegorz%20Alishahi%2C%20Afra%20Representation%20of%20linguistic%20form%20and%20function%20in%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference for Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kiros_et+al_2015_a",
            "entry": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems, pp. 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "Koniusz_et+al_2017_a",
            "entry": "Piotr Koniusz, Fei Yan, Philippe-Henri Gosselin, and Krystian Mikolajczyk. Higher-order occurrence pooling for bags-of-words: Visual concept detection. IEEE transactions on pattern analysis and machine intelligence, 39(2):313\u2013326, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koniusz%2C%20Piotr%20Yan%2C%20Fei%20Gosselin%2C%20Philippe-Henri%20Mikolajczyk%2C%20Krystian%20Higher-order%20occurrence%20pooling%20for%20bags-of-words%3A%20Visual%20concept%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koniusz%2C%20Piotr%20Yan%2C%20Fei%20Gosselin%2C%20Philippe-Henri%20Mikolajczyk%2C%20Krystian%20Higher-order%20occurrence%20pooling%20for%20bags-of-words%3A%20Visual%20concept%20detection%202017"
        },
        {
            "id": "Levy_et+al_2018_a",
            "entry": "Omer Levy, Kenton Lee, Nicholas FitzGerald, and Luke Zettlemoyer. Long short-term memory as a dynamically computed element-wise weighted sum. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 732\u2013739. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/ P18-2116.",
            "url": "http://aclweb.org/anthology/P18-2116",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Lee%2C%20Kenton%20FitzGerald%2C%20Nicholas%20Zettlemoyer%2C%20Luke%20Long%20short-term%20memory%20as%20a%20dynamically%20computed%20element-wise%20weighted%20sum%202018"
        },
        {
            "id": "Linzen_et+al_2016_a",
            "entry": "Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of LSTMs to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521\u2013 535, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20LSTMs%20to%20learn%20syntaxsensitive%20dependencies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20LSTMs%20to%20learn%20syntaxsensitive%20dependencies%202016"
        },
        {
            "id": "Merity_et+al_2016_a",
            "entry": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07843"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pp. 746\u2013751, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013"
        },
        {
            "id": "Montague_1974_a",
            "entry": "Richard Montague. English as a formal language. In Richard Thomason (ed.), Formal Philosophy. Selected papers by Richard Montague, pp. 188\u2013221. Yale University Press, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montague%2C%20Richard%20English%20as%20a%20formal%20language%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montague%2C%20Richard%20English%20as%20a%20formal%20language%201974"
        },
        {
            "id": "Newell_1980_a",
            "entry": "Allen Newell. Physical symbol systems. Cognitive Science, 4(2):135\u2013183, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newell%2C%20Allen%20Physical%20symbol%20systems%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newell%2C%20Allen%20Physical%20symbol%20systems%201980"
        },
        {
            "id": "Norman_et+al_2006_a",
            "entry": "Kenneth A. Norman, Sean M. Polyn, Greg J. Detre, and James V. Haxby. Beyond mind-reading: multi-voxel pattern analysis of fMRI data. Trends in Cognitive Sciences, 10(9):424\u2013430, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Norman%2C%20Kenneth%20A.%20Polyn%2C%20Sean%20M.%20Detre%2C%20Greg%20J.%20Haxby%2C%20James%20V.%20Beyond%20mind-reading%3A%20multi-voxel%20pattern%20analysis%20of%20fMRI%20data%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Norman%2C%20Kenneth%20A.%20Polyn%2C%20Sean%20M.%20Detre%2C%20Greg%20J.%20Haxby%2C%20James%20V.%20Beyond%20mind-reading%3A%20multi-voxel%20pattern%20analysis%20of%20fMRI%20data%202006"
        },
        {
            "id": "Omlin_1996_a",
            "entry": "Christian W. Omlin and C. Lee Giles. Extraction of rules from discrete-time recurrent neural networks. Neural Networks, 9(1):41\u201352, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omlin%2C%20Christian%20W.%20Giles%2C%20C.Lee%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omlin%2C%20Christian%20W.%20Giles%2C%20C.Lee%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996"
        },
        {
            "id": "Palangi_et+al_2018_a",
            "entry": "Hamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. Question-answering with grammatically-interpretable representations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17090.",
            "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17090",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palangi%2C%20Hamid%20Smolensky%2C%20Paul%20He%2C%20Xiaodong%20Deng%2C%20Li%20Question-answering%20with%20grammatically-interpretable%20representations%202018-02-02"
        },
        {
            "id": "Parikh_et+al_2016_a",
            "entry": "Ankur Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2249\u20132255. Association for Computational Linguistics, 2016. doi: 10.18653/v1/D16-1244. URL http://aclweb.org/anthology/D16-1244.",
            "crossref": "https://dx.doi.org/10.18653/v1/D16-1244",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/D16-1244"
        },
        {
            "id": "Pavlick_2017_a",
            "entry": "Ellie Pavlick. Compositional lexical semantics in natural language inference. PhD thesis, University of Pennsylvania, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pavlick%2C%20Ellie%20Compositional%20lexical%20semantics%20in%20natural%20language%20inference%202017"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Plate_1995_a",
            "entry": "Tony A. Plate. Holographic reduced representations. IEEE Transactions on Neural networks, 6(3): 623\u2013641, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Plate%2C%20Tony%20A.%20Holographic%20reduced%20representations%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Plate%2C%20Tony%20A.%20Holographic%20reduced%20representations%201995"
        },
        {
            "id": "Poliak_et+al_2018_a",
            "entry": "Adam Poliak, Yonatan Belinkov, James Glass, and Benjamin Van Durme. On the evaluation of semantic phenomena in neural machine translation using natural language inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 513\u2013523. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-2082.",
            "url": "http://aclweb.org/anthology/N18-2082",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poliak%2C%20Adam%20Belinkov%2C%20Yonatan%20Glass%2C%20James%20Durme%2C%20Benjamin%20Van%20On%20the%20evaluation%20of%20semantic%20phenomena%20in%20neural%20machine%20translation%20using%20natural%20language%20inference%202018"
        },
        {
            "id": "Pollack_1990_a",
            "entry": "Jordan B. Pollack. Recursive distributed representations. Artificial Intelligence, 46(1-2):77\u2013105, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pollack%2C%20Jordan%20B.%20Recursive%20distributed%20representations%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pollack%2C%20Jordan%20B.%20Recursive%20distributed%20representations%201990"
        },
        {
            "id": "Schlag_2018_a",
            "entry": "Imanol Schlag and Jurgen Schmidhuber. Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems, pp. 10003\u201310014, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Learning%20to%20reason%20with%20third%20order%20tensor%20products%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Learning%20to%20reason%20with%20third%20order%20tensor%20products%202018"
        },
        {
            "id": "Schuster_1997_a",
            "entry": "Mike Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673\u20132681, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuster%2C%20Mike%20Paliwal%2C%20Kuldip%20K.%20Bidirectional%20recurrent%20neural%20networks%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuster%2C%20Mike%20Paliwal%2C%20Kuldip%20K.%20Bidirectional%20recurrent%20neural%20networks%201997"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural MT learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1526\u20131534. Association for Computational Linguistics, 2016. doi: 10.18653/v1/D16-1159. URL http://www.aclweb.org/anthology/D16-1159.",
            "crossref": "https://dx.doi.org/10.18653/v1/D16-1159",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/D16-1159"
        },
        {
            "id": "Sirovich_1987_a",
            "entry": "Lawrence Sirovich and Michael Kirby. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3):519\u2013524, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sirovich%2C%20Lawrence%20Kirby%2C%20Michael%20Low-dimensional%20procedure%20for%20the%20characterization%20of%20human%20faces%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sirovich%2C%20Lawrence%20Kirby%2C%20Michael%20Low-dimensional%20procedure%20for%20the%20characterization%20of%20human%20faces%201987"
        },
        {
            "id": "Smolensky_1990_a",
            "entry": "Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1-2):159\u2013216, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20Tensor%20product%20variable%20binding%20and%20the%20representation%20of%20symbolic%20structures%20in%20connectionist%20systems%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20Paul%20Tensor%20product%20variable%20binding%20and%20the%20representation%20of%20symbolic%20structures%20in%20connectionist%20systems%201990"
        },
        {
            "id": "Smolensky_2012_a",
            "entry": "Paul Smolensky. Symbolic functions from neural computation. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 370(1971):3543\u20133569, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20Symbolic%20functions%20from%20neural%20computation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20Paul%20Symbolic%20functions%20from%20neural%20computation%202012"
        },
        {
            "id": "Socher_et+al_2010_a",
            "entry": "Richard Socher, Christopher D. Manning, and Andrew Y. Ng. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Learning%20continuous%20phrase%20representations%20and%20syntactic%20parsing%20with%20recursive%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Learning%20continuous%20phrase%20representations%20and%20syntactic%20parsing%20with%20recursive%20neural%20networks%202010"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1556\u20131566, Beijing, China, July 2015. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1150.",
            "url": "http://www.aclweb.org/anthology/P15-1150",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tai%2C%20Kai%20Sheng%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Improved%20semantic%20representations%20from%20tree-structured%20long%20short-term%20memory%20networks%202015-07"
        },
        {
            "id": "Tang_et+al_2018_a",
            "entry": "Shuai Tang, Paul Smolensky, and Virginia R de Sa. Learning distributed representations of symbolic structure using binding and unbinding operations. NeurIPS Workshop https://openreview.net/forum?id=r1zvGR6jjm and arXiv preprint arXiv:1810.12456, 2018.",
            "url": "https://openreview.net/forum?id=r1zvGR6jjm",
            "arxiv_url": "https://arxiv.org/pdf/1810.12456"
        },
        {
            "id": "Tenenbaum_2000_a",
            "entry": "Joshua B Tenenbaum and William T Freeman. Separating style and content with bilinear models. Neural computation, 12(6):1247\u20131283, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20Joshua%20B.%20Freeman%2C%20William%20T.%20Separating%20style%20and%20content%20with%20bilinear%20models%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20Joshua%20B.%20Freeman%2C%20William%20T.%20Separating%20style%20and%20content%20with%20bilinear%20models%202000"
        },
        {
            "id": "Turk_1991_a",
            "entry": "Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3 (1):71\u201386, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turk%2C%20Matthew%20Pentland%2C%20Alex%20Eigenfaces%20for%20recognition.%20Journal%20of%20cognitive%20neuroscience%2C%203%201991"
        },
        {
            "id": "Vasilescu_2002_a",
            "entry": "M. Alex O. Vasilescu and Demetri Terzopoulos. Multilinear image analysis for facial recognition. In Proceedings of the 16th International Conference on Pattern Recognition, 2002., volume 2, pp. 511\u2013514. IEEE, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilescu%2C%20M.Alex%20O.%20Terzopoulos%2C%20Demetri%20Multilinear%20image%20analysis%20for%20facial%20recognition%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilescu%2C%20M.Alex%20O.%20Terzopoulos%2C%20Demetri%20Multilinear%20image%20analysis%20for%20facial%20recognition%202002"
        },
        {
            "id": "Vasilescu_2005_a",
            "entry": "M. Alex O. Vasilescu and Demetri Terzopoulos. Multilinear independent components analysis. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, pp. 547\u2013553. IEEE, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilescu%2C%20M.Alex%20O.%20Terzopoulos%2C%20Demetri%20Multilinear%20independent%20components%20analysis%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilescu%2C%20M.Alex%20O.%20Terzopoulos%2C%20Demetri%20Multilinear%20independent%20components%20analysis%202005"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998\u20136008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017"
        },
        {
            "id": "Weiss_et+al_2018_a",
            "entry": "Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. In ICML, pp. 5244\u20135253, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weiss%2C%20Gail%20Goldberg%2C%20Yoav%20Yahav%2C%20Eran%20Extracting%20automata%20from%20recurrent%20neural%20networks%20using%20queries%20and%20counterexamples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weiss%2C%20Gail%20Goldberg%2C%20Yoav%20Yahav%2C%20Eran%20Extracting%20automata%20from%20recurrent%20neural%20networks%20using%20queries%20and%20counterexamples%202018"
        },
        {
            "id": "Wickelgren_1969_a",
            "entry": "Wayne A. Wickelgren. Context-sensitive coding, associative memory, and serial order in (speech) behavior. Psychological Review, 76(1):1\u201315, 1969.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wickelgren%2C%20Wayne%20A.%20Context-sensitive%20coding%2C%20associative%20memory%2C%20and%20serial%20order%20in%20%28speech%29%20behavior%201969",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wickelgren%2C%20Wayne%20A.%20Context-sensitive%20coding%2C%20associative%20memory%2C%20and%20serial%20order%20in%20%28speech%29%20behavior%201969"
        },
        {
            "id": "Two_1990_a",
            "entry": "Two of the parameters that must be provided to the TPDN are the dimensionality of the filler embeddings and the dimensionality of the role embeddings. We explore the effects of these parameters in Figure 4. For the role embeddings, substitution accuracy increases noticeably with each increase in dimensionality until the dimensionality hits 6, where accuracy plateaus. This behavior is likely due to the fact that the reversal seq2seq network is most likely to employ right-to-left roles, which involves 6 possible roles in this setting. A dimensionality of 6 is therefore the minimum embedding size needed to make the role vectors linearly independent; linear independence is an important property for the fidelity of a tensor product representation (Smolensky, 1990). The accuracy also generally increases as filler dimensionality increases, but there is a less clear point where it plateaus for the fillers than for the roles.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Two%20of%20the%20parameters%20that%20must%20be%20provided%20to%20the%20TPDN%20are%20the%20dimensionality%20of%20the%20filler%20embeddings%20and%20the%20dimensionality%20of%20the%20role%20embeddings%20We%20explore%20the%20effects%20of%20these%20parameters%20in%20Figure%204%20For%20the%20role%20embeddings%20substitution%20accuracy%20increases%20noticeably%20with%20each%20increase%20in%20dimensionality%20until%20the%20dimensionality%20hits%206%20where%20accuracy%20plateaus%20This%20behavior%20is%20likely%20due%20to%20the%20fact%20that%20the%20reversal%20seq2seq%20network%20is%20most%20likely%20to%20employ%20righttoleft%20roles%20which%20involves%206%20possible%20roles%20in%20this%20setting%20A%20dimensionality%20of%206%20is%20therefore%20the%20minimum%20embedding%20size%20needed%20to%20make%20the%20role%20vectors%20linearly%20independent%20linear%20independence%20is%20an%20important%20property%20for%20the%20fidelity%20of%20a%20tensor%20product%20representation%20Smolensky%201990%20The%20accuracy%20also%20generally%20increases%20as%20filler%20dimensionality%20increases%20but%20there%20is%20a%20less%20clear%20point%20where%20it%20plateaus%20for%20the%20fillers%20than%20for%20the%20roles",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Two%20of%20the%20parameters%20that%20must%20be%20provided%20to%20the%20TPDN%20are%20the%20dimensionality%20of%20the%20filler%20embeddings%20and%20the%20dimensionality%20of%20the%20role%20embeddings%20We%20explore%20the%20effects%20of%20these%20parameters%20in%20Figure%204%20For%20the%20role%20embeddings%20substitution%20accuracy%20increases%20noticeably%20with%20each%20increase%20in%20dimensionality%20until%20the%20dimensionality%20hits%206%20where%20accuracy%20plateaus%20This%20behavior%20is%20likely%20due%20to%20the%20fact%20that%20the%20reversal%20seq2seq%20network%20is%20most%20likely%20to%20employ%20righttoleft%20roles%20which%20involves%206%20possible%20roles%20in%20this%20setting%20A%20dimensionality%20of%206%20is%20therefore%20the%20minimum%20embedding%20size%20needed%20to%20make%20the%20role%20vectors%20linearly%20independent%20linear%20independence%20is%20an%20important%20property%20for%20the%20fidelity%20of%20a%20tensor%20product%20representation%20Smolensky%201990%20The%20accuracy%20also%20generally%20increases%20as%20filler%20dimensionality%20increases%20but%20there%20is%20a%20less%20clear%20point%20where%20it%20plateaus%20for%20the%20fillers%20than%20for%20the%20roles"
        },
        {
            "id": "The_1995_a",
            "entry": "The body of the paper focused on using the tensor product (fi \u2297 ri, see Figure 2b) as the operation for binding fillers to roles. There are other conceivable binding operations. Here we test two alternatives, both of which can be viewed as special cases of the tensor product or as related to it: circular convolution, which is used in holographic reduced representations (Plate, 1995), and elementwise product (fi ri). Both of these are restricted such that roles and fillers must have the same embedding dimension (Nf = Nr). We first try setting this dimension to 20, which is what was used as both the role and filler dimension in all tensor product experiments with digit sequences.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20body%20of%20the%20paper%20focused%20on%20using%20the%20tensor%20product%20fi%20%20ri%20see%20Figure%202b%20as%20the%20operation%20for%20binding%20fillers%20to%20roles%20There%20are%20other%20conceivable%20binding%20operations%20Here%20we%20test%20two%20alternatives%20both%20of%20which%20can%20be%20viewed%20as%20special%20cases%20of%20the%20tensor%20product%20or%20as%20related%20to%20it%20circular%20convolution%20which%20is%20used%20in%20holographic%20reduced%20representations%20Plate%201995%20and%20elementwise%20product%20fi%20ri%20Both%20of%20these%20are%20restricted%20such%20that%20roles%20and%20fillers%20must%20have%20the%20same%20embedding%20dimension%20Nf%20%20Nr%20We%20first%20try%20setting%20this%20dimension%20to%2020%20which%20is%20what%20was%20used%20as%20both%20the%20role%20and%20filler%20dimension%20in%20all%20tensor%20product%20experiments%20with%20digit%20sequences"
        }
    ]
}
