{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE TURING COMPLETENESS OF MODERN NEURAL NETWORK ARCHITECTURES",
        "author": "Jorge Perez, Javier Marinkovic, Pablo Barcelo Department of Computer Science, Universidad de Chile & IMFD Chile {jperez,jmarinkovic,pbarcelo}@dcc.uchile.cl",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyGBdo0qFm"
        },
        "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (<a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\">Kaiser & Sutskever, 2016</a>). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results."
    },
    "keywords": [
        {
            "term": "finite automaton",
            "url": "https://en.wikipedia.org/wiki/finite_automaton"
        },
        {
            "term": "external memory",
            "url": "https://en.wikipedia.org/wiki/external_memory"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "ICLR",
            "url": "https://en.wikipedia.org/wiki/ICLR"
        },
        {
            "term": "Neural Turing Machine",
            "url": "https://en.wikipedia.org/wiki/Neural_Turing_Machine"
        },
        {
            "term": "turing completeness",
            "url": "https://en.wikipedia.org/wiki/turing_completeness"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "automata",
            "url": "https://en.wikipedia.org/wiki/automata"
        },
        {
            "term": "computational power",
            "url": "https://en.wikipedia.org/wiki/computational_power"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks",
        "NTM": "Neural Turing Machine"
    },
    "highlights": [
        "There is an increasing interest in designing neural network architectures capable of learning algorithms from examples (<a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\"><a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\">Grefenstette et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\"><a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\">Joulin & Mikolov, 2015</a></a>; <a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\"><a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\">Kaiser & Sutskever, 2016</a></a>; <a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\"><a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\">Kurach et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\"><a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\">Dehghani et al, 2018</a></a>)",
        "Turing completeness often follows for these networks as they can be seen as a control unit with access to an unbounded memory; as such, they are capable of simulating any Turing machine",
        "For the case of the Transformer we provide a direct simulation of a Turing machine, while for the case of the Neural GPU our result follows by simulating standard sequence-to-sequence recurrent neural networks",
        "One strategy has been the addition of inductive biases in the form of external memory, being the Neural Turing Machine (NTM) (<a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a>) a paradigmatic example",
        "Our results in this paper are somehow orthogonal to the previous argument; we show that one can leverage the dense representations of the Neural GPU cells to obtain Turing completeness without requiring to add cells beyond the ones used to store the input",
        "Our proof of Turing completeness for the Transformer requires the presence of residual connections, i.e., the +xi, +ai, +yi, and +pi summands in Equations (6-11), while our proof for Neural GPUs heavily relies on the gating mechanism"
    ],
    "key_statements": [
        "There is an increasing interest in designing neural network architectures capable of learning algorithms from examples (<a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\"><a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\">Grefenstette et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\"><a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\">Joulin & Mikolov, 2015</a></a>; <a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\"><a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\">Kaiser & Sutskever, 2016</a></a>; <a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\"><a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\">Kurach et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\"><a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\">Dehghani et al, 2018</a></a>)",
        "Turing completeness often follows for these networks as they can be seen as a control unit with access to an unbounded memory; as such, they are capable of simulating any Turing machine",
        "The view proposed by Siegelmann & Sontag shows that it is possible to release the full computational power of recurrent neural networks without arbitrarily increasing its model complexity",
        "For the case of the Transformer we provide a direct simulation of a Turing machine, while for the case of the Neural GPU our result follows by simulating standard sequence-to-sequence recurrent neural networks",
        "One strategy has been the addition of inductive biases in the form of external memory, being the Neural Turing Machine (NTM) (<a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a>) a paradigmatic example",
        "Our results in this paper are somehow orthogonal to the previous argument; we show that one can leverage the dense representations of the Neural GPU cells to obtain Turing completeness without requiring to add cells beyond the ones used to store the input",
        "We have presented an analysis of the Turing completeness of two popular neural architectures for sequence-processing tasks; namely, the Transformer, based on attention, and the Neural GPU, based on recurrent convolutions",
        "Our proof of Turing completeness for the Transformer requires the presence of residual connections, i.e., the +xi, +ai, +yi, and +pi summands in Equations (6-11), while our proof for Neural GPUs heavily relies on the gating mechanism"
    ],
    "summary": [
        "There is an increasing interest in designing neural network architectures capable of learning algorithms from examples (<a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\"><a class=\"ref-link\" id=\"cGrefenstette_et+al_2015_a\" href=\"#rGrefenstette_et+al_2015_a\">Grefenstette et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\"><a class=\"ref-link\" id=\"cJoulin_2015_a\" href=\"#rJoulin_2015_a\">Joulin & Mikolov, 2015</a></a>; <a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\"><a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\">Kaiser & Sutskever, 2016</a></a>; <a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\"><a class=\"ref-link\" id=\"cKurach_et+al_2016_a\" href=\"#rKurach_et+al_2016_a\">Kurach et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\"><a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\">Dehghani et al, 2018</a></a>).",
        "The complete Tansformer A Transformer network receives an input sequence X, a seed vector y0, and a value r \u2208 N.",
        "As for the case of the embedding functions, we require the positional encoding pos(i) to be computable by a Turing machine working in linear time w.r.t. the size of i.",
        "There are several other details in the construction, in particular, at the beginning of the computation, the decoder needs to attend to the encoder and copy the input symbols so they can later be processed as described above.",
        "The complete construction uses one encoder layer, three decoder layers and vectors of dimension d = 2|Q| + 4|\u03a3| + 11 to store one-hot representations of states, symbols and some additional working space.",
        "The general architecture that we presented closely follows that of Vaswani et al (2017), some choices for functions and parameters in our positive results are different to the usual choices in practice.",
        "Functions U (\u00b7), R(\u00b7), and F (\u00b7) are defined as a convolution of its input with a 4-dimensional kernel bank with shape plus a bias tensor, followed by a point-wise transformation f (K \u2217 S + B)",
        "Bidimensional tensors and piecewise linear activations <a class=\"ref-link\" id=\"cFreivalds_2018_a\" href=\"#rFreivalds_2018_a\">Freivalds & Liepins (2018</a>) simplified Neural GPUs and proved that, by considering piecewise linear activations and bidimensional input tensors instead of the original smooth activations and tridimensional tensors used by <a class=\"ref-link\" id=\"cKaiser_2016_a\" href=\"#rKaiser_2016_a\">Kaiser & Sutskever (2016</a>), it is possible to achieve substantially better results in terms of training time and generalization.",
        "Our Turing completeness proof relies on a bidimensional tensor and uses piecewise linear activations, providing theoretical evidence that these simplifications retain the full expressiveness of Neural GPUs while simplifying its practical applicability.",
        "We have presented an analysis of the Turing completeness of two popular neural architectures for sequence-processing tasks; namely, the Transformer, based on attention, and the Neural GPU, based on recurrent convolutions.",
        "Our proof of Turing completeness for the Transformer requires the presence of residual connections, i.e., the +xi, +ai, +yi, and +pi summands in Equations (6-11), while our proof for Neural GPUs heavily relies on the gating mechanism.",
        "We closely follow their original definitions, some choices for functions and parameters in our positive results are different to the usual choices in practice, most notably, the use of hard attention for the case of the Transformer, and the piecewise linear activation functions for both architectures."
    ],
    "headline": "We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer and the Neural GPU",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409.0473.",
            "url": "http://arxiv.org/abs/1409.0473",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. In NAACL-HLT 2018, pp. 2261\u20132271, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yining%20Gilroy%2C%20Sorcha%20Maletti%2C%20Andreas%20May%2C%20Jonathan%20Recurrent%20neural%20networks%20as%20weighted%20language%20recognizers%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yining%20Gilroy%2C%20Sorcha%20Maletti%2C%20Andreas%20May%2C%20Jonathan%20Recurrent%20neural%20networks%20as%20weighted%20language%20recognizers%202018"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Cybenko_1989_a",
            "entry": "George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS, 2(4):303\u2013314, 1989. doi: 10.1007/BF02551274. URL https://doi.org/10.1007/BF02551274.",
            "crossref": "https://dx.doi.org/10.1007/BF02551274",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/BF02551274"
        },
        {
            "id": "Dehghani_et+al_2018_a",
            "entry": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. CoRR, abs/1807.03819, 2018. URL https://arxiv.org/abs/1807.03819.",
            "url": "https://arxiv.org/abs/1807.03819",
            "arxiv_url": "https://arxiv.org/pdf/1807.03819"
        },
        {
            "id": "Freivalds_2018_a",
            "entry": "Karlis Freivalds and Renars Liepins. Improving the neural GPU architecture for algorithm learning. In Neural Abstract Machines & Program Induction (NAMPI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freivalds%2C%20Karlis%20Liepins%2C%20Renars%20Improving%20the%20neural%20GPU%20architecture%20for%20algorithm%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freivalds%2C%20Karlis%20Liepins%2C%20Renars%20Improving%20the%20neural%20GPU%20architecture%20for%20algorithm%20learning%202018"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Grefenstette_et+al_2015_a",
            "entry": "Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp. 1828\u20131836, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06-27"
        },
        {
            "id": "He_et+al_2016_b",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 630\u2013645, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiming%20He%20Xiangyu%20Zhang%20Shaoqing%20Ren%20and%20Jian%20Sun%20Identity%20mappings%20in%20deep%20residual%20networks%20In%20Computer%20Vision%20%20ECCV%202016%20%2014th%20European%20Conference%20Amsterdam%20The%20Netherlands%20October%201114%202016%20Proceedings%20Part%20IV%20pp%20630645%202016b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiming%20He%20Xiangyu%20Zhang%20Shaoqing%20Ren%20and%20Jian%20Sun%20Identity%20mappings%20in%20deep%20residual%20networks%20In%20Computer%20Vision%20%20ECCV%202016%20%2014th%20European%20Conference%20Amsterdam%20The%20Netherlands%20October%201114%202016%20Proceedings%20Part%20IV%20pp%20630645%202016b"
        },
        {
            "id": "Joulin_2015_a",
            "entry": "Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural information processing systems, pp. 190\u2013198, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015"
        },
        {
            "id": "Kaiser_2016_a",
            "entry": "Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiser%2C%20Lukasz%20Sutskever%2C%20Ilya%20Neural%20GPUs%20learn%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiser%2C%20Lukasz%20Sutskever%2C%20Ilya%20Neural%20GPUs%20learn%20algorithms%202016"
        },
        {
            "id": "Kleene_1956_a",
            "entry": "S. C. Kleene. Representation of events in nerve nets and finite automata. In Claude Shannon and John McCarthy (eds.), Automata Studies, pp. 3\u201341. Princeton University Press, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleene%2C%20S.C.%20Representation%20of%20events%20in%20nerve%20nets%20and%20finite%20automata%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleene%2C%20S.C.%20Representation%20of%20events%20in%20nerve%20nets%20and%20finite%20automata%201956"
        },
        {
            "id": "Kurach_et+al_2016_a",
            "entry": "Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurach%2C%20Karol%20Andrychowicz%2C%20Marcin%20Sutskever%2C%20Ilya%20Neural%20random-access%20machines%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurach%2C%20Karol%20Andrychowicz%2C%20Marcin%20Sutskever%2C%20Ilya%20Neural%20random-access%20machines%202016"
        },
        {
            "id": "Mcculloch_1943_a",
            "entry": "Warren McCulloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:127\u2013147, 1943.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCulloch%2C%20Warren%20Pitts%2C%20Walter%20A%20logical%20calculus%20of%20ideas%20immanent%20in%20nervous%20activity%201943",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCulloch%2C%20Warren%20Pitts%2C%20Walter%20A%20logical%20calculus%20of%20ideas%20immanent%20in%20nervous%20activity%201943"
        },
        {
            "id": "Ollinger_2012_a",
            "entry": "Nicolas Ollinger. Universalities in cellular automata. In Handbook of Natural Computing, pp. 189\u2013 229. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolas%20Ollinger%20Universalities%20in%20cellular%20automata%20In%20Handbook%20of%20Natural%20Computing%20pp%20189%20229%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolas%20Ollinger%20Universalities%20in%20cellular%20automata%20In%20Handbook%20of%20Natural%20Computing%20pp%20189%20229%202012"
        },
        {
            "id": "Price_et+al_2016_a",
            "entry": "Eric Price, Wojciech Zaremba, and Ilya Sutskever. Extensions and limitations of the neural GPU. CoRR, abs/1611.00736, 2016. URL http://arxiv.org/abs/1611.00736.",
            "url": "http://arxiv.org/abs/1611.00736",
            "arxiv_url": "https://arxiv.org/pdf/1611.00736"
        },
        {
            "id": "Shaw_et+al_2018_a",
            "entry": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL-HLT, pp. 464\u2013468, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaw%2C%20Peter%20Uszkoreit%2C%20Jakob%20Vaswani%2C%20Ashish%20Self-attention%20with%20relative%20position%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaw%2C%20Peter%20Uszkoreit%2C%20Jakob%20Vaswani%2C%20Ashish%20Self-attention%20with%20relative%20position%20representations%202018"
        },
        {
            "id": "Siegelmann_1992_a",
            "entry": "Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT, pp. 440\u2013449, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siegelmann%2C%20Hava%20T.%20Sontag%2C%20Eduardo%20D.%20On%20the%20computational%20power%20of%20neural%20nets%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siegelmann%2C%20Hava%20T.%20Sontag%2C%20Eduardo%20D.%20On%20the%20computational%20power%20of%20neural%20nets%201992"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. J. Comput.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Hava%20T%20Siegelmann%20and%20Eduardo%20D%20Sontag%20On%20the%20computational%20power%20of%20neural%20nets%20J%20Comput",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Hava%20T%20Siegelmann%20and%20Eduardo%20D%20Sontag%20On%20the%20computational%20power%20of%20neural%20nets%20J%20Comput"
        },
        {
            "id": "Syst_1995_a",
            "entry": "Syst. Sci., 50(1):132\u2013150, 1995. Alvy Ray Smith III. Simple computation-universal cellular spaces. Journal of the ACM (JACM), 18 (3):339\u2013353, 1971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998\u20136008, 2017. Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision RNNs for language recognition. In ACL 2018, pp. 740\u2013745, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syst%20Alvy%20Ray%20Smith%20III.%20Simple%20computation-universal%20cellular%20spaces%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syst%20Alvy%20Ray%20Smith%20III.%20Simple%20computation-universal%20cellular%20spaces%201995"
        },
        {
            "id": "The_1995_a",
            "entry": "The idea presented above allows one to linearly simulate M, that is, each step of M is simulated with a constant number of steps of the corresponding RNN. Siegelmann & Sontag show that, with a refinement of the above encoding one can simulate M in real-time, that is, a single step of M is simulated with a single step of the recurrent network. The 10m + 30 is the bound given by a simulation with slow-down of two. See the original paper for details (Siegelmann & Sontag, 1995).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20idea%20presented%20above%20allows%20one%20to%20linearly%20simulate%20M%20that%20is%20each%20step%20of%20M%20is%20simulated%20with%20a%20constant%20number%20of%20steps%20of%20the%20corresponding%20RNN%20Siegelmann%20%20Sontag%20show%20that%20with%20a%20refinement%20of%20the%20above%20encoding%20one%20can%20simulate%20M%20in%20realtime%20that%20is%20a%20single%20step%20of%20M%20is%20simulated%20with%20a%20single%20step%20of%20the%20recurrent%20network%20The%2010m%20%2030%20is%20the%20bound%20given%20by%20a%20simulation%20with%20slowdown%20of%20two%20See%20the%20original%20paper%20for%20details%20Siegelmann%20%20Sontag%201995"
        },
        {
            "id": "Published_2019_b",
            "entry": "Published as a conference paper at ICLR 2019 Applying function O(\u00b7) plus the residual connection (Equation (11)) we have zi = O(ai) + ai = O([cw, cw]) + [cw, cw] = [cw \u2212 cw, \u2212cw] + [cw, cw] = [cw, 0]",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Applying%20function%20O%20plus%20the%20residual%20connection%20Equation%2011%20we%20have%20zi%20%20Oai%20%20ai%20%20Ocw%20cw%20%20cw%20cw%20%20cw%20%20cw%20cw%20%20cw%20cw%20%20cw%200"
        },
        {
            "id": "2",
            "entry": "2. If i > n and M has never written at index k, then s(i) = #, the blank symbol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20i%20%20n%20and%20M%20has%20never%20written%20at%20index%20k%20then%20si%20%20%20the%20blank%20symbol"
        },
        {
            "id": "3",
            "entry": "3. In other case, that is, if i > n and time i is not the first time that M is pointing to index k, then s(i) is the last symbol written by M at index k.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20other%20case%20that%20is%20if%20i%20%20n%20and%20time%20i%20is%20not%20the%20first%20time%20that%20M%20is%20pointing%20to%20index%20k%20then%20si%20is%20the%20last%20symbol%20written%20by%20M%20at%20index%20k",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20other%20case%20that%20is%20if%20i%20%20n%20and%20time%20i%20is%20not%20the%20first%20time%20that%20M%20is%20pointing%20to%20index%20k%20then%20si%20is%20the%20last%20symbol%20written%20by%20M%20at%20index%20k"
        },
        {
            "id": "1",
            "entry": "1. There exists f1: Q|Q|+|\u03a3| \u2192 Q|Q||\u03a3| such that f1([ q, s ]) = (q, s). 2. There exists f\u03b4: Q|Q||\u03a3| \u2192 Q2|Q||\u03a3| such that f\u03b4( (q, s) ) = \u03b4(q, s). 3. There exists f2: Q2|Q||\u03a3| \u2192 Q|Q|+|\u03a3|+1 such that f2( (q, s, m) ) = [ q, s, m ].",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=There%20exists%20f1%20QQ%CE%A3%20%20QQ%CE%A3%20such%20that%20f1%20q%20s%20%20%20q%20s%202%20There%20exists%20f%CE%B4%20QQ%CE%A3%20%20Q2Q%CE%A3%20such%20that%20f%CE%B4%20q%20s%20%20%20%CE%B4q%20s%203%20There%20exists%20f2%20Q2Q%CE%A3%20%20QQ%CE%A31%20such%20that%20f2%20q%20s%20m%20%20%20%20q%20s%20m"
        },
        {
            "id": "1",
            "entry": "1. We use this transformation just to represent m(i\u22121) with a value between 0 and 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20use%20this%20transformation%20just%20to%20represent%20mi1%20with%20a%20value%20between%200%20and%201"
        },
        {
            "id": "0",
            "entry": "0. Then we have that \u03b5i\u03b5j1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Then%20we%20have%20that%20%CE%B5i%CE%B5j1"
        },
        {
            "id": "1",
            "entry": "1. This implies that ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20implies%20that"
        }
    ]
}
