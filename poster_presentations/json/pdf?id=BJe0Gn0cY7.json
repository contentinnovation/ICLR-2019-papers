{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Generating Sentences from a Continuous Space",
        "author": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, Samy Bengio",
        "date": 2016,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJe0Gn0cY7",
            "doi": "10.18653/v1/k16-1002"
        },
        "journal": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning",
        "abstract": "Due to the phenomenon of \u201cposterior collapse,\u201d current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective. We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. Our proposed \u03b4-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate our method\u2019s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 \u00d7 32."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "latent variable model",
            "url": "https://en.wikipedia.org/wiki/latent_variable_model"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "ISSN",
            "url": "https://en.wikipedia.org/wiki/ISSN"
        },
        {
            "term": "continuous space",
            "url": "https://en.wikipedia.org/wiki/continuous_space"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "ELBO": "evidence lower bound",
        "VAEs": "variational autoencoders",
        "vMF": "von Mises-Fisher",
        "ACN": "Associative Compression Networks"
    },
    "highlights": [
        "Deep latent variable models trained with amortized variational inference have led to advances in representation learning on high-dimensional datasets (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a>; <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a>)",
        "We propose \u03b4-variational autoencoders, a simple framework for selecting variational families that prevent posterior collapse without altering the evidence lower bound training objective or weakening the decoder",
        "To encourage the generative model to leverage the latents for future timesteps, we introduce an anti-causal structure for the encoder where the parameters of the variational posterior for a timestep cannot depend on past observations (Fig. 2)",
        "We have demonstrated that \u03b4-variational autoencoders provide a simple, intuitive, and effective solution to posterior collapse in latent variable models, enabling them to be paired with powerful decoders"
    ],
    "key_statements": [
        "Deep latent variable models trained with amortized variational inference have led to advances in representation learning on high-dimensional datasets (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a>; <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a>)",
        "We propose \u03b4-variational autoencoders, a simple framework for selecting variational families that prevent posterior collapse without altering the evidence lower bound training objective or weakening the decoder",
        "To encourage the generative model to leverage the latents for future timesteps, we introduce an anti-causal structure for the encoder where the parameters of the variational posterior for a timestep cannot depend on past observations (Fig. 2)",
        "We have demonstrated that \u03b4-variational autoencoders provide a simple, intuitive, and effective solution to posterior collapse in latent variable models, enabling them to be paired with powerful decoders"
    ],
    "summary": [
        "Deep latent variable models trained with amortized variational inference have led to advances in representation learning on high-dimensional datasets (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\"><a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a></a>; <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\"><a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a></a>).",
        "If the autoregressive decoder is expressive enough to model the data density, the model can learn to ignore the latent variables, resulting in a trivial posterior that collapses to the prior.",
        "We demonstrate the effectiveness of this approach at learning latent-variable models with powerful decoders on images (CIFAR-10, and ImageNet 32 \u00d7 32), and text (LM1B).",
        "We achieve state of the art log-likelihood results with image models by introducing a sequential latent-variable model with an anti-causal encoder structure.",
        "To encourage the generative model to leverage the latents for future timesteps, we introduce an anti-causal structure for the encoder where the parameters of the variational posterior for a timestep cannot depend on past observations (Fig. 2).",
        "We use sequential latent variables to generate the image row by row.",
        "Another difference between our work and previous sequential latent variable models is our proposed anti-causal structure for the inference network.",
        "Other works use auxiliary tasks such as secondary low-resolution reconstructions with non-autoregressive decoders (<a class=\"ref-link\" id=\"cLucas_2017_a\" href=\"#rLucas_2017_a\">Lucas & Verbeek, 2017</a>) or predicting the state of the backward LSTM in the encoder (<a class=\"ref-link\" id=\"cGoyal_et+al_2017_a\" href=\"#rGoyal_et+al_2017_a\">Goyal et al, 2017</a>) to encourage the utilization of latent variables.",
        "The results on CIFAR-10 demonstrate the effect of the auxiliary prior on improving the efficiency of the latent code; it leads to more than 50% reduction in the rate of the model to achieve the same performance.",
        "To better highlight the difference between models and to put into perspective the amount of information that latent variables capture about images, the rate and distortion results in Fig. 4a are reported in bits per images.",
        "We repeated these experiments with both anti-causal and non-causal structures but without imposing a committed information rate or using other mitigation strategies, and found that neither structure by itself is able to mitigate the posterior collapse issue; in both cases the KL divergence drops to negligible values (< 10\u22128 bits per dimension) only after a few thousand training steps, and never recovers.",
        "Our model achieves slightly worse log-likelihood compared to its autoregressive counterpart (Table 2), but makes considerable use of latent variables, as demonstrated by the samples and interpolations in Appendix H.",
        "We have demonstrated that \u03b4-VAEs provide a simple, intuitive, and effective solution to posterior collapse in latent variable models, enabling them to be paired with powerful decoders.",
        "We do not require changes to the objective or weakening of the decoder, and we can learn useful representations as well as achieving state-of-the-art likelihoods."
    ],
    "headline": "We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for largescale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI\u201916, pp. 265\u2013283, Berkeley, CA, USA, 2016. USENIX Association. ISBN 978-1-931971-33-1. URL http://dl.acm.org/citation.cfm?id=3026877.3026899.",
            "url": "http://dl.acm.org/citation.cfm?id=3026877.3026899",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20largescale%20machine%20learning%202016"
        },
        {
            "id": "Alemi_et+al_2017_a",
            "entry": "Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy. Fixing a Broken ELBO. 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1711.00464.",
            "url": "http://arxiv.org/abs/1711.00464",
            "arxiv_url": "https://arxiv.org/pdf/1711.00464"
        },
        {
            "id": "Alemi_et+al_2018_a",
            "entry": "Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information bottleneck. arXiv preprint arXiv:1807.00906, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.00906"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Babaeizadeh_et+al_2017_a",
            "entry": "Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational video prediction. CoRR, abs/1710.11252, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11252"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating Sentences from a Continuous Space. 2015. doi: 10.18653/v1/K16-1002. URL http://arxiv.org/abs/1511.06349.",
            "crossref": "https://dx.doi.org/10.18653/v1/K16-1002",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "Chelba_et+al_2013_a",
            "entry": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.3005"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder. In Iclr, pp. 1\u201314, nov 2016. URL http://arxiv.org/abs/1611.02731.",
            "url": "http://arxiv.org/abs/1611.02731",
            "arxiv_url": "https://arxiv.org/pdf/1611.02731"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An Improved Autoregressive Generative Model. pp. 12\u201317, 2017. URL http://arxiv.org/abs/1712.09763.",
            "url": "http://arxiv.org/abs/1712.09763",
            "arxiv_url": "https://arxiv.org/pdf/1712.09763"
        },
        {
            "id": "Chung_et+al_2015_a",
            "entry": "Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio. A Recurrent Latent Variable Model for Sequential Data. Advances in Neural Information Processing Systems 28 (NIPS 2015), pp. 8, 2015. ISSN 10495258. URL http://arxiv.org/abs/1506.02216.",
            "url": "http://arxiv.org/abs/1506.02216",
            "arxiv_url": "https://arxiv.org/pdf/1506.02216"
        },
        {
            "id": "Cover_2006_a",
            "entry": "Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, New York, NY, USA, 2006. ISBN 0471241954.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20Information%20Theory%20%28Wiley%20Series%20in%20Telecommunications%20and%20Signal%20Processing%29%202006"
        },
        {
            "id": "Davidson_et+al_2018_a",
            "entry": "Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00891"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Denton_2018_a",
            "entry": "Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. CoRR, abs/1802.07687, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07687"
        },
        {
            "id": "Dieng_et+al_2018_a",
            "entry": "Adji B Dieng, Yoon Kim, Alexander M Rush, and David M Blei. Avoiding latent variable collapse with generative skip models. arXiv preprint arXiv:1807.04863, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04863"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre Cote, Nan Rosemary Ke, and Yoshua Bengio. Z-Forcing: Training Stochastic Recurrent Networks. (Nips), 2017. ISSN 10495258. URL http://arxiv.org/abs/1711.05411.",
            "url": "http://arxiv.org/abs/1711.05411",
            "arxiv_url": "https://arxiv.org/pdf/1711.05411"
        },
        {
            "id": "Graves_2014_a",
            "entry": "Alex Graves and Jacob Menick. Associative Compression Networks for Representation Learning. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Menick%2C%20Jacob%20Associative%20Compression%20Networks%20for%20Representation%20Learning%202014"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3549\u20133557. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf.",
            "url": "http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karol%20Gregor%20Frederic%20Besse%20Danilo%20Jimenez%20Rezende%20Ivo%20Danihelka%20and%20Daan%20Wierstra%20Towards%20conceptual%20compression%20In%20D%20D%20Lee%20M%20Sugiyama%20U%20V%20Luxburg%20I%20Guyon%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2029%20pp%2035493557%20Curran%20Associates%20Inc%202016%20URL%20httppapersnipsccpaper6542towardsconceptualcompressionpdf"
        },
        {
            "id": "Gulrajani_et+al_2016_a",
            "entry": "Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. PixelVAE: A Latent Variable Model for Natural Images. pp. 1\u20139, 2016. ISSN 0066-4308. doi: 10.1146/annurev.psych.53.100901.135239. URL http://arxiv.org/abs/1611.05013.",
            "crossref": "https://dx.doi.org/10.1146/annurev.psych.53.100901.135239",
            "arxiv_url": "https://arxiv.org/pdf/1611.05013"
        },
        {
            "id": "Guu_et+al_2017_a",
            "entry": "Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating Sentences by Editing Prototypes. 6:437\u2013450, 2017. ISSN 1938-7228. URL http://arxiv.org/abs/1709.08878.",
            "url": "http://arxiv.org/abs/1709.08878",
            "arxiv_url": "https://arxiv.org/pdf/1709.08878"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. \u03b2-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20%CE%B2-VAE%3A%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20%CE%B2-VAE%3A%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%202017"
        },
        {
            "id": "Hoffman_et+al_2016_a",
            "entry": "Matthew D Hoffman, Matthew J Johnson, and Google Brain. ELBO surgery: yet another way to carve up the variational evidence lower bound. Advances in Approximate Bayesian Inference, NIPS Workshop, (5), 2016. URL http://approximateinference.org/accepted/ HoffmanJohnson2016.pdf.",
            "url": "http://approximateinference.org/accepted/HoffmanJohnson2016.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Johnson%2C%20Matthew%20J.%20Brain%2C%20Google%20ELBO%20surgery%3A%20yet%20another%20way%20to%20carve%20up%20the%20variational%20evidence%20lower%20bound%202016"
        },
        {
            "id": "Rezende_2018_a",
            "entry": "D. Jimenez Rezende and F. Viola. Taming VAEs. ArXiv e-prints, October 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.Jimenez%20Viola%2C%20F.%20Taming%20VAEs.%20ArXiv%20e-prints%202018-10"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. Semi-amortized variational autoencoders. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2678\u20132687, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/kim18e.html.",
            "url": "http://proceedings.mlr.press/v80/kim18e.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Wiseman%2C%20Sam%20Miller%2C%20Andrew%20Sontag%2C%20David%20Semi-amortized%20variational%20autoencoders%202018-07"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2011_a",
            "entry": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving Variational Inference with Inverse Autoregressive Flow. (2011), 2016. ISSN 10495258. URL http://arxiv.org/abs/1606.04934.",
            "url": "http://arxiv.org/abs/1606.04934",
            "arxiv_url": "https://arxiv.org/pdf/1606.04934"
        },
        {
            "id": "Krizhevsky_et+al_0000_a",
            "entry": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/\u0303kriz/cifar.html.",
            "url": "http://www.cs.toronto.edu/\u0303kriz/cifar.html"
        },
        {
            "id": "Larsen_et+al_2016_a",
            "entry": "Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In International Conference on Machine Learning, pp. 1558\u20131566, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016"
        },
        {
            "id": "Lucas_2017_a",
            "entry": "Thomas Lucas and Jakob Verbeek. Auxiliary guided autoregressive variational autoencoders. arXiv preprint arXiv:1711.11479, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11479"
        },
        {
            "id": "Ostrovski_et+al_2018_a",
            "entry": "Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3936\u20133945, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/ostrovski18a.html.",
            "url": "http://proceedings.mlr.press/v80/ostrovski18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ostrovski%2C%20Georg%20Dabney%2C%20Will%20Munos%2C%20Remi%20Autoregressive%20quantile%20networks%20for%20generative%20modeling%202018-07"
        },
        {
            "id": "Parmar_et+al_2018_a",
            "entry": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image Transformer. 2018. ISSN 1938-7228. URL http://arxiv.org/abs/1802.05751.",
            "url": "http://arxiv.org/abs/1802.05751",
            "arxiv_url": "https://arxiv.org/pdf/1802.05751"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML\u201915, pp. 1530\u20131538. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045281.",
            "url": "http://dl.acm.org/citation.cfm?id=3045118.3045281",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. 32, 2014. ISSN 10495258. doi: 10.1051/ 0004-6361/201527329. URL http://arxiv.org/abs/1401.4082.",
            "crossref": "https://dx.doi.org/10.1051/0004-6361/201527329",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "Roy_et+al_2018_a",
            "entry": "Aurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on vector quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11063"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. pp. 1\u20139, 2017. ISSN 9781467324755. URL http://arxiv.org/abs/1701.05517.",
            "url": "http://arxiv.org/abs/1701.05517",
            "arxiv_url": "https://arxiv.org/pdf/1701.05517"
        },
        {
            "id": "Tomczak_2017_a",
            "entry": "Jakub M. Tomczak and Max Welling. VAE with a vampprior. CoRR, abs/1705.07120, 2017. URL http://arxiv.org/abs/1705.07120.",
            "url": "http://arxiv.org/abs/1705.07120",
            "arxiv_url": "https://arxiv.org/pdf/1705.07120"
        },
        {
            "id": "Turner_2007_a",
            "entry": "R. E. Turner and M. Sahani. A maximum-likelihood interpretation for slow feature analysis. Neural Computation, 19(4):1022\u20131038, 2007. ISSN 0899-7667. doi: http://dx.doi.org/10.1162/neco.2007.19.4.1022.",
            "crossref": "https://dx.doi.org/10.1162/neco.2007.19.4.1022",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/neco.2007.19.4.1022"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016a. URL http://arxiv.org/abs/1601.06759.",
            "url": "http://arxiv.org/abs/1601.06759",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Van_et+al_2016_b",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In International Conference on Machine Learning, volume 48, pp. 1747\u20131756, 2016b. ISBN 9781510829008. URL http://arxiv.org/abs/1601.06759.",
            "url": "http://arxiv.org/abs/1601.06759",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Van_et+al_2017_a",
            "entry": "Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.",
            "url": "http://arxiv.org/abs/1711.00937",
            "arxiv_url": "https://arxiv.org/pdf/1711.00937"
        },
        {
            "id": "Van_2008_a",
            "entry": "L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.J.P.%20Hinton%2C%20G.E.%20Visualizing%20high-dimensional%20data%20using%20t-sne%202008"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. (Nips), 2017. ISSN 0140-525X. doi: 10.1017/S0140525X16001837. URL http://arxiv.org/abs/1706.03762.",
            "crossref": "https://dx.doi.org/10.1017/S0140525X16001837",
            "arxiv_url": "https://arxiv.org/pdf/1706.03762"
        },
        {
            "id": "Vaswani_et+al_2018_a",
            "entry": "Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Lukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        },
        {
            "id": "Wiskott_2002_a",
            "entry": "Laurenz Wiskott and Terrence J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Comput., 14(4):715\u2013770, April 2002. ISSN 0899-7667. doi: 10.1162/ 089976602317318938. URL http://dx.doi.org/10.1162/089976602317318938.",
            "crossref": "https://dx.doi.org/10.1162/089976602317318938",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/089976602317318938"
        },
        {
            "id": "Xu_2018_a",
            "entry": "Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. Proc. Empirical Methods in Natural Language Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Jiacheng%20Durrett%2C%20Greg%20Spherical%20latent%20spaces%20for%20stable%20variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Jiacheng%20Durrett%2C%20Greg%20Spherical%20latent%20spaces%20for%20stable%20variational%20autoencoders%202018"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. 2017. URL http://arxiv.org/abs/1702.08139.",
            "url": "http://arxiv.org/abs/1702.08139",
            "arxiv_url": "https://arxiv.org/pdf/1702.08139"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing Learning and Inference in Variational Autoencoders. ICML Workshop, 2017. URL http://arxiv.org/abs/1706.02262.",
            "url": "http://arxiv.org/abs/1706.02262",
            "arxiv_url": "https://arxiv.org/pdf/1706.02262"
        }
    ]
}
