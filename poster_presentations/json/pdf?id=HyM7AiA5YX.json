{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "COMPLEMENT OBJECTIVE TRAINING",
        "author": "Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng Juan",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyM7AiA5YX"
        },
        "abstract": "Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks."
    },
    "keywords": [
        {
            "term": "Neural machine translation",
            "url": "https://en.wikipedia.org/wiki/Neural_machine_translation"
        },
        {
            "term": "natural language understanding",
            "url": "https://en.wikipedia.org/wiki/natural_language_understanding"
        },
        {
            "term": "primary objective",
            "url": "https://en.wikipedia.org/wiki/primary_objective"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "complement class",
            "url": "https://en.wikipedia.org/wiki/complement_class"
        },
        {
            "term": "cross entropy",
            "url": "https://en.wikipedia.org/wiki/cross_entropy"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "COT": "Complement Objective Training",
        "SVHN": "Street View House Numbers",
        "NLU": "natural language understanding",
        "NMT": "Neural machine translation",
        "FGSM": "Fast Gradient Sign Method"
    },
    "highlights": [
        "Statistical learning algorithms work by optimizing towards a training objective",
        "The popularity of deep neural networks has given rise to the use of cross entropy (<a class=\"ref-link\" id=\"cKullback_1951_a\" href=\"#rKullback_1951_a\">Kullback & Leibler, 1951</a>) as its primary training objective, since minimizing cross entropy is essentially equivalent to maximizing likelihood for disjoint classes",
        "We propose Complement Objective Training (COT), a new training paradigm that achieves this optimization goal without compromising the model\u2019s primary objective",
        "For Fast Gradient Sign Method white-box attacks on Complement Objective Training, adversarial perturbations are generated based on the sum of the primary gradient and the complement gradient, both gradients from the model trained by Complement Objective Training",
        "We study Complement Objective Training (COT), a new training paradigm that optimizes the complement objective in addition to the primary objective",
        "Complement Objective Training can be extended in several ways: first, in this paper, the complement objective is chosen to be the complement entropy"
    ],
    "key_statements": [
        "Statistical learning algorithms work by optimizing towards a training objective",
        "The popularity of deep neural networks has given rise to the use of cross entropy (<a class=\"ref-link\" id=\"cKullback_1951_a\" href=\"#rKullback_1951_a\">Kullback & Leibler, 1951</a>) as its primary training objective, since minimizing cross entropy is essentially equivalent to maximizing likelihood for disjoint classes",
        "We propose Complement Objective Training (COT), a new training paradigm that achieves this optimization goal without compromising the model\u2019s primary objective",
        "For Fast Gradient Sign Method white-box attacks on Complement Objective Training, adversarial perturbations are generated based on the sum of the primary gradient and the complement gradient, both gradients from the model trained by Complement Objective Training",
        "We conduct experiments on Fast Gradient Sign Method transfer attacks, which use the adversarial examples from a baseline model to attack and test the robustness of the model trained by Complement Objective Training",
        "We conjecture that since the main goal of the complement gradients is to neutralize the probabilities of incorrect classes, the complement gradients may \u201cpush away\u201d primary gradients when forming adversarial perturbations, which might partially answer why Complement Objective Training is more robust to Fast Gradient Sign Method white-box attacks compared to the baseline",
        "We study Complement Objective Training (COT), a new training paradigm that optimizes the complement objective in addition to the primary objective",
        "We propose complement entropy as the complement objective for neutralizing the effects of complement classes",
        "Models trained using Complement Objective Training demonstrate superior performance compared to the baseline models",
        "We find that Complement Objective Training makes the models robust to single-step adversarial attacks",
        "Complement Objective Training can be extended in several ways: first, in this paper, the complement objective is chosen to be the complement entropy",
        "The exploration of Complement Objective Training on broader applications remains as an open research question"
    ],
    "summary": [
        "Statistical learning algorithms work by optimizing towards a training objective.",
        "Figure 2 further illustrates the embeddings of CIFAR10 images calculated from ResNet-110 using two training paradigms: cross entropy and COT.",
        "Experimental results show that models trained by COT are more robust to adversarial attacks.",
        "Training with cross entropy as the primary objective aims at maximizing the predicted probability of the ground-truth class yg in Eq(1).",
        "We perform experiments to evaluate the robustness of the model trained by COT when attacked by adversarial examples.",
        "For all the experiments conducted in this paper, we use this normalized complement entropy as the complement objective to improve the baselines without further tuning of learning rates.",
        "Table 2 shows the experimental results and confirms that COT consistently improves the baseline models with the biggest improvement being the ResNet-110 with 11.7% reduction on the error rate.",
        "Table 3 provides the experimental results, which demonstrate that COT consistently improves the performance of all baseline models.",
        "The model trained by COT gives the best testing results when the beam width is 3, while the baseline uses 10 as the best beam width.",
        "Table 5 illustrates the experimental results, showing COT improves testing BLEU scores compared to the baseline NMT model on both greedy decoder and the beam search decoder.",
        "To set up FGSM white-box attacks on a baseline model, adversarial examples are generated using the gradients calculated based on the primary objective of the baseline model.",
        "For FGSM white-box attacks on COT, adversarial perturbations are generated based on the sum of the primary gradient and the complement gradient, both gradients from the model trained by COT.",
        "We conduct experiments on FGSM transfer attacks, which use the adversarial examples from a baseline model to attack and test the robustness of the model trained by COT.",
        "We conjecture that since the main goal of the complement gradients is to neutralize the probabilities of incorrect classes, the complement gradients may \u201cpush away\u201d primary gradients when forming adversarial perturbations, which might partially answer why COT is more robust to FGSM white-box attacks compared to the baseline.",
        "Only the primary objective of the baseline model is used to calculate the gradients for generating adversarial examples.",
        "The complement gradients are not considered when generating adversarial examples in the transfer attack, and this might be the reason why models trained by COT are more robust to transfer attacks.",
        "In this work, we show using complement objective help defend single-step adversarial attacks; the behavior of COT on more advanced adversarial attacks deserves further investigation and is left as another future work"
    ],
    "headline": "In addition to the primary objective, training using a complement objective that leverages information from the complement classes can be effective in improving model performance",
    "reference_links": [
        {
            "id": "Cettolo_et+al_2015_a",
            "entry": "Mauro Cettolo, Jan Niehues, Sebastian Stu ker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. The IWSLT 2015 evaluation campaign. In ICSLP\u201915, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mauro%20Cettolo%20Jan%20Niehues%20Sebastian%20Stu%20ker%20Luisa%20Bentivogli%20Roldano%20Cattoni%20and%20Marcello%20Federico%20The%20IWSLT%202015%20evaluation%20campaign%20In%20ICSLP15%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mauro%20Cettolo%20Jan%20Niehues%20Sebastian%20Stu%20ker%20Luisa%20Bentivogli%20Roldano%20Cattoni%20and%20Marcello%20Federico%20The%20IWSLT%202015%20evaluation%20campaign%20In%20ICSLP15%202015"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP\u201914, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bougares%2C%20Fethi%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bougares%2C%20Fethi%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. ImageNet: A large-scale hierarchical image database. In CVPR\u201909, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20jia%20Li%2C%20Li%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20jia%20Li%2C%20Li%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS\u201914. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS14%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS14%202014"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR\u201915, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV\u201916, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks"
        },
        {
            "id": "He_et+al_0000_b",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR\u201916, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "Huang_et+al_0000_a",
            "entry": "Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get M for free. In ICLR\u201917, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles"
        },
        {
            "id": "Huang_et+al_0000_b",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR\u201917, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML\u201915, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS\u201912, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kullback_1951_a",
            "entry": "Solomon Kullback and Richard A. Leibler. On information and sufficiency. Ann. Math. Statist., 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kullback%2C%20Solomon%20Leibler%2C%20Richard%20A.%20On%20information%20and%20sufficiency%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kullback%2C%20Solomon%20Leibler%2C%20Richard%20A.%20On%20information%20and%20sufficiency%201951"
        },
        {
            "id": "Kurakin_et+al_2017_a",
            "entry": "Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In ICLR\u201917 Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20J.%20Bengio%2C%20Samy%20Adversarial%20examples%20in%20the%20physical%20world%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20J.%20Bengio%2C%20Samy%20Adversarial%20examples%20in%20the%20physical%20world%202017"
        },
        {
            "id": "Luong_et+al_2015_a",
            "entry": "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. In EMNLP\u201915, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Minh-Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attentionbased%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Minh-Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attentionbased%20neural%20machine%20translation%202015"
        },
        {
            "id": "Mitchell_1997_a",
            "entry": "Tom M Mitchell et al. Machine learning. Burr Ridge, IL: McGraw Hill, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitchell%2C%20Tom%20M.%20Machine%20learning%201997"
        },
        {
            "id": "Netzer_et+al_2011_a",
            "entry": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS\u201911 Workshop, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR\u201915, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202014"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS\u201914, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Szegedy_et+al_2014_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR\u201914, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "Warden_2018_a",
            "entry": "Pete Warden. Speech Commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03209"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Saining Xie, Ross B. Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR\u201917, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20B.%20Dollar%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20B.%20Dollar%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC\u201916, 2016. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond empirical risk minimization. In ICLR\u201918, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        }
    ]
}
