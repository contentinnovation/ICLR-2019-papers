{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "CBOW IS NOT ALL YOU NEED: COMBINING CBOW WITH THE COMPOSITIONAL MATRIX SPACE MODEL",
        "author": "Florian Mai Idiap Research Institute Martigny, Switzerland florian.mai@idiap.ch",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1MgjoR9tQ"
        },
        "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW\u2019s word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a learning algorithm for the Continuous Matrix Space Model (<a class=\"ref-link\" id=\"cRudolph_2010_a\" href=\"#rRudolph_2010_a\">Rudolph & Giesbrecht, 2010</a>), which we call Continual Multiplication of Words (CMOW). Our algorithm is an adaptation of word2vec (<a class=\"ref-link\" id=\"cMikolov_et+al_2013_a\" href=\"#rMikolov_et+al_2013_a\">Mikolov et al., 2013a</a>), so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW\u2019s strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%."
    },
    "keywords": [
        {
            "term": "sentence embedding",
            "url": "https://en.wikipedia.org/wiki/sentence_embedding"
        },
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "word order",
            "url": "https://en.wikipedia.org/wiki/word_order"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "CBOW": "Continuous Bag of Words",
        "CMOW": "Continual Multiplication Of Words",
        "RNNs": "recurrent neural networks",
        "CMSM": "Compositional Matrix Space Model",
        "WC": "Word Content",
        "BShift": "Bigram Shift",
        "STS-B": "sentence representation benchmark"
    },
    "highlights": [
        "Word embeddings are perceived as one of the most impactful contributions from unsupervised representation learning to natural language processing from the past few years (<a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\"><a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\">Goth, 2016</a></a>)",
        "While Continuous Bag of Words yields the highest performance at word content memorization, Continual Multiplication Of Words outperforms Continuous Bag of Words at all other tasks",
        "We have presented the first efficient, unsupervised learning scheme for the word order aware Compositional Matrix Space Model",
        "We showed that the resulting sentence embeddings capture linguistic features that are complementary to Continuous Bag of Words embeddings",
        "We thereupon presented a hybrid model with Continuous Bag of Words that is able to combine the complementary strengths of both models to yield an improved downstream task performance, in particular on tasks that depend on word order information",
        "Our model narrows the gap in terms of representational power between simple word embedding based sentence encoders and highly non-linear recurrent sentence encoders"
    ],
    "key_statements": [
        "Word embeddings are perceived as one of the most impactful contributions from unsupervised representation learning to natural language processing from the past few years (<a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\"><a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\">Goth, 2016</a></a>)",
        "We show that Compositional Matrix Space Model can be trained in a similar way as the well-known Continuous Bag of Words model of word2vec (<a class=\"ref-link\" id=\"cMikolov_et+al_2013_a\" href=\"#rMikolov_et+al_2013_a\">Mikolov et al, 2013a</a>)",
        "We present the first unsupervised training scheme for Compositional Matrix Space Model, which we call Continual Multiplication Of Words (CMOW)",
        "Key elements of our scheme are an initialization strategy and training objective that are designed for training Compositional Matrix Space Model. (2) We quantitatively demonstrate that the strengths of the resulting embedding model are complementary to classical Continuous Bag of Words embeddings",
        "We report the results of two more models: H-Continuous Bag of Words is the 400-dimensional Continuous Bag of Words component trained in Hybrid and H-Continual Multiplication Of Words is the respective Continual Multiplication Of Words component",
        "While Continuous Bag of Words yields the highest performance at word content memorization, Continual Multiplication Of Words outperforms Continuous Bag of Words at all other tasks",
        "Table 2 shows the scores from the supervised downstream tasks",
        "Our jointly trained model is not more than 0.8 points below the better one of Continuous Bag of Words and Continual Multiplication Of Words on any of the considered supervised downstream tasks",
        "Continual Multiplication Of Words does not in general supersede Continuous Bag of Words embeddings. This can be explained by the fact that Continuous Bag of Words is stronger at word content memorization, which is known to highly correlate with the performance on most downstream tasks (Conneau et al, 2018)",
        "The differences are small. This can be explained by the fact that most tasks in the SentEval framework mainly depend on word content memorization (Conneau et al, 2018), where the hybrid model does not improve upon Continuous Bag of Words",
        "We have presented the first efficient, unsupervised learning scheme for the word order aware Compositional Matrix Space Model",
        "We showed that the resulting sentence embeddings capture linguistic features that are complementary to Continuous Bag of Words embeddings",
        "We thereupon presented a hybrid model with Continuous Bag of Words that is able to combine the complementary strengths of both models to yield an improved downstream task performance, in particular on tasks that depend on word order information",
        "Our model narrows the gap in terms of representational power between simple word embedding based sentence encoders and highly non-linear recurrent sentence encoders"
    ],
    "summary": [
        "Word embeddings are perceived as one of the most impactful contributions from unsupervised representation learning to natural language processing from the past few years (<a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\"><a class=\"ref-link\" id=\"cGoth_2016_a\" href=\"#rGoth_2016_a\">Goth, 2016</a></a>).",
        "We propose a hybrid model to jointly learn the word vectors of CBOW and the word matrices for CMOW.",
        "CBOW-CMOW retains CBOW\u2019s ability to memorize word content while at the same time improves the performance on the linguistic probing tasks by 8%.",
        "The hybrid model improves the performance over CBOW by 1.2% on supervised downstream tasks, and by 0.5% on the unsupervised tasks.",
        "For CMOW, modifying the objective leads to a large improvement on downstream tasks by 20.8% on average, while it does not make a difference for CBOW.",
        "The simplest combination is to train CBOW and CMOW separately and concatenate the resulting sentence embeddings at test time.",
        "The model uses logistic regression to predict the missing word from the concatenation of CBOW and CMOW embeddings.",
        "We conducted experiments to evaluate the effect of using our proposed models for training CMSMs. we describe the experimental setup and present the results on linguistic probing as well as downstream tasks.",
        "We have trained five different models: CBOW and CMOW with d = 20 and d = 28, which lead to 400-dimensional and 784-dimensional word embeddings, respectively.",
        "Our jointly trained model is not more than 0.8 points below the better one of CBOW and CMOW on any of the considered supervised downstream tasks.",
        "By choosing the target word of the objective at random, the performance of CMOW on downstream tasks improved by 20.8% on average.",
        "We observe that CMOW embeddings better encode the linguistic properties of sentences than CBOW.",
        "This can be explained by the fact that CBOW is stronger at word content memorization, which is known to highly correlate with the performance on most downstream tasks (Conneau et al, 2018).",
        "Our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously.",
        "This can be explained by the fact that most tasks in the SentEval framework mainly depend on word content memorization (Conneau et al, 2018), where the hybrid model does not improve upon CBOW.",
        "We have presented the first efficient, unsupervised learning scheme for the word order aware Compositional Matrix Space Model.",
        "We thereupon presented a hybrid model with CBOW that is able to combine the complementary strengths of both models to yield an improved downstream task performance, in particular on tasks that depend on word order information.",
        "Our model narrows the gap in terms of representational power between simple word embedding based sentence encoders and highly non-linear recurrent sentence encoders"
    ],
    "headline": "In order to address this shortcoming, we propose a learning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words",
    "reference_links": [
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20A%20simple%20but%20tough-to-beat%20baseline%20for%20sentence%20embeddings%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20A%20simple%20but%20tough-to-beat%20baseline%20for%20sentence%20embeddings%202017"
        },
        {
            "id": "Asaadi_2017_a",
            "entry": "Shima Asaadi and Sebastian Rudolph. Gradual learning of matrix-space models of language for sentiment analysis. In Rep4NLP@ACL, pp. 178\u2013185. Association for Computational Linguistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asaadi%2C%20Shima%20Rudolph%2C%20Sebastian%20Gradual%20learning%20of%20matrix-space%20models%20of%20language%20for%20sentiment%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asaadi%2C%20Shima%20Rudolph%2C%20Sebastian%20Gradual%20learning%20of%20matrix-space%20models%20of%20language%20for%20sentiment%20analysis%202017"
        },
        {
            "id": "Cer_et+al_2018_a",
            "entry": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11175"
        },
        {
            "id": "Chung_2018_a",
            "entry": "WooJin Chung and Samuel R Bowman. The lifted matrix-space model for semantic composition. In Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20WooJin%20Bowman%2C%20Samuel%20R.%20The%20lifted%20matrix-space%20model%20for%20semantic%20composition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20WooJin%20Bowman%2C%20Samuel%20R.%20The%20lifted%20matrix-space%20model%20for%20semantic%20composition%202018"
        },
        {
            "id": "Conneau_2018_a",
            "entry": "Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. In LREC. European Language Resources Association (ELRA), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Senteval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Senteval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018"
        },
        {
            "id": "Conneau_et+al_2017_a",
            "entry": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u0131c Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In EMNLP, pp. 670\u2013680. Association for Computational Linguistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017"
        },
        {
            "id": "Conneau_et+al_2018_b",
            "entry": "Alexis Conneau, Lo\u0131c Barrault, Guillaume Lample, German Kruszewski, and Marco Baroni. What you can cram into a single \\$&!#* vector: Probing sentence embeddings for linguistic properties. In ACL (1), pp. 2126\u20132136. Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Barrault%2C%20Lo%C4%B1c%20Lample%2C%20Guillaume%20Kruszewski%2C%20German%20What%20you%20can%20cram%20into%20a%20single%20%5C%24%26%21%23%2A%20vector%3A%20Probing%20sentence%20embeddings%20for%20linguistic%20properties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Barrault%2C%20Lo%C4%B1c%20Lample%2C%20Guillaume%20Kruszewski%2C%20German%20What%20you%20can%20cram%20into%20a%20single%20%5C%24%26%21%23%2A%20vector%3A%20Probing%20sentence%20embeddings%20for%20linguistic%20properties%202018"
        },
        {
            "id": "Gan_et+al_2017_a",
            "entry": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. Learning generic sentence representations using convolutional neural networks. In EMNLP, pp. 2390\u2013 2400. Association for Computational Linguistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Zhe%20Pu%2C%20Yunchen%20Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20Learning%20generic%20sentence%20representations%20using%20convolutional%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Zhe%20Pu%2C%20Yunchen%20Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20Learning%20generic%20sentence%20representations%20using%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, volume 9 of JMLR Proceedings, pp. 249\u2013256. JMLR.org, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goldberg_2017_a",
            "entry": "Yoav Goldberg. Neural Network Methods for Natural Language Processing. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2017. doi: 10.2200/ S00762ED1V01Y201703HLT037.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberg%2C%20Yoav%20Neural%20Network%20Methods%20for%20Natural%20Language%20Processing.%20Synthesis%20Lectures%20on%20Human%20Language%20Technologies%202017"
        },
        {
            "id": "Goth_2016_a",
            "entry": "Gregory Goth. Deep or shallow, NLP is breaking out. Commun. ACM, 59(3):13\u201316, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goth%2C%20Gregory%20Deep%20or%20shallow%2C%20NLP%20is%20breaking%20out%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goth%2C%20Gregory%20Deep%20or%20shallow%2C%20NLP%20is%20breaking%20out%202016"
        },
        {
            "id": "Edward_2011_a",
            "entry": "Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical compositional distributional model of meaning. In EMNLP, pp. 1394\u20131404. ACL, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edward%20Grefenstette%20and%20Mehrnoosh%20Sadrzadeh%20Experimental%20support%20for%20a%20categorical%20compositional%20distributional%20model%20of%20meaning%20In%20EMNLP%20pp%2013941404%20ACL%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edward%20Grefenstette%20and%20Mehrnoosh%20Sadrzadeh%20Experimental%20support%20for%20a%20categorical%20compositional%20distributional%20model%20of%20meaning%20In%20EMNLP%20pp%2013941404%20ACL%202011"
        },
        {
            "id": "Han_et+al_2013_a",
            "entry": "Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. Umbc ebiquitycore: Semantic textual similarity systems. In *SEM@NAACL-HLT, pp. 44\u201352. Association for Computational Linguistics, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Lushan%20Kashyap%2C%20Abhay%20L.%20Finin%2C%20Tim%20Mayfield%2C%20James%20Umbc%20ebiquitycore%3A%20Semantic%20textual%20similarity%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Lushan%20Kashyap%2C%20Abhay%20L.%20Finin%2C%20Tim%20Mayfield%2C%20James%20Umbc%20ebiquitycore%3A%20Semantic%20textual%20similarity%20systems%202013"
        },
        {
            "id": "Henao_et+al_2018_a",
            "entry": "Ricardo Henao, Chunyuan Li, Lawrence Carin, Qinliang Su, Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, and Yizhe Zhang. Baseline needs more love: On simple wordembedding-based models and associated pooling mechanisms. In ACL (1), pp. 440\u2013450. Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20Carin%2C%20Lawrence%20Su%2C%20Qinliang%20Baseline%20needs%20more%20love%3A%20On%20simple%20wordembedding-based%20models%20and%20associated%20pooling%20mechanisms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20Carin%2C%20Lawrence%20Su%2C%20Qinliang%20Baseline%20needs%20more%20love%3A%20On%20simple%20wordembedding-based%20models%20and%20associated%20pooling%20mechanisms%202018"
        },
        {
            "id": "Hill_et+al_2016_a",
            "entry": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In HLT-NAACL, pp. 1367\u20131377. The Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hill%2C%20Felix%20Cho%2C%20Kyunghyun%20Korhonen%2C%20Anna%20Learning%20distributed%20representations%20of%20sentences%20from%20unlabelled%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hill%2C%20Felix%20Cho%2C%20Kyunghyun%20Korhonen%2C%20Anna%20Learning%20distributed%20representations%20of%20sentences%20from%20unlabelled%20data%202016"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kiros_et+al_2015_a",
            "entry": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In NIPS, pp. 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20Zemel%2C%20Richard%20S.%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20Zemel%2C%20Richard%20S.%20Skip-thought%20vectors%202015"
        },
        {
            "id": "Logeswaran_2018_a",
            "entry": "Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Logeswaran%2C%20Lajanugen%20Lee%2C%20Honglak%20An%20efficient%20framework%20for%20learning%20sentence%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Logeswaran%2C%20Lajanugen%20Lee%2C%20Honglak%20An%20efficient%20framework%20for%20learning%20sentence%20representations%202018"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In NIPS, pp. 6297\u20136308, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%20pp%2062976308%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%20pp%2062976308%202017"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Workshop at the International Conference on Learning Representations, 2013a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Chen%2C%20Kai%20Corrado%2C%20Greg%20Dean%2C%20Jeffrey%20Efficient%20estimation%20of%20word%20representations%20in%20vector%20space%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Chen%2C%20Kai%20Corrado%2C%20Greg%20Dean%2C%20Jeffrey%20Efficient%20estimation%20of%20word%20representations%20in%20vector%20space%202013"
        },
        {
            "id": "Mikolov_et+al_2013_b",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pp. 3111\u20133119, 2013b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Gregory%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Gregory%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mikolov_et+al_2018_a",
            "entry": "Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. In LREC. European Language Resources Association (ELRA), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Grave%2C%20Edouard%20Bojanowski%2C%20Piotr%20Puhrsch%2C%20Christian%20Advances%20in%20pre-training%20distributed%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Grave%2C%20Edouard%20Bojanowski%2C%20Piotr%20Puhrsch%2C%20Christian%20Advances%20in%20pre-training%20distributed%20word%20representations%202018"
        },
        {
            "id": "Nie_et+al_2017_a",
            "entry": "Allen Nie, Erin D Bennett, and Noah D Goodman. Dissent: Sentence representation learning from explicit discourse relations. arXiv preprint arXiv:1710.04334, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04334"
        },
        {
            "id": "Pagliardini_et+al_2018_a",
            "entry": "Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings using compositional n-gram features. In NAACL-HLT, pp. 528\u2013540. Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pagliardini%2C%20Matteo%20Gupta%2C%20Prakhar%20Jaggi%2C%20Martin%20Unsupervised%20learning%20of%20sentence%20embeddings%20using%20compositional%20n-gram%20features%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pagliardini%2C%20Matteo%20Gupta%2C%20Prakhar%20Jaggi%2C%20Martin%20Unsupervised%20learning%20of%20sentence%20embeddings%20using%20compositional%20n-gram%20features%202018"
        },
        {
            "id": "Perone_et+al_2018_a",
            "entry": "Christian S Perone, Roberto Silveira, and Thomas S Paula. Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06259"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL-HLT, pp. 2227\u20132237. Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Ruckle_et+al_2018_a",
            "entry": "Andreas Ruckle, Steffen Eger, Maxime Peyrard, and Iryna Gurevych. Concatenated pmean word embeddings as universal cross-lingual sentence representations. arXiv preprint arXiv:1803.01400, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01400"
        },
        {
            "id": "Ruder_2018_a",
            "entry": "Sebastian Ruder and Jeremy Howard. Universal language model fine-tuning for text classification. In ACL (1), pp. 328\u2013339. Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruder%2C%20Sebastian%20Howard%2C%20Jeremy%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruder%2C%20Sebastian%20Howard%2C%20Jeremy%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018"
        },
        {
            "id": "Rudolph_2010_a",
            "entry": "Sebastian Rudolph and Eugenie Giesbrecht. Compositional matrix-space models of language. In ACL, pp. 907\u2013916. The Association for Computer Linguistics, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudolph%2C%20Sebastian%20Giesbrecht%2C%20Eugenie%20Compositional%20matrix-space%20models%20of%20language%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudolph%2C%20Sebastian%20Giesbrecht%2C%20Eugenie%20Compositional%20matrix-space%20models%20of%20language%202010"
        },
        {
            "id": "Socher_et+al_2012_a",
            "entry": "Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositionality through recursive matrix-vector spaces. In EMNLP-CoNLL, pp. 1201\u20131211. ACL, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Huval%2C%20Brody%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Semantic%20compositionality%20through%20recursive%20matrix-vector%20spaces%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Huval%2C%20Brody%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Semantic%20compositionality%20through%20recursive%20matrix-vector%20spaces%202012"
        },
        {
            "id": "Subramanian_et+al_2018_a",
            "entry": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Subramanian%2C%20Sandeep%20Trischler%2C%20Adam%20Bengio%2C%20Yoshua%20Pal%2C%20Christopher%20J.%20Learning%20general%20purpose%20distributed%20sentence%20representations%20via%20large%20scale%20multi-task%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Subramanian%2C%20Sandeep%20Trischler%2C%20Adam%20Bengio%2C%20Yoshua%20Pal%2C%20Christopher%20J.%20Learning%20general%20purpose%20distributed%20sentence%20representations%20via%20large%20scale%20multi-task%20learning%202018"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, and Virginia R. de Sa. Rethinking skip-thought: A neighborhood based approach. In Rep4NLP@ACL, pp. 211\u2013218. Association for Computational Linguistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Shuai%20Jin%2C%20Hailin%20Fang%2C%20Chen%20Wang%2C%20Zhaowen%20Rethinking%20skip-thought%3A%20A%20neighborhood%20based%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Shuai%20Jin%2C%20Hailin%20Fang%2C%20Chen%20Wang%2C%20Zhaowen%20Rethinking%20skip-thought%3A%20A%20neighborhood%20based%20approach%202017"
        },
        {
            "id": "Wieting_2019_a",
            "entry": "John Wieting and Douwe Kiela. No training required: Exploring random encoders for sentence classification. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wieting%2C%20John%20Kiela%2C%20Douwe%20No%20training%20required%3A%20Exploring%20random%20encoders%20for%20sentence%20classification%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wieting%2C%20John%20Kiela%2C%20Douwe%20No%20training%20required%3A%20Exploring%20random%20encoders%20for%20sentence%20classification%202019"
        },
        {
            "id": "Wieting_et+al_2016_a",
            "entry": "John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic sentence embeddings. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wieting%2C%20John%20Bansal%2C%20Mohit%20Gimpel%2C%20Kevin%20Livescu%2C%20Karen%20Towards%20universal%20paraphrastic%20sentence%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wieting%2C%20John%20Bansal%2C%20Mohit%20Gimpel%2C%20Kevin%20Livescu%2C%20Karen%20Towards%20universal%20paraphrastic%20sentence%20embeddings%202016"
        },
        {
            "id": "Yessenalina_2011_a",
            "entry": "Ainur Yessenalina and Claire Cardie. Compositional matrix-space models for sentiment analysis. In EMNLP, pp. 172\u2013182. ACL, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yessenalina%2C%20Ainur%20Cardie%2C%20Claire%20Compositional%20matrix-space%20models%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yessenalina%2C%20Ainur%20Cardie%2C%20Claire%20Compositional%20matrix-space%20models%20for%20sentiment%20analysis%202011"
        }
    ]
}
