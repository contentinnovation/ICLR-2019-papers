{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SMALL NONLINEARITIES IN ACTIVATION FUNCTIONS",
        "author": "CREATE BAD LOCAL MINIMA IN NEURAL NETWORKS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rke_YiRct7"
        },
        "abstract": "We investigate the loss surface of neural networks. We prove that even for onehidden-layer networks with \u201cslightest\u201d nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \u201cno spurious local minima\u201d is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "real number",
            "url": "https://en.wikipedia.org/wiki/real_number"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "critical point",
            "url": "https://en.wikipedia.org/wiki/critical_point"
        }
    ],
    "abbreviations": {
        "W H+1": "W H+1:1 implies rank"
    },
    "highlights": [
        "Neural network training reduces to solving nonconvex empirical risk minimization problems, a task that is in general intractable",
        "We provide a counterexample nonlinear network and a dataset for which a wide range of nonlinear activations result in a local minimum that is strictly inferior to the global minimum with exactly zero empirical risk",
        "We investigated the loss surface of deep linear and nonlinear neural networks",
        "We proved two theorems showing existence of spurious local minima on nonlinear networks, which apply to almost all datasets (Theorem 1) and a wide class of activations (Theorem 2)",
        "Given that spurious local minima are common in neural networks, a valuable future research direction will be investigating how far local minima are from global minima in general, and how the size of the network affects this gap",
        "Another thing to note is that even though we showed the existence of spurious local minima in the whole parameter space, things can be different in restricted sets of parameter space"
    ],
    "key_statements": [
        "Neural network training reduces to solving nonconvex empirical risk minimization problems, a task that is in general intractable",
        "Success stories of deep learning suggest that local minima of the empirical risk could be close to global minima",
        "We study below whether nonlinear neural networks provably have spurious local minima",
        "We show in \u00a72 and \u00a73 that even for extremely simple nonlinear networks, one encounters spurious local minima",
        "With ReLU-like activation (1) and a few mild assumptions, Theorem 1 shows that there exist spurious local minima",
        "We provide a counterexample nonlinear network and a dataset for which a wide range of nonlinear activations result in a local minimum that is strictly inferior to the global minimum with exactly zero empirical risk",
        "Assuming that the hidden layers are at least as wide as either the input or output, we show that critical points of the loss with a multilinear parameterization inherit the type of critical points of the loss with a linear parameterization",
        "We show that for differentiable losses whose critical points are globally optimal, deep linear networks have only global minima or saddle points",
        "With additional assumption that critical points of 0 are global minima, <a class=\"ref-link\" id=\"cLaurent_2018_b\" href=\"#rLaurent_2018_b\">Laurent & Brecht (2018b</a>) showed that \u201clocal min is global\u201d property holds for linear neural networks; our Corollay 5 gives a simple and efficient test condition as well as proving there are only global minima and saddles, which is clearly stronger.\n5 DISCUSSION AND FUTURE WORK",
        "We investigated the loss surface of deep linear and nonlinear neural networks",
        "We proved two theorems showing existence of spurious local minima on nonlinear networks, which apply to almost all datasets (Theorem 1) and a wide class of activations (Theorem 2)",
        "We concluded by Theorem 4, showing a general result studying the behavior of critical points in multilinearly parametrized functions, which unifies other existing results on linear neural networks",
        "Given that spurious local minima are common in neural networks, a valuable future research direction will be investigating how far local minima are from global minima in general, and how the size of the network affects this gap",
        "Another thing to note is that even though we showed the existence of spurious local minima in the whole parameter space, things can be different in restricted sets of parameter space"
    ],
    "summary": [
        "Neural network training reduces to solving nonconvex empirical risk minimization problems, a task that is in general intractable.",
        "For piecewise linear and nonnegative homogeneous activation functions (e.g., ReLU), we prove in Theorem 1 that if linear models cannot perfectly fit the data, one can construct infinitely many local minima that are not global.",
        "Noticing that most real world datasets cannot be perfectly fit with linear models, Theorem 1 shows that when we use the activation hs+,s\u2212 , the empirical risk has bad local minima for almost all datasets that one may encounter in practice.",
        "It is worth comparing our result with <a class=\"ref-link\" id=\"cLaurent_2018_a\" href=\"#rLaurent_2018_a\">Laurent & Brecht (2018a</a>), who use hinge loss based classification and assume linear separability to prove \u201cno spurious local minima\u201d for Leaky-ReLU networks.",
        "To construct parameters (Wj, \u0303bj)j2=1 that have strictly smaller risk than (Wj, \u02c6bj)j2=12j=1 is a spurious local minimum), we make the sign of inputs to the hidden nodes different depending on data.",
        "We provide a counterexample nonlinear network and a dataset for which a wide range of nonlinear activations result in a local minimum that is strictly inferior to the global minimum with exactly zero empirical risk.",
        "Recalling again s+ = 1 + and s\u2212 = 1, this means that even with the \u201cslightest\u201d nonlinearity in activation function, the network has a global minimum with risk zero while there exists a bad local minimum that performs just as linear least squares models.",
        "For models without bias parameters, (Wj)2j=1 is still a spurious local minimum, showing that <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al (2018</a>) fails to extend to empirical risks and non-unit weight vectors.",
        "Our result generalizes previous works on linear networks such as <a class=\"ref-link\" id=\"cKawaguchi_2016_a\" href=\"#rKawaguchi_2016_a\">Kawaguchi (2016</a>); <a class=\"ref-link\" id=\"cYun_et+al_2018_a\" href=\"#rYun_et+al_2018_a\">Yun et al (2018</a>); <a class=\"ref-link\" id=\"cZhou_2018_a\" href=\"#rZhou_2018_a\">Zhou & Liang (2018</a>), because it provides conditions for global optimality for a broader range of loss functions without assumptions on datasets.",
        "With additional assumption that critical points of 0 are global minima, <a class=\"ref-link\" id=\"cLaurent_2018_b\" href=\"#rLaurent_2018_b\">Laurent & Brecht (2018b</a>) showed that \u201clocal min is global\u201d property holds for linear neural networks; our Corollay 5 gives a simple and efficient test condition as well as proving there are only global minima and saddles, which is clearly stronger.",
        "We proved two theorems showing existence of spurious local minima on nonlinear networks, which apply to almost all datasets (Theorem 1) and a wide class of activations (Theorem 2).",
        "We concluded by Theorem 4, showing a general result studying the behavior of critical points in multilinearly parametrized functions, which unifies other existing results on linear neural networks."
    ],
    "headline": "We investigate the loss surface of neural networks",
    "reference_links": [
        {
            "id": "Allen-Zhu_et+al_2018_a",
            "entry": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. arXiv preprint arXiv:1811.03962, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.03962"
        },
        {
            "id": "Baldi_1989_a",
            "entry": "Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53\u201358, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "Bengio_et+al_2006_a",
            "entry": "Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Advances in neural information processing systems, pp. 123\u2013130, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoshua%20Bengio%20Nicolas%20L%20Roux%20Pascal%20Vincent%20Olivier%20Delalleau%20and%20Patrice%20Marcotte%20Convex%20neural%20networks%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20123130%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoshua%20Bengio%20Nicolas%20L%20Roux%20Pascal%20Vincent%20Olivier%20Delalleau%20and%20Patrice%20Marcotte%20Convex%20neural%20networks%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20123130%202006"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with gaussian inputs. In International Conference on Machine Learning, pp. 605\u2013614, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20ConvNet%20with%20gaussian%20inputs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20ConvNet%20with%20gaussian%20inputs%202017"
        },
        {
            "id": "Brutzkus_et+al_2018_a",
            "entry": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20SGD%20learns%20overparameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20SGD%20learns%20overparameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Du_et+al_0000_a",
            "entry": "Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1811.03804"
        },
        {
            "id": "Du_et+al_2018_a",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer CNN: Dont be afraid of spurious local minima. In International Conference on Machine Learning, pp. 1338\u20131347, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Singh%2C%20Aarti%20Gradient%20descent%20learns%20one-hidden-layer%20CNN%3A%20Dont%20be%20afraid%20of%20spurious%20local%20minima%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Singh%2C%20Aarti%20Gradient%20descent%20learns%20one-hidden-layer%20CNN%3A%20Dont%20be%20afraid%20of%20spurious%20local%20minima%202018"
        },
        {
            "id": "Du_et+al_0000_b",
            "entry": "Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018c.",
            "arxiv_url": "https://arxiv.org/pdf/1810.02054"
        },
        {
            "id": "Feizi_et+al_2017_a",
            "entry": "Soheil Feizi, Hamid Javadi, Jesse Zhang, and David Tse. Porcupine neural networks:(almost) all local optima are global. arXiv preprint arXiv:1710.02196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.02196"
        },
        {
            "id": "Freeman_2017_a",
            "entry": "C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017"
        },
        {
            "id": "Haeffele_2017_a",
            "entry": "Benjamin D Haeffele and Rene Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331\u20137339, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Rene%20Global%20optimality%20in%20neural%20network%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Rene%20Global%20optimality%20in%20neural%20network%20training%202017"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Klambauer_et+al_2017_a",
            "entry": "Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, pp. 972\u2013981, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20Gunter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Self-normalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20Gunter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Self-normalizing%20neural%20networks%202017"
        },
        {
            "id": "Krantz_2002_a",
            "entry": "Steven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science & Business Media, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krantz%2C%20Steven%20G.%20Parks%2C%20Harold%20R.%20A%20primer%20of%20real%20analytic%20functions%202002"
        },
        {
            "id": "Laurent_2018_a",
            "entry": "Thomas Laurent and James Brecht. The multilinear structure of ReLU networks. In International Conference on Machine Learning, pp. 2914\u20132922, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Thomas%20Brecht%2C%20James%20The%20multilinear%20structure%20of%20ReLU%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Thomas%20Brecht%2C%20James%20The%20multilinear%20structure%20of%20ReLU%20networks%202018"
        },
        {
            "id": "Laurent_2018_b",
            "entry": "Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are global. In International Conference on Machine Learning, pp. 2908\u20132913, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Thomas%20Brecht%2C%20James%20Deep%20linear%20networks%20with%20arbitrary%20loss%3A%20All%20local%20minima%20are%20global%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Thomas%20Brecht%2C%20James%20Deep%20linear%20networks%20with%20arbitrary%20loss%3A%20All%20local%20minima%20are%20global%202018"
        },
        {
            "id": "Li_2018_a",
            "entry": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8168\u2013 8177, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Liang%2C%20Yingyu%20Learning%20overparameterized%20neural%20networks%20via%20stochastic%20gradient%20descent%20on%20structured%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Liang%2C%20Yingyu%20Learning%20overparameterized%20neural%20networks%20via%20stochastic%20gradient%20descent%20on%20structured%20data%202018"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. In Advances in Neural Information Processing Systems, pp. 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20ReLU%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20ReLU%20activation%202017"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant. Adding one neuron can eliminate all bad local minima. In Advances in Neural Information Processing Systems, pp. 4355\u20134365, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Shiyu%20Sun%2C%20Ruoyu%20Lee%2C%20Jason%20D.%20Srikant%2C%20R.%20Adding%20one%20neuron%20can%20eliminate%20all%20bad%20local%20minima%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Shiyu%20Sun%2C%20Ruoyu%20Lee%2C%20Jason%20D.%20Srikant%2C%20R.%20Adding%20one%20neuron%20can%20eliminate%20all%20bad%20local%20minima%202018"
        },
        {
            "id": "Liang_et+al_2018_b",
            "entry": "Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of neural networks for binary classification. In International Conference on Machine Learning, pp. 2840\u20132849, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Shiyu%20Sun%2C%20Ruoyu%20Li%2C%20Yixuan%20Srikant%2C%20Rayadurgam%20Understanding%20the%20loss%20surface%20of%20neural%20networks%20for%20binary%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Shiyu%20Sun%2C%20Ruoyu%20Li%2C%20Yixuan%20Srikant%2C%20Rayadurgam%20Understanding%20the%20loss%20surface%20of%20neural%20networks%20for%20binary%20classification%202018"
        },
        {
            "id": "Lu_2017_a",
            "entry": "Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08580"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 2603\u20132612, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep CNNs. In International Conference on Machine Learning, pp. 3727\u20133736, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20CNNs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20CNNs%202018"
        },
        {
            "id": "Safran_2018_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In International Conference on Machine Learning, pp. 4430\u20134438, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20ReLU%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20ReLU%20neural%20networks%202018"
        },
        {
            "id": "Shamir_2018_a",
            "entry": "Ohad Shamir. Are ResNets provably better than linear predictors? In Advances in Neural Information Processing Systems, pp. 505\u2013514, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Are%20ResNets%20provably%20better%20than%20linear%20predictors%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Are%20ResNets%20provably%20better%20than%20linear%20predictors%3F%202018"
        },
        {
            "id": "Soltanolkotabi_2007_a",
            "entry": "Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In Advances in Neural Information Processing Systems, pp. 2007\u20132017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Learning%20ReLUs%20via%20gradient%20descent%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Learning%20ReLUs%20via%20gradient%20descent%202007"
        },
        {
            "id": "Soudry_2016_a",
            "entry": "Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "Swirszcz_et+al_2016_a",
            "entry": "Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of neural networks. arXiv preprint arXiv:1611.06310, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06310"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In International Conference on Machine Learning, pp. 3404\u20133413, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20ReLU%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20ReLU%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017"
        },
        {
            "id": "Venturi_et+al_2018_a",
            "entry": "Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06384"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Gang Wang, Georgios B Giannakis, and Jie Chen. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04685"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Chenwei Wu, Jiajun Luo, and Jason D Lee. No spurious local minima in a two hidden unit ReLU network. In International Conference on Learning Representations Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Chenwei%20Luo%2C%20Jiajun%20Lee%2C%20Jason%20D.%20No%20spurious%20local%20minima%20in%20a%20two%20hidden%20unit%20ReLU%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Chenwei%20Luo%2C%20Jiajun%20Lee%2C%20Jason%20D.%20No%20spurious%20local%20minima%20in%20a%20two%20hidden%20unit%20ReLU%20network%202018"
        },
        {
            "id": "Xie_et+al_2016_a",
            "entry": "Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv preprint arXiv:1611.03131, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03131"
        },
        {
            "id": "Yu_1995_a",
            "entry": "Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300\u20131303, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Xiao-Hu%20Chen%2C%20Guo-An%20On%20the%20local%20minima%20free%20condition%20of%20backpropagation%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Xiao-Hu%20Chen%2C%20Guo-An%20On%20the%20local%20minima%20free%20condition%20of%20backpropagation%20learning%201995"
        },
        {
            "id": "Yun_et+al_2018_a",
            "entry": "Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yun%2C%20Chulhee%20Sra%2C%20Suvrit%20Jadbabaie%2C%20Ali%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yun%2C%20Chulhee%20Sra%2C%20Suvrit%20Jadbabaie%2C%20Ali%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202018"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07808"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In International Conference on Machine Learning, pp. 4140\u20134149, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20Kai%20Song%2C%20Zhao%20Jain%2C%20Prateek%20Bartlett%2C%20Peter%20L.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20Kai%20Song%2C%20Zhao%20Jain%2C%20Prateek%20Bartlett%2C%20Peter%20L.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape properties. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Yi%20Liang%2C%20Yingbin%20Critical%20points%20of%20neural%20networks%3A%20Analytical%20forms%20and%20landscape%20properties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Yi%20Liang%2C%20Yingbin%20Critical%20points%20of%20neural%20networks%3A%20Analytical%20forms%20and%20landscape%20properties%202018"
        },
        {
            "id": "Zhou_et+al_2019_a",
            "entry": "Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global minimum in deep learning via star-convex path. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Yi%20Yang%2C%20Junjie%20Zhang%2C%20Huishuai%20Liang%2C%20Yingbin%20SGD%20converges%20to%20global%20minimum%20in%20deep%20learning%20via%20star-convex%20path%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Yi%20Yang%2C%20Junjie%20Zhang%2C%20Huishuai%20Liang%2C%20Yingbin%20SGD%20converges%20to%20global%20minimum%20in%20deep%20learning%20via%20star-convex%20path%202019"
        },
        {
            "id": "Zou_et+al_2018_a",
            "entry": "Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.08888"
        },
        {
            "id": "2",
            "entry": "2. If yj is non-duplicate, meaning that yj\u22121 < yj < yj+1, yj = yj holds.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20yj%20is%20nonduplicate%20meaning%20that%20yj1%20%20yj%20%20yj1%20yj%20%20yj%20holds"
        },
        {
            "id": "3",
            "entry": "3. If yj is duplicate, i:yi=yj (yi \u2212 yi) = 0 holds.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20yj%20is%20duplicate%20iyiyj%20yi%20%20yi%20%200%20holds"
        },
        {
            "id": "4",
            "entry": "4. There exists at least one duplicate yj such that, for that yj, there exist at least two different i\u2019s that satisfy yi = yj and yi = yi.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=There%20exists%20at%20least%20one%20duplicate%20yj%20such%20that%20for%20that%20yj%20there%20exist%20at%20least%20two%20different%20is%20that%20satisfy%20yi%20%20yj%20and%20yi%20%20yi"
        },
        {
            "id": "1",
            "entry": "1. If all the yj\u2019s are distinct and J = \u2205, by definition of J, yj = yj for all j. This violates our assumption that linear models cannot perfectly fit Y.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20all%20the%20yjs%20are%20distinct%20and%20J%20%20%20by%20definition%20of%20J%20yj%20%20yj%20for%20all%20j%20This%20violates%20our%20assumption%20that%20linear%20models%20cannot%20perfectly%20fit%20Y"
        },
        {
            "id": "2",
            "entry": "2. If we have yj = yj for a non-duplicate yj, at least one of the following statements must hold: i\u2264j\u22121(yi \u2212 yi) = 0 or i\u2264j(yi \u2212 yi) = 0, meaning that j \u2212 1 \u2208 J or j \u2208 J.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=If%20we%20have%20yj%20%20yj%20for%20a%20nonduplicate%20yj%20at%20least%20one%20of%20the%20following%20statements%20must%20hold%20ij1yi%20%20yi%20%200%20or%20ijyi%20%20yi%20%200%20meaning%20that%20j%20%201%20%20J%20or%20j%20%20J"
        },
        {
            "id": "4",
            "entry": "4. Since i:yi=yj (yi \u2212 yi) = 0 holds for any duplicate yj, if yi = yi holds for one i then there must be at least two of them that satisfies yi = yi. If this doesn\u2019t hold for all duplicate yi, with Part 2 this means that yj = yj holds for all j. This violates our assumption that linear models cannot perfectly fit Y.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Since%20iyiyj%20yi%20%20yi%20%200%20holds%20for%20any%20duplicate%20yj%20if%20yi%20%20yi%20holds%20for%20one%20i%20then%20there%20must%20be%20at%20least%20two%20of%20them%20that%20satisfies%20yi%20%20yi%20If%20this%20doesnt%20hold%20for%20all%20duplicate%20yi%20with%20Part%202%20this%20means%20that%20yj%20%20yj%20holds%20for%20all%20j%20This%20violates%20our%20assumption%20that%20linear%20models%20cannot%20perfectly%20fit%20Y"
        },
        {
            "id": "2",
            "entry": "2. The constant M is the largest xi 2 among all the indices, and g is one fourth times the minimum gap between all distinct values of yi.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20constant%20M%20is%20the%20largest%20xi%202%20among%20all%20the%20indices%20and%20g%20is%20one%20fourth%20times%20the%20minimum%20gap%20between%20all%20distinct%20values%20of%20yi"
        },
        {
            "id": "0",
            "entry": "0. Similarly, for all i such that yi > y\u2217 = yj2, yi",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Similarly%20for%20all%20i%20such%20that%20yi%20%20y%20%20yj2%20yi"
        },
        {
            "id": "0",
            "entry": "0. For j \u2208 J\u2265\u2217 (j \u2264 j1), we know yj = y\u2217, so yj \u2212 \u03b1vT xj \u2212 \u03b2 =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20j%20%20J%20j%20%20j1%20we%20know%20yj%20%20y%20so%20yj%20%20%CE%B1vT%20xj%20%20%CE%B2"
        },
        {
            "id": "0",
            "entry": "0. Also, for j \u2208 J<\u2217 (j > j1), yj \u2212 \u03b1vT xj \u2212 \u03b2 =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Also%20for%20j%20%E2%88%88%20J%3C%E2%88%97%20%28j%20%3E%20j1%29%2C%20yj%20%E2%88%92%20%CE%B1vT%20xj%20%E2%88%92%20%CE%B2%20%3D"
        },
        {
            "id": "0",
            "entry": "0. This finishes the case analysis and proves the first statements of the lemma. For the proof of this corollary, we present the values of real numbers that satisfy assumptions (C2.1)\u2013 (C2.7), for each activation function listed in the corollary: sigmoid, tanh, arctan, exponential linear units (ELU, Clevert et al. (2015)), scaled exponential linear units (SELU, Klambauer et al. (2017)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20finishes%20the%20case%20analysis%20and%20proves%20the%20first%20statements%20of%20the%20lemma%20For%20the%20proof%20of%20this%20corollary%20we%20present%20the%20values%20of%20real%20numbers%20that%20satisfy%20assumptions%20C21%20C27%20for%20each%20activation%20function%20listed%20in%20the%20corollary%20sigmoid%20tanh%20arctan%20exponential%20linear%20units%20ELU%20Clevert%20et%20al%202015%20scaled%20exponential%20linear%20units%20SELU%20Klambauer%20et%20al%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=This%20finishes%20the%20case%20analysis%20and%20proves%20the%20first%20statements%20of%20the%20lemma%20For%20the%20proof%20of%20this%20corollary%20we%20present%20the%20values%20of%20real%20numbers%20that%20satisfy%20assumptions%20C21%20C27%20for%20each%20activation%20function%20listed%20in%20the%20corollary%20sigmoid%20tanh%20arctan%20exponential%20linear%20units%20ELU%20Clevert%20et%20al%202015%20scaled%20exponential%20linear%20units%20SELU%20Klambauer%20et%20al%202017"
        },
        {
            "id": "Among_2002_a",
            "entry": "Among them, (C2.4)\u2013(C2.5) follow because sigmoid function is an real analytic function Krantz & Parks (2002).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Among%20them%20C24C25%20follow%20because%20sigmoid%20function%20is%20an%20real%20analytic%20function%20Krantz%20%20Parks%202002"
        },
        {
            "id": "0",
            "entry": "0. Also, due to definitions of \u2206j\u2019s, CH\u2212j\u2217+2 =(vj\u2217 vjT\u2217+1)(vj\u2217+1vjT\u2217+2) \u00b7 \u00b7 \u00b7 (vH vHT +1)\u2207 0(W H+1:1)(v0v1T )(Wj\u2217\u22121:2)T",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Also%20due%20to%20definitions%20of%20%E2%88%86j%E2%80%99s%2C%20CH%E2%88%92j%E2%88%97%2B2%20%3D%28vj%E2%88%97%20vjT%E2%88%97%2B1%29%28vj%E2%88%97%2B1vjT%E2%88%97%2B2%29%20%C2%B7%20%C2%B7%20%C2%B7"
        },
        {
            "id": "0",
            "entry": "0. Then, for all j ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Then%20for%20all%20j"
        }
    ]
}
