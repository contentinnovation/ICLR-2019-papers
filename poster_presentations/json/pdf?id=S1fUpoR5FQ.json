{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR",
        "author": "DEEP LEARNING",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1fUpoR5FQ"
        },
        "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-theart result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available. 1"
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "convex functions",
            "url": "https://en.wikipedia.org/wiki/convex_functions"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "QHM": "quasi-hyperbolic momentum",
        "NAG": "Nesterov (1983)\u2019s accelerated gradient",
        "SNV": "SYNTHESIZED NESTEROV VARIANTS",
        "AggMo": "Aggregated Momentum",
        "NMT": "neural machine translation"
    },
    "highlights": [
        "Stochastic gradient descent (SGD) serves as the optimizer of choice for many recent advances in deep learning across domains (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>)",
        "Algorithms Starting with gradient variance reduction as an informal and speculative motivation, we introduce the quasi-hyperbolic momentum (QHM) optimization algorithm in Section 3",
        "Practical validation and considerations In Section 6, we empirically demonstrate that quasi-hyperbolic momentum and QHAdam provide superior optimization in a variety of deep learning settings",
        "We present numerous connections between quasi-hyperbolic momentum and other optimization algorithms",
        "Alternative PID setting In Appendix E, we briefly discuss another PID setting by <a class=\"ref-link\" id=\"cAn_et+al_2018_a\" href=\"#rAn_et+al_2018_a\">An et al (2018</a>) and relate the resulting optimization algorithm to quasi-hyperbolic momentum",
        "quasi-hyperbolic momentum recovers the Robust Momentum method, which is a specific parameterization of SYNTHESIZED NESTEROV VARIANTS (<a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\">Cyrus et al, 2018</a>)"
    ],
    "key_statements": [
        "Stochastic gradient descent (SGD) serves as the optimizer of choice for many recent advances in deep learning across domains (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>)",
        "Algorithms Starting with gradient variance reduction as an informal and speculative motivation, we introduce the quasi-hyperbolic momentum (QHM) optimization algorithm in Section 3",
        "Practical validation and considerations In Section 6, we empirically demonstrate that quasi-hyperbolic momentum and QHAdam provide superior optimization in a variety of deep learning settings",
        "We present numerous connections between quasi-hyperbolic momentum and other optimization algorithms",
        "Alternative PID setting In Appendix E, we briefly discuss another PID setting by <a class=\"ref-link\" id=\"cAn_et+al_2018_a\" href=\"#rAn_et+al_2018_a\">An et al (2018</a>) and relate the resulting optimization algorithm to quasi-hyperbolic momentum",
        "quasi-hyperbolic momentum recovers the Robust Momentum method, which is a specific parameterization of SYNTHESIZED NESTEROV VARIANTS (<a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\">Cyrus et al, 2018</a>)",
        "Unifying two-state optimization algorithms These connections demonstrate that many two-state optimization algorithms are functionally similar or equivalent to each other",
        "We offer some practical suggestions for deep learning practitioners, those who default to momentum, <a class=\"ref-link\" id=\"cNesterov_1983_a\" href=\"#rNesterov_1983_a\">Nesterov (1983</a>)\u2019s accelerated gradient, or Adam with \u03b2 = 0.9 as a rule of thumb:"
    ],
    "summary": [
        "Stochastic gradient descent (SGD) serves as the optimizer of choice for many recent advances in deep learning across domains (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>).",
        "In Section 4, we connect QHM with plain SGD, momentum, Nesterov\u2019s accelerated gradient, PID control algorithms (<a class=\"ref-link\" id=\"cRecht_2018_a\" href=\"#rRecht_2018_a\">Recht, 2018</a>; <a class=\"ref-link\" id=\"cAn_et+al_2018_a\" href=\"#rAn_et+al_2018_a\">An et al, 2018</a>), synthesized Nesterov variants (<a class=\"ref-link\" id=\"cLessard_et+al_2016_a\" href=\"#rLessard_et+al_2016_a\">Lessard et al, 2016</a>), noise-robust momentum (<a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\"><a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\">Cyrus et al, 2018</a></a>), Triple Momentum (<a class=\"ref-link\" id=\"cScoy_et+al_2018_a\" href=\"#rScoy_et+al_2018_a\">Scoy et al, 2018</a>), and least-squares acceleration of SGD (<a class=\"ref-link\" id=\"cKidambi_et+al_2018_a\" href=\"#rKidambi_et+al_2018_a\">Kidambi et al, 2018</a>).",
        "We characterize the set of optimization algorithms that QHM recovers.",
        "Practical validation and considerations In Section 6, we empirically demonstrate that QHM and QHAdam provide superior optimization in a variety of deep learning settings.",
        "Plain SGD The SGD algorithm, parameterized by learning rate \u03b1 \u2208 R, uses the update rule: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b1 \u00b7 \u2207Lt",
        "Momentum The momentum algorithm, parameterized by \u03b1 \u2208 R and \u03b2 \u2208 R, uses the update rule: gt+1 \u2190 \u03b2 \u00b7 gt + (1 \u2212 \u03b2) \u00b7 \u2207Lt",
        "We propose and discuss the quasi-hyperbolic momentum (QHM) algorithm.",
        "This sheds light on the somewhat unintuitive NAG algorithm, providing a natural interpretation of NAG\u2019s update rule as a \u03b2-weighted average between momentum and plain SGD.",
        "Alternative PID setting In Appendix E, we briefly discuss another PID setting by <a class=\"ref-link\" id=\"cAn_et+al_2018_a\" href=\"#rAn_et+al_2018_a\">An et al (2018</a>) and relate the resulting optimization algorithm to QHM.",
        "QHM recovers the Robust Momentum method, which is a specific parameterization of SNV (<a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\"><a class=\"ref-link\" id=\"cCyrus_et+al_2018_a\" href=\"#rCyrus_et+al_2018_a\">Cyrus et al, 2018</a></a>).",
        "This motivates their proposal of the AccSGD algorithm, which yields faster convergence over momentum and NAG in certain least-squares regression settings.",
        "Since QH\u221aM recovers Triple Momentum, QHM recovers the global linear convergence rate of 1 \u2212 1/ \u03ba for strongly convex, smooth loss functions.",
        "In Appendix D, we characterize the set of two-state optimization algorithms recoverable by QHM.",
        "Our hope here is to provide future work with a routine conversion to QHM so that they may leverage the accessibility and efficiency benefits, as well as the many connections to other algorithms.",
        "The gradient distribution is often \u201cspiky\u201d; Adam training often fails to converge due to a very small number of large parameter updates.",
        "We offer some practical suggestions for deep learning practitioners, those who default to momentum, NAG, or Adam with \u03b2 = 0.9 as a rule of thumb:",
        "Consider using QHM or QHAdam, instead of momentum, NAG, or Adam.",
        "Convergence results for QHM in a reasonably general stochastic setting would be appealing, we are not aware of compelling analogous results for momentum or NAG.",
        "We hope that practitioners and researchers will find these algorithms both practically useful and interesting as a subject of further study"
    ],
    "headline": "We propose the quasi-hyperbolic momentum algorithm as an extremely simple alteration of momentum stochastic gradient descent, averaging a plain stochastic gradient descent step with a momentum step",
    "reference_links": [
        {
            "id": "Aastrom_1995_a",
            "entry": "K. J. Aastrom and T. Hagglund. PID Controllers: Theory, Design, and Tuning. Instrument Society of America, Research Triangle Park, NC, 2 edition, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aastrom%2C%20K.J.%20Hagglund%2C%20T.%20PID%20Controllers%3A%20Theory%2C%20Design%2C%20and%20Tuning%201995"
        },
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Allen-Zhu_2017_a",
            "entry": "Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of Machine Learning Research, 18:221:1\u2013221:51, 2017. URL http://jmlr.org/papers/v18/papers/v18/16-410.html.",
            "url": "http://jmlr.org/papers/v18/papers/v18/16-410.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "An_et+al_2018_a",
            "entry": "Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller approach for stochastic optimization of deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=An%2C%20Wangpeng%20Wang%2C%20Haoqian%20Sun%2C%20Qingyun%20A%20pid%20controller%20approach%20for%20stochastic%20optimization%20of%20deep%20networks%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=An%2C%20Wangpeng%20Wang%2C%20Haoqian%20Sun%2C%20Qingyun%20A%20pid%20controller%20approach%20for%20stochastic%20optimization%20of%20deep%20networks%202018-06"
        },
        {
            "id": "Baydin_et+al_2018_a",
            "entry": "Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkrsAzWAb.",
            "url": "https://openreview.net/forum?id=BkrsAzWAb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baydin%2C%20Atilim%20Gunes%20Cornish%2C%20Robert%20Rubio%2C%20David%20Martinez%20Schmidt%2C%20Mark%20Online%20learning%20rate%20adaptation%20with%20hypergradient%20descent%202018"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540.",
            "url": "http://arxiv.org/abs/1606.01540",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Chung_1961_a",
            "entry": "Shin-ho Chung and Richard J Hernstein. Relative and absolute strength of response as a function of frequency of reinforcement 1, 2. Journal of the experimental analysis of behavior, 4(3):267\u2013272, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Shin-ho%20Hernstein%2C%20Richard%20J.%20Relative%20and%20absolute%20strength%20of%20response%20as%20a%20function%20of%20frequency%20of%20reinforcement%201%2C%202%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Shin-ho%20Hernstein%2C%20Richard%20J.%20Relative%20and%20absolute%20strength%20of%20response%20as%20a%20function%20of%20frequency%20of%20reinforcement%201%2C%202%201961"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andrevan Schaik. EMNIST: an extension of MNIST to handwritten letters. CoRR, abs/1702.05373, 2017. URL http://arxiv.org/abs/1702.05373.",
            "url": "http://arxiv.org/abs/1702.05373",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "Cyrus_et+al_2018_a",
            "entry": "Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference, ACC 2018, Milwaukee, WI, USA, June 27-29, 2018, pp. 1376\u20131381, 2018. doi: 10.23919/ACC.2018. 8430824. URL https://doi.org/10.23919/ACC.2018.8430824.",
            "crossref": "https://dx.doi.org/10.23919/ACC.2018.8430824",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.23919/ACC.2018.8430824"
        },
        {
            "id": "Dauphin_et+al_2016_a",
            "entry": "Yann N Dauphin, Angela Fan, Michael Auli Auli, and David Grangier. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08083"
        },
        {
            "id": "Defazio_et+al_2014_a",
            "entry": "Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20R.%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20R.%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014-12-08"
        },
        {
            "id": "Dozat_2016_a",
            "entry": "Timothy Dozat. Incorporating nesterov momentum into adam. ICLR Workshop, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dozat%2C%20Timothy%20Incorporating%20nesterov%20momentum%20into%20adam%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dozat%2C%20Timothy%20Incorporating%20nesterov%20momentum%20into%20adam%202016"
        },
        {
            "id": "Fujimoto_et+al_2018_a",
            "entry": "Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09477"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677.",
            "url": "http://arxiv.org/abs/1706.02677",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 630\u2013645, 2016b. doi: 10.1007/ 978-3-319-46493-0\\ 38. URL https://doi.org/10.1007/978-3-319-46493-0_38.",
            "crossref": "https://dx.doi.org/10.1007/978-3-319-46493-0\\",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/978-3-319-46493-0%5C"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning: Lecture 6a, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Srivastava%2C%20Nitish%20Swersky%2C%20Kevin%20Neural%20networks%20for%20machine%20learning%3A%20Lecture%206a%202012"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448\u2013456, 2015. URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.",
            "url": "http://jmlr.org/proceedings/papers/v37/ioffe15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20ICML%202015%20Lille%20France%20611%20July%202015%20pp%20448456%202015%20URL%20httpjmlrorgproceedingspapersv37ioffe15html"
        },
        {
            "id": "Jain_et+al_2017_a",
            "entry": "Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent. CoRR, abs/1704.08227, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08227"
        },
        {
            "id": "Johnson_2013_a",
            "entry": "Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pp. 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013-12-05"
        },
        {
            "id": "Kidambi_et+al_2018_a",
            "entry": "Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJTutzbA-.",
            "url": "https://openreview.net/forum?id=rJTutzbA-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kidambi%2C%20Rahul%20Netrapalli%2C%20Praneeth%20Jain%2C%20Prateek%20Kakade%2C%20Sham%20M.%20On%20the%20insufficiency%20of%20existing%20momentum%20schemes%20for%20stochastic%20optimization%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of 3rd International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Laibson_1997_a",
            "entry": "David Laibson. Golden eggs and hyperbolic discounting. The Quarterly Journal of Economics, 112 (2):443\u2013478, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laibson%2C%20David%20Golden%20eggs%20and%20hyperbolic%20discounting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laibson%2C%20David%20Golden%20eggs%20and%20hyperbolic%20discounting%201997"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The mnist database of handwritten digits. 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20The%20mnist%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "Lessard_et+al_2016_a",
            "entry": "Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57\u201395, 2016. doi: 10.1137/15M1009597. URL https://doi.org/10.1137/15M1009597.",
            "crossref": "https://dx.doi.org/10.1137/15M1009597",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/15M1009597"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2101\u20132110, 2017. URL http://proceedings.mlr.press/v70/li17f.html.",
            "url": "http://proceedings.mlr.press/v70/li17f.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Qianxiao%20Tai%2C%20Cheng%20E%2C%20Weinan%20Stochastic%20modified%20equations%20and%20adaptive%20stochastic%20gradient%20algorithms%202017-08-06"
        },
        {
            "id": "Liu_2017_a",
            "entry": "Kuang Liu. Pytorch cifar. https://github.com/kuangliu/pytorch-cifar, 2017.",
            "url": "https://github.com/kuangliu/pytorch-cifar"
        },
        {
            "id": "Loizou_2017_a",
            "entry": "Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. CoRR, abs/1712.09677, 2017. URL http://arxiv.org/abs/1712.09677.",
            "url": "http://arxiv.org/abs/1712.09677",
            "arxiv_url": "https://arxiv.org/pdf/1712.09677"
        },
        {
            "id": "Loshchilov_2017_a",
            "entry": "Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101.",
            "url": "http://arxiv.org/abs/1711.05101",
            "arxiv_url": "https://arxiv.org/pdf/1711.05101"
        },
        {
            "id": "Lucas_et+al_2018_a",
            "entry": "James Lucas, Richard S. Zemel, and Roger Grosse. Aggregated momentum: Stability through passive damping. CoRR, abs/1804.00325, 2018. URL http://arxiv.org/abs/1804.00325.",
            "url": "http://arxiv.org/abs/1804.00325",
            "arxiv_url": "https://arxiv.org/pdf/1804.00325"
        },
        {
            "id": "Merity_et+al_2016_a",
            "entry": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.",
            "url": "http://arxiv.org/abs/1609.07843",
            "arxiv_url": "https://arxiv.org/pdf/1609.07843"
        },
        {
            "id": "Mitliagkas_et+al_2016_a",
            "entry": "Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, and Christopher Re. Asynchrony begets momentum, with an application to deep learning. In 54th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2016, Monticello, IL, USA, September 27-30, 2016, pp. 997\u2013 1004, 2016. doi: 10.1109/ALLERTON.2016.7852343. URL https://doi.org/10.1109/ ALLERTON.2016.7852343.",
            "crossref": "https://dx.doi.org/10.1109/ALLERTON.2016.7852343",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ALLERTON.2016.7852343"
        },
        {
            "id": "Nesterov_1983_a",
            "entry": "Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/k 2). In Dokl. Akad. Nauk SSSR, volume 269, pp. 543\u2013547, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20E.%20A%20method%20for%20solving%20the%20convex%20programming%20problem%20with%20convergence%20rate%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20E.%20A%20method%20for%20solving%20the%20convex%20programming%20problem%20with%20convergence%20rate%201983"
        },
        {
            "id": "Ott_et+al_2018_a",
            "entry": "Myle Ott, Sergey Edunov, David Grangier Grangier, and Michael Auli. Scaling neural machine translation. arXiv preprint arXiv:1806.00187, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00187"
        },
        {
            "id": "Paszke_et+al_2016_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Pytorch examples. https://github.com/pytorch/examples, 2016.",
            "url": "https://github.com/pytorch/examples"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Edmund_1968_a",
            "entry": "Edmund S Phelps and Robert A Pollak. On second-best national saving and game-equilibrium growth. The Review of Economic Studies, 35(2):185\u2013199, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edmund%2C%20S.%20Phelps%20and%20Robert%20A%20Pollak.%20On%20second-best%20national%20saving%20and%20game-equilibrium%20growth%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edmund%2C%20S.%20Phelps%20and%20Robert%20A%20Pollak.%20On%20second-best%20national%20saving%20and%20game-equilibrium%20growth%201968"
        },
        {
            "id": "Polyak_1964_a",
            "entry": "BT Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "Recht_2018_a",
            "entry": "Ben Recht. The best things in life are model free. argmin (personal blog), 2018. URL http://www.argmin.net/2018/04/19/pid/.",
            "url": "http://www.argmin.net/2018/04/19/pid/"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ryQu7f-RZ.",
            "url": "https://openreview.net/forum?id=ryQu7f-RZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Roux_2018_a",
            "entry": "Nicolas Le Roux, Reza Babanezhad, and Pierre-Antoine Manzagol. Online variance-reducing optimization, 2018. URL https://openreview.net/forum?id=r1qKBtJvG.",
            "url": "https://openreview.net/forum?id=r1qKBtJvG"
        },
        {
            "id": "Ruder_2016_a",
            "entry": "Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04747"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and FeiFei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Schmidt_et+al_2013_a",
            "entry": "Mark W. Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing finite sums with the stochastic average gradient. CoRR, abs/1309.2388, 2013. URL http://arxiv.org/abs/1309.2388.",
            "url": "http://arxiv.org/abs/1309.2388",
            "arxiv_url": "https://arxiv.org/pdf/1309.2388"
        },
        {
            "id": "Scoy_et+al_2018_a",
            "entry": "Bryan Van Scoy, Randy A. Freeman, and Kevin M. Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2 (1):49\u201354, 2018. doi: 10.1109/LCSYS.2017.2722406. URL https://doi.org/10.1109/ LCSYS.2017.2722406.",
            "crossref": "https://dx.doi.org/10.1109/LCSYS.2017.2722406",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/LCSYS.2017.2722406"
        },
        {
            "id": "Shang_et+al_2017_a",
            "entry": "Fanhua Shang, Yuanyuan Liu, James Cheng, and Jiacheng Zhuo. Fast stochastic variance reduced gradient method with momentum acceleration for machine learning. CoRR, abs/1703.07948, 2017. URL http://arxiv.org/abs/1703.07948.",
            "url": "http://arxiv.org/abs/1703.07948",
            "arxiv_url": "https://arxiv.org/pdf/1703.07948"
        },
        {
            "id": "Sutskever_et+al_2013_a",
            "entry": "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026\u20135033, 2012. doi: 10.1109/IROS.2012. 6386109. URL https://doi.org/10.1109/IROS.2012.6386109.",
            "crossref": "https://dx.doi.org/10.1109/IROS.2012.6386109",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IROS.2012.6386109"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6000\u20136010, 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.",
            "url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaswani%2C%20Ashish%20Shazeer%2C%20Noam%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20Attention%20is%20all%20you%20need%202017-12"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 2048\u20132057, 2015. URL http://jmlr.org/proceedings/papers/v37/xuc15.html.",
            "url": "http://jmlr.org/proceedings/papers/v37/xuc15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015-07-06"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09541"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Jian Zhang, Ioannis Mitliagkas, and Christopher Re. Yellowfin and the art of momentum tuning. CoRR, abs/1706.03471, 2017. URL http://arxiv.org/abs/1706.03471.",
            "url": "http://arxiv.org/abs/1706.03471",
            "arxiv_url": "https://arxiv.org/pdf/1706.03471"
        }
    ]
}
