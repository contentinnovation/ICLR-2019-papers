{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DISTRIBUTION-INTERPOLATION TRADE OFF IN GENERATIVE MODELS",
        "author": "Damian Lesniak, Jagiellonian University",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyMhLo0qKQ"
        },
        "abstract": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors \u2013 regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions."
    },
    "keywords": [
        {
            "term": "multidimensional probability distribution",
            "url": "https://en.wikipedia.org/wiki/multidimensional_probability_distribution"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "linear interpolation",
            "url": "https://en.wikipedia.org/wiki/linear_interpolation"
        },
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        },
        {
            "term": "prior distribution",
            "url": "https://en.wikipedia.org/wiki/prior_distribution"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "cauchy distribution",
            "url": "https://en.wikipedia.org/wiki/cauchy_distribution"
        }
    ],
    "abbreviations": {
        "VAEs": "Variational Auto-Encoders",
        "GANs": "Generative Adversarial Networks"
    },
    "highlights": [
        "Generative latent variable models have grown to be a very popular research topic, with Variational Auto-Encoders (VAEs) (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\"><a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a></a>) and Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) gaining a lot of interest in the last few years",
        "It should be noted that if D is large enough, the distribution of the norms of vectors sampled from the D-dimensional Cauchy distribution has a low density near zero \u2013 to the normal and uniform distributions \u2013 but linear interpolations do not oversample this part of the latent space, due to the heavy-tailed nature of the Cauchy distribution",
        "We investigated the properties of multidimensional probability distributions in the context of generative models",
        "We found out that there is a certain trade-off: it is impossible to define a latent probability distribution with a finite mean and the linear interpolation invariance property",
        "The D-dimensional Cauchy distribution serves as an example of a latent probability distribution that remains unchanged by linear interpolation, at the cost of poor model performance, due to the heavytailed nature",
        "Instead of using the Cauchy distribution as the latent distribution, we propose to use it to define nonlinear interpolations that have the distribution matching property"
    ],
    "key_statements": [
        "Generative latent variable models have grown to be a very popular research topic, with Variational Auto-Encoders (VAEs) (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\"><a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a></a>) and Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) gaining a lot of interest in the last few years",
        "In section 2.1 we show that if the linear interpolation does not change the latent probability distribution, it must be trivial or \"pathological\"",
        "Figure 1 presents a decoded linear interpolations between random latent vectors using a DCGAN model trained on the CelebA dataset for the Cauchy distribution and the distribution from <a class=\"ref-link\" id=\"cKilcher_et+al_2017_a\" href=\"#rKilcher_et+al_2017_a\">Kilcher et al (2017</a>)",
        "It should be noted that if D is large enough, the distribution of the norms of vectors sampled from the D-dimensional Cauchy distribution has a low density near zero \u2013 to the normal and uniform distributions \u2013 but linear interpolations do not oversample this part of the latent space, due to the heavy-tailed nature of the Cauchy distribution",
        "We investigated the properties of multidimensional probability distributions in the context of generative models",
        "We found out that there is a certain trade-off: it is impossible to define a latent probability distribution with a finite mean and the linear interpolation invariance property",
        "The D-dimensional Cauchy distribution serves as an example of a latent probability distribution that remains unchanged by linear interpolation, at the cost of poor model performance, due to the heavytailed nature",
        "Instead of using the Cauchy distribution as the latent distribution, we propose to use it to define nonlinear interpolations that have the distribution matching property"
    ],
    "summary": [
        "Generative latent variable models have grown to be a very popular research topic, with Variational Auto-Encoders (VAEs) (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\"><a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a></a>) and Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) gaining a lot of interest in the last few years.",
        "The solution is, either, to change the latent distribution so that the linear interpolation will not cause a distribution mismatch, or redefine the shortest path property.",
        "Let us assume that we want to train a generative model which has a D-dimensional latent space and a fixed latent probability distribution, defined by a random variable Z.",
        "If Z defines a distribution on the D-dimensional latent space, Z(1) and Z(2) are independent and distributed identically to Z, and for every \u03bb \u2208 [0, 1] the random variable f L(Z(1), Z(2), \u03bb) := (1 \u2212 \u03bb)Z(1) + \u03bbZ(2) is distributed identically to Z, we will say that Z has the linear interpolation invariance property, or that linear interpolation does not change the distribution of Z.",
        "Figure 1 presents a decoded linear interpolations between random latent vectors using a DCGAN model trained on the CelebA dataset for the Cauchy distribution and the distribution from <a class=\"ref-link\" id=\"cKilcher_et+al_2017_a\" href=\"#rKilcher_et+al_2017_a\">Kilcher et al (2017</a>).",
        "It should be noted that if D is large enough, the distribution of the norms of vectors sampled from the D-dimensional Cauchy distribution has a low density near zero \u2013 to the normal and uniform distributions \u2013 but linear interpolations do not oversample this part of the latent space, due to the heavy-tailed nature of the Cauchy distribution.",
        "In case of the Cauchy distribution we observed issues with generating images if the norm of the sampled latent vector was relatively large.",
        "If Z defines a distribution on the D-dimensional latent space, Z(1) and Z(2) are independent and distributed identically to Z, and for every \u03bb \u2208 [0, 1] the random variable fZ(1),Z(2) (\u03bb) is distributed identically to Z, we will say that the interpolation f has the distribution matching property in conjunction with Z, or that the interpolation f does not change the distribution of Z.",
        "Figure 3 shows comparison of the Cauchy-linear and the spherical Cauchy-linear interpolations on a two-dimensional plane for pairs of vectors sampled from different probability distributions.",
        "Figure 4 is an illustration of distribution matching property for Cauchy-linear interpolation.",
        "We found out that there is a certain trade-off: it is impossible to define a latent probability distribution with a finite mean and the linear interpolation invariance property.",
        "The D-dimensional Cauchy distribution serves as an example of a latent probability distribution that remains unchanged by linear interpolation, at the cost of poor model performance, due to the heavytailed nature.",
        "Instead of using the Cauchy distribution as the latent distribution, we propose to use it to define nonlinear interpolations that have the distribution matching property.",
        "The assumption of the shortest path being a straight line must be relaxed, but our scheme is general enough to provide a way of incorporating other desirable properties"
    ],
    "headline": "We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models",
    "reference_links": [
        {
            "id": "Agustsson_et+al_2017_a",
            "entry": "Eirikur Agustsson, Alexander Sage, Radu Timofte, and Luc Van Gool. Optimal transport maps for distribution preserving operations on latent spaces of generative models. arXiv:1711.01970, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01970"
        },
        {
            "id": "Georgios_2017_a",
            "entry": "Georgios Arvanitidis, Lars Kai Hansen, and S\u00f8ren Hauberg. Latent space oddity: on the curvature of deep generative models. arXiv:1710.11379, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11379"
        },
        {
            "id": "Ferenc_2017_a",
            "entry": "Husz\u00e1r Ferenc. Gaussian distributions are soap bubbles. http://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/, 2017. Accessed:2018-09-12.",
            "url": "http://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Ioffe_0000_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Kilcher_et+al_2017_a",
            "entry": "Yannic Kilcher, Aurelien Lucchi, and Thomas Hofmann. Semantic interpolation in implicit models. arXiv:1710.11381, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11381"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730\u20133738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv:1301.3781, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "Nolan_2018_a",
            "entry": "John Nolan. Stable Distributions - Models for Heavy Tailed Data. Birkhauser, Boston, 2018. In progress, Chapter 1 online at http://fs2.american.edu/jpnolan/www/stable/stable.html.",
            "url": "http://fs2.american.edu/jpnolan/www/stable/stable.html"
        },
        {
            "id": "Radford_et+al_0000_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Shoemake_1985_a",
            "entry": "Ken Shoemake. Animating rotation with quaternion curves. In ACM SIGGRAPH computer graphics, volume 19, pp. 245\u2013254. ACM, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shoemake%2C%20Ken%20Animating%20rotation%20with%20quaternion%20curves%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shoemake%2C%20Ken%20Animating%20rotation%20with%20quaternion%20curves%201985"
        },
        {
            "id": "Random_2017_a",
            "entry": "Kyle Siegrist. Random. http://www.randomservices.org/random/special/ Cauchy.html, 2017. Accessed:2018-09-12.",
            "url": "http://www.randomservices.org/random/special/Cauchy.html"
        },
        {
            "id": "White_2016_a",
            "entry": "Tom White. Sampling generative networks: Notes on a few effective techniques. arXiv:1609.04468, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04468"
        },
        {
            "id": "All_2015_a",
            "entry": "All experiments were conducted using a DCGAN model (Radford et al., 2015), in which the generator network consisted of a linear layer with 8192 neurons, followed by four convolution transposition layers, each using 5 \u00d7 5 filters and strides of 2, with number of filters in order of layers: 256, 128, 64, 3. Except for the output layer, where tanh activation function was used, all previous layers used ReLU. Discriminator\u2019s architecture mirrored the one from the generator, with a single exception of using leaky ReLU instead of vanilla ReLU function for all except the last layer. No batch normalisation was used in both networks. Adam optimiser with learning rate of 2e\u22124 and momentum set to 0.5 was used. Batch size 64 was used throughout all experiments. If not explicitly stated otherwise, latent space dimension was set to 100. For the CelebA dataset we resized the input images to 64 \u00d7 64.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=All%20experiments%20were%20conducted%20using%20a%20DCGAN%20model%20Radford%20et%20al%202015%20in%20which%20the%20generator%20network%20consisted%20of%20a%20linear%20layer%20with%208192%20neurons%20followed%20by%20four%20convolution%20transposition%20layers%20each%20using%205%20%205%20filters%20and%20strides%20of%202%20with%20number%20of%20filters%20in%20order%20of%20layers%20256%20128%2064%203%20Except%20for%20the%20output%20layer%20where%20tanh%20activation%20function%20was%20used%20all%20previous%20layers%20used%20ReLU%20Discriminators%20architecture%20mirrored%20the%20one%20from%20the%20generator%20with%20a%20single%20exception%20of%20using%20leaky%20ReLU%20instead%20of%20vanilla%20ReLU%20function%20for%20all%20except%20the%20last%20layer%20No%20batch%20normalisation%20was%20used%20in%20both%20networks%20Adam%20optimiser%20with%20learning%20rate%20of%202e4%20and%20momentum%20set%20to%2005%20was%20used%20Batch%20size%2064%20was%20used%20throughout%20all%20experiments%20If%20not%20explicitly%20stated%20otherwise%20latent%20space%20dimension%20was%20set%20to%20100%20For%20the%20CelebA%20dataset%20we%20resized%20the%20input%20images%20to%2064%20%2064"
        },
        {
            "id": "1",
            "entry": "1. The random variables",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20random%20variables"
        },
        {
            "id": "4",
            "entry": "4. The random variable f SCL",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20random%20variable%20f%20SCL"
        },
        {
            "id": "5",
            "entry": "5. The random variable f SCL ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20random%20variable%20f%20SCL"
        }
    ]
}
