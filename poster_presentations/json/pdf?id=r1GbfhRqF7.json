{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "KERNEL CHANGE-POINT DETECTION WITH AUXILIARY DEEP GENERATIVE MODELS",
        "author": "Wei-Cheng Chang, Chun-Liang Li, Yiming Yang & Barnab\u00e1s P\u00f3czos Carnegie Mellon University Pittsburgh, PA 15213, USA {wchang,chunlial,yiming,bapoczos}@cs.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1GbfhRqF7"
        },
        "abstract": "Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for two-sample test has been studied, the insufficient samples in change point detection problem hinders the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel twosample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies."
    },
    "keywords": [
        {
            "term": "time series",
            "url": "https://en.wikipedia.org/wiki/time_series"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "receiver operating characteristic",
            "url": "https://en.wikipedia.org/wiki/receiver_operating_characteristic"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "CPD": "change point detection",
        "MMD": "maximum mean discrepancy",
        "RKHS": "reproducing kernel Hilbert space",
        "RNNs": "recurrent neural networks",
        "ROC": "receiver operating characteristic",
        "NSF": "National Science Foundation"
    },
    "highlights": [
        "Detecting changes in the temporal evolution of a system in time series analysis has attracted considerable attention in machine learning and data mining for decades (Basseville et al, 1993; <a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\"><a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\">Brodsky & Darkhovsky, 2013</a></a>)",
        "KL-change point detection shows significant improvement over the other methods on all the datasets, except being in a second place on the Yahoo dataset, with 2% lower AUC compared to the leading ARGP",
        "Notice that OPT-maximum mean discrepancy performs not so good compared to KL-change point detection, which again verifies our simulated example in Sec. 3 that directly applying existing kernel learning approaches with insufficient samples may not be suitable for realworld change point detection task",
        "We propose KL-change point detection, a new kernel learning framework for two-sample test by optimizing a lower bound of test power with a auxiliary generator, to resolve the issue of insufficient samples in changepoints detection",
        "The deep kernel parametrization of KL-change point detection combines the latent space of recurrent neural networks with RBF kernels that effectively detect a variety of change-points from different real-world applications",
        "Extensive evaluation of our new approach along with strong baseline methods on benchmark datasets shows the outstanding performance of the proposed method in retrospective change point detection"
    ],
    "key_statements": [
        "Detecting changes in the temporal evolution of a system in time series analysis has attracted considerable attention in machine learning and data mining for decades (Basseville et al, 1993; <a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\"><a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\">Brodsky & Darkhovsky, 2013</a></a>)",
        "As shown in Fig. 1, we focus on the retrospective change point detection (<a class=\"ref-link\" id=\"cTakeuchi_2006_a\" href=\"#rTakeuchi_2006_a\">Takeuchi & Yamanishi, 2006</a>; <a class=\"ref-link\" id=\"cLi_et+al_2015_a\" href=\"#rLi_et+al_2015_a\">Li et al, 2015a</a>), which allows a flexible time window to react on the change-points",
        "Kernel two-sample test has been applied to time series change point detection with some success",
        "We propose KL-change point detection, a kernel learning framework for time series change point detection",
        "We propose to optimize a lower bound of the test power via an auxiliary generative model, which aims at serving as a surrogate of the abnormal events",
        "In Section 4, we present a deep kernel parametrization of our framework, which endows a data-driven kernel for the kernel two-sample test",
        "We review maximum mean discrepancy (MMD) and its use to two-sample test, which are two cornerstones in this work",
        "KL-change point detection shows significant improvement over the other methods on all the datasets, except being in a second place on the Yahoo dataset, with 2% lower AUC compared to the leading ARGP",
        "Notice that OPT-maximum mean discrepancy performs not so good compared to KL-change point detection, which again verifies our simulated example in Sec. 3 that directly applying existing kernel learning approaches with insufficient samples may not be suitable for realworld change point detection task",
        "In Table 2, we studied a kernel learning baseline, OPT-maximum mean discrepancy, that optimizing an recurrent neural networks parameterized kernel by controlling type-I error without the surrogate distribution",
        "Retrospective-change point detection (ARGP-BOCPD, RDR-KCPD, Mstats-KCPD) have better results compared to real-time change point detection (ARMA, ARGP, recurrent neural networks,LSTNet), which is not the case in real-world datasets",
        "This suggests low reconstruction error does not necessarily lead to good change point detection accuracies",
        "KL-change point detection finds a better kernel than maximum mean discrepancy-codespace by optimizing the lower bound of the test power",
        "We propose KL-change point detection, a new kernel learning framework for two-sample test by optimizing a lower bound of test power with a auxiliary generator, to resolve the issue of insufficient samples in changepoints detection",
        "The deep kernel parametrization of KL-change point detection combines the latent space of recurrent neural networks with RBF kernels that effectively detect a variety of change-points from different real-world applications",
        "Extensive evaluation of our new approach along with strong baseline methods on benchmark datasets shows the outstanding performance of the proposed method in retrospective change point detection"
    ],
    "summary": [
        "Detecting changes in the temporal evolution of a system in time series analysis has attracted considerable attention in machine learning and data mining for decades (Basseville et al, 1993; <a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\"><a class=\"ref-link\" id=\"cBrodsky_2013_a\" href=\"#rBrodsky_2013_a\">Brodsky & Darkhovsky, 2013</a></a>).",
        "Kernel two-sample test has been applied to time series CPD with some success.",
        "Kernel selection by optimizing the test power, is not directly applicable for time series CPD due to insufficient samples, as we discuss in Section 3.",
        "The difficulty of choosing kernels via optimizing test power in Eq (2) is that we have very limited samples from the abnormal distribution Q.",
        "This toy example not only suggesets that optimizing kernel with surrogate distribution G leads to better test power when samples from Q are insufficient, but demonstrates that the effectiveness of our kernel selection objective holds without introducing any autoregressive/RNN modeling to control the Type-I error.",
        "Notice that OPT-MMD performs not so good compared to KL-CPD, which again verifies our simulated example in Sec. 3 that directly applying existing kernel learning approaches with insufficient samples may not be suitable for realworld CPD task.",
        "KL-CPD, instead, leverages RNN to extract useful contexts and encodes time series in a discriminative embedding on which kernel two-sample test is used to detection changing points.",
        "In Table 2, we studied a kernel learning baseline, OPT-MMD, that optimizing an RNN parameterized kernel by controlling type-I error without the surrogate distribution.",
        "From Table 2, we can observe KL-CPD is better than other RNN alternatives, such as LSTNet. Those performance gaps between KL-CPD, OPT-MMD and other RNN works indicate the proposed maximizing testing power framework via an auxiliary distribution serves as a good surrogate for kernel selection.",
        "To further explore the performance of KL-CPD with controlled experiments, we follow other time series CPD papers (<a class=\"ref-link\" id=\"cTakeuchi_2006_a\" href=\"#rTakeuchi_2006_a\">Takeuchi & Yamanishi, 2006</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2013_a\" href=\"#rLiu_et+al_2013_a\">Liu et al, 2013</a>; <a class=\"ref-link\" id=\"cMatteson_2014_a\" href=\"#rMatteson_2014_a\">Matteson & James, 2014</a>) to create three simulated datasets each with a representative change-point characteristic: jumping mean, scaling variance, and alternating between two mixtures of Gaussian (Gaussian-Mixtures).",
        "MMD-codespace and KL-CPD are conducting kernel two-sample test on a learned low dimension codespace, which moderately alleviates this issue.",
        "KL-CPD finds a better kernel than MMD-codespace by optimizing the lower bound of the test power.",
        "We propose KL-CPD, a new kernel learning framework for two-sample test by optimizing a lower bound of test power with a auxiliary generator, to resolve the issue of insufficient samples in changepoints detection.",
        "The deep kernel parametrization of KL-CPD combines the latent space of RNNs with RBF kernels that effectively detect a variety of change-points from different real-world applications.",
        "With simulation analysis in addition we can see that the new method not only boosts the kernel power but evades the performance degradation as data dimensionality increases"
    ],
    "headline": "We propose KL-change point detection, a novel kernel learning framework for time series change point detection that optimizes a lower bound of test power via an auxiliary generative model",
    "reference_links": [
        {
            "id": "Al-Shedivat_et+al_2017_a",
            "entry": "Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, and Eric P Xing. Learning scalable deep kernels with recurrent structure. JMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Shedivat%2C%20Maruan%20Wilson%2C%20Andrew%20Gordon%20Saatchi%2C%20Yunus%20Zhiting%20Hu%2C%20and%20Eric%20P%20Xing.%20Learning%20scalable%20deep%20kernels%20with%20recurrent%20structure%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Shedivat%2C%20Maruan%20Wilson%2C%20Andrew%20Gordon%20Saatchi%2C%20Yunus%20Zhiting%20Hu%2C%20and%20Eric%20P%20Xing.%20Learning%20scalable%20deep%20kernels%20with%20recurrent%20structure%202017"
        },
        {
            "id": "Arbel_et+al_2018_a",
            "entry": "Michael Arbel, Dougal J Sutherland, Miko\u0142aj Binkowski, and Arthur Gretton. On gradient regularizers for mmd gans. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arbel%2C%20Michael%20Sutherland%2C%20Dougal%20J.%20Binkowski%2C%20Miko%C5%82aj%20Gretton%2C%20Arthur%20On%20gradient%20regularizers%20for%20mmd%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arbel%2C%20Michael%20Sutherland%2C%20Dougal%20J.%20Binkowski%2C%20Miko%C5%82aj%20Gretton%2C%20Arthur%20On%20gradient%20regularizers%20for%20mmd%20gans%202018"
        },
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sanjeev%20Arora%20Rong%20Ge%20Yingyu%20Liang%20Tengyu%20Ma%20and%20Yi%20Zhang%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20gans%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sanjeev%20Arora%20Rong%20Ge%20Yingyu%20Liang%20Tengyu%20Ma%20and%20Yi%20Zhang%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20gans%20In%20ICML%202017"
        },
        {
            "id": "Basseville_1993_a",
            "entry": "Mich\u00e8le Basseville, Igor V Nikiforov, et al. Detection of abrupt changes: theory and application. Prentice Hall Englewood Cliffs, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Basseville%2C%20Mich%C3%A8le%20Nikiforov%2C%20Igor%20V.%20Detection%20of%20abrupt%20changes%3A%20theory%20and%20application%201993"
        },
        {
            "id": "Basu_2007_a",
            "entry": "Sabyasachi Basu and Martin Meckesheimer. Automatic outlier detection for time series: an application to sensor data. Knowledge and Information Systems, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Basu%2C%20Sabyasachi%20Meckesheimer%2C%20Martin%20Automatic%20outlier%20detection%20for%20time%20series%3A%20an%20application%20to%20sensor%20data%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Basu%2C%20Sabyasachi%20Meckesheimer%2C%20Martin%20Automatic%20outlier%20detection%20for%20time%20series%3A%20an%20application%20to%20sensor%20data%202007"
        },
        {
            "id": "Binkowski_et+al_2018_a",
            "entry": "Miko\u0142aj Binkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Binkowski%2C%20Miko%C5%82aj%20Sutherland%2C%20Dougal%20J.%20Arbel%2C%20Michael%20Gretton%2C%20Arthur%20Demystifying%20mmd%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Binkowski%2C%20Miko%C5%82aj%20Sutherland%2C%20Dougal%20J.%20Arbel%2C%20Michael%20Gretton%2C%20Arthur%20Demystifying%20mmd%20gans%202018"
        },
        {
            "id": "Box_2013_a",
            "entry": "George Box. Box and jenkins: time series analysis, forecasting and control. A Very British Affair, ser. Palgrave Advanced Texts in Econometrics. Palgrave Macmillan UK, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20George%20Box%20and%20jenkins%3A%20time%20series%20analysis%2C%20forecasting%20and%20control.%20A%20Very%20British%20Affair%2C%20ser.%20Palgrave%20Advanced%20Texts%20in%20Econometrics%202013"
        },
        {
            "id": "Brodsky_2013_a",
            "entry": "E Brodsky and Boris S Darkhovsky. Nonparametric methods in change point problems. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brodsky%2C%20E.%20Darkhovsky%2C%20Boris%20S.%20Nonparametric%20methods%20in%20change%20point%20problems%202013"
        },
        {
            "id": "Candela_et+al_2003_a",
            "entry": "Joaquin Quinonero Candela, Agathe Girard, Jan Larsen, and Carl Edward Rasmussen. Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting. In ICASSP, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candela%2C%20Joaquin%20Quinonero%20Girard%2C%20Agathe%20Larsen%2C%20Jan%20Rasmussen%2C%20Carl%20Edward%20Propagation%20of%20uncertainty%20in%20bayesian%20kernel%20models-application%20to%20multiple-step%20ahead%20forecasting%202003"
        },
        {
            "id": "Chandola_et+al_2009_a",
            "entry": "Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Dziugaite_et+al_2015_a",
            "entry": "Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015"
        },
        {
            "id": "Gardner_et+al_2006_a",
            "entry": "Andrew B Gardner, Abba M Krieger, George Vachtsevanos, and Brian Litt. One-class novelty detection for seizure analysis from intracranial eeg. JMLR, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20Andrew%20B.%20Krieger%2C%20Abba%20M.%20Vachtsevanos%2C%20George%20Litt%2C%20Brian%20One-class%20novelty%20detection%20for%20seizure%20analysis%20from%20intracranial%20eeg%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20Andrew%20B.%20Krieger%2C%20Abba%20M.%20Vachtsevanos%2C%20George%20Litt%2C%20Brian%20One-class%20novelty%20detection%20for%20seizure%20analysis%20from%20intracranial%20eeg%202006"
        },
        {
            "id": "Gretton_et+al_2007_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Sch\u00f6lkopf, and Alex J Smola. A kernel method for the two-sample-problem. In NIPS, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. JMLR, 2012a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "Gretton_et+al_2012_b",
            "entry": "Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Sejdinovic%2C%20Dino%20Strathmann%2C%20Heiko%20Balakrishnan%2C%20Sivaraman%20Optimal%20kernel%20choice%20for%20large-scale%20two-sample%20tests%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Sejdinovic%2C%20Dino%20Strathmann%2C%20Heiko%20Balakrishnan%2C%20Sivaraman%20Optimal%20kernel%20choice%20for%20large-scale%20two-sample%20tests%202012"
        },
        {
            "id": "Gustafsson_1996_a",
            "entry": "Fredrik Gustafsson. The marginalized likelihood ratio test for detecting abrupt changes. IEEE Transactions on automatic control, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gustafsson%2C%20Fredrik%20The%20marginalized%20likelihood%20ratio%20test%20for%20detecting%20abrupt%20changes%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gustafsson%2C%20Fredrik%20The%20marginalized%20likelihood%20ratio%20test%20for%20detecting%20abrupt%20changes%201996"
        },
        {
            "id": "Gustafsson_2000_a",
            "entry": "Fredrik Gustafsson and Fredrik Gustafsson. Adaptive filtering and change detection. Citeseer, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gustafsson%2C%20Fredrik%20Gustafsson%2C%20Fredrik%20Adaptive%20filtering%20and%20change%20detection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gustafsson%2C%20Fredrik%20Gustafsson%2C%20Fredrik%20Adaptive%20filtering%20and%20change%20detection%202000"
        },
        {
            "id": "Harchaoui_et+al_2009_a",
            "entry": "Zaid Harchaoui, Eric Moulines, and Francis R Bach. Kernel change-point analysis. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harchaoui%2C%20Zaid%20Moulines%2C%20Eric%20Bach%2C%20Francis%20R.%20Kernel%20change-point%20analysis%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harchaoui%2C%20Zaid%20Moulines%2C%20Eric%20Bach%2C%20Francis%20R.%20Kernel%20change-point%20analysis%202009"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Kawahara_et+al_2007_a",
            "entry": "Yoshinobu Kawahara, Takehisa Yairi, and Kazuo Machida. Change-point detection in time-series data based on subspace identification. In ICDM. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawahara%2C%20Yoshinobu%20Yairi%2C%20Takehisa%20Machida%2C%20Kazuo%20Change-point%20detection%20in%20time-series%20data%20based%20on%20subspace%20identification.%20In%20ICDM%202007"
        },
        {
            "id": "Lai_et+al_2018_a",
            "entry": "Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In SIGIR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Guokun%20Chang%2C%20Wei-Cheng%20Yang%2C%20Yiming%20Liu%2C%20Hanxiao%20Modeling%20long-and%20short-term%20temporal%20patterns%20with%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Guokun%20Chang%2C%20Wei-Cheng%20Yang%2C%20Yiming%20Liu%2C%20Hanxiao%20Modeling%20long-and%20short-term%20temporal%20patterns%20with%20deep%20neural%20networks%202018"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab\u00e1s P\u00f3czos. Mmd gan: Towards deeper understanding of moment matching network. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Shuang Li, Yao Xie, Hanjun Dai, and Le Song. M-statistic for kernel change-point detection. In NIPS, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Shuang%20Xie%2C%20Yao%20Dai%2C%20Hanjun%20Song%2C%20Le%20M-statistic%20for%20kernel%20change-point%20detection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Shuang%20Xie%2C%20Yao%20Dai%2C%20Hanjun%20Song%2C%20Le%20M-statistic%20for%20kernel%20change-point%20detection%202015"
        },
        {
            "id": "Li_et+al_2015_b",
            "entry": "Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In ICML, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "Liu_et+al_2013_a",
            "entry": "Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in timeseries data by relative density-ratio estimation. Neural Networks, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Song%20Yamada%2C%20Makoto%20Collier%2C%20Nigel%20Sugiyama%2C%20Masashi%20Change-point%20detection%20in%20timeseries%20data%20by%20relative%20density-ratio%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Song%20Yamada%2C%20Makoto%20Collier%2C%20Nigel%20Sugiyama%2C%20Masashi%20Change-point%20detection%20in%20timeseries%20data%20by%20relative%20density-ratio%20estimation%202013"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Yusha Liu, Chun-Liang Li, and Barnab\u00e1s P\u00f3czos. Classifier two-sample test for video anomaly detections. In BMVC, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yusha%20Li%2C%20Chun-Liang%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Classifier%20two-sample%20test%20for%20video%20anomaly%20detections%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yusha%20Li%2C%20Chun-Liang%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Classifier%20two-sample%20test%20for%20video%20anomaly%20detections%202018"
        },
        {
            "id": "Matteson_2014_a",
            "entry": "David S Matteson and Nicholas A James. A nonparametric approach for multiple change point analysis of multivariate data. Journal of the American Statistical Association, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matteson%2C%20David%20S.%20James%2C%20Nicholas%20A.%20A%20nonparametric%20approach%20for%20multiple%20change%20point%20analysis%20of%20multivariate%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matteson%2C%20David%20S.%20James%2C%20Nicholas%20A.%20A%20nonparametric%20approach%20for%20multiple%20change%20point%20analysis%20of%20multivariate%20data%202014"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Mroueh_et+al_2018_a",
            "entry": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev gan. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Youssef%20Mroueh%20ChunLiang%20Li%20Tom%20Sercu%20Anant%20Raj%20and%20Yu%20Cheng%20Sobolev%20gan%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Youssef%20Mroueh%20ChunLiang%20Li%20Tom%20Sercu%20Anant%20Raj%20and%20Yu%20Cheng%20Sobolev%20gan%20In%20ICLR%202018"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Pepelyshev_2015_a",
            "entry": "Andrey Pepelyshev and Aleksey S Polunchenko. Real-time financial surveillance via quickest change-point detection methods. arXiv preprint arXiv:1509.01570, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01570"
        },
        {
            "id": "Reeves_et+al_2007_a",
            "entry": "Jaxk Reeves, Jien Chen, Xiaolan L Wang, Robert Lund, and Qi Qi Lu. A review and comparison of changepoint detection techniques for climate data. Journal of Applied Meteorology and Climatology, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reeves%2C%20Jaxk%20Chen%2C%20Jien%20Wang%2C%20Xiaolan%20L.%20Lund%2C%20Robert%20A%20review%20and%20comparison%20of%20changepoint%20detection%20techniques%20for%20climate%20data%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reeves%2C%20Jaxk%20Chen%2C%20Jien%20Wang%2C%20Xiaolan%20L.%20Lund%2C%20Robert%20A%20review%20and%20comparison%20of%20changepoint%20detection%20techniques%20for%20climate%20data%202007"
        },
        {
            "id": "Saat_et+al_2010_a",
            "entry": "Yunus Saat\u00e7i, Ryan Turner, and Carl Edward Rasmussen. Gaussian process change point models. In ICML, June 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saat%C3%A7i%2C%20Yunus%20Turner%2C%20Ryan%20Rasmussen%2C%20Carl%20Edward%20Gaussian%20process%20change%20point%20models%202010-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saat%C3%A7i%2C%20Yunus%20Turner%2C%20Ryan%20Rasmussen%2C%20Carl%20Edward%20Gaussian%20process%20change%20point%20models%202010-06"
        },
        {
            "id": "Sutherland_et+al_2017_a",
            "entry": "Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutherland%2C%20Dougal%20J.%20Tung%2C%20Hsiao-Yu%20Strathmann%2C%20Heiko%20De%2C%20Soumyajit%20Generative%20models%20and%20model%20criticism%20via%20optimized%20maximum%20mean%20discrepancy%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutherland%2C%20Dougal%20J.%20Tung%2C%20Hsiao-Yu%20Strathmann%2C%20Heiko%20De%2C%20Soumyajit%20Generative%20models%20and%20model%20criticism%20via%20optimized%20maximum%20mean%20discrepancy%202017"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Takeuchi_2006_a",
            "entry": "Jun-ichi Takeuchi and Kenji Yamanishi. A unifying framework for detecting outliers and change points from time series. IEEE transactions on Knowledge and Data Engineering, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takeuchi%2C%20Jun-ichi%20Yamanishi%2C%20Kenji%20A%20unifying%20framework%20for%20detecting%20outliers%20and%20change%20points%20from%20time%20series%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takeuchi%2C%20Jun-ichi%20Yamanishi%2C%20Kenji%20A%20unifying%20framework%20for%20detecting%20outliers%20and%20change%20points%20from%20time%20series%202006"
        },
        {
            "id": "Wang_et+al_2011_a",
            "entry": "Yao Wang, Chunguo Wu, Zhaohua Ji, Binghong Wang, and Yanchun Liang. Non-parametric change-point method for differential gene expression detection. PloS one, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yao%20Wu%2C%20Chunguo%20Ji%2C%20Zhaohua%20Wang%2C%20Binghong%20Non-parametric%20change-point%20method%20for%20differential%20gene%20expression%20detection%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yao%20Wu%2C%20Chunguo%20Ji%2C%20Zhaohua%20Wang%2C%20Binghong%20Non-parametric%20change-point%20method%20for%20differential%20gene%20expression%20detection%202011"
        },
        {
            "id": "Wilson_2016_a",
            "entry": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In AISTATS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning%202016"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Zhao Xu, Kristian Kersting, and Lorenzo von Ritter. Stochastic online anomaly analysis for streaming time series. In IJCAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Zhao%20Kersting%2C%20Kristian%20von%20Ritter%2C%20Lorenzo%20Stochastic%20online%20anomaly%20analysis%20for%20streaming%20time%20series%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Zhao%20Kersting%2C%20Kristian%20von%20Ritter%2C%20Lorenzo%20Stochastic%20online%20anomaly%20analysis%20for%20streaming%20time%20series%202017"
        },
        {
            "id": "Yamanishi_2002_a",
            "entry": "Kenji Yamanishi and Jun-ichi Takeuchi. A unifying framework for detecting outliers and change points from non-stationary time series data. In SIGKDD. ACM, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamanishi%2C%20Kenji%20Takeuchi%2C%20Jun-ichi%20A%20unifying%20framework%20for%20detecting%20outliers%20and%20change%20points%20from%20non-stationary%20time%20series%20data%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamanishi%2C%20Kenji%20Takeuchi%2C%20Jun-ichi%20A%20unifying%20framework%20for%20detecting%20outliers%20and%20change%20points%20from%20non-stationary%20time%20series%20data%202002"
        },
        {
            "id": "Yamanishi_et+al_2004_a",
            "entry": "Kenji Yamanishi, Jun-Ichi Takeuchi, Graham Williams, and Peter Milne. On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms. Data Mining and Knowledge Discovery, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamanishi%2C%20Kenji%20Takeuchi%2C%20Jun-Ichi%20Williams%2C%20Graham%20Milne%2C%20Peter%20On-line%20unsupervised%20outlier%20detection%20using%20finite%20mixtures%20with%20discounting%20learning%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamanishi%2C%20Kenji%20Takeuchi%2C%20Jun-Ichi%20Williams%2C%20Graham%20Milne%2C%20Peter%20On-line%20unsupervised%20outlier%20detection%20using%20finite%20mixtures%20with%20discounting%20learning%20algorithms%202004"
        },
        {
            "id": "Although_2017_a",
            "entry": "Although our proposed method KL-CPD has a similar objective function as appeared in MMD GAN (Li et al., 2017), we would like to point out the underlying interpretation and motivations are radically different, as summarized below.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Although%20our%20proposed%20method%20KLCPD%20has%20a%20similar%20objective%20function%20as%20appeared%20in%20MMD%20GAN%20Li%20et%20al%202017%20we%20would%20like%20to%20point%20out%20the%20underlying%20interpretation%20and%20motivations%20are%20radically%20different%20as%20summarized%20below"
        },
        {
            "id": "Mk_2017_a",
            "entry": "The first difference is the interpretation of inner maximization problem maxk Mk(P, G). MMD GANs (Li et al., 2017; Binkowski et al., 2018) treat whole maximization problem maxk Mk(P, G) as a new probabilistic distance, which can also be viewed as an extension of integral probability metric (IPM). The properties of the distance is also studied in Li et al. (2017); Arbel et al. (2018). A follow-up work (Arbel et al., 2018) by combining Mroueh et al. (2018) push maxk Mk(P, G) further to be a scaled distance with gradient norm. However, the maximization problem (4) of this paper defines the lower bound of the test power, which also takes the variance of the empirical estimate into account, instead of the distance.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mk%28P%2C%20G%29.%20MMD%20GANs%20%28Li%20The%20first%20difference%20is%20the%20interpretation%20of%20inner%20maximization%20problem%20maxk%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mk%28P%2C%20G%29.%20MMD%20GANs%20%28Li%20The%20first%20difference%20is%20the%20interpretation%20of%20inner%20maximization%20problem%20maxk%202017"
        },
        {
            "id": "Regarding_2015_a",
            "entry": "Regarding the goals, MMD GAN aims to learn a generative model that approximates the underlying data distribution P of interests. All the works (Dziugaite et al., 2015; Li et al., 2015b; Sutherland et al., 2017; Li et al., 2017; Binkowski et al., 2018; Arbel et al., 2018) use MMD or maxk Mk(P, G) to define distance, then try to optimize G to be as closed to P as possible. However, that is not the goal of this paper, where G is just an auxiliary generative model which needs to satisfies Eq. (3). Instead, we aim to find the most powerful k for conducting hypothesis test. In practice, we still optimize G toward P because we usually have no prior knowledge (sufficient samples) about Q, and we want to ensure the lower bound still hold for many possible Q (e.g. Q can be also similar to P). However, even with this reason, we still adopt early stopping to prevent the auxiliary G from being exactly the same as P.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Regarding%20the%20goals%20MMD%20GAN%20aims%20to%20learn%20a%20generative%20model%20that%20approximates%20the%20underlying%20data%20distribution%20P%20of%20interests%20All%20the%20works%20Dziugaite%20et%20al%202015%20Li%20et%20al%202015b%20Sutherland%20et%20al%202017%20Li%20et%20al%202017%20Binkowski%20et%20al%202018%20Arbel%20et%20al%202018%20use%20MMD%20or%20maxk%20MkP%20G%20to%20define%20distance%20then%20try%20to%20optimize%20G%20to%20be%20as%20closed%20to%20P%20as%20possible%20However%20that%20is%20not%20the%20goal%20of%20this%20paper%20where%20G%20is%20just%20an%20auxiliary%20generative%20model%20which%20needs%20to%20satisfies%20Eq%203%20Instead%20we%20aim%20to%20find%20the%20most%20powerful%20k%20for%20conducting%20hypothesis%20test%20In%20practice%20we%20still%20optimize%20G%20toward%20P%20because%20we%20usually%20have%20no%20prior%20knowledge%20sufficient%20samples%20about%20Q%20and%20we%20want%20to%20ensure%20the%20lower%20bound%20still%20hold%20for%20many%20possible%20Q%20eg%20Q%20can%20be%20also%20similar%20to%20P%20However%20even%20with%20this%20reason%20we%20still%20adopt%20early%20stopping%20to%20prevent%20the%20auxiliary%20G%20from%20being%20exactly%20the%20same%20as%20P",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Regarding%20the%20goals%20MMD%20GAN%20aims%20to%20learn%20a%20generative%20model%20that%20approximates%20the%20underlying%20data%20distribution%20P%20of%20interests%20All%20the%20works%20Dziugaite%20et%20al%202015%20Li%20et%20al%202015b%20Sutherland%20et%20al%202017%20Li%20et%20al%202017%20Binkowski%20et%20al%202018%20Arbel%20et%20al%202018%20use%20MMD%20or%20maxk%20MkP%20G%20to%20define%20distance%20then%20try%20to%20optimize%20G%20to%20be%20as%20closed%20to%20P%20as%20possible%20However%20that%20is%20not%20the%20goal%20of%20this%20paper%20where%20G%20is%20just%20an%20auxiliary%20generative%20model%20which%20needs%20to%20satisfies%20Eq%203%20Instead%20we%20aim%20to%20find%20the%20most%20powerful%20k%20for%20conducting%20hypothesis%20test%20In%20practice%20we%20still%20optimize%20G%20toward%20P%20because%20we%20usually%20have%20no%20prior%20knowledge%20sufficient%20samples%20about%20Q%20and%20we%20want%20to%20ensure%20the%20lower%20bound%20still%20hold%20for%20many%20possible%20Q%20eg%20Q%20can%20be%20also%20similar%20to%20P%20However%20even%20with%20this%20reason%20we%20still%20adopt%20early%20stopping%20to%20prevent%20the%20auxiliary%20G%20from%20being%20exactly%20the%20same%20as%20P"
        },
        {
            "id": "2http://www.cc.gatech.edu/~borg/ijcv_psslds/_0000_b",
            "entry": "2http://www.cc.gatech.edu/~borg/ijcv_psslds/ 3http://mldata.org/repository/data/viewslug/fish_killer/ 4http://hasc.jp/hc2011 5https://webscope.sandbox.yahoo.com/catalog.php?datatype=s",
            "url": "http://www.cc.gatech.edu/~borg/ijcv_psslds/3http://mldata.org/repository/data/viewslug/fish_killer/4http://hasc.jp/hc2011"
        },
        {
            "id": "Published_2017_a",
            "entry": "Published as a conference paper at ICLR 2019 Our algorithms are implemented in Python (PyTorch Paszke et al. (2017)), and running on Nvidia GeForce GTX 1080 Ti GPUs. Datasets and experiment code are publicly available. For all the baseline methods we used the released source code, include MATLAB code6 for ARMA, ARGP and ARGP-BOCPD, Pytorch code7 for RNN and LSTNet, MATLAB code8 for RDR-KCPD, and MATLAB code 9 for Mstats-KCPD. B.5 CONDITIONAL SAMPLES OF KL-CPD",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Our%20algorithms%20are%20implemented%20in%20Python%20PyTorch%20Paszke%20et%20al%202017%20and%20running%20on%20Nvidia%20GeForce%20GTX%201080%20Ti%20GPUs%20Datasets%20and%20experiment%20code%20are%20publicly%20available%20For%20all%20the%20baseline%20methods%20we%20used%20the%20released%20source%20code%20include%20MATLAB%20code6%20for%20ARMA%20ARGP%20and%20ARGPBOCPD%20Pytorch%20code7%20for%20RNN%20and%20LSTNet%20MATLAB%20code8%20for%20RDRKCPD%20and%20MATLAB%20code%209%20for%20MstatsKCPD%20B5%20CONDITIONAL%20SAMPLES%20OF%20KLCPD",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Our%20algorithms%20are%20implemented%20in%20Python%20PyTorch%20Paszke%20et%20al%202017%20and%20running%20on%20Nvidia%20GeForce%20GTX%201080%20Ti%20GPUs%20Datasets%20and%20experiment%20code%20are%20publicly%20available%20For%20all%20the%20baseline%20methods%20we%20used%20the%20released%20source%20code%20include%20MATLAB%20code6%20for%20ARMA%20ARGP%20and%20ARGPBOCPD%20Pytorch%20code7%20for%20RNN%20and%20LSTNet%20MATLAB%20code8%20for%20RDRKCPD%20and%20MATLAB%20code%209%20for%20MstatsKCPD%20B5%20CONDITIONAL%20SAMPLES%20OF%20KLCPD"
        }
    ]
}
