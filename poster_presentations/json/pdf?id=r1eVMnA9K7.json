{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNSUPERVISED CONTROL THROUGH NON-PARAMETRIC DISCRIMINATIVE REWARDS",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1eVMnA9K7"
        },
        "abstract": "Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptuallyspecified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains \u2013 Atari, the DeepMind Control Suite and DeepMind Lab."
    },
    "keywords": [
        {
            "term": "extrinsic reward",
            "url": "https://en.wikipedia.org/wiki/extrinsic_reward"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "GVFs": "generalized value functions",
        "UVFAs": "Universal Value Function Approximators",
        "GAN": "Generative Adversarial Networks",
        "UPNs": "Universal Planning Networks",
        "VIC": "Variational Intrinsic Control"
    },
    "highlights": [
        "The best performing methods on many reinforcement learning benchmark problems combine model-free reinforcement learning methods with policies represented using deep neural networks (<a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\">Horgan et al, 2018</a></a>; <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\"><a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al, 2018</a></a>)",
        "Despite reaching or surpassing human-level performance on many challenging tasks, deep model-free reinforcement learning methods that learn purely from the reward signal learn in a way that differs greatly from the manner in which humans learn",
        "A human player quickly learns which aspects of the environment are under their control as well as how to control them, as evidenced by their ability to rapidly adapt to novel reward functions (<a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\">Lake et al, 2017</a>)",
        "We demonstrate the effectiveness of our approach on three domains \u2013 Atari games, continuous control tasks from the DeepMind Control Suite, and DeepMind Lab",
        "In this work we focus on learning only from the stream of actions and observations in order to forego the need for an extrinsic reward function",
        "We have presented a system that can learn to achieve goals, specified in the form of observations from the environment, in a purely unsupervised fashion, i.e. without any extrinsic rewards or expert demonstrations"
    ],
    "key_statements": [
        "The best performing methods on many reinforcement learning benchmark problems combine model-free reinforcement learning methods with policies represented using deep neural networks (<a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\">Horgan et al, 2018</a></a>; <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\"><a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al, 2018</a></a>)",
        "Despite reaching or surpassing human-level performance on many challenging tasks, deep model-free reinforcement learning methods that learn purely from the reward signal learn in a way that differs greatly from the manner in which humans learn",
        "A human player quickly learns which aspects of the environment are under their control as well as how to control them, as evidenced by their ability to rapidly adapt to novel reward functions (<a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\">Lake et al, 2017</a>)",
        "We demonstrate the effectiveness of our approach on three domains \u2013 Atari games, continuous control tasks from the DeepMind Control Suite, and DeepMind Lab",
        "In this work we focus on learning only from the stream of actions and observations in order to forego the need for an extrinsic reward function",
        "AGILE (<a class=\"ref-link\" id=\"cBahdanau_et+al_2018_a\" href=\"#rBahdanau_et+al_2018_a\">Bahdanau et al, 2018</a>) learns an instruction-conditional policy where goals in a grid-world are specified in terms of predicates which should be satisfied, and a reward function is learned using a discriminator trained to distinguish states achieved by the policy from a dataset of instruction, goal state pairs",
        "We have presented a system that can learn to achieve goals, specified in the form of observations from the environment, in a purely unsupervised fashion, i.e. without any extrinsic rewards or expert demonstrations"
    ],
    "summary": [
        "The best performing methods on many reinforcement learning benchmark problems combine model-free reinforcement learning methods with policies represented using deep neural networks (<a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\"><a class=\"ref-link\" id=\"cHorgan_et+al_2018_a\" href=\"#rHorgan_et+al_2018_a\">Horgan et al, 2018</a></a>; <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\"><a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al, 2018</a></a>).",
        "We show how to learn a goal achievement reward function r(s; sg) that measures how similar state s is to state sg using a mutual information objective at the same time as learning \u03c0\u03b8(a|s; sg).",
        "We demonstrate the effectiveness of our approach on three domains \u2013 Atari games, continuous control tasks from the DeepMind Control Suite, and DeepMind Lab. We show that our agent learns to successfully achieve a wide variety of visually-specified goals, discovering underlying degrees of controllability of an environment in a purely unsupervised manner and without access to an extrinsic reward signal.",
        "We turn to the problem of learning a goal achievement reward function r\u03c6(s; sg) with parameters \u03c6 for high-dimensional state spaces.",
        "We can optimize this objective with respect to policy parameters \u03b8 by repeatedly generating trajectories and performing reinforcement learning updates on \u03c0\u03b8 with a reward of log q\u03c6 given at time T and 0 for other time steps.",
        "Goal achievement reward: We train a goal achievement reward function r(s; sg) used to compute rewards for the goal-conditioned policy based on a learned measure of state similarity.",
        "AGILE (<a class=\"ref-link\" id=\"cBahdanau_et+al_2018_a\" href=\"#rBahdanau_et+al_2018_a\">Bahdanau et al, 2018</a>) learns an instruction-conditional policy where goals in a grid-world are specified in terms of predicates which should be satisfied, and a reward function is learned using a discriminator trained to distinguish states achieved by the policy from a dataset of instruction, goal state pairs.",
        "Experiments showed that once a UPN is trained the state representation it learned can be used to construct a reward function for visually specified goals.",
        "We compared DISCERN to several baseline methods for learning goal-conditioned policies: Conditioned Autoencoder (AE): In order to interrogate the role of the discriminative reward learning criterion, we replace the discriminative criterion for embedding learning with an L2 reconstruction loss on ht; that is, in addition to \u03be\u03c6(\u00b7), we learn an inverse mapping \u03be\u03c6\u22121(\u00b7) with a separate set of parameters, and train both with the criterion ht \u2212 \u03be\u03c6\u22121) 2.",
        "We have presented a system that can learn to achieve goals, specified in the form of observations from the environment, in a purely unsupervised fashion, i.e. without any extrinsic rewards or expert demonstrations.",
        "A natural step is the incorporation of DISCERN into a deep hierarchical reinforcement learning setup (<a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al, 2017</a>; <a class=\"ref-link\" id=\"cLevy_et+al_2018_a\" href=\"#rLevy_et+al_2018_a\">Levy et al, 2018</a>; <a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a>) where a meta-policy for proposing goals is learned after or in tandem with a low-level controller, i.e. by optimizing an extrinsic reward signal."
    ],
    "headline": "We present an unsupervised learning algorithm to train agents to achieve perceptuallyspecified goals using only a stream of observations and actions",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems 30, pp. 5048\u20135058. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Bahdanau_et+al_2018_a",
            "entry": "Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. Learning to follow language instructions with adversarial reward induction. arXiv preprint arXiv:1806.01946, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01946"
        },
        {
            "id": "Barber_2004_a",
            "entry": "David Barber and Felix V. Agakov. Information maximization in noisy channels: A variational approach. In S. Thrun, L. K. Saul, and B. Scholkopf (eds.), Advances in Neural Information Processing Systems 16, pp. 201\u2013208. MIT Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barber%2C%20David%20Agakov%2C%20Felix%20V.%20Information%20maximization%20in%20noisy%20channels%3A%20A%20variational%20approach%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barber%2C%20David%20Agakov%2C%20Felix%20V.%20Information%20maximization%20in%20noisy%20channels%3A%20A%20variational%20approach%202004"
        },
        {
            "id": "Beattie_et+al_2016_a",
            "entry": "Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler, Andrew Lefrancq, Simon Green, V\u0131ctor Valdes, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03801"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Bengio_2003_a",
            "entry": "Yoshua Bengio and Jean-Sebastien Senecal. Quick training of probabilistic neural nets by importance sampling. In Proceedings of the conference on Artificial Intelligence and Statistics (AISTATS), 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Senecal%2C%20Jean-Sebastien%20Quick%20training%20of%20probabilistic%20neural%20nets%20by%20importance%20sampling%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Senecal%2C%20Jean-Sebastien%20Quick%20training%20of%20probabilistic%20neural%20nets%20by%20importance%20sampling%202003"
        },
        {
            "id": "Chiappa_et+al_2018_a",
            "entry": "Silvia Chiappa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiappa%2C%20Silvia%20Racaniere%2C%20Sebastien%20Wierstra%2C%20Daan%20Mohamed%2C%20Shakir%20Recurrent%20environment%20simulators%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiappa%2C%20Silvia%20Racaniere%2C%20Sebastien%20Wierstra%2C%20Daan%20Mohamed%2C%20Shakir%20Recurrent%20environment%20simulators%202018"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06070"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Florensa%2C%20Carlos%20Duan%2C%20Yan%20Abbeel%2C%20Pieter%20Stochastic%20neural%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Florensa%2C%20Carlos%20Duan%2C%20Yan%20Abbeel%2C%20Pieter%20Stochastic%20neural%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Ganin_et+al_2018_a",
            "entry": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. arXiv preprint arXiv:1804.01118, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01118"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20Rezende%2C%20Danilo%20Jimenez%20Wierstra%2C%20Daan%20Variational%20intrinsic%20control%202016"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "Shane Gu, Tim Lillicrap, Ilya Sutskever, and Sergei Levine. Continuous deep q-learning with model-based acceleration. In Proc. of ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shane%20Lillicrap%2C%20Tim%20Sutskever%2C%20Ilya%20Levine%2C%20Sergei%20Continuous%20deep%20q-learning%20with%20model-based%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shane%20Lillicrap%2C%20Tim%20Sutskever%2C%20Ilya%20Levine%2C%20Sergei%20Continuous%20deep%20q-learning%20with%20model-based%20acceleration%202016"
        },
        {
            "id": "Ha_2018_a",
            "entry": "David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10122"
        },
        {
            "id": "Held_et+al_2017_a",
            "entry": "David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06366"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Horgan_et+al_2018_a",
            "entry": "Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. Distributed prioritized experience replay. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horgan%2C%20Dan%20Quan%2C%20John%20Budden%2C%20David%20Barth-Maron%2C%20Gabriel%20Distributed%20prioritized%20experience%20replay%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horgan%2C%20Dan%20Quan%2C%20John%20Budden%2C%20David%20Barth-Maron%2C%20Gabriel%20Distributed%20prioritized%20experience%20replay%202018"
        },
        {
            "id": "Kaelbling_1993_a",
            "entry": "Leslie Pack Kaelbling. Learning to achieve goals. In Proceedings of the 13th International Joint Conference on Artificial Intelligence, pp. 1094\u20131099, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20Leslie%20Pack%20Learning%20to%20achieve%20goals%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20Leslie%20Pack%20Learning%20to%20achieve%20goals%201993"
        },
        {
            "id": "Kulesza_2012_a",
            "entry": "Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends R in Machine Learning, 5(2\u20133):123\u2013286, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Determinantal%20point%20processes%20for%20machine%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Determinantal%20point%20processes%20for%20machine%20learning%202012"
        },
        {
            "id": "Lafferty_et+al_2001_a",
            "entry": "John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. pp. 282\u2013289, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lafferty%2C%20John%20McCallum%2C%20Andrew%20Pereira%2C%20Fernando%20C.N.%20Conditional%20random%20fields%3A%20Probabilistic%20models%20for%20segmenting%20and%20labeling%20sequence%20data%202001"
        },
        {
            "id": "Lake_et+al_2017_a",
            "entry": "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017"
        },
        {
            "id": "Laversanne-Finot_et+al_2018_a",
            "entry": "Adrien Laversanne-Finot, Alexandre Pere, and Pierre-Yves Oudeyer. Curiosity driven exploration of learned disentangled goal spaces. In Conference on Robot Learning, pp. 487\u2013504, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laversanne-Finot%2C%20Adrien%20Pere%2C%20Alexandre%20Oudeyer%2C%20Pierre-Yves%20Curiosity%20driven%20exploration%20of%20learned%20disentangled%20goal%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laversanne-Finot%2C%20Adrien%20Pere%2C%20Alexandre%20Oudeyer%2C%20Pierre-Yves%20Curiosity%20driven%20exploration%20of%20learned%20disentangled%20goal%20spaces%202018"
        },
        {
            "id": "Levy_et+al_2018_a",
            "entry": "Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight. arXiv preprint arXiv:1805.08180, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08180"
        },
        {
            "id": "Lin_1993_a",
            "entry": "Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Long-Ji%20Reinforcement%20learning%20for%20robots%20using%20neural%20networks%201993"
        },
        {
            "id": "Machado_2016_a",
            "entry": "Marlos C Machado and Michael Bowling. Learning purposeful behaviour in the absence of rewards. arXiv preprint arXiv:1605.07700, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07700"
        },
        {
            "id": "Machado_et+al_2017_a",
            "entry": "Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. In International Conference on Machine Learning, pp. 2295\u20132304, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Bowling%2C%20Michael%20A%20laplacian%20framework%20for%20option%20discovery%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Bowling%2C%20Michael%20A%20laplacian%20framework%20for%20option%20discovery%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mobahi_et+al_2009_a",
            "entry": "Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep learning from temporal coherence in video. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 737\u2013744. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mobahi%2C%20Hossein%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Deep%20learning%20from%20temporal%20coherence%20in%20video%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mobahi%2C%20Hossein%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Deep%20learning%20from%20temporal%20coherence%20in%20video%202009"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. arXiv preprint arXiv:1805.08296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Nair_et+al_2018_a",
            "entry": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. arXiv preprint arXiv:1807.04742, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04742"
        },
        {
            "id": "Nair_2006_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Inferring motor programs from images of handwritten digits. In Y. Weiss, B. Scholkopf, and J. C. Platt (eds.), Advances in Neural Information Processing Systems 18, pp. 515\u2013522. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Inferring%20motor%20programs%20from%20images%20of%20handwritten%20digits%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Inferring%20motor%20programs%20from%20images%20of%20handwritten%20digits%202006"
        },
        {
            "id": "Oh_et+al_2015_a",
            "entry": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems 28, pp. 2863\u20132871. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. arXiv preprint arXiv:1804.08606, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08606"
        },
        {
            "id": "Pere_et+al_2018_a",
            "entry": "Alexandre Pere, Sebastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learning of goal spaces for intrinsically motivated goal exploration. arXiv preprint arXiv:1803.00781, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00781"
        },
        {
            "id": "Riedmiller_et+al_2018_a",
            "entry": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playingsolving sparse reward tasks from scratch. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riedmiller%2C%20Martin%20Hafner%2C%20Roland%20Lampe%2C%20Thomas%20Neunert%2C%20Michael%20Learning%20by%20playingsolving%20sparse%20reward%20tasks%20from%20scratch%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riedmiller%2C%20Martin%20Hafner%2C%20Roland%20Lampe%2C%20Thomas%20Neunert%2C%20Michael%20Learning%20by%20playingsolving%20sparse%20reward%20tasks%20from%20scratch%202018"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pp. 1312\u20131320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20value%20function%20approximators%20In%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20value%20function%20approximators%20In%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015"
        },
        {
            "id": "Sermanet_et+al_2017_a",
            "entry": "Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Eric Jang, Stefan Schaal, Jasmine Hsu, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. arXiv preprint arXiv:1704.06888, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06888"
        },
        {
            "id": "Srinivas_et+al_2018_a",
            "entry": "Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20Aravind%20Jabri%2C%20Allan%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Universal%20planning%20networks%202018"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Introduction%20to%20Reinforcement%20Learning%201998"
        },
        {
            "id": "Sutton_et+al_2011_a",
            "entry": "Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 761\u2013768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011"
        },
        {
            "id": "Tassa_et+al_2018_a",
            "entry": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Veeriah_et+al_2018_a",
            "entry": "Vivek Veeriah, Junhyuk Oh, and Satinder Singh. Many-goals reinforcement learning. arXiv preprint arXiv:1806.09605, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09605"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In International Conference on Machine Learning, pp. 3540\u20133549, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630\u20133638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Wang_et+al_1995_a",
            "entry": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling network architectures for deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1995\u20132003, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20van%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20van%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995"
        }
    ]
}
