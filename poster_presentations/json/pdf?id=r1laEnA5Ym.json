{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A VARIATIONAL INEQUALITY PERSPECTIVE ON GENERATIVE ADVERSARIAL NETWORKS",
        "author": "Gauthier Gidel",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1laEnA5Ym"
        },
        "abstract": "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam."
    },
    "keywords": [
        {
            "term": "super resolution",
            "url": "https://en.wikipedia.org/wiki/super_resolution"
        },
        {
            "term": "variational inequality",
            "url": "https://en.wikipedia.org/wiki/variational_inequality"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "convex analysis",
            "url": "https://en.wikipedia.org/wiki/convex_analysis"
        },
        {
            "term": "new formulation",
            "url": "https://en.wikipedia.org/wiki/New_Formulation"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial networks",
        "SGD": "stochastic gradient descent",
        "VIP": "variational inequality problem",
        "VIPs": "variational inequality problems",
        "OMD": "Optimistic mirror descent"
    },
    "highlights": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) form a generative modeling approach known for producing realistic natural images (<a class=\"ref-link\" id=\"cKarras_et+al_2018_a\" href=\"#rKarras_et+al_2018_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2018_a\" href=\"#rKarras_et+al_2018_a\">Karras et al, 2018</a></a>) as well as high quality super-resolution (Ledig et al, 2017) and style transfer (Zhu et al, 2017)",
        "Much recent work has tried to tackle these training difficulties, usually by proposing new formulations of the Generative adversarial networks objective (<a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a>). Each of these formulations can be understood as a two-player game, in the sense of game theory (Von Neumann and Morgenstern, 1944), and can be addressed as a variational inequality problem (VIP) (<a class=\"ref-link\" id=\"cHarker_1990_a\" href=\"#rHarker_1990_a\">Harker and Pang, 1990</a>), a framework that encompasses traditional saddle point optimization algorithms (<a class=\"ref-link\" id=\"cKorpelevich_1976_a\" href=\"#rKorpelevich_1976_a\">Korpelevich, 1976</a>)",
        "We present two techniques from this literature, namely averaging and extrapolation, widely used to solve variational inequality problem but which have not been explored in the context of Generative adversarial networks before.1",
        "We introduce a technique, called extrapolation from the past, that only requires one gradient computation per update compared to extrapolation which requires to compute the gradient twice, rediscovering, with a variational inequality problem perspective, a particular case of optimistic mirror descent (Rakhlin and Sridharan, 2013)",
        "We newly addressed Generative adversarial networks objectives in the framework of variational inequality",
        "The presented techniques address a fundamental problem in Generative adversarial networks training in a principled way, and are orthogonal to the design of new Generative adversarial networks architectures and objectives"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>) form a generative modeling approach known for producing realistic natural images (<a class=\"ref-link\" id=\"cKarras_et+al_2018_a\" href=\"#rKarras_et+al_2018_a\">Karras et al, 2018</a>) as well as high quality super-resolution (Ledig et al, 2017) and style transfer (Zhu et al, 2017)",
        "Much recent work has tried to tackle these training difficulties, usually by proposing new formulations of the Generative adversarial networks objective (<a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a>). Each of these formulations can be understood as a two-player game, in the sense of game theory (Von Neumann and Morgenstern, 1944), and can be addressed as a variational inequality problem (VIP) (<a class=\"ref-link\" id=\"cHarker_1990_a\" href=\"#rHarker_1990_a\">Harker and Pang, 1990</a>), a framework that encompasses traditional saddle point optimization algorithms (<a class=\"ref-link\" id=\"cKorpelevich_1976_a\" href=\"#rKorpelevich_1976_a\">Korpelevich, 1976</a>)",
        "We present two techniques from this literature, namely averaging and extrapolation, widely used to solve variational inequality problem but which have not been explored in the context of Generative adversarial networks before.1",
        "We extend standard Generative adversarial networks training methods such as stochastic gradient descent or Adam into variants that incorporate these techniques (Alg. 4 is new)",
        "We introduce a technique, called extrapolation from the past, that only requires one gradient computation per update compared to extrapolation which requires to compute the gradient twice, rediscovering, with a variational inequality problem perspective, a particular case of optimistic mirror descent (Rakhlin and Sridharan, 2013)",
        "We have showed that both saddle point optimization and non-zero sum game optimization, which encompass the large majority of Generative adversarial networks variants proposed in the literature, can be cast as variational inequality problem",
        "We present and analyze three algorithms that are variants of stochastic gradient descent that are appropriate to solve (VIP)",
        "We newly addressed Generative adversarial networks objectives in the framework of variational inequality",
        "The presented techniques address a fundamental problem in Generative adversarial networks training in a principled way, and are orthogonal to the design of new Generative adversarial networks architectures and objectives"
    ],
    "summary": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) form a generative modeling approach known for producing realistic natural images (<a class=\"ref-link\" id=\"cKarras_et+al_2018_a\" href=\"#rKarras_et+al_2018_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2018_a\" href=\"#rKarras_et+al_2018_a\">Karras et al, 2018</a></a>) as well as high quality super-resolution (Ledig et al, 2017) and style transfer (Zhu et al, 2017).",
        "We present two techniques from this literature, namely averaging and extrapolation, widely used to solve VIPs but which have not been explored in the context of GANs before.1",
        "We explain that the oscillations of basic SGD for GAN training previously noticed (<a class=\"ref-link\" id=\"cGoodfellow_2016_a\" href=\"#rGoodfellow_2016_a\">Goodfellow, 2016</a>) can be explained by standard variational inequality optimization results and we illustrate how averaging and extrapolation can fix this issue.",
        "\u00a73 presents standard techniques and extrapolation from the past to optimize variational inequalities in a batch setting.",
        "We have showed that both saddle point optimization and non-zero sum game optimization, which encompass the large majority of GAN variants proposed in the literature, can be cast as VIPs. we turn to suitable optimization techniques for such problems.",
        "One thing to notice is that the operator of a bilinear objective is not strongly monotone, but in that case one can use the standard extrapolation method (14) which converges linearly for a bilinear game (Tseng, 1995, Cor. 3.3).",
        "Alg. 1 (AvgSGD) presents the stochastic gradient method with averaging, which reduces to the standard SGD updates for the two-player games used in the GAN literature, but returning an average of the iterates.",
        "SGD with extrapolation provides better convergence guarantees but requires two gradient computations and samples per iteration.",
        "1 and 3, if E\u03be[F ] is L-Lipschitz SGD with extrapolation from the past using a constant step-size \u03b7 \u2264 \u221a1 , gives that the averaged iterates converge as, 2 3L",
        "Note that in the case of a two-player game (3), the previous convergence results can be generalized to gradient updates with a different step-size for each player by rescaling the objectives LG and LD by a different scaling factor.",
        "A linearly convergent variance reduced version of the stochastic gradient method has been proposed by <a class=\"ref-link\" id=\"cPalaniappan_2016_a\" href=\"#rPalaniappan_2016_a\">Palaniappan and Bach (2016</a>) for strongly monotone variational inequalities.",
        "Several methods to stabilize GANs consist in transforming a zero-sum formulation into a more general game that can no longer be cast as a saddle point problem.",
        "We will compare the following optimization algorithms: baselines are SGD and Adam using either simultaneous updates on the generator and on the discriminator or k updates on the discriminator alternating with 1 update on the generator.9 Variants that use extrapolation are denoted ExtraSGD (Alg. 2) and ExtraAdam (Alg. 4).",
        "We evaluate the proposed techniques in the context of GAN training, which is a challenging stochastic optimization problem where the objectives of both players are non-convex.",
        "They are likely to be widely applicable, and benefit future development of GANs"
    ],
    "headline": "We present two techniques from this literature, namely averaging and extrapolation, widely used to solve variational inequality problem but which have not been explored in the context of Generative adversarial networks before.1",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Atkinson_2003_a",
            "entry": "K. E. Atkinson. An introduction to numerical analysis. John Wiley & Sons, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atkinson%2C%20K.E.%20An%20introduction%20to%20numerical%20analysis%202003"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20optimization%202004"
        },
        {
            "id": "Bruck_1977_a",
            "entry": "R. E. Bruck. On the weak convergence of an ergodic iteration for the solution of variational inequalities for monotone operators in Hilbert space. Journal of Mathematical Analysis and Applications, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruck%2C%20R.E.%20On%20the%20weak%20convergence%20of%20an%20ergodic%20iteration%20for%20the%20solution%20of%20variational%20inequalities%20for%20monotone%20operators%20in%20Hilbert%20space%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruck%2C%20R.E.%20On%20the%20weak%20convergence%20of%20an%20ergodic%20iteration%20for%20the%20solution%20of%20variational%20inequalities%20for%20monotone%20operators%20in%20Hilbert%20space%201977"
        },
        {
            "id": "Chen_1997_a",
            "entry": "G. H. Chen and R. T. Rockafellar. Convergence rates in forward\u2013backward splitting. SIAM Journal on Optimization, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20G.H.%20Rockafellar%2C%20R.T.%20Convergence%20rates%20in%20forward%E2%80%93backward%20splitting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20G.H.%20Rockafellar%2C%20R.T.%20Convergence%20rates%20in%20forward%E2%80%93backward%20splitting%201997"
        },
        {
            "id": "Chiang_et+al_2012_a",
            "entry": "C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In COLT, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiang%2C%20C.-K.%20Yang%2C%20T.%20Lee%2C%20C.-J.%20Mahdavi%2C%20M.%20Online%20optimization%20with%20gradual%20variations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiang%2C%20C.-K.%20Yang%2C%20T.%20Lee%2C%20C.-J.%20Mahdavi%2C%20M.%20Online%20optimization%20with%20gradual%20variations%202012"
        },
        {
            "id": "Crespi_et+al_2005_a",
            "entry": "G. P. Crespi, A. Guerraggio, and M. Rocca. Minty variational inequality and optimization: scalar and vector case. In Generalized Convexity, Generalized Monotonicity and Applications, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crespi%2C%20G.P.%20Guerraggio%2C%20A.%20Rocca%2C%20M.%20Minty%20variational%20inequality%20and%20optimization%3A%20scalar%20and%20vector%20case%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crespi%2C%20G.P.%20Guerraggio%2C%20A.%20Rocca%2C%20M.%20Minty%20variational%20inequality%20and%20optimization%3A%20scalar%20and%20vector%20case%202005"
        },
        {
            "id": "Daskalakis_et+al_2018_a",
            "entry": "C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20C.%20Ilyas%2C%20A.%20Syrgkanis%2C%20V.%20Zeng%2C%20H.%20Training%20GANs%20with%20optimism%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20C.%20Ilyas%2C%20A.%20Syrgkanis%2C%20V.%20Zeng%2C%20H.%20Training%20GANs%20with%20optimism%202018"
        },
        {
            "id": "Fedus_et+al_2018_a",
            "entry": "W. Fedus, M. Rosca, B. Lakshminarayanan, A. M. Dai, S. Mohamed, and I. Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20W.%20Rosca%2C%20M.%20Lakshminarayanan%2C%20B.%20Dai%2C%20A.M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20W.%20Rosca%2C%20M.%20Lakshminarayanan%2C%20B.%20Dai%2C%20A.M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "Gidel_et+al_2017_a",
            "entry": "G. Gidel, T. Jebara, and S. Lacoste-Julien. Frank-Wolfe algorithms for saddle point problems. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidel%2C%20G.%20Jebara%2C%20T.%20Lacoste-Julien%2C%20S.%20Frank-Wolfe%20algorithms%20for%20saddle%20point%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidel%2C%20G.%20Jebara%2C%20T.%20Lacoste-Julien%2C%20S.%20Frank-Wolfe%20algorithms%20for%20saddle%20point%20problems%202017"
        },
        {
            "id": "Gidel_et+al_2019_a",
            "entry": "G. Gidel, R. Askari Hemmat, P. Mohammad, H. Gabriel, L. R\u00e9mi, L.-J. Simon, and M. Ioannis. Negative momentum for improved game dynamics. In AISTATS, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidel%2C%20G.%20Hemmat%2C%20R.Askari%20Mohammad%2C%20P.%20Gabriel%2C%20H.%20Negative%20momentum%20for%20improved%20game%20dynamics%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidel%2C%20G.%20Hemmat%2C%20R.Askari%20Mohammad%2C%20P.%20Gabriel%2C%20H.%20Negative%20momentum%20for%20improved%20game%20dynamics%202019"
        },
        {
            "id": "Goodfellow_2016_a",
            "entry": "I. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "Grnarova_et+al_2018_a",
            "entry": "P. Grnarova, K. Y. Levy, A. Lucchi, T. Hofmann, and A. Krause. An online learning approach to generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grnarova%2C%20P.%20Levy%2C%20K.Y.%20Lucchi%2C%20A.%20Hofmann%2C%20T.%20An%20online%20learning%20approach%20to%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grnarova%2C%20P.%20Levy%2C%20K.Y.%20Lucchi%2C%20A.%20Hofmann%2C%20T.%20An%20online%20learning%20approach%20to%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "Harker_1990_a",
            "entry": "P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications. Mathematical programming, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harker%2C%20P.T.%20Pang%2C%20J.-S.%20Finite-dimensional%20variational%20inequality%20and%20nonlinear%20complementarity%20problems%3A%20a%20survey%20of%20theory%2C%20algorithms%20and%20applications%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harker%2C%20P.T.%20Pang%2C%20J.-S.%20Finite-dimensional%20variational%20inequality%20and%20nonlinear%20complementarity%20problems%3A%20a%20survey%20of%20theory%2C%20algorithms%20and%20applications%201990"
        },
        {
            "id": "Hazan_et+al_2017_a",
            "entry": "E. Hazan, K. Singh, and C. Zhang. Efficient regret minimization in non-convex games. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Singh%2C%20K.%20Zhang%2C%20C.%20Efficient%20regret%20minimization%20in%20non-convex%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Singh%2C%20K.%20Zhang%2C%20C.%20Efficient%20regret%20minimization%20in%20non-convex%20games%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "A_2017_a",
            "entry": "A. Iusem, A. Jofr\u00e9, R. I. Oliveira, and P. Thompson. Extragradient method with variance reduction for stochastic variational inequalities. SIAM Journal on Optimization, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Iusem%2C%20A.%20Jofr%C3%A9%2C%20R.%20I.%20Oliveira%20Thompson%2C%20P.%20Extragradient%20method%20with%20variance%20reduction%20for%20stochastic%20variational%20inequalities%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Iusem%2C%20A.%20Jofr%C3%A9%2C%20R.%20I.%20Oliveira%20Thompson%2C%20P.%20Extragradient%20method%20with%20variance%20reduction%20for%20stochastic%20variational%20inequalities%202017"
        },
        {
            "id": "Juditsky_et+al_2011_a",
            "entry": "A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Juditsky%2C%20A.%20Nemirovski%2C%20A.%20Tauvel%2C%20C.%20Solving%20variational%20inequalities%20with%20stochastic%20mirror-prox%20algorithm%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Juditsky%2C%20A.%20Nemirovski%2C%20A.%20Tauvel%2C%20C.%20Solving%20variational%20inequalities%20with%20stochastic%20mirror-prox%20algorithm%202011"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Korpelevich_1976_a",
            "entry": "G. Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korpelevich%2C%20G.%20The%20extragradient%20method%20for%20finding%20saddle%20points%20and%20other%20problems%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Korpelevich%2C%20G.%20The%20extragradient%20method%20for%20finding%20saddle%20points%20and%20other%20problems%201976"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master\u2019s thesis, University of Toronto, Canada, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Larsson_1994_a",
            "entry": "T. Larsson and M. Patriksson. A class of gap functions for variational inequalities. Math. Program., 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsson%2C%20T.%20Patriksson%2C%20M.%20A%20class%20of%20gap%20functions%20for%20variational%20inequalities%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsson%2C%20T.%20Patriksson%2C%20M.%20A%20class%20of%20gap%20functions%20for%20variational%20inequalities%201994"
        },
        {
            "id": "C_2017_a",
            "entry": "C. Ledig, L. Theis, F. Husz\u00e1r, J. Caballero, A. Cunningham, A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20Ledig%20L%20Theis%20F%20Husz%C3%A1r%20J%20Caballero%20A%20Cunningham%20A%20Acosta%20A%20P%20Aitken%20A%20Tejani%20J%20Totz%20Z%20Wang%20et%20al%20Photorealistic%20single%20image%20superresolution%20using%20a%20generative%20adversarial%20network%20In%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20Ledig%20L%20Theis%20F%20Husz%C3%A1r%20J%20Caballero%20A%20Cunningham%20A%20Acosta%20A%20P%20Aitken%20A%20Tejani%20J%20Totz%20Z%20Wang%20et%20al%20Photorealistic%20single%20image%20superresolution%20using%20a%20generative%20adversarial%20network%20In%20CVPR%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Y. Li, A. Schwing, K.-C. Wang, and R. Zemel. Dualing GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Schwing%2C%20A.%20Wang%2C%20K.-C.%20Zemel%2C%20R.%20Dualing%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Schwing%2C%20A.%20Wang%2C%20K.-C.%20Zemel%2C%20R.%20Dualing%20GANs%202017"
        },
        {
            "id": "Mertikopoulos_et+al_2019_a",
            "entry": "P. Mertikopoulos, H. Zenati, B. Lecouat, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Mirror descent in saddle-point problems: going the extra (gradient) mile. In ICLR, 2019. To appear.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mertikopoulos%2C%20P.%20Zenati%2C%20H.%20Lecouat%2C%20B.%20Foo%2C%20C.-S.%20Mirror%20descent%20in%20saddle-point%20problems%3A%20going%20the%20extra%20%28gradient%29%20mile%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mertikopoulos%2C%20P.%20Zenati%2C%20H.%20Lecouat%2C%20B.%20Foo%2C%20C.-S.%20Mirror%20descent%20in%20saddle-point%20problems%3A%20going%20the%20extra%20%28gradient%29%20mile%202019"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "L. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20GANs%202017"
        },
        {
            "id": "Mescheder_et+al_2018_a",
            "entry": "L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20L.%20Geiger%2C%20A.%20Nowozin%2C%20S.%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20L.%20Geiger%2C%20A.%20Nowozin%2C%20S.%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018"
        },
        {
            "id": "Metz_et+al_2017_a",
            "entry": "L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metz%2C%20L.%20Poole%2C%20B.%20Pfau%2C%20D.%20Sohl-Dickstein%2C%20J.%20Unrolled%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metz%2C%20L.%20Poole%2C%20B.%20Pfau%2C%20D.%20Sohl-Dickstein%2C%20J.%20Unrolled%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Kataoka%2C%20T.%20Koyama%2C%20M.%20Yoshida%2C%20Y.%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Kataoka%2C%20T.%20Koyama%2C%20M.%20Yoshida%2C%20Y.%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "Ozdaglar_2009_a",
            "entry": "A. Nedicand A. Ozdaglar. Subgradient methods for saddle-point problems. J Optim Theory Appl, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ozdaglar%2C%20A.Nedicand%20A.%20Subgradient%20methods%20for%20saddle-point%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ozdaglar%2C%20A.Nedicand%20A.%20Subgradient%20methods%20for%20saddle-point%20problems%202009"
        },
        {
            "id": "Nemirovski_2004_a",
            "entry": "A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM J. Optim., 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004"
        },
        {
            "id": "Nemirovski_et+al_2009_a",
            "entry": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "Nesterov_1983_a",
            "entry": "Y. Nesterov. Introductory Lectures On Convex Optimization. Springer, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20Lectures%20On%20Convex%20Optimization%201983"
        },
        {
            "id": "Nesterov_2007_a",
            "entry": "Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. Math. Program., 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Dual%20extrapolation%20and%20its%20applications%20to%20solving%20variational%20inequalities%20and%20related%20problems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Dual%20extrapolation%20and%20its%20applications%20to%20solving%20variational%20inequalities%20and%20related%20problems%202007"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Palaniappan_2016_a",
            "entry": "B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016"
        },
        {
            "id": "Polyak_1963_a",
            "entry": "B. T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.T.%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.T.%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Rakhlin_1951_a",
            "entry": "A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In COLT, 2013. H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 1951. T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In NIPS, 2016. I. Sutskever. Training recurrent neural networks. PhD thesis, 2013. P. Tseng. On linear convergence of iterative methods for the variational inequality problem. Journal of Computational and Applied Mathematics, 1995. J. Von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20A.%20Sridharan%2C%20K.%20Online%20learning%20with%20predictable%20sequences%201951"
        },
        {
            "id": "Yadav_et+al_2018_a",
            "entry": "A. Yadav, S. Shah, Z. Xu, D. Jacobs, and T. Goldstein. Stabilizing adversarial nets with prediction methods. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yadav%2C%20A.%20Shah%2C%20S.%20Xu%2C%20Z.%20Jacobs%2C%20D.%20Stabilizing%20adversarial%20nets%20with%20prediction%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yadav%2C%20A.%20Shah%2C%20S.%20Xu%2C%20Z.%20Jacobs%2C%20D.%20Stabilizing%20adversarial%20nets%20with%20prediction%20methods%202018"
        },
        {
            "id": "Yaz_et+al_2014_a",
            "entry": "Y. Yaz\u0131c\u0131, C.-S. Foo, S. Winkler, K.-H. Yap, G. Piliouras, and V. Chandrasekhar. The unusual effectiveness of averaging in GAN training. In ICLR, 2019. To appear. F. Yousefian, A. Nedic, and U. V. Shanbhag. Optimal robust smoothing extragradient algorithms for stochastic variational inequality problems. In CDC. IEEE, 2014. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Yaz%C4%B1c%C4%B1%20CS%20Foo%20S%20Winkler%20KH%20Yap%20G%20Piliouras%20and%20V%20Chandrasekhar%20The%20unusual%20effectiveness%20of%20averaging%20in%20GAN%20training%20In%20ICLR%202019%20To%20appear%20F%20Yousefian%20A%20Nedic%20and%20U%20V%20Shanbhag%20Optimal%20robust%20smoothing%20extragradient%20algorithms%20for%20stochastic%20variational%20inequality%20problems%20In%20CDC%20IEEE%202014%20JY%20Zhu%20T%20Park%20P%20Isola%20and%20A%20A%20Efros%20Unpaired%20imagetoimage%20translation%20using%20cycleconsistent%20adversarial%20networks%20In%20ICCV%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Yaz%C4%B1c%C4%B1%20CS%20Foo%20S%20Winkler%20KH%20Yap%20G%20Piliouras%20and%20V%20Chandrasekhar%20The%20unusual%20effectiveness%20of%20averaging%20in%20GAN%20training%20In%20ICLR%202019%20To%20appear%20F%20Yousefian%20A%20Nedic%20and%20U%20V%20Shanbhag%20Optimal%20robust%20smoothing%20extragradient%20algorithms%20for%20stochastic%20variational%20inequality%20problems%20In%20CDC%20IEEE%202014%20JY%20Zhu%20T%20Park%20P%20Isola%20and%20A%20A%20Efros%20Unpaired%20imagetoimage%20translation%20using%20cycleconsistent%20adversarial%20networks%20In%20ICCV%202017"
        },
        {
            "id": "This_2004_a",
            "entry": "This is standard convex analysis result which can be found for instance in (Boyd and Vandenberghe, 2004). The following lemma is also standard in convex analysis and its proof uses similar arguments as the proof of Lemma 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20is%20standard%20convex%20analysis%20result%20which%20can%20be%20found%20for%20instance%20in%20Boyd%20and%20Vandenberghe%202004%20The%20following%20lemma%20is%20also%20standard%20in%20convex%20analysis%20and%20its%20proof%20uses%20similar%20arguments%20as%20the%20proof%20of%20Lemma%201"
        },
        {
            "id": "This_2019_a",
            "entry": "This reduction has already been proposed by Gidel et al. (2019). For completeness, we reproduce here similar arguments. The following lemma is a bit more general than the result provided by Gidel et al. (2019). It states that the study of a wide class of unconstrained first order method on (63) can be reduced to the study of the method on (36), with potentially rescaled step-sizes.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20reduction%20has%20already%20been%20proposed%20by%20Gidel%20et%20al%202019%20For%20completeness%20we%20reproduce%20here%20similar%20arguments%20The%20following%20lemma%20is%20a%20bit%20more%20general%20than%20the%20result%20provided%20by%20Gidel%20et%20al%202019%20It%20states%20that%20the%20study%20of%20a%20wide%20class%20of%20unconstrained%20first%20order%20method%20on%2063%20can%20be%20reduced%20to%20the%20study%20of%20the%20method%20on%2036%20with%20potentially%20rescaled%20stepsizes",
            "oa_query": "https://api.scholarcy.com/oa_version?query=This%20reduction%20has%20already%20been%20proposed%20by%20Gidel%20et%20al%202019%20For%20completeness%20we%20reproduce%20here%20similar%20arguments%20The%20following%20lemma%20is%20a%20bit%20more%20general%20than%20the%20result%20provided%20by%20Gidel%20et%20al%202019%20It%20states%20that%20the%20study%20of%20a%20wide%20class%20of%20unconstrained%20first%20order%20method%20on%2063%20can%20be%20reduced%20to%20the%20study%20of%20the%20method%20on%2036%20with%20potentially%20rescaled%20stepsizes"
        }
    ]
}
