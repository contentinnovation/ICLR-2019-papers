{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PROXYLESSNAS: DIRECT NEURAL ARCHITECTURE SEARCH ON TARGET TASK AND HARDWARE",
        "author": "Han Cai, Ligeng Zhu, Song Han Massachusetts Institute of Technology {hancai, ligeng, songhan}@mit.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HylVB3AqYm"
        },
        "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.1"
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "directed acyclic graph",
            "url": "https://en.wikipedia.org/wiki/directed_acyclic_graph"
        },
        {
            "term": "Neural architecture search",
            "url": "https://en.wikipedia.org/wiki/Neural_architecture_search"
        }
    ],
    "abbreviations": {
        "NAS": "Neural architecture search",
        "DAG": "directed acyclic graph",
        "MBConv": "mobile inverted bottleneck convolution"
    },
    "highlights": [
        "Neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition (Zoph et al, 2018; <a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\"><a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\">Cai et al, 2018a</a></a>; Liu et al, 2018a; Zhong et al, 2018) and language modeling (Zoph & Le, 2017)",
        "Top-performing blocks are stacked and transferred to the large-scale target task. This paradigm has been widely adopted in subsequent Neural architecture search algorithms (Liu et al, 2018a;b; Real et al, 2018; Cai et al, 2018b; Liu et al, 2018c; Tan et al, 2018; Luo et al, 2018)",
        "We demonstrate the effectiveness of our proposed method on two benchmark datasets (CIFAR-10 and ImageNet) for the image classification task",
        "Table 3 reports the results on GPU, where we find that our ProxylessNAS can still achieve superior performances compared to both human-designed and automatically searched architectures",
        "We introduced ProxylessNAS that can directly learn neural network architectures on the target task and target hardware without any proxy",
        "We reduced the search cost (GPU-hours and GPU memory) of Neural architecture search to the same level of normal training using path binarization"
    ],
    "key_statements": [
        "Neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition (Zoph et al, 2018; <a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\"><a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\">Cai et al, 2018a</a></a>; Liu et al, 2018a; Zhong et al, 2018) and language modeling (Zoph & Le, 2017)",
        "Top-performing blocks are stacked and transferred to the large-scale target task. This paradigm has been widely adopted in subsequent Neural architecture search algorithms (Liu et al, 2018a;b; Real et al, 2018; Cai et al, 2018b; Liu et al, 2018c; Tan et al, 2018; Luo et al, 2018)",
        "We propose a simple and effective solution to the aforementioned limitations, called ProxylessNAS, which directly learns the architectures on the target task and hardware instead of with\n1Pretrained models and evaluation code are released at https://github.com/MIT-HAN-LAB/ProxylessNAS",
        "We reduce the computational cost (GPU hours aGnPdUGHoPuUrs memory)GoPfUaMrecmhoirtyecture search to the same level of regular training in the following ways",
        "We propose a gradient-based approach to train these binarized parameters based on BinaryConnect (Courbariaux et al, 2015)",
        "To handle non-differentiable hardware objectives for learning specialized network architectures on target hardware, we model network latency as a continuous function and optimize it as regularization loss",
        "We demonstrate the effectiveness of our proposed method on two benchmark datasets (CIFAR-10 and ImageNet) for the image classification task",
        "Table 3 reports the results on GPU, where we find that our ProxylessNAS can still achieve superior performances compared to both human-designed and automatically searched architectures",
        "We introduced ProxylessNAS that can directly learn neural network architectures on the target task and target hardware without any proxy",
        "We reduced the search cost (GPU-hours and GPU memory) of Neural architecture search to the same level of normal training using path binarization"
    ],
    "summary": [
        "Neural architecture search (NAS) has demonstrated much success in automating neural network architecture design for various deep learning tasks, such as image recognition (Zoph et al, 2018; <a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\"><a class=\"ref-link\" id=\"cCai_et+al_2018_a\" href=\"#rCai_et+al_2018_a\">Cai et al, 2018a</a></a>; Liu et al, 2018a; Zhong et al, 2018) and language modeling (Zoph & Le, 2017).",
        "To handle non-differentiable hardware objectives for learning specialized network architectures on target hardware, we model network latency as a continuous function and optimize it as regularization loss.",
        "Our work is most closely related to One-Shot (<a class=\"ref-link\" id=\"cBender_et+al_2018_a\" href=\"#rBender_et+al_2018_a\">Bender et al, 2018</a>) and DARTS (Liu et al, 2018c), both of which get rid of the meta-controller by modeling NAS as a single training process of an over-parameterized network that comprises all candidate paths.",
        "As illustrated in Eq (3) and Figure 2, by using the binary gates rather than real-valued path weights (Liu et al, 2018c), only one path of activation is active in memory at run-time and the memory requirement of training the over-parameterized network is reduced to the same level of training a compact model.",
        "Figure 2 illustrates the training procedure of the weight parameters and binarized architecture pa- MITRed-1 rameters in the over-parameterized network.",
        "Directly using Eq (4) to update the architecture parameters would require roughly N times GPU memory compared to training a compact model.",
        "Regardless of the value of N , only two paths are involved in each update step of the architecture parameters, and thereby the memory requirement is reduced to the same level of training a compact model.",
        "Unlike previous NAS works (Zoph et al, 2018; Liu et al, 2018c) that first learn CNN blocks on CIFAR-10 under small-scale setting, transfer the learned block to ImageNet or CIFAR-10 under large-scale setting by repeatedly stacking it, we directly learn the architectures on the target task and target hardware (GPU, CPU and mobile phone) while allowing each block to be specified.",
        "We randomly sample 5,000 images from the training set as a validation set for learning architecture parameters which are updated using the Adam optimizer with an initial learning rate of 0.006 for the gradient-based algorithm (Section 3.2.1) and 0.01 for the REINFORCEbased algorithm (Section 3.3.2).",
        "Compared with PathLevel EAS (Cai et al, 2018b) that explores the tree-structured architecture space, both Proxyless-G and Proxyless-R achieves similar or lower test error rate results with half fewer parameters.",
        "Figure 6 demonstrates the detailed architectures of our searched CNN models on three hardware platforms: GPU/CPU/Mobile.",
        "April we allow specializing network architectures for different platforms by directly incorporating the measured hardware latency into optimization objectives.",
        "We compared the optimized models on CPU/GPU/mobile and raised the awareness of the needs of specializing neural network architecture for different hardware architectures."
    ],
    "headline": "We present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms",
    "reference_links": [
        {
            "id": "Bender_et+al_2018_a",
            "entry": "Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018"
        },
        {
            "id": "Brock_et+al_2018_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018"
        },
        {
            "id": "Cai_et+al_2018_a",
            "entry": "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. In AAAI, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018"
        }
    ]
}
