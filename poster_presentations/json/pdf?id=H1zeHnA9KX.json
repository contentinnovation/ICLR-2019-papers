{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "REPRESENTING FORMAL LANGUAGES: A COMPARISON BETWEEN FINITE AUTOMATA AND RECURRENT NEURAL NETWORKS",
        "author": "Joshua J. Michalenko Rice University jjm,@rice.edu",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1zeHnA9KX"
        },
        "abstract": "We investigate the internal representations that a recurrent neural network (RNN) uses while learning to recognize a regular formal language. Specifically, we train a RNN on positive and negative examples from a regular language, and ask if there is a simple decoding function that maps states of this RNN to states of the minimal deterministic finite automaton (MDFA) for the language. Our experiments show that such a decoding function indeed exists, and that it maps states of the RNN not to MDFA states, but to states of an abstraction obtained by clustering small sets of MDFA states into \u201csuperstates\u201d. A qualitative analysis reveals that the abstraction often has a simple interpretation. Overall, the results suggest a strong structural relationship between internal representations used by RNNs and finite automata, and explain the well-known ability of RNNs to recognize formal grammatical structure."
    },
    "keywords": [
        {
            "term": "regular language",
            "url": "https://en.wikipedia.org/wiki/regular_language"
        },
        {
            "term": "deterministic finite automaton",
            "url": "https://en.wikipedia.org/wiki/deterministic_finite_automaton"
        },
        {
            "term": "finite state machine",
            "url": "https://en.wikipedia.org/wiki/finite_state_machine"
        },
        {
            "term": "minimal DFA",
            "url": "https://en.wikipedia.org/wiki/minimal_DFA"
        },
        {
            "term": "formal language",
            "url": "https://en.wikipedia.org/wiki/formal_language"
        },
        {
            "term": "Nondeterministic Finite Automaton",
            "url": "https://en.wikipedia.org/wiki/Nondeterministic_Finite_Automaton"
        },
        {
            "term": "automata",
            "url": "https://en.wikipedia.org/wiki/automata"
        },
        {
            "term": "finite automata",
            "url": "https://en.wikipedia.org/wiki/finite_automata"
        },
        {
            "term": "finite automaton",
            "url": "https://en.wikipedia.org/wiki/finite_automaton"
        },
        {
            "term": "Area under the curve",
            "url": "https://en.wikipedia.org/wiki/Area_under_the_curve"
        },
        {
            "term": "grammatical structure",
            "url": "https://en.wikipedia.org/wiki/grammatical_structure"
        },
        {
            "term": "small set",
            "url": "https://en.wikipedia.org/wiki/small_set"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        },
        {
            "term": "finite state automaton",
            "url": "https://en.wikipedia.org/wiki/finite_state_automaton"
        }
    ],
    "abbreviations": {
        "RNN": "recurrent neural network",
        "MDFA": "minimal DFA",
        "RNNs": "Recurrent neural networks",
        "FA": "finite automata",
        "DFA": "Deterministic Finite Automaton",
        "NFA": "Nondeterministic Finite Automaton",
        "AUC": "Area under the curve",
        "LDC": "linear decoding"
    },
    "highlights": [
        "Recurrent neural networks (RNNs) seem \u201cunreasonably\u201d effective at modeling patterns in noisy realworld sequences",
        "We ask: Can the internal knowledge representations of recurrent neural network trained to recognize formal languages be mapped to the states of automata-theoretic models that are traditionally used to define these same formal languages? we investigate this question for the class of regular languages, or formal languages accepted by finite automata (FA)",
        "We have studied how recurrent neural network trained to recognize regular formal languages represent knowledge in their hidden state",
        "We have asked if this internal representation can be decoded into canonical, minimal Deterministic Finite Automaton that exactly recognizes the language, and can be seen to be the \"ground truth\"",
        "We have shown that a linear function does a remarkably good job at performing such a decoding. This decoder maps states of the recurrent neural network not to minimal Deterministic Finite Automaton states, but to states of an abstraction obtained by clustering small sets of minimal Deterministic Finite Automaton states into \"abstractions\"",
        "The results suggest a strong structural relationship between internal representations used by recurrent neural network and finite automata, and explain the well-known ability of recurrent neural network to recognize formal grammatical structure"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) seem \u201cunreasonably\u201d effective at modeling patterns in noisy realworld sequences",
        "We propose a new way of understanding how trained recurrent neural network represent grammatical structure, by comparing them to finite automata that solve the same language recognition task",
        "We ask: Can the internal knowledge representations of recurrent neural network trained to recognize formal languages be mapped to the states of automata-theoretic models that are traditionally used to define these same formal languages? we investigate this question for the class of regular languages, or formal languages accepted by finite automata (FA)",
        "Since there exist infinitely many finite automata that accept the same language, we focus on the minimal deterministic finite automaton (MDFA) \u2014 the deterministic finite automaton (DFA) with the smallest possible number of states \u2013 that perfectly recognizes the language",
        "We find the abstraction has low \u201ccoarseness\", in the sense that only a few of the minimal Deterministic Finite Automaton states need be clustered, and a qualitative analysis reveals that the abstractions often have simple interpretations",
        "WHY ABSTRACTIONS ARE NECESSARY Given the information above, how is the hidden state space of the RL organized? One hypothesis that is consistent with the observations above is that the trained recurrent neural network reflects a coarse-grained abstraction of the state space Q0 (Figure 1), rather than the minimal Deterministic Finite Automaton states themselves.1",
        "We have studied how recurrent neural network trained to recognize regular formal languages represent knowledge in their hidden state",
        "We have asked if this internal representation can be decoded into canonical, minimal Deterministic Finite Automaton that exactly recognizes the language, and can be seen to be the \"ground truth\"",
        "We have shown that a linear function does a remarkably good job at performing such a decoding. This decoder maps states of the recurrent neural network not to minimal Deterministic Finite Automaton states, but to states of an abstraction obtained by clustering small sets of minimal Deterministic Finite Automaton states into \"abstractions\"",
        "The results suggest a strong structural relationship between internal representations used by recurrent neural network and finite automata, and explain the well-known ability of recurrent neural network to recognize formal grammatical structure"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) seem \u201cunreasonably\u201d effective at modeling patterns in noisy realworld sequences.",
        "Given a dataset of input strings D \u2282 \u03a3\u2217, we can define the decoding accuracy of a map ffor an abstraction ALn from RNN RL by: \u03c1f(RL, ALn )",
        "For a given decoding function fand NFA ALn , we want to check whether the RNN transition on a is mapped to the abstraction of the MDFA transition on a.",
        "One hypothesis that is consistent with the observations above is that the trained RNN reflects a coarse-grained abstraction of the state space Q0 (Figure 1), rather than the MDFA states themselves.1 To test this hypothesis, we propose a simple greedy algorithm to find an abstraction mapping \u03b1: (a) given an NFA AnL with n unique states in Qn, consider all (n \u2212 1)-partitions of Qn\u22121; (b) select the partition with the highest decoding accuracy; (c) Repeat this iterative merging process until only a 2-partition remains.",
        "Figure 5: 5a (Left): Average linear decoder testing accuracy as a function of coarseness (The number of times \u03b1 is applied), sorted by the number of nodes in the MDFA.",
        "Results showing the relationship between high decoding accuracy \u03c1f(RL, ALn ) as a function of coarseness is presented in Figure 5a conditioned on the number of nodes in the original MDFA.",
        "Both of these figures showcase that for a given DFA, in general we can find a low coarseness NFA that the hidden state space of RL can be decoded to with high accuracy.",
        "6b (Right): Average transitional accuracy vs coarseness, sorted by the number of nodes in the MDFA while using a non-linear decoder.",
        "Figure 7: 7a (Top): The MDFA of the SIMPLE EMAILS language with a dendrogram representing the the sequence of abstractions created while using a linear decoder.",
        "8b (Right): Linear decoder accuracies as a function of coarseness for the DATES language corresponding to Figure 7b.",
        "One possible and very likely reason for this is the network has learned an abstraction of the pattern [a-d]* and uses the same hidden state space regardless of location in string to recognize this pattern, which has been indicated in past work (<a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\">Karpathy et al, 2015</a>).",
        "Our new interpretation of H reveals some new intuitions, empirically backed by our decoding and transitional accuracy scores, regarding how the RNN RL structures the hidden state space H in the task of language recognition.",
        "We have studied how RNNs trained to recognize regular formal languages represent knowledge in their hidden state.",
        "We intend to explore more complex and richer classes of formal languages, such as context-free languages and recursively enumerable languages, and their neural analogs"
    ],
    "headline": "We investigate the internal representations that a recurrent neural network uses while learning to recognize a regular formal language",
    "reference_links": [
        {
            "id": "Astrand_et+al_2014_a",
            "entry": "E. Astrand., P. Enel, G. Ibos, P. F. Dominey, P. Baraduc, and S. B. Hamed. Comparison of classifiers for decoding sensory and cognitive information from prefrontal neuronal populations. PLOS ONE, 9:1\u201314, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Astrand.%2C%20E.%20Enel%2C%20P.%20Ibos%2C%20G.%20Dominey%2C%20P.F.%20Comparison%20of%20classifiers%20for%20decoding%20sensory%20and%20cognitive%20information%20from%20prefrontal%20neuronal%20populations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Astrand.%2C%20E.%20Enel%2C%20P.%20Ibos%2C%20G.%20Dominey%2C%20P.F.%20Comparison%20of%20classifiers%20for%20decoding%20sensory%20and%20cognitive%20information%20from%20prefrontal%20neuronal%20populations%202014"
        },
        {
            "id": "Ayache_et+al_2018_a",
            "entry": "S. Ayache, R. Eyraud, and N. Goudian. Explaining black boxes on sequential data using weighted automata. In 14th International Conference on Grammatical Inference, pages 81\u2013103, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ayache%2C%20S.%20Eyraud%2C%20R.%20Goudian%2C%20N.%20Explaining%20black%20boxes%20on%20sequential%20data%20using%20weighted%20automata%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ayache%2C%20S.%20Eyraud%2C%20R.%20Goudian%2C%20N.%20Explaining%20black%20boxes%20on%20sequential%20data%20using%20weighted%20automata%202018"
        },
        {
            "id": "Das_1993_a",
            "entry": "S. Das and M. Mozer. A unified gradient-descent/clustering architecture for finite state machine induction. In Advances in Neural Information Processing Systems, pages 19\u201326, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20S.%20Mozer%2C%20M.%20A%20unified%20gradient-descent/clustering%20architecture%20for%20finite%20state%20machine%20induction%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20S.%20Mozer%2C%20M.%20A%20unified%20gradient-descent/clustering%20architecture%20for%20finite%20state%20machine%20induction%201993"
        },
        {
            "id": "Firoiu_et+al_1998_a",
            "entry": "L. Firoiu, T. Oates, and P. R. Cohen. Learning deterministic finite automaton with a recurrent neural network. In 4th International Conference on Grammatical Inference, pages 90\u2013101, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Firoiu%2C%20L.%20Oates%2C%20T.%20Cohen%2C%20P.R.%20Learning%20deterministic%20finite%20automaton%20with%20a%20recurrent%20neural%20network%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Firoiu%2C%20L.%20Oates%2C%20T.%20Cohen%2C%20P.R.%20Learning%20deterministic%20finite%20automaton%20with%20a%20recurrent%20neural%20network%201998"
        },
        {
            "id": "Funahashi_1993_a",
            "entry": "K. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks, 6(6):801\u2013806, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Funahashi%2C%20K.%20Nakamura%2C%20Y.%20Approximation%20of%20dynamical%20systems%20by%20continuous%20time%20recurrent%20neural%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Funahashi%2C%20K.%20Nakamura%2C%20Y.%20Approximation%20of%20dynamical%20systems%20by%20continuous%20time%20recurrent%20neural%20networks%201993"
        },
        {
            "id": "Gers_2001_a",
            "entry": "F. A. Gers and J. Schmidhuber. LSTM recurrent networks learn simple context-free and contextsensitive languages. IEEE Transactions on Neural Networks, 12(6):pages 1333\u20131340, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20LSTM%20recurrent%20networks%20learn%20simple%20context-free%20and%20contextsensitive%20languages%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20LSTM%20recurrent%20networks%20learn%20simple%20context-free%20and%20contextsensitive%20languages%202001"
        },
        {
            "id": "Giles_et+al_1991_a",
            "entry": "C. L. Giles, C. B. Miller, D. Chen, G. Sun, H. Chen, and Y. Lee. Extracting and learning an unknown grammar with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 317\u2013324, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giles%2C%20C.L.%20Miller%2C%20C.B.%20Chen%2C%20D.%20Sun%2C%20G.%20Extracting%20and%20learning%20an%20unknown%20grammar%20with%20recurrent%20neural%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giles%2C%20C.L.%20Miller%2C%20C.B.%20Chen%2C%20D.%20Sun%2C%20G.%20Extracting%20and%20learning%20an%20unknown%20grammar%20with%20recurrent%20neural%20networks%201991"
        },
        {
            "id": "Giles_et+al_1992_a",
            "entry": "C. L. Giles, C. B. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee. Learning and extracting finite state automata with second-order recurrent neural networks. Neural Computation, 4(3):pages 393\u2013405, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giles%2C%20C.L.%20Miller%2C%20C.B.%20Chen%2C%20D.%20Chen%2C%20H.%20Learning%20and%20extracting%20finite%20state%20automata%20with%20second-order%20recurrent%20neural%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giles%2C%20C.L.%20Miller%2C%20C.B.%20Chen%2C%20D.%20Chen%2C%20H.%20Learning%20and%20extracting%20finite%20state%20automata%20with%20second-order%20recurrent%20neural%20networks%201992"
        },
        {
            "id": "Hopcroft_1979_a",
            "entry": "J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Languages and Computation. Addison-Wesley, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopcroft%2C%20J.E.%20Ullman%2C%20J.D.%20Introduction%20to%20Automata%20Theory%2C%20Languages%20and%20Computation%201979"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "A. Karpathy, J. Johnson, and F. Li. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Kombrink_et+al_2011_a",
            "entry": "S. Kombrink, T. Mikolov, M. Karafi\u00e1t, and L. Burget. Recurrent neural network based language modeling in meeting recognition. In 12th Annual Conference of the International Speech Communication Association, pages 2877\u20132880, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kombrink%2C%20S.%20Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Recurrent%20neural%20network%20based%20language%20modeling%20in%20meeting%20recognition%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kombrink%2C%20S.%20Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Recurrent%20neural%20network%20based%20language%20modeling%20in%20meeting%20recognition%202011"
        },
        {
            "id": "Krakovna_2016_a",
            "entry": "V. Krakovna and F. Doshi-Velez. Increasing the interpretability of recurrent neural networks using hidden markov models. arXiv preprint arXiv:1611.05934, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05934"
        },
        {
            "id": "Lawrence_et+al_2000_a",
            "entry": "S. Lawrence, C. L. Giles, and S. Fong. Natural language grammatical inference with recurrent neural networks. IEEE Transactions on Knowledge and Data Engineering, 12(1):pages 126\u2013140, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20S.%20Giles%2C%20C.L.%20Fong%2C%20S.%20Natural%20language%20grammatical%20inference%20with%20recurrent%20neural%20networks%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20S.%20Giles%2C%20C.L.%20Fong%2C%20S.%20Natural%20language%20grammatical%20inference%20with%20recurrent%20neural%20networks%202000"
        },
        {
            "id": "V_2008_a",
            "entry": "L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9: 2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Miclet_1996_a",
            "entry": "L. Miclet and C. de la Higuera. Grammatical Inference: Learning Syntax from Sentences. Springer, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miclet%2C%20L.%20de%20la%20Higuera%2C%20C.%20Grammatical%20Inference%3A%20Learning%20Syntax%20from%20Sentences%201996"
        },
        {
            "id": "M_2017_a",
            "entry": "A. M\u00f8ller. dk.brics.automaton \u2013 finite-state automata and regular expressions for Java, 2017. http://www.brics.dk/automaton/.",
            "url": "http://www.brics.dk/automaton/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%B8ller%2C%20A.%20dk.brics.automaton%20%E2%80%93%20finite-state%20automata%20and%20regular%20expressions%20for%202017"
        },
        {
            "id": "Neto_et+al_1997_a",
            "entry": "J. P. G. Neto, H. T. Siegelmann, J. F. Costa, and C. P. S. Araujo. Turing universality of neural nets (revisited). In 6th International Workshop on Computer Aided Systems Theory, pages 361\u2013366, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neto%2C%20J.P.G.%20Siegelmann%2C%20H.T.%20Costa%2C%20J.F.%20Araujo%2C%20C.P.S.%20Turing%20universality%20of%20neural%20nets%20%28revisited%29%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neto%2C%20J.P.G.%20Siegelmann%2C%20H.T.%20Costa%2C%20J.F.%20Araujo%2C%20C.P.S.%20Turing%20universality%20of%20neural%20nets%20%28revisited%29%201997"
        },
        {
            "id": "Omlin_0000_a",
            "entry": "C. W. Omlin and C. L. Giles. Constructing deterministic finite-state automata in recurrent neural networks. Journal of the Association of Computing Machinery, JACM, 43(6):pages 937\u2013972, 1996a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omlin%2C%20C.W.%20Giles%2C%20C.L.%20Constructing%20deterministic%20finite-state%20automata%20in%20recurrent%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omlin%2C%20C.W.%20Giles%2C%20C.L.%20Constructing%20deterministic%20finite-state%20automata%20in%20recurrent%20neural%20networks"
        },
        {
            "id": "Omlin_1996_a",
            "entry": "C. W. Omlin and C. L. Giles. Extraction of rules from discrete-time recurrent neural networks. Neural Networks, 9(1):41\u201352, 1996b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omlin%2C%20C.W.%20Giles%2C%20C.L.%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omlin%2C%20C.W.%20Giles%2C%20C.L.%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996"
        },
        {
            "id": "Rabusseau_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 G. Rabusseau, T. Li, and D. Precup. Connecting weighted automata and recurrent neural networks through spectral learning. arXiv preprint arXiv:1807.01406, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01406"
        },
        {
            "id": "Tino_et+al_1998_a",
            "entry": "P. Tino, B. G. Horne, and C. L. Giles. Finite state machines and recurrent neural networks \u2014 automata and dynamical systems approaches. In Neural Networks and Pattern Recognition, pages 171 \u2013 219. 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tino%2C%20P.%20Horne%2C%20B.G.%20Giles%2C%20C.L.%20Finite%20state%20machines%20and%20recurrent%20neural%20networks%20%E2%80%94%20automata%20and%20dynamical%20systems%20approaches%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tino%2C%20P.%20Horne%2C%20B.G.%20Giles%2C%20C.L.%20Finite%20state%20machines%20and%20recurrent%20neural%20networks%20%E2%80%94%20automata%20and%20dynamical%20systems%20approaches%201998"
        },
        {
            "id": "Weiss_et+al_2018_a",
            "entry": "G. Weiss, Y. Goldberg, and E. Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. In Proceedings of the 35th International Conference on Machine Learning, ICML, pages 5244\u20135253, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weiss%2C%20G.%20Goldberg%2C%20Y.%20Yahav%2C%20E.%20Extracting%20automata%20from%20recurrent%20neural%20networks%20using%20queries%20and%20counterexamples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weiss%2C%20G.%20Goldberg%2C%20Y.%20Yahav%2C%20E.%20Extracting%20automata%20from%20recurrent%20neural%20networks%20using%20queries%20and%20counterexamples%202018"
        }
    ]
}
