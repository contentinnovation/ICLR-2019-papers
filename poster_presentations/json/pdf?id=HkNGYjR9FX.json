{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING RECURRENT BINARY/TERNARY WEIGHTS",
        "author": "Arash Ardakani, Zhengyun Ji, Sean C. Smithson, Brett H. Meyer & Warren J. Gross Department of Electrical and Computer Engineering, McGill University, Montreal, Canada {arash.ardakani, zhengyun.ji, sean.smithson}@mail.mcgill.ca {brett.meyer, warren.gross}@mcgill.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkNGYjR9FX"
        },
        "abstract": "Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all multiply-accumulate operations by simple accumulations, bringing significant benefits to custom hardware in terms of silicon area and power consumption. On the software side, we evaluate the performance (in terms of accuracy) of our method using long short-term memories (LSTMs) and gated recurrent units (GRUs) on various sequential models including sequence classification and language modeling. We demonstrate that our method achieves competitive results on the aforementioned tasks while using binary/ternary weights during the runtime. On the hardware side, we present custom hardware for accelerating the recurrent computations of LSTMs with binary/ternary weights. Ultimately, we show that LSTMs with binary/ternary weights can achieve up to 12\u00d7 memory saving and 10\u00d7 inference speedup compared to the full-precision hardware implementation design."
    },
    "keywords": [
        {
            "term": "application-specific integrated circuit",
            "url": "https://en.wikipedia.org/wiki/application-specific_integrated_circuit"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent neural networks",
        "LSTMs": "long short-term memories",
        "GRUs": "gated recurrent units",
        "CNNs": "Convolutional neural networks",
        "TTQ": "trained ternary quantization",
        "LAB": "Learning-Recurrent-Binary-Ternary-Weights aware binarization",
        "LAQ": "loss-aware quantization",
        "LSTM": "Long Short-Term Memory",
        "GRU": "Gated Recurrent Unit",
        "BPC": "Bits Per Character",
        "ASIC": "application-specific integrated circuit"
    },
    "highlights": [
        "Convolutional neural networks (CNNs) have surpassed human-level accuracy in various complex tasks by obtaining a hierarchical representation with increasing levels of abstraction (<a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\"><a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\">Bengio (2009</a></a>); <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">Lecun et al (2015</a></a>))",
        "We present custom hardware to accelerate the recurrent computations of Recurrent neural networks with binary or ternary weights",
        "We evaluate the performance of the proposed long short-term memories with binary/ternary weights on different temporal tasks to show the generality of our method",
        "All the models reported in Table 1 use an long short-term memories layer with 1000, 512 and 512 units on a sequence length of 100 for the experiments on Penn Treebank (<a class=\"ref-link\" id=\"cMarcus_et+al_1993_a\" href=\"#rMarcus_et+al_1993_a\">Marcus et al (1993</a>)), War & Peace (<a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\"><a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\">Karpathy et al (2015</a></a>)) and Linux Kernel (<a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\"><a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\">Karpathy et al (2015</a></a>)) corpora, respectively",
        "We introduced a method that learns recurrent binary/ternary weights and eliminates most of the full-precision multiplications of the recurrent computations during the inference",
        "We showed that learning recurrent binary/ternary weights brings a major benefit to custom hardware implementations by replacing full-precision multipliers with hardware-friendly multiplexers and reducing the memory bandwidth"
    ],
    "key_statements": [
        "Convolutional neural networks (CNNs) have surpassed human-level accuracy in various complex tasks by obtaining a hierarchical representation with increasing levels of abstraction (<a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\"><a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\">Bengio (2009</a></a>); <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">Lecun et al (2015</a></a>))",
        "We propose a method that learns recurrent binary and ternary weights in Recurrent neural networks during the training phase and eliminates the need for full-precision multiplications during the inference time",
        "We present custom hardware to accelerate the recurrent computations of Recurrent neural networks with binary or ternary weights",
        "We mainly focus on the long short-term memories architecture to learn recurrent binary/ternary weights due to their prevalent use in both academia and industry",
        "We evaluate the performance of the proposed long short-term memories with binary/ternary weights on different temporal tasks to show the generality of our method",
        "All the models reported in Table 1 use an long short-term memories layer with 1000, 512 and 512 units on a sequence length of 100 for the experiments on Penn Treebank (<a class=\"ref-link\" id=\"cMarcus_et+al_1993_a\" href=\"#rMarcus_et+al_1993_a\">Marcus et al (1993</a>)), War & Peace (<a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\"><a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\">Karpathy et al (2015</a></a>)) and Linux Kernel (<a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\"><a class=\"ref-link\" id=\"cKarpathy_et+al_2015_a\" href=\"#rKarpathy_et+al_2015_a\">Karpathy et al (2015</a></a>)) corpora, respectively",
        "The experimental results show that our model with binary/ternary weights outperforms all the existing quantized models in terms of prediction accuracy",
        "Our ternarized model achieves the same Bits Per Character values on War & Peace and Penn Treebank datasets as the full-precision model while requiring 32\u00d7 less memory footprint",
        "We examine the prediction accuracy of our method over the medium and large models introduced by <a class=\"ref-link\" id=\"cZaremba_et+al_2014_a\" href=\"#rZaremba_et+al_2014_a\">Zaremba et al (2014</a>): the medium model contains an long short-term memories layer of size 650 and the large model contains two long short-term memories layers of size 1500",
        "For the long short-term memories with recurrent binary/ternary weights, a 12-bit fixed-point representation is only used for activations and multipliers in the MAC units are replaced with low-cost multiplexers",
        "We introduced a method that learns recurrent binary/ternary weights and eliminates most of the full-precision multiplications of the recurrent computations during the inference",
        "We showed that learning recurrent binary/ternary weights brings a major benefit to custom hardware implementations by replacing full-precision multipliers with hardware-friendly multiplexers and reducing the memory bandwidth",
        "We introduced two application-specific integrated circuit implementations: low-power and high-throughput implementations"
    ],
    "summary": [
        "Convolutional neural networks (CNNs) have surpassed human-level accuracy in various complex tasks by obtaining a hierarchical representation with increasing levels of abstraction (<a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\"><a class=\"ref-link\" id=\"cBengio_2009_a\" href=\"#rBengio_2009_a\">Bengio (2009</a></a>); <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">Lecun et al (2015</a></a>)).",
        "There are no binary/ternary models that can perform different temporal tasks while achieving similar prediction accuracy to its full-precision counterpart is missing in literature.",
        "We evaluate the performance of the proposed LSTMs with binary/ternary weights on different temporal tasks to show the generality of our method.",
        "While our binary model uses a lower bit precision and fewer operations for the recurrent computations compared to the alternating models, its loss of accuracy is small.",
        "The simulation results show that our Attentive Reader with binary/ternary weights yields similar accuracy rate to its full-precision counterpart while requiring 32\u00d7 smaller memory footprint.",
        "To show the effect of the probabilistic quantization on the prediction accuracy of temporal tasks, we adopted the ternarized network trained for the character-level language modeling tasks on the Penn Treebank corpus.",
        "While we have only applied our binarization/ternarization method on LSTMs, our method can be used to binarize/ternarize other recurrent architectures such as GRUs. To show the versatility of our method, we repeat the character-level language modeling task performed in Section 5.1 using GRUs on the Penn Treebank, War & Peace and Linux Kernel corpora.",
        "The simulation results show that our method can successfully binarize/ternarize the recurrent weights of GRUs. As a final note, we have investigated the effect of using different batch sizes on the prediction accuracy of our binarized/ternarized models.",
        "We trained an LSTM of size 1000 over a sequence length of 100 and different batch sizes to perform the character-level language modeling task on the Penn Treebank corpus.",
        "For the LSTMs with recurrent binary/ternary weights, a 12-bit fixed-point representation is only used for activations and multipliers in the MAC units are replaced with low-cost multiplexers.",
        "They show that using recurrent binary/ternary weights results in up to 9\u00d7 lower power and 10.6\u00d7 lower silicon area compared to the baseline when performing the inference computations at 400 MHz. For the high-speed design, we consider the same silicon area and power consumption for both the fullprecision and binary/ternary-precision models.",
        "We introduced a method that learns recurrent binary/ternary weights and eliminates most of the full-precision multiplications of the recurrent computations during the inference.",
        "We showed that learning recurrent binary/ternary weights brings a major benefit to custom hardware implementations by replacing full-precision multipliers with hardware-friendly multiplexers and reducing the memory bandwidth.",
        "The former architecture can save up to 9\u00d7 power consumption and the latter speeds up the recurrent computations by a factor of 10"
    ],
    "headline": "To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of Recurrent neural networks",
    "reference_links": [
        {
            "id": "Ardakani_et+al_2016_a",
            "entry": "Arash Ardakani, Carlo Condo, and Warren J Gross. Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks. 5th International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1611.01427.",
            "url": "https://arxiv.org/abs/1611.01427",
            "arxiv_url": "https://arxiv.org/pdf/1611.01427"
        },
        {
            "id": "Bengio_et+al_1994_a",
            "entry": "Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. Trans. Neur. Netw., 5(2):157\u2013166, March 1994. ISSN 1045-9227. doi: 10.1109/72.279181. URL http://dx.doi.org/10.1109/72.279181.",
            "crossref": "https://dx.doi.org/10.1109/72.279181",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/72.279181"
        },
        {
            "id": "Bengio_2009_a",
            "entry": "Yoshua Bengio. Learning Deep Architectures for AI. Found. Trends Mach. Learn., 2(1):1\u2013127, January 2009. ISSN 1935-8237. doi: 10.1561/2200000006. URL http://dx.doi.org/10.1561/2200000006.",
            "crossref": "https://dx.doi.org/10.1561/2200000006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1561/2200000006"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, et al. Dadiannao: A machine-learning supercomputer. In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 609\u2013622. IEEE Computer Society, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yunji%20Luo%2C%20Tao%20Liu%2C%20Shaoli%20Zhang%2C%20Shijin%20Dadiannao%3A%20A%20machine-learning%20supercomputer%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yunji%20Luo%2C%20Tao%20Liu%2C%20Shaoli%20Zhang%2C%20Shijin%20Dadiannao%3A%20A%20machine-learning%20supercomputer%202014"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merri\u00ebnboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724\u20131734, Doha, Qatar, October 2014a. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D14-1179.",
            "url": "http://www.aclweb.org/anthology/D14-1179",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014-10"
        },
        {
            "id": "Cho_et+al_2014_b",
            "entry": "Kyunghyun Cho, Bart van Merri\u00ebnboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724\u20131734, Doha, Qatar, October 2014b. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D14-1179.",
            "url": "http://www.aclweb.org/anthology/D14-1179",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014-10"
        },
        {
            "id": "Cooijmans_et+al_2016_a",
            "entry": "Tim Cooijmans, Nicolas Ballas, C\u00e9sar Laurent, and Aaron C. Courville. Recurrent batch normalization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.",
            "url": "http://arxiv.org/abs/1603.09025",
            "arxiv_url": "https://arxiv.org/pdf/1603.09025"
        },
        {
            "id": "Courbariaux_2016_a",
            "entry": "Matthieu Courbariaux and Yoshua Bengio. BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. CoRR, abs/1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "Courbariaux_et+al_2015_a",
            "entry": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. CoRR, abs/1511.00363, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.00363"
        },
        {
            "id": "Glorot_0000_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks"
        },
        {
            "id": "International_2010_a",
            "entry": "International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR. URL http://proceedings.mlr.press/v9/glorot10a.html.",
            "url": "http://proceedings.mlr.press/v9/glorot10a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20volume%209%20of%20Proceedings%20of%20Machine%20Learning%20Research%20pp%20249256%20Chia%20Laguna%20Resort%20Sardinia%20Italy%201315%20May%202010%20PMLR%20URL%20httpproceedingsmlrpressv9glorot10ahtml"
        },
        {
            "id": "Graves_2013_a",
            "entry": "Alex Graves. Generating Sequences With Recurrent Neural Networks. CoRR, abs/1308.0850, 2013. URL http://dblp.uni-trier.de/db/journals/corr/corr1308.html# Graves13.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1308.html#",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Y. Guo, A. Yao, H. Zhao, and Y. Chen. Network sketching: Exploiting binary structure in deep cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4040\u20134048, July 2017. doi: 10.1109/CVPR.2017.430.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2017.430",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2017.430"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both Weights and Connections for Efficient Neural Networks. CoRR, abs/1506.02626, 2015. URL http://arxiv.org/abs/1506.02626.",
            "url": "http://arxiv.org/abs/1506.02626",
            "arxiv_url": "https://arxiv.org/pdf/1506.02626"
        },
        {
            "id": "Hermann_et+al_2015_a",
            "entry": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 1693\u20131701. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf.",
            "url": "http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hermann%2C%20Karl%20Moritz%20Kocisky%2C%20Tomas%20Grefenstette%2C%20Edward%20Espeholt%2C%20Lasse%20Teaching%20machines%20to%20read%20and%20comprehend%202015"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hou_2018_a",
            "entry": "Lu Hou and James T. Kwok. Loss-aware weight quantization of deep networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkrSv0lA-.",
            "url": "https://openreview.net/forum?id=BkrSv0lA-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hou%2C%20Lu%20Kwok%2C%20James%20T.%20Loss-aware%20weight%20quantization%20of%20deep%20networks%202018"
        },
        {
            "id": "Hou_et+al_2016_a",
            "entry": "Lu Hou, Quanming Yao, and James T. Kwok. Loss-aware binarization of deep networks. CoRR, abs/1611.01600, 2016. URL http://arxiv.org/abs/1611.01600.",
            "url": "http://arxiv.org/abs/1611.01600",
            "arxiv_url": "https://arxiv.org/pdf/1611.01600"
        },
        {
            "id": "Hubara_et+al_2016_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 4107\u20134115. Curran Associates, Inc., 2016a. URL http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf.",
            "url": "http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Itay%20Hubara%20Matthieu%20Courbariaux%20Daniel%20Soudry%20Ran%20ElYaniv%20and%20Yoshua%20Bengio%20Binarized%20neural%20networks%20In%20D%20D%20Lee%20M%20Sugiyama%20U%20V%20Luxburg%20I%20Guyon%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2029%20pp%2041074115%20Curran%20Associates%20Inc%202016a%20URL%20httppapersnipsccpaper6573binarizedneuralnetworkspdf"
        },
        {
            "id": "Hubara_et+al_1609_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. arXiv eprints, abs/1609.07061, September 2016b. URL https://arxiv.org/abs/1609.07061.",
            "url": "https://arxiv.org/abs/1609.07061",
            "arxiv_url": "https://arxiv.org/pdf/1609.07061"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch Normalization: accelerating Deep Network Training by Reducing Internal Covariate Shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167.",
            "url": "http://arxiv.org/abs/1502.03167",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Jaderberg_et+al_2014_a",
            "entry": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014"
        },
        {
            "id": "Annual_2017_a",
            "entry": "Annual International Symposium on Computer Architecture, ISCA \u201917, pp. 1\u201312, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4892-8. doi: 10.1145/3079856.3080246. URL http://doi.acm.org/10.1145/3079856.3080246.",
            "crossref": "https://dx.doi.org/10.1145/3079856.3080246",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3079856.3080246"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Fei-Fei Li. Visualizing and understanding recurrent networks. CoRR, abs/1506.02078, 2015. URL http://dblp.uni-trier.de/db/journals/corr/corr1506.html#KarpathyJL15.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1506.html#KarpathyJL15",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Kim_2016_a",
            "entry": "Minje Kim and Paris Smaragdis. Bitwise Neural Networks. CoRR, abs/1601.06071, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06071"
        },
        {
            "id": "Laurent_et+al_2016_a",
            "entry": "C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio. Batch normalized recurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2657\u20132661, March 2016. doi: 10.1109/ICASSP.2016.7472159.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2016.7472159",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2016.7472159"
        },
        {
            "id": "Le_et+al_2015_a",
            "entry": "Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "Lebedev_et+al_2014_a",
            "entry": "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. CoRR, abs/1412.6553, 2014. URL http://arxiv.org/abs/1412.6553.",
            "url": "http://arxiv.org/abs/1412.6553",
            "arxiv_url": "https://arxiv.org/pdf/1412.6553"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pp. 2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Yann Lecun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 5 2015. ISSN 0028-0836. doi: 10.1038/nature14539.",
            "crossref": "https://dx.doi.org/10.1038/nature14539",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14539"
        },
        {
            "id": "Li_2016_a",
            "entry": "Fengfu Li and Bin Liu. Ternary weight networks. CoRR, abs/1605.04711, 2016. URL http://arxiv.org/abs/1605.04711.",
            "url": "http://arxiv.org/abs/1605.04711",
            "arxiv_url": "https://arxiv.org/pdf/1605.04711"
        },
        {
            "id": "Lin_et+al_2015_a",
            "entry": "Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural Networks with Few Multiplications. CoRR, abs/1510.03009, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.03009"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Baoyuan Liu, Min Wang, H. Foroosh, M. Tappen, and M. Penksy. Sparse convolutional neural networks. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 806\u2013814, June 2015. doi: 10.1109/CVPR.2015.7298681.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2015.7298681",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2015.7298681"
        },
        {
            "id": "Marcus_et+al_1993_a",
            "entry": "Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Comput. Linguist., 19(2):313\u2013330, June 1993. ISSN 0891-2017. URL http://dl.acm.org/citation.cfm?id=972470.972475.",
            "url": "http://dl.acm.org/citation.cfm?id=972470.972475",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993-06"
        },
        {
            "id": "Tom_2010_a",
            "entry": "Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%C3%A1%C5%A1%20Mikolov%20Martin%20Karafi%C3%A1t%20Luk%C3%A1%C5%A1%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Recurrent%20neural%20network%20based%20language%20model%20In%20Eleventh%20Annual%20Conference%20of%20the%20International%20Speech%20Communication%20Association%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%C3%A1%C5%A1%20Mikolov%20Martin%20Karafi%C3%A1t%20Luk%C3%A1%C5%A1%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Recurrent%20neural%20network%20based%20language%20model%20In%20Eleventh%20Annual%20Conference%20of%20the%20International%20Speech%20Communication%20Association%202010"
        },
        {
            "id": "Mikolov_et+al_2012_a",
            "entry": "Tom\u00e1\u0161 Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky. Subword language modeling with neural networks. preprint (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf), 8, 2012.",
            "url": "http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, pp. III\u20131310\u2013III\u20131318. JMLR.org, 2013. URL http://dl.acm.org/citation.cfm?id=3042817.3043083.",
            "url": "http://dl.acm.org/citation.cfm?id=3042817.3043083",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Rastegari_et+al_2016_a",
            "entry": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016. URL http://dblp.uni-trier.de/db/journals/corr/corr1603.html#RastegariORF16.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1603.html#RastegariORF16",
            "arxiv_url": "https://arxiv.org/pdf/1603.05279"
        },
        {
            "id": "Sainath_et+al_2013_a",
            "entry": "Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Lowrank matrix factorization for deep neural network training with high-dimensional output targets. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6655\u20136659. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20Tara%20N.%20Kingsbury%2C%20Brian%20Sindhwani%2C%20Vikas%20Arisoy%2C%20Ebru%20Lowrank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20Tara%20N.%20Kingsbury%2C%20Brian%20Sindhwani%2C%20Vikas%20Arisoy%2C%20Ebru%20Lowrank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201914, pp. 3104\u20133112, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969033.2969173.",
            "url": "http://dl.acm.org/citation.cfm?id=2969033.2969173",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Cheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E. Convolutional neural networks with low-rank regularization. CoRR, abs/1511.06067, 2015. URL http://dblp.uni-trier.de/db/journals/corr/corr1511.html#TaiXWE15.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1511.html#TaiXWE15",
            "arxiv_url": "https://arxiv.org/pdf/1511.06067"
        },
        {
            "id": "Vinyals_et+al_2014_a",
            "entry": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and Tell: A Neural Image Caption Generator, 2014. URL http://arxiv.org/abs/1411.4555.cite arxiv:1411.4555.",
            "url": "http://arxiv.org/abs/1411.4555.cite",
            "arxiv_url": "https://arxiv.org/pdf/1411.4555"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pp. 2082\u20132090, USA, 2016. Curran Associates Inc. ISBN 978-15108-3881-9. URL http://dl.acm.org/citation.cfm?id=3157096.3157329.",
            "url": "http://dl.acm.org/citation.cfm?id=3157096.3157329",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating Multi-bit Quantization for Recurrent Neural Networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= S19dR9x0b.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Chen%20Yao%2C%20Jianqiang%20Lin%2C%20Zhouchen%20Ou%2C%20Wenwu%20Alternating%20Multi-bit%20Quantization%20for%202018"
        },
        {
            "id": "Yang_et+al_2015_a",
            "entry": "Z. Yang, M. Moczulski, M. Denil, N. d. Freitas, A. Smola, L. Song, and Z. Wang. Deep Fried Convnets. In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1476\u20131483, Dec 2015. doi: 10.1109/ICCV.2015.173.",
            "crossref": "https://dx.doi.org/10.1109/ICCV.2015.173",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICCV.2015.173"
        },
        {
            "id": "Zaremba_et+al_2014_a",
            "entry": "Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. CoRR, abs/1409.2329, 2014. URL http://dblp.uni-trier.de/db/journals/corr/corr1409.html#ZarembaSV14.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1409.html#ZarembaSV14",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo, T. Chen, and Y. Chen. Cambricon-x: An accelerator for sparse neural networks. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 1\u201312, Oct 2016. doi: 10.1109/MICRO.2016.7783723.",
            "crossref": "https://dx.doi.org/10.1109/MICRO.2016.7783723",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/MICRO.2016.7783723"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160, 2016. URL http://arxiv.org/abs/1606.06160.",
            "url": "http://arxiv.org/abs/1606.06160",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. CoRR, abs/1612.01064, 2016. URL http://arxiv.org/abs/1612.01064.",
            "url": "http://arxiv.org/abs/1612.01064",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        }
    ]
}
