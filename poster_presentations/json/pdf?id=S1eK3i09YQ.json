{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GRADIENT DESCENT PROVABLY OPTIMIZES OVER-PARAMETERIZED NEURAL NETWORKS",
        "author": "Simon S. Du, Machine Learning Department Carnegie Mellon University ssdu@cs.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1eK3i09YQ"
        },
        "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        }
    ],
    "abbreviations": {
        "ReLU": "rectified linear unit",
        "GD": "gradient descent"
    },
    "highlights": [
        "Neural networks trained by first order methods have achieved a remarkable impact on many applications, but their theoretical properties are still mysteries",
        "We rigorously prove that as long as no two inputs are parallel and m is large enough, with randomly initialized a and W(0), gradient descent achieves zero training loss at a linear convergence rate, i.e., it finds a solution W(K) with L(W(K)) \u2264 in K = O) iterations.4",
        "<a class=\"ref-link\" id=\"cJacot_et+al_2018_a\" href=\"#rJacot_et+al_2018_a\">Jacot et al (2018</a>) established an asymptotic result showing for the multilayer fully-connected neural network with a smooth activation function, if every layer\u2019s weight matrix is infinitely wide, for finite training time, the convergence of gradient descent can be characterized by a kernel",
        "Our paper focuses on the two-layer neural network with rectified linear unit activation function and we are able to prove the Gram matrix is stable for infinite training time",
        "In this paper we show with over-parameterization, gradient descent provable converges to the global minimum of the empirical loss at a linear convergence rate",
        "We believe our approach can be generalized to deep neural networks"
    ],
    "key_statements": [
        "Neural networks trained by first order methods have achieved a remarkable impact on many applications, but their theoretical properties are still mysteries",
        "We rigorously prove that as long as no two inputs are parallel and m is large enough, with randomly initialized a and W(0), gradient descent achieves zero training loss at a linear convergence rate, i.e., it finds a solution W(K) with L(W(K)) \u2264 in K = O) iterations.4",
        "We can use this property to show most of the patterns do not change. Combining these insights we prove the first global quantitative convergence result of gradient descent on rectified linear unit activated neural networks for the empirical risk minimization problem",
        "<a class=\"ref-link\" id=\"cJacot_et+al_2018_a\" href=\"#rJacot_et+al_2018_a\">Jacot et al (2018</a>) established an asymptotic result showing for the multilayer fully-connected neural network with a smooth activation function, if every layer\u2019s weight matrix is infinitely wide, for finite training time, the convergence of gradient descent can be characterized by a kernel",
        "Our paper focuses on the two-layer neural network with rectified linear unit activation function and we are able to prove the Gram matrix is stable for infinite training time",
        "In this paper we show with over-parameterization, gradient descent provable converges to the global minimum of the empirical loss at a linear convergence rate",
        "We believe our approach can be generalized to deep neural networks"
    ],
    "summary": [
        "Neural networks trained by first order methods have achieved a remarkable impact on many applications, but their theoretical properties are still mysteries.",
        "These results often make strong assumptions on the labels and input distributions or do not imply why randomly initialized first order method can achieve zero training loss.",
        "We rigorously prove that as long as no two inputs are parallel and m is large enough, with randomly initialized a and W(0), gradient descent achieves zero training loss at a linear convergence rate, i.e., it finds a solution W(K) with L(W(K)) \u2264 in K = O) iterations.4 our theoretical result not only shows the global convergence but gives a quantitative convergence rate in terms of the desired accuracy.",
        "Combining these insights we prove the first global quantitative convergence result of gradient descent on ReLU activated neural networks for the empirical risk minimization problem.",
        "<a class=\"ref-link\" id=\"cJacot_et+al_2018_a\" href=\"#rJacot_et+al_2018_a\">Jacot et al (2018</a>) established an asymptotic result showing for the multilayer fully-connected neural network with a smooth activation function, if every layer\u2019s weight matrix is infinitely wide, for finite training time, the convergence of gradient descent can be characterized by a kernel.",
        "Our paper focuses on the two-layer neural network with ReLU activation function and we are able to prove the Gram matrix is stable for infinite training time.",
        "The most related paper is by <a class=\"ref-link\" id=\"cLi_2018_a\" href=\"#rLi_2018_a\">Li and Liang (2018</a>) who observed that when training a two-layer full connected neural network, most of the patterns (I wr xi \u2265 0 ) do not change over iterations, which we use to show the stability of the Gram matrix.",
        "The following theorem shows using gradient flow to jointly train both layers, we can still enjoy linear convergence rate towards zero loss.",
        "We show randomly initialized gradient descent with a constant positive step size converges to the global minimum at a linear rate.",
        "Theorem 4.1 shows even though the objective function is non-smooth and non-convex, gradient descent with a constant step size still enjoys a linear convergence rate.",
        "In this paper we show with over-parameterization, gradient descent provable converges to the global minimum of the empirical loss at a linear convergence rate.",
        "The key proof idea is to show the over-parameterization makes Gram matrix remain positive definite for all iterations, which in turn guarantees the linear convergence.",
        "Has a lower bounded least eigenvalue for all t, similar to Section 3, gradient flow converges to zero training loss at a linear convergence rate.",
        "Has a lower bounded least eigenvalue, gradient flow converges to zero training loss at a linear convergence rate.",
        "If we use the quadratic loss, we can compute dW(h)(t)"
    ],
    "headline": "For an m hidden node shallow neural network with rectified linear unit activation and n training data, we show as long as m is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function",
    "reference_links": [
        {
            "id": "Andoni_et+al_2014_a",
            "entry": "Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pages 1908\u20131916, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andoni%2C%20Alexandr%20Panigrahy%2C%20Rina%20Valiant%2C%20Gregory%20Zhang%2C%20Li%20Learning%20polynomials%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andoni%2C%20Alexandr%20Panigrahy%2C%20Rina%20Valiant%2C%20Gregory%20Zhang%2C%20Li%20Learning%20polynomials%20with%20neural%20networks%202014"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06509"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with gaussian inputs. In International Conference on Machine Learning, pages 605\u2013614, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20ConvNet%20with%20gaussian%20inputs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20ConvNet%20with%20gaussian%20inputs%202017"
        },
        {
            "id": "Chizat_2018_a",
            "entry": "Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09545"
        },
        {
            "id": "Daniely_2017_a",
            "entry": "Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Information Processing Systems, pages 2422\u20132430, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniely%2C%20Amit%20SGD%20learns%20the%20conjugate%20kernel%20class%20of%20the%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniely%2C%20Amit%20SGD%20learns%20the%20conjugate%20kernel%20class%20of%20the%20network%202017"
        },
        {
            "id": "Davis_et+al_2018_a",
            "entry": "Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method converges on tame functions. arXiv preprint arXiv:1804.07795, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07795"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. Proceedings of the 35th International Conference on Machine Learning, pages 1329\u20131338, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067\u20131077, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Jin%2C%20Chi%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20Gradient%20descent%20can%20take%20exponential%20time%20to%20escape%20saddle%20points%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Jin%2C%20Chi%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20Gradient%20descent%20can%20take%20exponential%20time%20to%20escape%20saddle%20points%202017"
        },
        {
            "id": "Du_et+al_2017_b",
            "entry": "Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06129"
        },
        {
            "id": "Du_et+al_0000_a",
            "entry": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00900"
        },
        {
            "id": "Du_et+al_2018_b",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. Proceedings of the 35th International Conference on Machine Learning, pages 1339\u20131348, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Poczos%2C%20Barnabas%20Gradient%20descent%20learns%20one-hidden-layer%20cnn%3A%20Don%E2%80%99t%20be%20afraid%20of%20spurious%20local%20minima%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Poczos%2C%20Barnabas%20Gradient%20descent%20learns%20one-hidden-layer%20cnn%3A%20Don%E2%80%99t%20be%20afraid%20of%20spurious%20local%20minima%202018"
        },
        {
            "id": "Freeman_2016_a",
            "entry": "C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01540"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2212 online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%88%92%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%88%92%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Haeffele_2015_a",
            "entry": "Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07540"
        },
        {
            "id": "Hardt_2016_a",
            "entry": "Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "Jacot_et+al_2018_a",
            "entry": "Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07572"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724\u20131732, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Li_2018_a",
            "entry": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.01204"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pages 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "Mei_et+al_0000_a",
            "entry": "Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. Proceedings of the National Academy of Sciences.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layers%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layers%20neural%20networks"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pages 2603\u20132612, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In International Conference on Machine Learning, pages 3727\u20133736, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20cnns%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20cnns%202018"
        },
        {
            "id": "Safran_2016_a",
            "entry": "Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pages 774\u2013782, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016"
        },
        {
            "id": "Safran_2018_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In International Conference on Machine Learning, pages 4433\u20134441, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20ReLU%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20ReLU%20neural%20networks%202018"
        },
        {
            "id": "Soltanolkotabi_2007_a",
            "entry": "Mahdi Soltanolkotabi. Learning ReLus via gradient descent. In Advances in Neural Information Processing Systems, pages 2007\u20132017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Learning%20ReLus%20via%20gradient%20descent%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Learning%20ReLus%20via%20gradient%20descent%202007"
        },
        {
            "id": "Soltanolkotabi_et+al_2018_a",
            "entry": "Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Javanmard%2C%20Adel%20Lee%2C%20Jason%20D.%20Theoretical%20insights%20into%20the%20optimization%20landscape%20of%20over-parameterized%20shallow%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Javanmard%2C%20Adel%20Lee%2C%20Jason%20D.%20Theoretical%20insights%20into%20the%20optimization%20landscape%20of%20over-parameterized%20shallow%20neural%20networks%202018"
        },
        {
            "id": "Soudry_2016_a",
            "entry": "Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In International Conference on Machine Learning, pages 3404\u20133413, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20ReLU%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20ReLU%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017"
        },
        {
            "id": "Tsuchida_et+al_2017_a",
            "entry": "Russell Tsuchida, Farbod Roosta-Khorasani, and Marcus Gallagher. Invariance of weight distributions in rectified mlps. arXiv preprint arXiv:1711.09090, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09090"
        },
        {
            "id": "Venturi_et+al_2018_a",
            "entry": "Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06384"
        },
        {
            "id": "Wilson_et+al_2016_a",
            "entry": "Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02635"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial Intelligence and Statistics, pages 1216\u20131224, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Bo%20Liang%2C%20Yingyu%20Song%2C%20Le%20Diverse%20neural%20network%20learns%20true%20target%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Bo%20Liang%2C%20Yingyu%20Song%2C%20Le%20Diverse%20neural%20network%20learns%20true%20target%20functions%202017"
        },
        {
            "id": "Yun_et+al_0000_a",
            "entry": "Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. A critical view of global optimality in deep learning. arXiv preprint arXiv:1802.03487, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03487"
        },
        {
            "id": "Yun_et+al_0000_b",
            "entry": "Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Efficiently testing local optimality and escaping saddles for relu networks. arXiv preprint arXiv:1809.10858, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1809.10858"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. NIN, 8:35\u201367. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Yi Zhou and Yingbin Liang. Critical points of neural networks: Analytical forms and landscape properties. arXiv preprint arXiv:1710.11205, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "2",
            "entry": "2. Integrating the gradient and using Lemma A.2, we can bound the distance from the initialization wr(t) \u2212 wr(0) 2 \u2264 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Integrating%20the%20gradient%20and%20using%20Lemma%20A2%20we%20can%20bound%20the%20distance%20from%20the%20initialization%20wrt%20%20wr0%202"
        }
    ]
}
