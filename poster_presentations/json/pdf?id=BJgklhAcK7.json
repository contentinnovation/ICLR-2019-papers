{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
        "author": "Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero & Raia Hadsell DeepMind, London, UK {andreirusu, dushyantr, sygi, vinyals, razp, osindero, raia}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJgklhAcK7"
        },
        "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "parameter space",
            "url": "https://en.wikipedia.org/wiki/parameter_space"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "model parameter",
            "url": "https://en.wikipedia.org/wiki/model_parameter"
        }
    ],
    "abbreviations": {
        "LEO": "Latent Embedding Optimization",
        "MAML": "Model-agnostic meta-learning"
    },
    "highlights": [
        "Humans have a remarkable ability to quickly grasp new concepts from a very small number of examples or a limited amount of experience, leveraging prior knowledge and context",
        "Optimization-based meta-learning approaches (<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi & Larochelle, 2017</a>; <a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>) aim to find a single set of model parameters that can be adapted with a few steps of gradient descent to individual tasks",
        "In this work we propose a new approach, named Latent Embedding Optimization (LEO), which learns a low-dimensional latent embedding of model parameters and performs optimization-based meta-learning in this space",
        "We demonstrate that Latent Embedding Optimization achieves state-of-the-art results on both the miniImageNet and tieredImageNet datasets, and run an ablation study and further analysis to show that both conditional parameter generation and optimization in latent space are critical for the success of the method",
        "Latent Embedding Optimization incorporates aspects of model-based and optimization-based meta-learning, producing parameters that are first conditioned on the input data and adapted by gradient descent.\n1Note that we omit the task subscript i from latent code z and input data xnk for clarity",
        "We have introduced Latent Embedding Optimization (LEO), a meta-learning technique which uses a parameter generative model to capture the diverse range of parameters useful for a distribution over tasks, and demonstrated a new state-of-the-art result on the challenging 5-way 1- and 5-shot miniImageNet and tieredImageNet classification problems"
    ],
    "key_statements": [
        "Humans have a remarkable ability to quickly grasp new concepts from a very small number of examples or a limited amount of experience, leveraging prior knowledge and context",
        "Optimization-based meta-learning approaches (<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi & Larochelle, 2017</a>; <a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>) aim to find a single set of model parameters that can be adapted with a few steps of gradient descent to individual tasks",
        "In this work we propose a new approach, named Latent Embedding Optimization (LEO), which learns a low-dimensional latent embedding of model parameters and performs optimization-based meta-learning in this space",
        "We demonstrate that Latent Embedding Optimization achieves state-of-the-art results on both the miniImageNet and tieredImageNet datasets, and run an ablation study and further analysis to show that both conditional parameter generation and optimization in latent space are critical for the success of the method",
        "Latent Embedding Optimization incorporates aspects of model-based and optimization-based meta-learning, producing parameters that are first conditioned on the input data and adapted by gradient descent.\n1Note that we omit the task subscript i from latent code z and input data xnk for clarity",
        "Following the Latent Embedding Optimization adaptation procedure (Algorithm 1) we use fine-tuning3 by performing a few steps of gradient-based adaptation directly in parameter space using the few-shot set Dtr. This is similar to the adaptation procedure of Model-agnostic meta-learning, or Meta-SGD (<a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al, 2017</a>) when the learning rates are learned, with the important difference that starting points of fine-tuning are custom generated by Latent Embedding Optimization for every task instance Ti",
        "We evaluated Latent Embedding Optimization on the \u201cmulti-view\u201d feature representation used by <a class=\"ref-link\" id=\"cQiao_et+al_2017_a\" href=\"#rQiao_et+al_2017_a\">Qiao et al (2017</a>) with miniImageNet, which involves significant data augmentation compared to the approaches in Table 1",
        "We have introduced Latent Embedding Optimization (LEO), a meta-learning technique which uses a parameter generative model to capture the diverse range of parameters useful for a distribution over tasks, and demonstrated a new state-of-the-art result on the challenging 5-way 1- and 5-shot miniImageNet and tieredImageNet classification problems"
    ],
    "summary": [
        "Humans have a remarkable ability to quickly grasp new concepts from a very small number of examples or a limited amount of experience, leveraging prior knowledge and context.",
        "Optimization-based meta-learning approaches (<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi & Larochelle, 2017</a>; <a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>) aim to find a single set of model parameters that can be adapted with a few steps of gradient descent to individual tasks.",
        "The choice of architecture, composed of an encoding process, and decoding process, enables us to perform the MAML gradient-based adaptation steps in the learned, low-dimensional embedding space of the parameter generative model (Figure 1).",
        "LEO incorporates aspects of model-based and optimization-based meta-learning, producing parameters that are first conditioned on the input data and adapted by gradient descent.",
        "Coupled with the small number of training samples (5-shot), the task is challenging for 2 main reasons: (1) learning a distribution over models becomes necessary, in order to account for the uncertainty introduced by noisy labels; (2) problem instances may be likely under both modes: in some cases a sine wave may fit the data as well as a line.",
        "Following the LEO adaptation procedure (Algorithm 1) we use fine-tuning3 by performing a few steps of gradient-based adaptation directly in parameter space using the few-shot set Dtr. This is similar to the adaptation procedure of MAML, or Meta-SGD (<a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al, 2017</a>) when the learning rates are learned, with the important difference that starting points of fine-tuning are custom generated by LEO for every task instance Ti. Empirically, we find that fine-tuning applies a very small change to the parameters with only a slight improvement in performance on supervised classification tasks.",
        "The main approach, labeled as LEO in the table, uses a stochastic parameter generator for several steps of latent embedding optimization, followed by fine-tuning steps in parameter space.",
        "To qualitatively characterize the learnt embedding space, we plot codes produced by the relational encoder before and after the LEO procedure, using a 5-way 1-shot model and 1000 task instances from the validation meta-set of miniImageNet. Figure 4 shows a t-SNE projection of class conditional encoder outputs zn as well as their respective final adapted versions zn.",
        "We have introduced Latent Embedding Optimization (LEO), a meta-learning technique which uses a parameter generative model to capture the diverse range of parameters useful for a distribution over tasks, and demonstrated a new state-of-the-art result on the challenging 5-way 1- and 5-shot miniImageNet and tieredImageNet classification problems.",
        "LEO achieves this by learning a lowdimensional data-dependent latent embedding, and performing gradient-based adaptation in this space, which means that it allows for a task-specific parameter initialization and can perform adaptation more effectively."
    ],
    "headline": "We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems, pp. 4331\u20134339, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016"
        },
        {
            "id": "M_2017_a",
            "entry": "M. Bauer, M. Rojas-Carulla, J. Bart\u0142omiej Swiatkowski, B. Scholkopf, and R. E. Turner. Discriminative k-shot learning using probabilistic models. ArXiv e-prints, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Bauer%20M%20RojasCarulla%20J%20Bart%C5%82omiej%20Swiatkowski%20B%20Scholkopf%20and%20R%20E%20Turner%20Discriminative%20kshot%20learning%20using%20probabilistic%20models%20ArXiv%20eprints%20June%202017"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.",
            "url": "http://arxiv.org/abs/1406.1078",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, June 2009a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009-06"
        },
        {
            "id": "Deng_et+al_2009_b",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Fei-Fei_et+al_2006_a",
            "entry": "Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fei-Fei%2C%20Li%20Fergus%2C%20Rob%20Perona%2C%20Pietro%20One-shot%20learning%20of%20object%20categories%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fei-Fei%2C%20Li%20Fergus%2C%20Rob%20Perona%2C%20Pietro%20One-shot%20learning%20of%20object%20categories%202006"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pp. 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Finn_et+al_2018_a",
            "entry": "Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. arXiv preprint arXiv:1806.02817, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02817"
        },
        {
            "id": "Garnelo_et+al_2018_a",
            "entry": "Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1704\u20131713, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018a. PMLR. URL http://proceedings.mlr.press/v80/garnelo18a.html.",
            "url": "http://proceedings.mlr.press/v80/garnelo18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garnelo%2C%20Marta%20Rosenbaum%2C%20Dan%20Maddison%2C%20Christopher%20Ramalho%2C%20Tiago%20Conditional%20neural%20processes%202018-07"
        },
        {
            "id": "Marta_et+al_2018_a",
            "entry": "Marta Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. M. A. Eslami, and Y. Whye Teh. Neural Processes. ArXiv e-prints, July 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marta%20Garnelo%20J%20Schwarz%20D%20Rosenbaum%20F%20Viola%20D%20J%20Rezende%20S%20M%20A%20Eslami%20and%20Y%20Whye%20Teh%20Neural%20Processes%20ArXiv%20eprints%20July%202018b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marta%20Garnelo%20J%20Schwarz%20D%20Rosenbaum%20F%20Viola%20D%20J%20Rezende%20S%20M%20A%20Eslami%20and%20Y%20Whye%20Teh%20Neural%20Processes%20ArXiv%20eprints%20July%202018b"
        },
        {
            "id": "Gidaris_2018_a",
            "entry": "Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. CoRR, abs/1804.09458, 2018. URL http://arxiv.org/abs/1804.09458.",
            "url": "http://arxiv.org/abs/1804.09458",
            "arxiv_url": "https://arxiv.org/pdf/1804.09458"
        },
        {
            "id": "Grant_et+al_2018_a",
            "entry": "Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradientbased meta-learning as hierarchical bayes. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grant%2C%20Erin%20Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Darrell%2C%20Trevor%20Recasting%20gradientbased%20meta-learning%20as%20hierarchical%20bayes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grant%2C%20Erin%20Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Darrell%2C%20Trevor%20Recasting%20gradientbased%20meta-learning%20as%20hierarchical%20bayes%202018"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20Beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20Beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017"
        },
        {
            "id": "Hinton_1987_a",
            "entry": "Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177\u2013186, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geoffrey%20E%20Hinton%20and%20David%20C%20Plaut%20Using%20fast%20weights%20to%20deblur%20old%20memories%20In%20Proceedings%20of%20the%20ninth%20annual%20conference%20of%20the%20Cognitive%20Science%20Society%20pp%20177186%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geoffrey%20E%20Hinton%20and%20David%20C%20Plaut%20Using%20fast%20weights%20to%20deblur%20old%20memories%20In%20Proceedings%20of%20the%20ninth%20annual%20conference%20of%20the%20Cognitive%20Science%20Society%20pp%20177186%201987"
        },
        {
            "id": "Sepp_2001_a",
            "entry": "Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In Proceedings of the International Conference on Artificial Neural Networks, ICANN \u201901, pp. 87\u201394, London, UK, UK, 2001. Springer-Verlag. ISBN 3-540-42486-5. URL http://dl.acm.org/citation.cfm?id=646258.684281.",
            "url": "http://dl.acm.org/citation.cfm?id=646258.684281",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sepp%20Hochreiter%2C%20A.Steven%20Younger%20Conwell%2C%20Peter%20R.%20Learning%20to%20learn%20using%20gradient%20descent%202001"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian Model-Agnostic Meta-Learning. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20T.%20Yoon%2C%20J.%20Dia%2C%20O.%20Kim%2C%20S.%20Bayesian%20Model-Agnostic%20Meta-Learning.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Koch_et+al_2015_a",
            "entry": "Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "Krueger_et+al_2017_a",
            "entry": "D. Krueger, C.-W. Huang, R. Islam, R. Turner, A. Lacoste, and A. Courville. Bayesian Hypernetworks. ArXiv e-prints, October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krueger%2C%20D.%20Huang%2C%20C.-W.%20Islam%2C%20R.%20Turner%2C%20R.%20Bayesian%20Hypernetworks.%20ArXiv%20e-prints%202017-10"
        },
        {
            "id": "Lacoste_et+al_2018_a",
            "entry": "A. Lacoste, B. Oreshkin, W. Chung, T. Boquet, N. Rostamzadeh, and D. Krueger. Uncertainty in Multitask Transfer Learning. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste%2C%20A.%20Oreshkin%2C%20B.%20Chung%2C%20W.%20Boquet%2C%20T.%20Uncertainty%20in%20Multitask%20Transfer%20Learning.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Lake_et+al_2011_a",
            "entry": "Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "Lee_2018_a",
            "entry": "Y. Lee and S. Choi. Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace. ArXiv e-prints, January 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Y.%20Choi%2C%20S.%20Gradient-Based%20Meta-Learning%20with%20Learned%20Layerwise%20Metric%20and%20Subspace.%20ArXiv%20e-prints%202018-01"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. CoRR, abs/1707.09835, 2017. URL http://arxiv.org/abs/1707.09835.",
            "url": "http://arxiv.org/abs/1707.09835",
            "arxiv_url": "https://arxiv.org/pdf/1707.09835"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot learning. arXiv preprint arXiv:1805.10002, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10002"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20metalearner%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20metalearner%202018"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, February 2015. ISSN 00280836. URL http://dx.doi.org/10.1038/nature14236.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, Tong Wang, and Adam Trischler. Learning rapid-temporal adaptations. CoRR, abs/1712.09926, 2017. URL http://arxiv.org/abs/1712.09926.",
            "url": "http://arxiv.org/abs/1712.09926",
            "arxiv_url": "https://arxiv.org/pdf/1712.09926"
        },
        {
            "id": "Nichol_2018_a",
            "entry": "Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Oreshkin_et+al_2018_a",
            "entry": "B. N. Oreshkin, P. Rodriguez, and A. Lacoste. TADAM: Task dependent adaptive metric for improved few-shot learning. ArXiv e-prints, May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oreshkin%2C%20B.N.%20Rodriguez%2C%20P.%20Lacoste%2C%20A.%20TADAM%3A%20Task%20dependent%20adaptive%20metric%20for%20improved%20few-shot%20learning.%20ArXiv%20e-prints%202018-05"
        },
        {
            "id": "Qiao_et+al_2017_a",
            "entry": "Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan Yuille. Few-shot image recognition by predicting parameters from activations. arXiv preprint arXiv:1706.03466, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03466"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Ren_et+al_2018_a",
            "entry": "Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJcSzz-CZ.",
            "url": "https://openreview.net/forum?id=HJcSzz-CZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Mengye%20Ravi%2C%20Sachin%20Triantafillou%2C%20Eleni%20Snell%2C%20Jake%20Meta-learning%20for%20semi-supervised%20few-shot%20classification%202018"
        },
        {
            "id": "Russakovsky_et+al_2014_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and FeiFei Li. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.",
            "url": "http://arxiv.org/abs/1409.0575",
            "arxiv_url": "https://arxiv.org/pdf/1409.0575"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pp. 1842\u20131850, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "Jurgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universitat Munchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Evolutionary%20principles%20in%20self-referential%20learning%2C%20or%20on%20learning%20how%20to%20learn%3A%20the%20meta-meta-%201987"
        },
        {
            "id": "Schmidhuber_2015_a",
            "entry": "Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354\u2013, October 2017. URL http://dx.doi.org/10.1038/nature24270.",
            "crossref": "https://dx.doi.org/10.1038/nature24270",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature24270"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. CoRR, abs/1703.05175, 2017. URL http://arxiv.org/abs/1703.05175.",
            "url": "http://arxiv.org/abs/1703.05175",
            "arxiv_url": "https://arxiv.org/pdf/1703.05175"
        },
        {
            "id": "Sung_et+al_2017_a",
            "entry": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. CoRR, abs/1711.06025, 2017. URL http://arxiv.org/abs/1711.06025.",
            "url": "http://arxiv.org/abs/1711.06025",
            "arxiv_url": "https://arxiv.org/pdf/1711.06025"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. CoRR, abs/1409.3215, 2014. URL http://arxiv.org/abs/1409.3215.",
            "url": "http://arxiv.org/abs/1409.3215",
            "arxiv_url": "https://arxiv.org/pdf/1409.3215"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630\u20133638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "T. Wu, J. Peurifoy, I. L. Chuang, and M. Tegmark. Meta-learning autoencoders for few-shot prediction. ArXiv e-prints, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20T.%20Peurifoy%2C%20J.%20Chuang%2C%20I.L.%20M.%20Tegmark.%20Meta-learning%20autoencoders%20for%20few-shot%20prediction.%20ArXiv%20e-prints%202018-07"
        },
        {
            "id": "Yosinski_et+al_2014_a",
            "entry": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? CoRR, abs/1411.1792, 2014. URL http://arxiv.org/abs/1411.1792.",
            "url": "http://arxiv.org/abs/1411.1792",
            "arxiv_url": "https://arxiv.org/pdf/1411.1792"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        },
        {
            "id": "Zagoruyko_2016_b",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016b. URL http://arxiv.org/abs/1605.07146.",
            "url": "http://arxiv.org/abs/1605.07146",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Fengwei Zhou, Bin Wu, and Zhenguo Li. Deep meta-learning: Learning to learn in the concept space. CoRR, abs/1802.03596, 2018. URL http://arxiv.org/abs/1802.03596.",
            "url": "http://arxiv.org/abs/1802.03596",
            "arxiv_url": "https://arxiv.org/pdf/1802.03596"
        },
        {
            "id": "We_2018_a",
            "entry": "We used the experimental setup of Finn et al. (2018) for 1D 5-shot noisy regression tasks. Inputs were sampled uniformly from [\u22125, 5]. A multimodal task distribution was used. Half of the problem instances were sinusoids with amplitude and phase sampled uniformly from [0.1, 5] and [0, \u03c0] respectively. The other half were lines with slope and intercept sampled uniformly from the interval [\u22123, 3]. Gaussian noise with standard deviation 0.3 was added to regression targets.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20used%20the%20experimental%20setup%20of%20Finn%20et%20al%202018%20for%201D%205shot%20noisy%20regression%20tasks%20Inputs%20were%20sampled%20uniformly%20from%205%205%20A%20multimodal%20task%20distribution%20was%20used%20Half%20of%20the%20problem%20instances%20were%20sinusoids%20with%20amplitude%20and%20phase%20sampled%20uniformly%20from%2001%205%20and%200%20%CF%80%20respectively%20The%20other%20half%20were%20lines%20with%20slope%20and%20intercept%20sampled%20uniformly%20from%20the%20interval%203%203%20Gaussian%20noise%20with%20standard%20deviation%2003%20was%20added%20to%20regression%20targets",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20used%20the%20experimental%20setup%20of%20Finn%20et%20al%202018%20for%201D%205shot%20noisy%20regression%20tasks%20Inputs%20were%20sampled%20uniformly%20from%205%205%20A%20multimodal%20task%20distribution%20was%20used%20Half%20of%20the%20problem%20instances%20were%20sinusoids%20with%20amplitude%20and%20phase%20sampled%20uniformly%20from%2001%205%20and%200%20%CF%80%20respectively%20The%20other%20half%20were%20lines%20with%20slope%20and%20intercept%20sampled%20uniformly%20from%20the%20interval%203%203%20Gaussian%20noise%20with%20standard%20deviation%2003%20was%20added%20to%20regression%20targets"
        }
    ]
}
