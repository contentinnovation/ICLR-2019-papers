{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Feature Visualization",
        "author": "Chris Olah, Alexander Mordvintsev, Ludwig Schubert",
        "date": 2017,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1EERs09YQ",
            "doi": "10.23915/distill.00007"
        },
        "journal": "Distill",
        "volume": "2",
        "abstract": "Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns. In order to quantitatively analyze such an intriguing phenomenon, we propose a concept alignment method based on how units respond to the replicated text. We conduct analyses with different architectures on multiple datasets for classification and translation tasks and provide new insights into how deep models understand natural language."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        }
    ],
    "abbreviations": {
        "NMT": "neural machine translation",
        "NLP": "natural language processing",
        "CNNs": "convolutional neural networks",
        "DoA": "degree of alignment",
        "DEL": "Delta of Expected Loss"
    },
    "highlights": [
        "Understanding and interpreting how deep neural networks process natural language is a crucial and challenging problem",
        "We newly propose a simple but highly effective concept alignment method that can discover which natural language concepts are aligned to each unit in the representation",
        "We show that the units of deep convolutional neural networks learned in natural language processing tasks could act as a natural language concept detector",
        "We proposed a simple but highly effective concept alignment method for character-level convolutional neural networks to confirm that each unit of the hidden layers serves as detectors of natural language concepts",
        "An interesting future direction is to extend the concept coverage from natural language to more abstract forms such as sentence structure, nuance, and tone. Another direction is to quantify the properties of individual units in other models widely used in natural language processing tasks",
        "Combining our definition of concepts with the attention mechanism (e.g. <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al (2015</a>)) could be a promising direction, because it can reveal how the representations are attended by the model to capture concepts, helping us better understand the decision-making process of popular deep models"
    ],
    "key_statements": [
        "Understanding and interpreting how deep neural networks process natural language is a crucial and challenging problem",
        "Some studies including <a class=\"ref-link\" id=\"cBau_et+al_2017_a\" href=\"#rBau_et+al_2017_a\">Bau et al (2017</a>); <a class=\"ref-link\" id=\"cFong_2018_a\" href=\"#rFong_2018_a\">Fong & Vedaldi (2018</a>); <a class=\"ref-link\" id=\"cOlah_et+al_2017_a\" href=\"#rOlah_et+al_2017_a\">Olah et al (2017</a>; 2018) have researched on what information is captured by individual or multiple units in visual representations learned for image recognition tasks",
        "They discovered the emergence of certain units that selectively activate to those specific concepts. Building upon these lines of research, we consider the following question: What natural language concepts are captured by each unit in the representations learned from natural language processing tasks?",
        "We newly propose a simple but highly effective concept alignment method that can discover which natural language concepts are aligned to each unit in the representation",
        "We show that the units of deep convolutional neural networks learned in natural language processing tasks could act as a natural language concept detector",
        "Without any additional labeled data or re-training process, we can discover, for each unit of the convolutional neural networks, natural language concepts including morphemes, words and phrases that are present in the training data",
        "We focus on convolutional neural networks (CNNs), their character-level variants",
        "Some natural language patterns such as morphemes, words, phrases frequently appear in the retrieved sentences, implying that those concepts might have a large attribution to the activation value of that unit.\n3.2",
        "Figure 4: 30 concepts selected by the number of aligned units in three encoding layers of ByteNet learned on the Europarl translation dataset and VDCNN learned on AG-News. [#] symbol denotes morpheme concepts",
        "Several units in the convolutional neural networks learned on natural language processing tasks respond selectively to specific natural language concepts, rather than getting activated in an uninterpretable way",
        "To confirm that it holds for representations learned in natural language processing tasks, we divide granularity of natural language concepts to the morpheme, word and N -gram phrase (N = 2, 3, 4, 5), and observe the number of units that they are aligned in different layers",
        "We proposed a simple but highly effective concept alignment method for character-level convolutional neural networks to confirm that each unit of the hidden layers serves as detectors of natural language concepts",
        "An interesting future direction is to extend the concept coverage from natural language to more abstract forms such as sentence structure, nuance, and tone. Another direction is to quantify the properties of individual units in other models widely used in natural language processing tasks",
        "Combining our definition of concepts with the attention mechanism (e.g. <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al (2015</a>)) could be a promising direction, because it can reveal how the representations are attended by the model to capture concepts, helping us better understand the decision-making process of popular deep models"
    ],
    "summary": [
        "Understanding and interpreting how deep neural networks process natural language is a crucial and challenging problem.",
        "Building upon these lines of research, we consider the following question: What natural language concepts are captured by each unit in the representations learned from NLP tasks?",
        "We show that the units of deep CNNs learned in NLP tasks could act as a natural language concept detector.",
        "Without any additional labeled data or re-training process, we can discover, for each unit of the CNN, natural language concepts including morphemes, words and phrases that are present in the training data.",
        "We first train a CNN model for each natural language task and retrieve training sentences that highly activate specific units.",
        "Some natural language patterns such as morphemes, words, phrases frequently appear in the retrieved sentences, implying that those concepts might have a large attribution to the activation value of that unit.",
        "Figure 3 shows examples of the top K sentences and the aligned concepts that are discovered by our method, for selected units.",
        "This suggests that, we train character-level CNNs with feeding sentences as the form of discrete symbols, individual units could capture natural language concepts sharing a similar semantic or grammatical role.",
        "Figure 4: 30 concepts selected by the number of aligned units in three encoding layers of ByteNet learned on the Europarl translation dataset and VDCNN learned on AG-News.",
        "Because the number of morphemes, words, and phrases present in training corpus is usually much greater than the number of units per layer, we do not expect to always align any natural language concepts in the corpus to one of the units.",
        "Several units in the CNN learned on NLP tasks respond selectively to specific natural language concepts, rather than getting activated in an uninterpretable way.",
        "To confirm that it holds for representations learned in NLP tasks, we divide granularity of natural language concepts to the morpheme, word and N -gram phrase (N = 2, 3, 4, 5), and observe the number of units that they are aligned in different layers.",
        "Figure 7-(a) shows the correlation between the frequency of each concept in the training corpus and the number of units where each concept is aligned in the last layer of the topic classification model learned on AG News dataset.",
        "We proposed a simple but highly effective concept alignment method for character-level CNNs to confirm that each unit of the hidden layers serves as detectors of natural language concepts.",
        "Combining our definition of concepts with the attention mechanism (e.g. <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al (2015</a></a>)) could be a promising direction, because it can reveal how the representations are attended by the model to capture concepts, helping us better understand the decision-making process of popular deep models"
    ],
    "headline": "In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Agarwal%2C%20Ashish%20Barham%2C%20Paul%20Brevdo%2C%20Eugene%20TensorFlow%3A%20Large-Scale%20Machine%20Learning%20on%20Heterogeneous%20Systems%202015"
        },
        {
            "id": "Adi_et+al_2017_a",
            "entry": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adi%2C%20Yossi%20Kermany%2C%20Einat%20Belinkov%2C%20Yonatan%20Lavi%2C%20Ofer%20Fine-grained%20Analysis%20of%20Sentence%20Embeddings%20Using%20Auxiliary%20Prediction%20Tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adi%2C%20Yossi%20Kermany%2C%20Einat%20Belinkov%2C%20Yonatan%20Lavi%2C%20Ofer%20Fine-grained%20Analysis%20of%20Sentence%20Embeddings%20Using%20Auxiliary%20Prediction%20Tasks%202017"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015"
        },
        {
            "id": "Bau_et+al_2017_a",
            "entry": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network Dissection: Quantifying Interpretability of Deep Visual Representations. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20Dissection%3A%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20Dissection%3A%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representations%202017"
        },
        {
            "id": "Bau_et+al_2019_a",
            "entry": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, and Antonio Torralba. Visualizing and Understanding Generative Adversarial Networks. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Bau%20JunYan%20Zhu%20Hendrik%20Strobelt%20Bolei%20Zhou%20Joshua%20B%20Tenenbaum%20William%20T%20Freeman%20and%20Antonio%20Torralba%20Visualizing%20and%20Understanding%20Generative%20Adversarial%20Networks%20In%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=David%20Bau%20JunYan%20Zhu%20Hendrik%20Strobelt%20Bolei%20Zhou%20Joshua%20B%20Tenenbaum%20William%20T%20Freeman%20and%20Antonio%20Torralba%20Visualizing%20and%20Understanding%20Generative%20Adversarial%20Networks%20In%20ICLR%202019"
        },
        {
            "id": "Belinkov_et+al_2017_a",
            "entry": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do Neural Machine Translation Models Learn about Morphology? In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yonatan%20Belinkov%20Nadir%20Durrani%20Fahim%20Dalvi%20Hassan%20Sajjad%20and%20James%20Glass%20What%20do%20Neural%20Machine%20Translation%20Models%20Learn%20about%20Morphology%20In%20ACL%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yonatan%20Belinkov%20Nadir%20Durrani%20Fahim%20Dalvi%20Hassan%20Sajjad%20and%20James%20Glass%20What%20do%20Neural%20Machine%20Translation%20Models%20Learn%20about%20Morphology%20In%20ACL%202017"
        },
        {
            "id": "Bojanowski_et+al_2017_a",
            "entry": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching Word Vectors with Subword Information. TACL, 5:135\u2013146, 2017. ISSN 2307-387X.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20Word%20Vectors%20with%20Subword%20Information%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20Word%20Vectors%20with%20Subword%20Information%202017"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder%E2%80%93%20Decoder%20for%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder%E2%80%93%20Decoder%20for%202014"
        },
        {
            "id": "Conneau_et+al_2017_a",
            "entry": "Alexis Conneau, Holger Schwenk, Lo\u0131c Barrault, and Yann Lecun. Very Deep Convolutional Networks for Text Classification. In EACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Lecun%2C%20Yann%20Very%20Deep%20Convolutional%20Networks%20for%20Text%20Classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Lecun%2C%20Yann%20Very%20Deep%20Convolutional%20Networks%20for%20Text%20Classification%202017"
        },
        {
            "id": "Conneau_et+al_2018_a",
            "entry": "Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u0131c Barrault, and Marco Baroni. What You Can Cram into a Single \\$&!#\u2217 Vector: Probing Sentence Embeddings for Linguistic Properties. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kruszewski%2C%20German%20Lample%2C%20Guillaume%20Barrault%2C%20Lo%C4%B1c%20What%20You%20Can%20Cram%20into%20a%20Single%20%5C%24%26%21%23%E2%88%97%20Vector%3A%20Probing%20Sentence%20Embeddings%20for%20Linguistic%20Properties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kruszewski%2C%20German%20Lample%2C%20Guillaume%20Barrault%2C%20Lo%C4%B1c%20What%20You%20Can%20Cram%20into%20a%20Single%20%5C%24%26%21%23%E2%88%97%20Vector%3A%20Probing%20Sentence%20Embeddings%20for%20Linguistic%20Properties%202018"
        },
        {
            "id": "Erhan_et+al_2009_a",
            "entry": "Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing Higher-layer Features of a Deep Network. University of Montreal, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Visualizing%20Higher-layer%20Features%20of%20a%20Deep%20Network%202009"
        },
        {
            "id": "Fong_2018_a",
            "entry": "Ruth Fong and Andrea Vedaldi. Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fong%2C%20Ruth%20Vedaldi%2C%20Andrea%20Net2Vec%3A%20Quantifying%20and%20Explaining%20how%20Concepts%20are%20Encoded%20by%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fong%2C%20Ruth%20Vedaldi%2C%20Andrea%20Net2Vec%3A%20Quantifying%20and%20Explaining%20how%20Concepts%20are%20Encoded%20by%202018"
        },
        {
            "id": "Kalchbrenner_et+al_2016_a",
            "entry": "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural Machine Translation in Linear Time. arXiv preprint arXiv:1610.10099, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.10099"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and Understanding Recurrent Networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Kim_et+al_2016_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-Aware Neural Language Models. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoon%20Kim%20Yacine%20Jernite%20David%20Sontag%20and%20Alexander%20M%20Rush%20CharacterAware%20Neural%20Language%20Models%20In%20AAAI%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoon%20Kim%20Yacine%20Jernite%20David%20Sontag%20and%20Alexander%20M%20Rush%20CharacterAware%20Neural%20Language%20Models%20In%20AAAI%202016"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kitaev_2018_a",
            "entry": "Nikita Kitaev and Dan Klein. Constituency Parsing with a Self-Attentive Encoder. ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20Parsing%20with%20a%20Self-Attentive%20Encoder%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20Parsing%20with%20a%20Self-Attentive%20Encoder%202018"
        },
        {
            "id": "Koehn_2005_a",
            "entry": "Philipp Koehn. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT summit, volume 5, pp. 79\u201386, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koehn%2C%20Philipp%20Europarl%3A%20A%20Parallel%20Corpus%20for%20Statistical%20Machine%20Translation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koehn%2C%20Philipp%20Europarl%3A%20A%20Parallel%20Corpus%20for%20Statistical%20Machine%20Translation%202005"
        },
        {
            "id": "Lehmann_et+al_2015_a",
            "entry": "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Soren Auer, et al. DBpedia\u2013a LargeScale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web, 6(2):167\u2013195, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20DBpedia%E2%80%93a%20LargeScale%2C%20Multilingual%20Knowledge%20Base%20Extracted%20from%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20DBpedia%E2%80%93a%20LargeScale%2C%20Multilingual%20Knowledge%20Base%20Extracted%20from%202015"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the Importance of Single Directions for Generalization. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20Importance%20of%20Single%20Directions%20for%20Generalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20Importance%20of%20Single%20Directions%20for%20Generalization%202018"
        },
        {
            "id": "Mou_et+al_2016_a",
            "entry": "Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How Transferable are Neural Networks in NLP Applications? In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lili%20Mou%20Zhao%20Meng%20Rui%20Yan%20Ge%20Li%20Yan%20Xu%20Lu%20Zhang%20and%20Zhi%20Jin%20How%20Transferable%20are%20Neural%20Networks%20in%20NLP%20Applications%20In%20EMNLP%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lili%20Mou%20Zhao%20Meng%20Rui%20Yan%20Ge%20Li%20Yan%20Xu%20Lu%20Zhang%20and%20Zhi%20Jin%20How%20Transferable%20are%20Neural%20Networks%20in%20NLP%20Applications%20In%20EMNLP%202016"
        },
        {
            "id": "Hierarchical_2011_a",
            "entry": "Daniel Mullner. Modern Hierarchical, Agglomerative Clustering Algorithms. arXiv preprint arXiv:1109.2378, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1109.2378"
        },
        {
            "id": "Olah_et+al_2017_a",
            "entry": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature Visualization. Distill, 2017. doi: 10.23915/distill.00007. https://distill.pub/2017/feature-visualization.",
            "crossref": "https://dx.doi.org/10.23915/distill.00007"
        },
        {
            "id": "Olah_et+al_2018_a",
            "entry": "Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The Building Blocks of Interpretability. Distill, 2018. doi: 10.23915/ distill.00010. https://distill.pub/2018/building-blocks.",
            "crossref": "https://dx.doi.org/10.23915/distill.00010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.23915/distill.00010"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20Vectors%20for%20Word%20Representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20Vectors%20for%20Word%20Representation%202014"
        },
        {
            "id": "Qian_et+al_2016_a",
            "entry": "Peng Qian, Xipeng Qiu, and Xuanjing Huang. Analyzing Linguistic Knowledge in Sequential Model of Sentence. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qian%2C%20Peng%20Qiu%2C%20Xipeng%20Huang%2C%20Xuanjing%20Analyzing%20Linguistic%20Knowledge%20in%20Sequential%20Model%20of%20Sentence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20Peng%20Qiu%2C%20Xipeng%20Huang%2C%20Xuanjing%20Analyzing%20Linguistic%20Knowledge%20in%20Sequential%20Model%20of%20Sentence%202016"
        },
        {
            "id": "Radford_et+al_2017_a",
            "entry": "Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to Generate Reviews and Discovering Sentiment. arXiv preprint arXiv:1704.01444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01444"
        },
        {
            "id": "Role_2011_a",
            "entry": "Franois Role and Mohamed Nadif. Handling the Impact of Low Frequency Events on Co-occurrence based Measures of Word Similarity. In KDIR, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Role%2C%20Franois%20Nadif%2C%20Mohamed%20Handling%20the%20Impact%20of%20Low%20Frequency%20Events%20on%20Co-occurrence%20based%20Measures%20of%20Word%20Similarity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Role%2C%20Franois%20Nadif%2C%20Mohamed%20Handling%20the%20Impact%20of%20Low%20Frequency%20Events%20on%20Co-occurrence%20based%20Measures%20of%20Word%20Similarity%202011"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Xing Shi, Kevin Knight, and Deniz Yuret. Why Neural Translations are the Right Length. In EMNLP, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%20Shi%20Kevin%20Knight%20and%20Deniz%20Yuret%20Why%20Neural%20Translations%20are%20the%20Right%20Length%20In%20EMNLP%202016a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%20Shi%20Kevin%20Knight%20and%20Deniz%20Yuret%20Why%20Neural%20Translations%20are%20the%20Right%20Length%20In%20EMNLP%202016a"
        },
        {
            "id": "Shi_et+al_2016_b",
            "entry": "Xing Shi, Inkit Padhi, and Kevin Knight. Does String-Based Neural MT Learn Source Syntax? In EMNLP, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%20Shi%20Inkit%20Padhi%20and%20Kevin%20Knight%20Does%20StringBased%20Neural%20MT%20Learn%20Source%20Syntax%20In%20EMNLP%202016b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%20Shi%20Inkit%20Padhi%20and%20Kevin%20Knight%20Does%20StringBased%20Neural%20MT%20Learn%20Source%20Syntax%20In%20EMNLP%202016b"
        },
        {
            "id": "Simonyan_et+al_2013_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside Convolutional Networks: Visualising Image Classification Models and Saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "Speer_et+al_2017_a",
            "entry": "Robert Speer, Joshua Chin, and Catherine Havasi. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Speer%2C%20Robert%20Chin%2C%20Joshua%20Havasi%2C%20Catherine%20ConceptNet%205.5%3A%20An%20Open%20Multilingual%20Graph%20of%20General%20Knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Speer%2C%20Robert%20Chin%2C%20Joshua%20Havasi%2C%20Catherine%20ConceptNet%205.5%3A%20An%20Open%20Multilingual%20Graph%20of%20General%20Knowledge%202017"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural Networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%202014"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng, and Shiyue Zhang. Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition. In ICASSP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Zhiyuan%20Shi%2C%20Ying%20Wang%2C%20Dong%20Feng%2C%20Yang%20Memory%20Visualization%20for%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Zhiyuan%20Shi%2C%20Ying%20Wang%2C%20Dong%20Feng%2C%20Yang%20Memory%20Visualization%20for%202017"
        },
        {
            "id": "Tiedemann_2012_a",
            "entry": "Jrg Tiedemann. Parallel Data, Tools and Interfaces in OPUS. In LREC. ELRA, 2012. ISBN 978-29517408-7-7.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tiedemann%2C%20Jrg%20Parallel%20Data%2C%20Tools%20and%20Interfaces%20in%20OPUS.%20In%20LREC%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tiedemann%2C%20Jrg%20Parallel%20Data%2C%20Tools%20and%20Interfaces%20in%20OPUS.%20In%20LREC%202012"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez, and \u0141 ukasz Kaiser. Attention is All You Need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Llion%20Jones%20Jakob%20Uszkoreit%20Aidan%20N%20Gomez%20and%20%C5%81%20ukasz%20Kaiser%20Attention%20is%20All%20You%20Need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Llion%20Jones%20Jakob%20Uszkoreit%20Aidan%20N%20Gomez%20and%20%C5%81%20ukasz%20Kaiser%20Attention%20is%20All%20You%20Need%20In%20NIPS%202017"
        },
        {
            "id": "Virpioja_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Sami Virpioja, Peter Smit, Stig-Arne Gronroos, and Mikko Kurimo. Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline. In Aalto University publication series. Department of Signal Processing and Acoustics, Aalto University, 2013. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text Classification. In NIPS, 2015. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object Detectors Emerge in Deep Scene CNNs. In ICLR, 2015. Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting Deep Visual Representations via Network Dissection. IEEE TPAMI, 2018a. Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable Basis Decomposition for Visual Explanation. In ECCV, 2018b. Xunjie Zhu, Tingfeng Li, and Gerard Melo. Exploring Semantic Properties of Sentence Embeddings. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Virpioja%2C%20Sami%20Smit%2C%20Peter%20Gronroos%2C%20Stig-Arne%20Kurimo%2C%20Mikko%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Virpioja%2C%20Sami%20Smit%2C%20Peter%20Gronroos%2C%20Stig-Arne%20Kurimo%2C%20Mikko%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Point-wise_0000_a",
            "entry": "Point-wise Mutual Information (PMI) is a measure of association used in information theory and statistics. The PMI of a pair of samples x and y sampled from random variables X and Y quantifies the discrepancy between the probability of their coincidence as follows: p(x, y)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pointwise%20Mutual%20Information%20PMI%20is%20a%20measure%20of%20association%20used%20in%20information%20theory%20and%20statistics%20The%20PMI%20of%20a%20pair%20of%20samples%20x%20and%20y%20sampled%20from%20random%20variables%20X%20and%20Y%20quantifies%20the%20discrepancy%20between%20the%20probability%20of%20their%20coincidence%20as%20follows%20px%20y"
        },
        {
            "id": "However_2011_a",
            "entry": "However, this metric has a bias of always preferring lengthy concepts even in earlier layers, which is not possible considering the receptive field of the convolution. Our intuition for this bias is consistent with Role & Nadif (2011), where it is a well-known problem with PMI, which is its tendency to give very high association scores to pairs involving low-frequency ones, as the denominator is small in such cases. If certain concept cn in top K sentences is very lengthy, then its frequency in the corpus p(cn) would get very small, and pmi(u, cn) would be large with regardless of correlation between u and cn.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=However%20this%20metric%20has%20a%20bias%20of%20always%20preferring%20lengthy%20concepts%20even%20in%20earlier%20layers%20which%20is%20not%20possible%20considering%20the%20receptive%20field%20of%20the%20convolution%20Our%20intuition%20for%20this%20bias%20is%20consistent%20with%20Role%20%20Nadif%202011%20where%20it%20is%20a%20wellknown%20problem%20with%20PMI%20which%20is%20its%20tendency%20to%20give%20very%20high%20association%20scores%20to%20pairs%20involving%20lowfrequency%20ones%20as%20the%20denominator%20is%20small%20in%20such%20cases%20If%20certain%20concept%20cn%20in%20top%20K%20sentences%20is%20very%20lengthy%20then%20its%20frequency%20in%20the%20corpus%20pcn%20would%20get%20very%20small%20and%20pmiu%20cn%20would%20be%20large%20with%20regardless%20of%20correlation%20between%20u%20and%20cn"
        }
    ]
}
