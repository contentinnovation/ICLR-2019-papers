{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Learning to Skim Text",
        "author": "Adams Wei Yu, Hongrae Lee, Quoc Le",
        "date": 2017,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1xf9jAqFQ",
            "doi": "10.18653/v1/p17-1172"
        },
        "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "abstract": "Recurrent neural networks (RNNs) can model natural language by sequentially \u201dreading\u201d input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as \u201dneural speed reading\u201d, either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word. A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text."
    },
    "keywords": [
        {
            "term": "text classification",
            "url": "https://en.wikipedia.org/wiki/text_classification"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "speed reading",
            "url": "https://en.wikipedia.org/wiki/speed_reading"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent neural networks",
        "FLOP": "floating point operations",
        "GRU": "Gated Recurrent Unit",
        "LSTM": "Long Short Term Memory",
        "NLP": "natural language processing",
        "FLOPs": "floating point operations",
        "FLOP-r": "FLOP reduction"
    },
    "highlights": [
        "Recurrent neural networks (RNNs) are a popular model for processing sequential data",
        "Hard attention mechanisms have been considered in many areas, ranging from computer vision (<a class=\"ref-link\" id=\"cMnih_et+al_2014_a\" href=\"#rMnih_et+al_2014_a\">Mnih et al, 2014</a>; <a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\"><a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\">Campos et al, 2018</a></a>) where the model learns what parts of the image it should focus on, to natural language processing (NLP), such as text classification and question answering (<a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a>; <a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\"><a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\">Campos et al, 2018</a></a>; <a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a>), where the model learns which part of a document it can ignore",
        "Our approach obtains similar accuracies compared to vanilla Long Short Term Memory, while in some cases reducing the reading percentage to below 20% (IMDB and DBPedia), and with the worst speed reading resulting in a reading percentage of 68.6% (Yelp)",
        "If floating point operations reductions are not reported in the original paper, we report the speed increase, which should be considered a lower bound on the floating point operations reduction",
        "We presented Structural-Jump-Long Short Term Memory, a recurrent neural network for speed reading",
        "StructuralJump-Long Short Term Memory is inspired by human speed reading, and can skip irrelevant words in important sections, while jumping past unimportant parts of a text"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) are a popular model for processing sequential data",
        "Hard attention mechanisms have been considered in many areas, ranging from computer vision (<a class=\"ref-link\" id=\"cMnih_et+al_2014_a\" href=\"#rMnih_et+al_2014_a\">Mnih et al, 2014</a>; <a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\"><a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\">Campos et al, 2018</a></a>) where the model learns what parts of the image it should focus on, to natural language processing (NLP), such as text classification and question answering (<a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a>; <a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\"><a class=\"ref-link\" id=\"cCampos_et+al_2018_a\" href=\"#rCampos_et+al_2018_a\">Campos et al, 2018</a></a>; <a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a>), where the model learns which part of a document it can ignore",
        "Our approach obtains similar accuracies compared to vanilla Long Short Term Memory, while in some cases reducing the reading percentage to below 20% (IMDB and DBPedia), and with the worst speed reading resulting in a reading percentage of 68.6% (Yelp)",
        "The speed reading behaviour varies across the datasets with no skipping on CBT-CN, CBT-NE, and Yelp, no jumping on Rotten Tomatoes, and a mix of skipping and jumping on the remaining datasets",
        "If floating point operations reductions are not reported in the original paper, we report the speed increase, which should be considered a lower bound on the floating point operations reduction",
        "To allow a consistent comparison of the effect of each speed reading model, we report the accuracy difference between each paper\u2019s reported full read and speed read accuracies",
        "We presented Structural-Jump-Long Short Term Memory, a recurrent neural network for speed reading",
        "StructuralJump-Long Short Term Memory is inspired by human speed reading, and can skip irrelevant words in important sections, while jumping past unimportant parts of a text"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) are a popular model for processing sequential data.",
        "An extensive experimental evaluation against all state-of-the-art speed reading models (<a class=\"ref-link\" id=\"cSeo_et+al_2018_a\" href=\"#rSeo_et+al_2018_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2018_a\" href=\"#rSeo_et+al_2018_a\">Seo et al, 2018</a></a>; <a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\"><a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a></a>; 2018; <a class=\"ref-link\" id=\"cFu_2018_a\" href=\"#rFu_2018_a\"><a class=\"ref-link\" id=\"cFu_2018_a\" href=\"#rFu_2018_a\">Fu & Ma, 2018</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>), shows that our Structural-JumpLSTM of dynamically spaced jumps and word level skipping leads to large FLOP reductions while maintaining the same or better reading accuracy than a vanilla LSTM that reads the full text.",
        "We present the first speed reading model that both jumps and skips.",
        "Structural-Jump-LSTM is optimized with regards to two different objectives: 1) Producing an output that can be used for classification, and 2) learning when to skip and jump based on the inputs and their context, such that the minimum number of read inputs gives the maximum accuracy.",
        "We use the same tasks and datasets used by the state-of-the-art in speed reading, and evaluate against all 5 state-of-the-art models (<a class=\"ref-link\" id=\"cSeo_et+al_2018_a\" href=\"#rSeo_et+al_2018_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2018_a\" href=\"#rSeo_et+al_2018_a\">Seo et al, 2018</a></a>; <a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\"><a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a></a>; 2018; <a class=\"ref-link\" id=\"cFu_2018_a\" href=\"#rFu_2018_a\"><a class=\"ref-link\" id=\"cFu_2018_a\" href=\"#rFu_2018_a\">Fu & Ma, 2018</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>) in addition to a vanilla LSTM full reading baseline.",
        "The model learns to skip some uninformative words, read those it deems important for predicting the target (Mean of transportation), and once it is certain of the prediction it starts jumping to the end of the sentences.",
        "Note that the state-of-the-art models use different network configurations of the RNN network and training schemes, resulting in different full read and speed read accuracies for the same dataset.",
        "The performance obtained by reading just the query has been reported to be very similar to using both the query and the 20 sentences (<a class=\"ref-link\" id=\"cHill_et+al_2016_a\" href=\"#rHill_et+al_2016_a\">Hill et al, 2016</a>), which could indicate a certain noise level in the data that speed reading models are able to identify and reduce the number of read words between the high information section of the text and the final prediction.",
        "We presented Structural-Jump-LSTM, a recurrent neural network for speed reading.",
        "StructuralJump-LSTM is inspired by human speed reading, and can skip irrelevant words in important sections, while jumping past unimportant parts of a text.",
        "Through an extensive experimental evaluation against all five state-of-the-art baselines, Structural-Jump-LSTM obtains the overall largest reduction in floating point operations, while maintaining the same accuracy or even improving it over a vanilla LSTM model that reads the full text.",
        "We contribute the first ever neural speed reading model that both skips and jumps over dynamically defined chunks of text without loss of effectiveness and with notable gains in efficiency.",
        "Future work includes investigating other reward functions, where most of the reward is not awarded in the end, and whether this would improve agent training by having a stronger signal spread throughout the text"
    ],
    "headline": "We present Structural-Jump-Long Short Term Memory: the first neural speed reading model to both skip and jump text during inference",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Campos_et+al_2018_a",
            "entry": "V\u0131ctor Campos, Brendan Jou, Xavier Giro-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Campos%2C%20V%C4%B1ctor%20Jou%2C%20Brendan%20Nieto%2C%20Xavier%20Giro-i%20Torres%2C%20Jordi%20Skip%20rnn%3A%20Learning%20to%20skip%20state%20updates%20in%20recurrent%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Campos%2C%20V%C4%B1ctor%20Jou%2C%20Brendan%20Nieto%2C%20Xavier%20Giro-i%20Torres%2C%20Jordi%20Skip%20rnn%3A%20Learning%20to%20skip%20state%20updates%20in%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "Cheng_et+al_2016_a",
            "entry": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Jianpeng%20Dong%2C%20Li%20Lapata%2C%20Mirella%20Long%20short-term%20memory-networks%20for%20machine%20reading%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Jianpeng%20Dong%2C%20Li%20Lapata%2C%20Mirella%20Long%20short-term%20memory-networks%20for%20machine%20reading%202016"
        },
        {
            "id": "Choi_et+al_2017_a",
            "entry": "Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 209\u2013220, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Eunsol%20Hewlett%2C%20Daniel%20Uszkoreit%2C%20Jakob%20Polosukhin%2C%20Illia%20Coarse-to-fine%20question%20answering%20for%20long%20documents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Eunsol%20Hewlett%2C%20Daniel%20Uszkoreit%2C%20Jakob%20Polosukhin%2C%20Illia%20Coarse-to-fine%20question%20answering%20for%20long%20documents%202017"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Fu_2018_a",
            "entry": "Tsu-Jui Fu and Wei-Yun Ma. Speed reading: Learning to read forbackward via shuttle. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Tsu-Jui%20Ma%2C%20Wei-Yun%20Speed%20reading%3A%20Learning%20to%20read%20forbackward%20via%20shuttle%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Tsu-Jui%20Ma%2C%20Wei-Yun%20Speed%20reading%3A%20Learning%20to%20read%20forbackward%20via%20shuttle%202018"
        },
        {
            "id": "Hill_et+al_2016_a",
            "entry": "Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children\u2019s books with explicit memory representations. In Proceedings of the 4th International Conference on Learning Representations (ICLR 2016), San Juan, Puerto Rico, May 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hill%2C%20Felix%20Bordes%2C%20Antoine%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20The%20goldilocks%20principle%3A%20Reading%20children%E2%80%99s%20books%20with%20explicit%20memory%20representations%202016-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hill%2C%20Felix%20Bordes%2C%20Antoine%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20The%20goldilocks%20principle%3A%20Reading%20children%E2%80%99s%20books%20with%20explicit%20memory%20representations%202016-05"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Zhengjie Huang, Zi Ye, Shuangyin Li, and Rong Pan. Length adaptive recurrent model for text classification. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pp. 1019\u20131027. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Zhengjie%20Ye%2C%20Zi%20Li%2C%20Shuangyin%20Pan%2C%20Rong%20Length%20adaptive%20recurrent%20model%20for%20text%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Zhengjie%20Ye%2C%20Zi%20Li%2C%20Shuangyin%20Pan%2C%20Rong%20Length%20adaptive%20recurrent%20model%20for%20text%20classification%202017"
        },
        {
            "id": "Johansen_2017_a",
            "entry": "Alexander Johansen and Richard Socher. Learning when to skim and when to read. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 257\u2013264, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johansen%2C%20Alexander%20Socher%2C%20Richard%20Learning%20when%20to%20skim%20and%20when%20to%20read%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johansen%2C%20Alexander%20Socher%2C%20Richard%20Learning%20when%20to%20skim%20and%20when%20to%20read%202017"
        },
        {
            "id": "Konda_2000_a",
            "entry": "Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pp. 1008\u20131014, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000"
        },
        {
            "id": "Lehmann_et+al_2015_a",
            "entry": "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Soren Auer, et al. Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6(2):167\u2013195, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20Dbpedia%E2%80%93a%20largescale%2C%20multilingual%20knowledge%20base%20extracted%20from%20wikipedia%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20Dbpedia%E2%80%93a%20largescale%2C%20multilingual%20knowledge%20base%20extracted%20from%20wikipedia%202015"
        },
        {
            "id": "Lioma_2008_a",
            "entry": "Christina Lioma and CJ Keith van Rijsbergen. Part of speech n-grams and information retrieval. French Review of applied linguistics, 13(1):9\u201322, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lioma%2C%20Christina%20van%20Rijsbergen%2C%20C.J.Keith%20Part%20of%20speech%20n-grams%20and%20information%20retrieval%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lioma%2C%20Christina%20van%20Rijsbergen%2C%20C.J.Keith%20Part%20of%20speech%20n-grams%20and%20information%20retrieval%202008"
        },
        {
            "id": "Maas_et+al_2011_a",
            "entry": "Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pp. 142\u2013150. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011"
        },
        {
            "id": "Mnih_et+al_2014_a",
            "entry": "Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances in neural information processing systems, pp. 2204\u20132212, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Neil_et+al_2016_a",
            "entry": "Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems, pp. 3882\u20133890, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neil%2C%20Daniel%20Pfeiffer%2C%20Michael%20Liu%2C%20Shih-Chii%20Phased%20lstm%3A%20Accelerating%20recurrent%20network%20training%20for%20long%20or%20event-based%20sequences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neil%2C%20Daniel%20Pfeiffer%2C%20Michael%20Liu%2C%20Shih-Chii%20Phased%20lstm%3A%20Accelerating%20recurrent%20network%20training%20for%20long%20or%20event-based%20sequences%202016"
        },
        {
            "id": "Pang_2005_a",
            "entry": "Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics, pp. 115\u2013124. Association for Computational Linguistics, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20Bo%20Lee%2C%20Lillian%20Seeing%20stars%3A%20Exploiting%20class%20relationships%20for%20sentiment%20categorization%20with%20respect%20to%20rating%20scales%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20Bo%20Lee%2C%20Lillian%20Seeing%20stars%3A%20Exploiting%20class%20relationships%20for%20sentiment%20categorization%20with%20respect%20to%20rating%20scales%202005"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Seo_et+al_2018_a",
            "entry": "Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural speed reading via skimrnn. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Neural%20speed%20reading%20via%20skimrnn%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Neural%20speed%20reading%20via%20skimrnn%202018"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Adams Wei Yu, Hongrae Lee, and Quoc Le. Learning to skim text. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1880\u20131890, 2017. doi: 10.18653/v1/P17-1172. URL http://www.aclweb.org/anthology/P17-1172.",
            "crossref": "https://dx.doi.org/10.18653/v1/P17-1172"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Keyi Yu, Yang Liu, Alexander G. Schwing, and Jian Peng. Fast and accurate text classification: Skimming, rereading and early stopping, 2018. URL https://openreview.net/forum?id=ryZ8sz-Ab.",
            "url": "https://openreview.net/forum?id=ryZ8sz-Ab"
        },
        {
            "id": "Zhang_et+al_2015_a",
            "entry": "Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pp. 649\u2013657, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015"
        }
    ]
}
