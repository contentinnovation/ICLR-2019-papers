{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION",
        "author": "Takayuki Osa University of Tokyo, Tokyo, Japan RIKEN AIP, Tokyo, Japan osa@mfg.t.u-tokyo.ac.jp",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hyl_vjC5KQ"
        },
        "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "robotic manipulation",
            "url": "https://en.wikipedia.org/wiki/robotic_manipulation"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "deep deterministic policy gradient",
            "url": "https://en.wikipedia.org/wiki/deep_deterministic_policy_gradient"
        },
        {
            "term": "Divide and Conquer",
            "url": "https://en.wikipedia.org/wiki/Divide_and_Conquer"
        },
        {
            "term": "analogy",
            "url": "https://en.wikipedia.org/wiki/analogy"
        }
    ],
    "abbreviations": {
        "HRL": "Hierarchical reinforcement learning",
        "RL": "Reinforcement learning",
        "MDP": "Markov decision process",
        "MI": "mutual information",
        "DDPG": "deep deterministic policy gradient",
        "RIM": "regularized information maximization",
        "VAT": "virtual adversarial training",
        "IOPG": "inferred option policy gradients",
        "DnC": "Divide and Conquer"
    },
    "highlights": [
        "Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>), robotic manipulation tasks (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>), and video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>)",
        "We propose an Hierarchical reinforcement learning method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return",
        "We first introduce the latent representation learning via advantage-weighted information maximization, and we describe the Hierarchical reinforcement learning framework based on deterministic option policies.\n3.1",
        "We proposed a novel Hierarchical reinforcement learning method, hierarchical reinforcement learning via advantage-weighted information maximization",
        "Our Hierarchical reinforcement learning framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for Hierarchical reinforcement learning and a monolithic policy for the standard Reinforcement learning",
        "The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks"
    ],
    "key_statements": [
        "Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>), robotic manipulation tasks (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>), and video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>)",
        "Hierarchical reinforcement learning (HRL) is a type of Reinforcement learning that leverages the hierarchical structure of a given task by learning a hierarchical policy (<a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999</a>; <a class=\"ref-link\" id=\"cDietterich_2000_a\" href=\"#rDietterich_2000_a\">Dietterich, 2000</a>)",
        "We present an Hierarchical reinforcement learning method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL",
        "We propose an Hierarchical reinforcement learning method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return",
        "We consider tasks that can be modeled as a Markov decision process (MDP), consisting of a state space S, an action space A, a reward function r : S \u00d7 A \u2192 R, an initial state distribution \u03c1(s0), and a transition probability p that defines the probability of transitioning from state st and action at at time t to state st+1",
        "We propose a novel Hierarchical reinforcement learning method based on advantage-weighted information maximization",
        "We first introduce the latent representation learning via advantage-weighted information maximization, and we describe the Hierarchical reinforcement learning framework based on deterministic option policies.\n3.1",
        "The concept underlying adInfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function",
        "<a class=\"ref-link\" id=\"cOsa_2018_a\" href=\"#rOsa_2018_a\">Osa & Sugiyama (2018</a>) proposed to learn a latent variable in Hierarchical reinforcement learning with importance sampling, their method is limited to episodic settings where only a single option is used in an episode",
        "In this study we developed adInfoHRL based on deterministic option policies",
        "We proposed a novel Hierarchical reinforcement learning method, hierarchical reinforcement learning via advantage-weighted information maximization",
        "The latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space",
        "Our Hierarchical reinforcement learning framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for Hierarchical reinforcement learning and a monolithic policy for the standard Reinforcement learning",
        "The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks"
    ],
    "summary": [
        "Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>), robotic manipulation tasks (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>), and video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>).",
        "We present an HRL method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL.",
        "We formulate the problem of learning a latent variable in a hierarchical policy as one of learning discrete and interpretable representations of states and actions.",
        "We propose the learning of a latent variable of a hierarchical policy as a discrete and hidden representation of the state-action space.",
        "To learn option policies that correspond to the modes of the advantage function, we introduce advantage-weighted importance.",
        "We formulate the problem of learning the latent variable o of a hierarchical policy as one of learning a latent representation of the state-action space.",
        "We first introduce the latent representation learning via advantage-weighted information maximization, and we describe the HRL framework based on deterministic option policies.",
        "Differentiating the objective function in Equation (14), we obtain the deterministic policy gradient of our option-policy \u03bc\u03b8o (s) given by",
        "The concept underlying adInfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function.",
        "The work by <a class=\"ref-link\" id=\"cHausman_et+al_2018_a\" href=\"#rHausman_et+al_2018_a\">Hausman et al (2018</a>) is closely related to the variational approach, and they proposed a method for learning a latent variable of a hierarchical policy via a variational bound.",
        "Our method learns the latent variable by maximizing MI with advantage-weighted importance.",
        "InfoGAIL can be interpreted as a method that divides the state-action space based on the density induced by an expert\u2019s policy by maximizing the regularized MI objective.",
        "The method proposed by <a class=\"ref-link\" id=\"cNeumann_2009_a\" href=\"#rNeumann_2009_a\">Neumann & Peters (2009</a>) employs the importance weight based on the advantage function for learning a monolithic policy, while our method uses a similar importance weight for learning a latent variable of a hierarchical policy.",
        "Osa & Sugiyama (2018) proposed to learn a latent variable in HRL with importance sampling, their method is limited to episodic settings where only a single option is used in an episode.",
        "The concept of dividing the state-action space via advantage-weighted importance can be applied to stochastic policy gradients as well.",
        "We proposed a novel HRL method, hierarchical reinforcement learning via advantage-weighted information maximization.",
        "The latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space.",
        "Our results suggested that our approach can improve the performance of TD3 in certain problem domains"
    ],
    "headline": "We propose an Hierarchical reinforcement learning method that learns a latent variable of a hierarchical policy using mutual information maximization",
    "reference_links": [
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "P. L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20P.L.%20Harb%2C%20J.%20Precup%2C%20D.%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20P.L.%20Harb%2C%20J.%20Precup%2C%20D.%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. In arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%20Brockman%20V%20Cheung%20L%20Pettersson%20J%20Schneider%20J%20Schulman%20J%20Tang%20and%20W%20Zaremba%20Openai%20gym%20In%20arXiv%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%20Brockman%20V%20Cheung%20L%20Pettersson%20J%20Schneider%20J%20Schulman%20J%20Tang%20and%20W%20Zaremba%20Openai%20gym%20In%20arXiv%202016"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Daniel_et+al_2016_a",
            "entry": "C. Daniel, G. Neumann, O. Kroemer, and J. Peters. Hierarchical relative entropy policy search. Journal of Machine Learning Research, 17:1\u201350, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20C.%20Neumann%2C%20G.%20Kroemer%2C%20O.%20Peters%2C%20J.%20Hierarchical%20relative%20entropy%20policy%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20C.%20Neumann%2C%20G.%20Kroemer%2C%20O.%20Peters%2C%20J.%20Hierarchical%20relative%20entropy%20policy%20search%202016"
        },
        {
            "id": "Dayan_1997_a",
            "entry": "P. Dayan and G. Hinton. Using expectation-maximization for reinforcement learning. Neural Computation, 9:271\u2013278, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20P.%20Hinton%2C%20G.%20Using%20expectation-maximization%20for%20reinforcement%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20P.%20Hinton%2C%20G.%20Using%20expectation-maximization%20for%20reinforcement%20learning%201997"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Dietterich_2000_a",
            "entry": "T. G Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research, 13:227\u2013303, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dietterich%2C%20T.G.%20Hierarchical%20reinforcement%20learning%20with%20the%20MAXQ%20value%20function%20decomposition%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dietterich%2C%20T.G.%20Hierarchical%20reinforcement%20learning%20with%20the%20MAXQ%20value%20function%20decomposition%202000"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "B. Eysenbach, A. Gupta, J. Ibarz, and Sergey Levine. Diversity is all you need: Learning diverse skills without a reward function. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eysenbach%2C%20B.%20Gupta%2C%20A.%20Ibarz%2C%20J.%20Levine%2C%20Sergey%20Diversity%20is%20all%20you%20need%3A%20Learning%20diverse%20skills%20without%20a%20reward%20function%202018"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "C. Florensa, Y. Duan, and P. Abbeel. Stochastic neural networks for hierarchical reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Florensa%2C%20C.%20Duan%2C%20Y.%20Abbeel%2C%20P.%20Stochastic%20neural%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Florensa%2C%20C.%20Duan%2C%20Y.%20Abbeel%2C%20P.%20Stochastic%20neural%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Frans_et+al_2018_a",
            "entry": "K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta learning shared hierarchies. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frans%2C%20K.%20Ho%2C%20J.%20Chen%2C%20X.%20Abbeel%2C%20P.%20Meta%20learning%20shared%20hierarchies%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frans%2C%20K.%20Ho%2C%20J.%20Chen%2C%20X.%20Abbeel%2C%20P.%20Meta%20learning%20shared%20hierarchies%202018"
        },
        {
            "id": "Fujimoto_et+al_2018_a",
            "entry": "S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actorcritic methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1587\u20131596, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/fujimoto18a.html.",
            "url": "http://proceedings.mlr.press/v80/fujimoto18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fujimoto%2C%20S.%20van%20Hoof%2C%20H.%20Meger%2C%20D.%20Addressing%20function%20approximation%20error%20in%20actorcritic%20methods%202018-07"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "D. Ghosh, A. Singh, A. Rajeswaran, V. Kumar, and S. Levine. Divide-and-conquer reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20D.%20Singh%2C%20A.%20Rajeswaran%2C%20A.%20Kumar%2C%20V.%20Divide-and-conquer%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20D.%20Singh%2C%20A.%20Rajeswaran%2C%20A.%20Kumar%2C%20V.%20Divide-and-conquer%20reinforcement%20learning%202018"
        },
        {
            "id": "Gomes_et+al_2010_a",
            "entry": "R. Gomes, A. Krause, and P. Perona. Discriminative clustering by regularized information maximization. In Advances in Neural Information Processing Systems (NIPS), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomes%2C%20R.%20Krause%2C%20A.%20Perona%2C%20P.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomes%2C%20R.%20Krause%2C%20A.%20Perona%2C%20P.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.",
            "url": "http://www.deeplearningbook.org"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "K. Gregor, D. Rezende, and D. Wierstra. Variational intrinsic control. arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20K.%20Rezende%2C%20D.%20Wierstra%2C%20D.%20Variational%20intrinsic%20control%202016"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine. Latent space policies for hierarchical reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20T.%20Hartikainen%2C%20K.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Latent%20space%20policies%20for%20hierarchical%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20T.%20Hartikainen%2C%20K.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Latent%20space%20policies%20for%20hierarchical%20reinforcement%20learning%202018"
        },
        {
            "id": "Haarnoja_et+al_2018_b",
            "entry": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the International Conference on Machine Learning (ICML), 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20T.%20Zhou%2C%20A.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20T.%20Zhou%2C%20A.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller. Learning an embedding space for transferable robot skills. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20K.%20Springenberg%2C%20J.T.%20Wang%2C%20Z.%20Heess%2C%20N.%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20K.%20Springenberg%2C%20J.T.%20Wang%2C%20Z.%20Heess%2C%20N.%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "Henderson_et+al_2018_a",
            "entry": "P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henderson%2C%20P.%20Islam%2C%20R.%20Bachman%2C%20P.%20Pineau%2C%20J.%20Deep%20reinforcement%20learning%20that%20matters%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henderson%2C%20P.%20Islam%2C%20R.%20Bachman%2C%20P.%20Pineau%2C%20J.%20Deep%20reinforcement%20learning%20that%20matters%202018"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama. Learning discrete representations via information maximizing self augmented training. In Proceedings of the International Conference on Machine Learning (ICML). 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20W.%20Miyato%2C%20T.%20Tokui%2C%20S.%20Matsumoto%2C%20E.%20Learning%20discrete%20representations%20via%20information%20maximizing%20self%20augmented%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20W.%20Miyato%2C%20T.%20Tokui%2C%20S.%20Matsumoto%2C%20E.%20Learning%20discrete%20representations%20via%20information%20maximizing%20self%20augmented%20training%202017"
        },
        {
            "id": "Kober_2011_a",
            "entry": "J. Kober and J. Peters. Policy search for motor primitives in robotics. Machine Learning, 84: 171\u2013203, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20J.%20Peters%2C%20J.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20J.%20Peters%2C%20J.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202011"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1\u201340, 2016. URL http://jmlr.org/papers/v17/15-522.html.",
            "url": "http://jmlr.org/papers/v17/15-522.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Y. Li, J. Song, and S. Ermon. InfoGAIL: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems (NIPS), pp. 3812\u20133822, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20InfoGAIL%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20InfoGAIL%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Miyato_et+al_2016_a",
            "entry": "T. Miyato, S. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual adversarial training. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu1, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu1%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu1%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "O. Nachum, S. Gu, H. Lee, and S. Levine. Data-efficient hierarchical reinforcement learning. arXiv, 2018. URL https://arxiv.org/abs/1805.08296.",
            "url": "https://arxiv.org/abs/1805.08296",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Neumann_2009_a",
            "entry": "G. Neumann and J. Peters. Fitted q-iteration by advantage weighted regression. In Advances in Neural Information Processing Systems (NIPS), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20G.%20Peters%2C%20J.%20Fitted%20q-iteration%20by%20advantage%20weighted%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20G.%20Peters%2C%20J.%20Fitted%20q-iteration%20by%20advantage%20weighted%20regression%202009"
        },
        {
            "id": "Osa_2018_a",
            "entry": "T. Osa and M. Sugiyama. Hierarchical policy search via return-weighted density estimation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osa%2C%20T.%20Sugiyama%2C%20M.%20Hierarchical%20policy%20search%20via%20return-weighted%20density%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osa%2C%20T.%20Sugiyama%2C%20M.%20Hierarchical%20policy%20search%20via%20return-weighted%20density%20estimation%202018"
        },
        {
            "id": "Osa_et+al_0000_a",
            "entry": "T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 7(1-2):1\u2013179, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osa%2C%20T.%20Pajarinen%2C%20J.%20Neumann%2C%20G.%20Bagnell%2C%20J.A.%20An%20algorithmic%20perspective%20on%20imitation%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osa%2C%20T.%20Pajarinen%2C%20J.%20Neumann%2C%20G.%20Bagnell%2C%20J.A.%20An%20algorithmic%20perspective%20on%20imitation%20learning"
        },
        {
            "id": "Osa_et+al_0000_b",
            "entry": "T. Osa, J. Peters, and G. Neumann. Hierarchical reinforcement learning of multiple grasping strategies with human instructions. Advanced Robotics, 32(18):955\u2013968, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osa%2C%20T.%20Peters%2C%20J.%20Neumann%2C%20G.%20Hierarchical%20reinforcement%20learning%20of%20multiple%20grasping%20strategies%20with%20human%20instructions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osa%2C%20T.%20Peters%2C%20J.%20Neumann%2C%20G.%20Hierarchical%20reinforcement%20learning%20of%20multiple%20grasping%20strategies%20with%20human%20instructions"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "J. Schulman, X. Chen, and P. Abbeel. Equivalence between policy gradients and soft q-learning. In arXiv, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Chen%2C%20X.%20Abbeel%2C%20P.%20Equivalence%20between%20policy%20gradients%20and%20soft%20q-learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Chen%2C%20X.%20Abbeel%2C%20P.%20Equivalence%20between%20policy%20gradients%20and%20soft%20q-learning%202017"
        },
        {
            "id": "Schulman_et+al_2017_b",
            "entry": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Wolski%2C%20F.%20Dhariwal%2C%20P.%20Radford%2C%20A.%20Proximal%20policy%20optimization%20algorithms%202017"
        },
        {
            "id": "Silver_et+al_2014_a",
            "entry": "D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "D. Silver, A. Huang, C. J Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Smith_et+al_2018_a",
            "entry": "M. J. A. Smith, H. Van Hoof, and J. Pineau. An inference-based policy gradient method for learning options. In Proceedings of the International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20M.J.A.%20Hoof%2C%20H.Van%20Pineau%2C%20J.%20An%20inference-based%20policy%20gradient%20method%20for%20learning%20options%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20M.J.A.%20Hoof%2C%20H.Van%20Pineau%2C%20J.%20An%20inference-based%20policy%20gradient%20method%20for%20learning%20options%202018"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "R. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.%20Precup%2C%20D.%20Singh%2C%20S.%20Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning.%20Artificial%20intelligence%201999"
        },
        {
            "id": "E_2012_a",
            "entry": "E. Todorov, T. Erez,, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E.%20Todorov%2C%20T.%20Erez%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E.%20Todorov%2C%20T.%20Erez%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Van_2008_a",
            "entry": "L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. FeUdal networks for hierarchical reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Ziebart_2010_a",
            "entry": "B. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Carnegie Mellon University, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.%20Modeling%20Purposeful%20Adaptive%20Behavior%20with%20the%20Principle%20of%20Maximum%20Causal%20Entropy%202010"
        },
        {
            "id": "where_2017_a",
            "entry": "where H(o) = p(o) log p(o)do and H(o|s, a) = p(o|s, a) log p(o|s, a)do. We make the empirical estimate of MI employed by Gomes et al. (2010); Hu et al. (2017) and modify it to employ the importance weight. The empirical estimate of MI with respect to the density induced by a policy \u03c0 is given by",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=where%20Ho%20%20po%20log%20podo%20and%20Hos%20a%20%20pos%20a%20log%20pos%20ado%20We%20make%20the%20empirical%20estimate%20of%20MI%20employed%20by%20Gomes%20et%20al%202010%20Hu%20et%20al%202017%20and%20modify%20it%20to%20employ%20the%20importance%20weight%20The%20empirical%20estimate%20of%20MI%20with%20respect%20to%20the%20density%20induced%20by%20a%20policy%20%CF%80%20is%20given%20by",
            "oa_query": "https://api.scholarcy.com/oa_version?query=where%20Ho%20%20po%20log%20podo%20and%20Hos%20a%20%20pos%20a%20log%20pos%20ado%20We%20make%20the%20empirical%20estimate%20of%20MI%20employed%20by%20Gomes%20et%20al%202010%20Hu%20et%20al%202017%20and%20modify%20it%20to%20employ%20the%20importance%20weight%20The%20empirical%20estimate%20of%20MI%20with%20respect%20to%20the%20density%20induced%20by%20a%20policy%20%CF%80%20is%20given%20by"
        }
    ]
}
