{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "K FOR THE PRICE OF 1: PARAMETER-EFFICIENT MULTI-TASK AND TRANSFER LEARNING",
        "author": "Pramod Kaushik Mudrarkarta, The University of Chicago pramodkm@uchicago.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJxvEh0cFQ"
        },
        "abstract": "We introduce a novel method that enables parameter-efficient transfer and multitask learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of finetuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine-tuning, we match singletask performance."
    },
    "keywords": [
        {
            "term": "transfer learning",
            "url": "https://en.wikipedia.org/wiki/transfer_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Deep neural networks have revolutionized many areas of machine intelligence and are used for many vision tasks that even few years ago were considered nearly impenetrable (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a></a>; Liu et al, 2016)",
        "Advances in neural networks and hardware is resulting in much of the computation being shifted to consumer devices, delivering faster response, and better security and privacy guarantees (Konecnyet al., 2016; <a class=\"ref-link\" id=\"cHoward_et+al_2017_a\" href=\"#rHoward_et+al_2017_a\">Howard et al, 2017</a>)",
        "Transfer learning We demonstrate that, by fine-tuning less than 35K parameters in MobilenetV2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a>) and InceptionV3 (<a class=\"ref-link\" id=\"cSzegedy_et+al_2016_a\" href=\"#rSzegedy_et+al_2016_a\">Szegedy et al, 2016</a>), our method leads to significant accuracy improvements over fine-tuning only the last layer (102K-1.2M parameters, depending on the number of classes) on multiple transfer learning tasks",
        "When combined with fine-tuning the last layer, we train less than 10% of the model\u2019s parameters in total.We show the effectiveness of our method over last-layer-based fine-tuning on transfer learning between completely different problems, namely COCO-trained SSD model (Liu et al, 2016) to classification over ImageNet (<a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\">Deng et al, 2009</a>)",
        "We show preliminary results on transfer learning across completely different types of tasks using MobilenetV2 and Single-Shot Multibox Detector (SSD) (Liu et al, 2016) networks",
        "We have demonstrated that using biases and scales alone allows pretrained neural networks to solve very different problems"
    ],
    "key_statements": [
        "Deep neural networks have revolutionized many areas of machine intelligence and are used for many vision tasks that even few years ago were considered nearly impenetrable (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a></a>; Liu et al, 2016)",
        "Advances in neural networks and hardware is resulting in much of the computation being shifted to consumer devices, delivering faster response, and better security and privacy guarantees (Konecnyet al., 2016; <a class=\"ref-link\" id=\"cHoward_et+al_2017_a\" href=\"#rHoward_et+al_2017_a\">Howard et al, 2017</a>)",
        "Transfer learning We demonstrate that, by fine-tuning less than 35K parameters in MobilenetV2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a>) and InceptionV3 (<a class=\"ref-link\" id=\"cSzegedy_et+al_2016_a\" href=\"#rSzegedy_et+al_2016_a\">Szegedy et al, 2016</a>), our method leads to significant accuracy improvements over fine-tuning only the last layer (102K-1.2M parameters, depending on the number of classes) on multiple transfer learning tasks",
        "When combined with fine-tuning the last layer, we train less than 10% of the model\u2019s parameters in total.We show the effectiveness of our method over last-layer-based fine-tuning on transfer learning between completely different problems, namely COCO-trained SSD model (Liu et al, 2016) to classification over ImageNet (<a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\">Deng et al, 2009</a>)",
        "We show preliminary results on transfer learning across completely different types of tasks using MobilenetV2 and Single-Shot Multibox Detector (SSD) (Liu et al, 2016) networks",
        "We have demonstrated that using biases and scales alone allows pretrained neural networks to solve very different problems"
    ],
    "summary": [
        "Deep neural networks have revolutionized many areas of machine intelligence and are used for many vision tasks that even few years ago were considered nearly impenetrable (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a></a>; Liu et al, 2016).",
        "For ImageNet (<a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\"><a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\">Deng et al, 2009</a></a>), we show that we can simultaneously train MobilenetV2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\"><a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a></a>) models operating at 5 different resolution scales, 224, 192, 160, 128 and 96, while sharing more than 98% of the parameters and resulting in the same or higher accuracy as individually trained models.",
        "They are lightweight - for instance, they account for less than 3% of MobilenetV2\u2019s parameters when training on ImageNet. we describe how model patches can be used in transfer and multi-task learning.",
        "One family of approaches (<a class=\"ref-link\" id=\"cYosinski_et+al_2014_a\" href=\"#rYosinski_et+al_2014_a\">Yosinski et al, 2014</a>; <a class=\"ref-link\" id=\"cDonahue_et+al_2014_a\" href=\"#rDonahue_et+al_2014_a\">Donahue et al, 2014</a>) widely used by practitioners for domain adaptation and transfer learning is based on fine-tuning only the last layer of a neural network to solve a new task.",
        "While the core idea of our method is based on learning small model patches, we see significant boost in performance when we fine-tune the patch along with last layer (Section 5).",
        "Experiments (Section 5) show that model-patch based fine-tuning, especially with the scale-andbias patch, is comparable and sometimes better than last-layer-based fine-tuning, despite utilizing a significantly smaller set of parameters.",
        "We evaluate the performance of our method in both transfer and multi-task learning using the image recognition networks MobilenetV2 (<a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\"><a class=\"ref-link\" id=\"cSandler_et+al_2018_a\" href=\"#rSandler_et+al_2018_a\">Sandler et al, 2018</a></a>) and InceptionV3 (<a class=\"ref-link\" id=\"cSzegedy_et+al_2016_a\" href=\"#rSzegedy_et+al_2016_a\">Szegedy et al, 2016</a>) and a variety of datasets: ImageNet (<a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\"><a class=\"ref-link\" id=\"cDeng_et+al_2009_a\" href=\"#rDeng_et+al_2009_a\">Deng et al, 2009</a></a>), CIFAR-10/100 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky, 2009</a>), Cars (Krause",
        "To demonstrate the expressivity of the biases and scales, we perform an experiment on MobilenetV2, where we learn only the scale-and-bias patch while keeping the rest of the parameters frozen at their initial random state.",
        "We see that fine-tuning only the scale-and-bias patch results in comparable accuracies as fine-tuning only the last layer while using fewer parameters.",
        "In our experiments (Appendix B.2, Figure 9) we observed the opposite behavior when fine-tuning only small model patches: the accuracy grows as learning rate increases.",
        "An overview of results on MobilenetV2 with different learning rates and model patches is shown in Figure 3.",
        "Multi-task validation accuracy using a separate S/B patch for each model, is comparable to singletask accuracy, while considerably better than the setup that only uses separate logit-layer for each task, while using only using 1% more parameters.",
        "We introduced a new way of performing transfer and multi-task learning where we patch only a very small fraction of model parameters, that leads to high accuracy on very different tasks, compared to traditional methods."
    ],
    "headline": "We introduce a novel method that enables parameter-efficient transfer and multitask learning with deep neural networks",
    "reference_links": [
        {
            "id": "Olah_et+al_2015_a",
            "entry": "Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning for recommendation. arXiv preprint arXiv:1802.07876, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07876"
        },
        {
            "id": "Chollet_2017_a",
            "entry": "Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chollet%2C%20Francois%20Xception%3A%20Deep%20learning%20with%20depthwise%20separable%20convolutions%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chollet%2C%20Francois%20Xception%3A%20Deep%20learning%20with%20depthwise%20separable%20convolutions%202017-07"
        },
        {
            "id": "Cui_et+al_2018_a",
            "entry": "Y. Cui, Y. Song, C. Sun, A. Howard, and S. Belongie. Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20Y.%20Song%2C%20Y.%20Sun%2C%20C.%20Howard%2C%20A.%20Large%20Scale%20Fine-Grained%20Categorization%20and%20Domain-Specific%20Transfer%20Learning.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Donahue_et+al_2014_a",
            "entry": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 647\u2013655, Bejing, China, 22\u201324 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/donahue14.html.",
            "url": "http://proceedings.mlr.press/v32/donahue14.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014-06"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "Hoffer_et+al_2018_a",
            "entry": "Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the last weight layer. CoRR, abs/1801.04540, 2018. URL http://arxiv.org/abs/1801.04540.",
            "url": "http://arxiv.org/abs/1801.04540",
            "arxiv_url": "https://arxiv.org/pdf/1801.04540"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017. URL http://arxiv.org/abs/1704.04861.",
            "url": "http://arxiv.org/abs/1704.04861",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, pp. 448\u2013456. JMLR.org, July 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20Learning%20%20Volume%2037%20pp%20448456%20JMLRorg%20July%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20Learning%20%20Volume%2037%20pp%20448456%20JMLRorg%20July%202015"
        },
        {
            "id": "Kim_et+al_2013_a",
            "entry": "Sangwook Kim, Swathi Kavuri, and Minho Lee. Deep network with support vector machines. In Minho Lee, Akira Hirose, Zeng-Guang Hou, and Rhee Man Kil (eds.), Neural Information Processing, pp. 458\u2013465, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Sangwook%20Kavuri%2C%20Swathi%20Lee%2C%20Minho%20Deep%20network%20with%20support%20vector%20machines%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Sangwook%20Kavuri%2C%20Swathi%20Lee%2C%20Minho%20Deep%20network%20with%20support%20vector%20machines%202013"
        },
        {
            "id": "Jakub_et+al_2016_a",
            "entry": "Jakub Konecny, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016. URL https://arxiv.org/abs/1610.05492.",
            "url": "https://arxiv.org/abs/1610.05492",
            "arxiv_url": "https://arxiv.org/pdf/1610.05492"
        },
        {
            "id": "Krause_et+al_2013_a",
            "entry": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 554\u2013561, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Jonathan%20Stark%2C%20Michael%20Deng%2C%20Jia%20Fei-Fei%2C%20Li%203d%20object%20representations%20for%20finegrained%20categorization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krause%2C%20Jonathan%20Stark%2C%20Michael%20Deng%2C%20Jia%20Fei-Fei%2C%20Li%203d%20object%20representations%20for%20finegrained%20categorization%202013"
        },
        {
            "id": "Krishnamoorthi_2018_a",
            "entry": "R. Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnamoorthi%2C%20R.%20Quantizing%20deep%20convolutional%20networks%20for%20efficient%20inference%3A%20A%20whitepaper.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pp. 1106\u20131114, 2012. URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",
            "url": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012-12-03"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. CoRR, abs/1603.04779, 2016. URL http://arxiv.org/abs/1603.04779.",
            "url": "http://arxiv.org/abs/1603.04779",
            "arxiv_url": "https://arxiv.org/pdf/1603.04779"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. arXiv preprint arXiv:1807.11164, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.11164"
        },
        {
            "id": "Maji_et+al_2013_a",
            "entry": "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1306.5151"
        },
        {
            "id": "Montufar_et+al_2014_a",
            "entry": "Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2924\u20132932. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf.",
            "url": "http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "Nichol_2018_a",
            "entry": "Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Nilsback_2008_a",
            "entry": "M-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12"
        },
        {
            "id": "Rosenfeld_2018_a",
            "entry": "Amir Rosenfeld and John K. Tsotsos. Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing. CoRR, abs/1802.00844, 2018. URL http://arxiv.org/abs/1802.00844.",
            "url": "http://arxiv.org/abs/1802.00844",
            "arxiv_url": "https://arxiv.org/pdf/1802.00844"
        },
        {
            "id": "Sandler_et+al_2018_a",
            "entry": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04381"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.",
            "url": "http://arxiv.org/abs/1409.1556",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Streeter_2018_a",
            "entry": "Matthew Streeter. Approximation algorithms for cascading prediction models. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Streeter%2C%20Matthew%20Approximation%20algorithms%20for%20cascading%20prediction%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Streeter%2C%20Matthew%20Approximation%20algorithms%20for%20cascading%20prediction%20models%202018"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Yosinski_et+al_2014_a",
            "entry": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201914, pp. 3320\u20133328, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969033.2969197.",
            "url": "http://dl.acm.org/citation.cfm?id=2969033.2969197",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017. URL http://arxiv.org/abs/1707.01083.",
            "url": "http://arxiv.org/abs/1707.01083",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Lapedriza%2C%20Agata%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Lapedriza%2C%20Agata%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017"
        }
    ]
}
