{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SPECTRAL INFERENCE NETWORKS: UNIFYING DEEP AND SPECTRAL LEARNING",
        "author": "David Pfau, Stig Petersen, Ashish Agarwal, David G. T. Barrett, & Kimberly L. Stachenfeld",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJzqpj09YQ",
            "doi": "10.6084/m9.figshare.6197069.v1"
        },
        "abstract": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "linear operator",
            "url": "https://en.wikipedia.org/wiki/linear_operator"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "proto-value functions",
            "url": "https://en.wikipedia.org/wiki/proto-value_functions"
        }
    ],
    "abbreviations": {
        "SpIN": "Spectral Inference Networks",
        "PVFs": "proto-value functions",
        "SFA": "Slow Feature Analysis",
        "VMC": "Variational Quantum Monte Carlo"
    },
    "highlights": [
        "Spectral algorithms are central to machine learning and scientific computing",
        "We propose a way to approximate eigenfunctions of linear operators on highdimensional function spaces with neural networks, which we call Spectral Inference Networks (SpIN)",
        "The closest related work in machine learning on finding eigenfunctions by optimization of parametric models is Slow Feature Analysis (SFA) (Wiskott & Sejnowski, 2002), which is a special case of Spectral Inference Networks",
        "We have shown that a single unified framework is able to compute spectral decompositions by stochastic gradient descent on domains relevant to physics and machine learning",
        "The physics application presented here is on a fairly simple system, and we hope that Spectral Inference Nets can be fruitfully applied to more complex physical systems for which computational solutions are not yet available",
        "The representations learned on video data show nontrivial structure and sensitivity to meaningful properties of the scene. These representations could be used for many downstream tasks, such as object tracking, gesture recognition, or faster exploration and subgoal discovery in reinforcement learning"
    ],
    "key_statements": [
        "Spectral algorithms are central to machine learning and scientific computing",
        "For systems where the linear operator of interest can be represented as a reasonably-sized matrix, full eigendecomposition can be achieved in O(n3) time (<a class=\"ref-link\" id=\"cPan_et+al_1998_a\" href=\"#rPan_et+al_1998_a\">Pan et al, 1998</a>), and in cases where the matrix is too large to diagonalize completely, iterative algorithms based on Krylov subspace methods can efficiently compute a fixed number of eigenvectors by repeated application of matrix-vector products (<a class=\"ref-link\" id=\"cGolub_2012_a\" href=\"#rGolub_2012_a\">Golub & Van Loan, 2012</a>)",
        "We propose a way to approximate eigenfunctions of linear operators on highdimensional function spaces with neural networks, which we call Spectral Inference Networks (SpIN)",
        "The earliest work on stochastic PCA is that of \u201cOja\u2019s rule\u201d (<a class=\"ref-link\" id=\"cOja_1982_a\" href=\"#rOja_1982_a\">Oja, 1982</a>), which is a Hebbian learning rule that converges to the first principal component, and a wide variety of online SVD algorithms have appeared since. Most of these stochastic spectral algorithms are concerned with learning fixed-size eigenvectors from online data, while we are concerned with cases where the eigenfunctions are over a space too large to be represented efficiently with a fixed-size vector",
        "The closest related work in machine learning on finding eigenfunctions by optimization of parametric models is Slow Feature Analysis (SFA) (Wiskott & Sejnowski, 2002), which is a special case of Spectral Inference Networks",
        "The expression in Eq 14 is a nonlinear function of multiple expectations, so naively replacing \u03a0, \u03a3, L, \u039b and their gradients with empirical estimates will be biased. This makes learning Spectral Inference Networks more difficult than standard problems in machine learning for which unbiased gradient estimates are available",
        "The full algorithm for training Spectral Inference Networks is given in Alg. 1, with TensorFlow pseudocode in the supplementary material in Sec",
        "As a first experiment to demonstrate the correctness of the method on a problem with a known solution, we investigated the use of Spectral Inference Networks for solving the Schrodinger equation for a two-dimensional hydrogen atom",
        "We found it critical to set the decay rate for RMSProp to be slower than the decay \u03b2 used for the moving average of the covariance in Spectral Inference Networks, and expect the same would be true for other adaptive gradient methods",
        "We have shown that a single unified framework is able to compute spectral decompositions by stochastic gradient descent on domains relevant to physics and machine learning",
        "The physics application presented here is on a fairly simple system, and we hope that Spectral Inference Nets can be fruitfully applied to more complex physical systems for which computational solutions are not yet available",
        "The representations learned on video data show nontrivial structure and sensitivity to meaningful properties of the scene. These representations could be used for many downstream tasks, such as object tracking, gesture recognition, or faster exploration and subgoal discovery in reinforcement learning"
    ],
    "summary": [
        "Spectral algorithms are central to machine learning and scientific computing.",
        "By choosing a function approximator known to work well in a certain domain, such as convolutional neural networks for vision, we may be able to bias the learned representation towards reasonable solutions in a way that is difficult to encode by choice of kernel.",
        "We propose a way to approximate eigenfunctions of linear operators on highdimensional function spaces with neural networks, which we call Spectral Inference Networks (SpIN).",
        "This significantly extends prior work on unsupervised learning without a generative model and we expect will be useful in scaling many applications of spectral methods.",
        "Sec 2 provides a review of related work on spectral learning and stochastic optimization of approximate eigenfunctions.",
        "Sec. 3 defines the objective function for Spectral Inference Networks, framing eigenfunction problems as an optimization problem.",
        "Sec. 4 describes the algorithm for training Spectral Inference Networks using bilevel optimization and a custom gradient to learn ordered eigenfunctions simultaneously.",
        "Most of these stochastic spectral algorithms are concerned with learning fixed-size eigenvectors from online data, while we are concerned with cases where the eigenfunctions are over a space too large to be represented efficiently with a fixed-size vector.",
        "The closest related work in machine learning on finding eigenfunctions by optimization of parametric models is Slow Feature Analysis (SFA) (Wiskott & Sejnowski, 2002), which is a special case of SpIN.",
        "Spectral decompositions have been used in combination with the kernelized Stein gradient estimator to better learn implicit generative models like GANs (<a class=\"ref-link\" id=\"cShi_et+al_2018_a\" href=\"#rShi_et+al_2018_a\">Shi et al, 2018</a>).",
        "This makes learning Spectral Inference Networks more difficult than standard problems in machine learning for which unbiased gradient estimates are available.",
        "By replacing \u03a3 and J\u03a3 with a moving average in Eq 14, we can cast learning Spectral Inference Networks as exactly this kind of bilevel problem.",
        "The full algorithm for training Spectral Inference Networks is given in Alg. 1, with TensorFlow pseudocode in the supplementary material in Sec. B.",
        "We present empirical results on a quantum mechanics problem with a known closedform solution, and an example of unsupervised feature learning from video without a generative",
        "As a first experiment to demonstrate the correctness of the method on a problem with a known solution, we investigated the use of SpIN for solving the Schrodinger equation for a two-dimensional hydrogen atom.",
        "We have shown that a single unified framework is able to compute spectral decompositions by stochastic gradient descent on domains relevant to physics and machine learning."
    ],
    "headline": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization",
    "reference_links": [
        {
            "id": "Anandkumar_et+al_2012_a",
            "entry": "Anima Anandkumar, Dean P Foster, Daniel J Hsu, Sham M Kakade, and Yi-Kai Liu. A Spectral Algorithm for Latent Dirichlet Allocation. In Advances in Neural Information Processing Systems, pp. 917\u2013925, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Anima%20Foster%2C%20Dean%20P.%20Hsu%2C%20Daniel%20J.%20Kakade%2C%20Sham%20M.%20A%20Spectral%20Algorithm%20for%20Latent%20Dirichlet%20Allocation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Anima%20Foster%2C%20Dean%20P.%20Hsu%2C%20Daniel%20J.%20Kakade%2C%20Sham%20M.%20A%20Spectral%20Algorithm%20for%20Latent%20Dirichlet%20Allocation%202012"
        },
        {
            "id": "Barreto_et+al_2017_a",
            "entry": "Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor Features for Transfer in Reinforcement Learning. In Advances in Neural Information Processing Systems, pp. 4055\u20134065, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20Andre%20Dabney%2C%20Will%20Munos%2C%20Remi%20Hunt%2C%20Jonathan%20J.%20Successor%20Features%20for%20Transfer%20in%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20Andre%20Dabney%2C%20Will%20Munos%2C%20Remi%20Hunt%2C%20Jonathan%20J.%20Successor%20Features%20for%20Transfer%20in%20Reinforcement%20Learning%202017"
        },
        {
            "id": "Belkin_2002_a",
            "entry": "Mikhail Belkin and Partha Niyogi. Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering. In Advances in Neural Information Processing Systems, pp. 585\u2013591, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20Eigenmaps%20and%20Spectral%20Techniques%20for%20Embedding%20and%20Clustering%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20Eigenmaps%20and%20Spectral%20Techniques%20for%20Embedding%20and%20Clustering%202002"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20Evaluation%20Platform%20for%20General%20Agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20Evaluation%20Platform%20for%20General%20Agents%202013"
        },
        {
            "id": "Bengio_et+al_2004_a",
            "entry": "Yoshua Bengio, Jean-Francois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas L Roux, and Marie Ouimet. Out-of-sample Extensions for LLE, IsoMap, MDS, Eigenmaps, and Spectral Clustering. In Advances in Neural Information Processing Systems, pp. 177\u2013184, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Paiement%2C%20Jean-Francois%20Vincent%2C%20Pascal%20Delalleau%2C%20Olivier%20Out-of-sample%20Extensions%20for%20LLE%2C%20IsoMap%2C%20MDS%2C%20Eigenmaps%2C%20and%20Spectral%20Clustering%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Paiement%2C%20Jean-Francois%20Vincent%2C%20Pascal%20Delalleau%2C%20Olivier%20Out-of-sample%20Extensions%20for%20LLE%2C%20IsoMap%2C%20MDS%2C%20Eigenmaps%2C%20and%20Spectral%20Clustering%202004"
        },
        {
            "id": "Bertsekas_2014_a",
            "entry": "Dimitri P Bertsekas. Constrained Optimization and Lagrange Multiplier methods. Academic press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Constrained%20Optimization%20and%20Lagrange%20Multiplier%20methods%202014"
        },
        {
            "id": "Blunt_et+al_2015_a",
            "entry": "NS Blunt, Ali Alavi, and George H Booth. Krylov-Projected Quantum Monte Carlo Method. Physical Review Letters, 115(5):050603, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=NS%20Blunt%20Ali%20Alavi%20and%20George%20H%20Booth%20KrylovProjected%20Quantum%20Monte%20Carlo%20Method%20Physical%20Review%20Letters%201155050603%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=NS%20Blunt%20Ali%20Alavi%20and%20George%20H%20Booth%20KrylovProjected%20Quantum%20Monte%20Carlo%20Method%20Physical%20Review%20Letters%201155050603%202015"
        },
        {
            "id": "Boots_et+al_2011_a",
            "entry": "Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the Learning-Planning Loop with Predictive State Representations. The International Journal of Robotics Research, 30(7):954\u2013966, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boots%2C%20Byron%20Siddiqi%2C%20Sajid%20M.%20Gordon%2C%20Geoffrey%20J.%20Closing%20the%20Learning-Planning%20Loop%20with%20Predictive%20State%20Representations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boots%2C%20Byron%20Siddiqi%2C%20Sajid%20M.%20Gordon%2C%20Geoffrey%20J.%20Closing%20the%20Learning-Planning%20Loop%20with%20Predictive%20State%20Representations%202011"
        },
        {
            "id": "Borkar_1997_a",
            "entry": "Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5): 291\u2013294, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20Vivek%20S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borkar%2C%20Vivek%20S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997"
        },
        {
            "id": "Bruna_et+al_2014_a",
            "entry": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs. In Proceedings of the 2nd International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruna%2C%20Joan%20Zaremba%2C%20Wojciech%20Szlam%2C%20Arthur%20LeCun%2C%20Yann%20Spectral%20Networks%20and%20Locally%20Connected%20Networks%20on%20Graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruna%2C%20Joan%20Zaremba%2C%20Wojciech%20Szlam%2C%20Arthur%20LeCun%2C%20Yann%20Spectral%20Networks%20and%20Locally%20Connected%20Networks%20on%20Graphs%202014"
        },
        {
            "id": "Carleo_2017_a",
            "entry": "Giuseppe Carleo and Matthias Troyer. Solving the Quantum Many-Body Problem with Artificial Neural Networks. Science, 355(6325):602\u2013606, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carleo%2C%20Giuseppe%20Troyer%2C%20Matthias%20Solving%20the%20Quantum%20Many-Body%20Problem%20with%20Artificial%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carleo%2C%20Giuseppe%20Troyer%2C%20Matthias%20Solving%20the%20Quantum%20Many-Body%20Problem%20with%20Artificial%20Neural%20Networks%202017"
        },
        {
            "id": "Choo_et+al_2018_a",
            "entry": "Kenny Choo, Giuseppe Carleo, Nicolas Regnault, and Titus Neupert. Symmetries and many-body excitations with neural-network quantum states. Physical review letters, 121(16):167204, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choo%2C%20Kenny%20Carleo%2C%20Giuseppe%20Regnault%2C%20Nicolas%20Neupert%2C%20Titus%20Symmetries%20and%20many-body%20excitations%20with%20neural-network%20quantum%20states%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choo%2C%20Kenny%20Carleo%2C%20Giuseppe%20Regnault%2C%20Nicolas%20Neupert%2C%20Titus%20Symmetries%20and%20many-body%20excitations%20with%20neural-network%20quantum%20states%202018"
        },
        {
            "id": "Cranmer_et+al_0000_a",
            "entry": "Kyle Cranmer, Duccio Pappadopulo, and Siavash Golkar. Quantum Inference and Quantum Flows. doi:10.6084/m9.figshare.6197069.v1, 2018.",
            "crossref": "https://dx.doi.org/10.6084/m9.figshare.6197069.v1",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.6084/m9.figshare.6197069.v1"
        },
        {
            "id": "Daskalakis_et+al_2018_a",
            "entry": "Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. A Converse to Banach\u2019s Fixed Point Theorem and its CLS Completeness. In 50th Annual ACM Symposium on Theory of Computing, 2018. URL http://arxiv.org/abs/1702.07339.",
            "url": "http://arxiv.org/abs/1702.07339",
            "arxiv_url": "https://arxiv.org/pdf/1702.07339"
        },
        {
            "id": "Edelman_et+al_1998_a",
            "entry": "Alan Edelman, Tomas A Arias, and Steven T Smith. The Geometry of Algorithms with Orthogonality Constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303\u2013353, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edelman%2C%20Alan%20Arias%2C%20Tomas%20A.%20Smith%2C%20Steven%20T.%20The%20Geometry%20of%20Algorithms%20with%20Orthogonality%20Constraints%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edelman%2C%20Alan%20Arias%2C%20Tomas%20A.%20Smith%2C%20Steven%20T.%20The%20Geometry%20of%20Algorithms%20with%20Orthogonality%20Constraints%201998"
        },
        {
            "id": "Foulkes_et+al_2001_a",
            "entry": "WMC Foulkes, L Mitas, RJ Needs, and G Rajagopal. Quantum Monte Carlo Simulations of Solids. Reviews of Modern Physics, 73(1):33, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foulkes%2C%20W.M.C.%20Mitas%2C%20L.%20Needs%2C%20R.J.%20Rajagopal%2C%20G.%20Quantum%20Monte%20Carlo%20Simulations%20of%20Solids%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foulkes%2C%20W.M.C.%20Mitas%2C%20L.%20Needs%2C%20R.J.%20Rajagopal%2C%20G.%20Quantum%20Monte%20Carlo%20Simulations%20of%20Solids%202001"
        },
        {
            "id": "Franzius_et+al_2007_a",
            "entry": "Mathias Franzius, Henning Sprekeler, and Laurenz Wiskott. Slowness and Sparseness Lead to Place, Head-Direction, and Spatial-View Cells. PLoS Computational Biology, 3(8):e166, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franzius%2C%20Mathias%20Sprekeler%2C%20Henning%20Wiskott%2C%20Laurenz%20Slowness%20and%20Sparseness%20Lead%20to%20Place%2C%20Head-Direction%2C%20and%20Spatial-View%20Cells%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franzius%2C%20Mathias%20Sprekeler%2C%20Henning%20Wiskott%2C%20Laurenz%20Slowness%20and%20Sparseness%20Lead%20to%20Place%2C%20Head-Direction%2C%20and%20Spatial-View%20Cells%202007"
        },
        {
            "id": "Giles_2008_a",
            "entry": "Mike Giles. An Extended Collection of Matrix Derivative Results for Forward and Reverse Mode Automatic Differentiation. Oxford University Computing Laboratory, Numerical Analysis Report, 08(01), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giles%2C%20Mike%20An%20Extended%20Collection%20of%20Matrix%20Derivative%20Results%20for%20Forward%20and%20Reverse%20Mode%20Automatic%20Differentiation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giles%2C%20Mike%20An%20Extended%20Collection%20of%20Matrix%20Derivative%20Results%20for%20Forward%20and%20Reverse%20Mode%20Automatic%20Differentiation%202008"
        },
        {
            "id": "Golub_2012_a",
            "entry": "Gene H Golub and Charles F Van Loan. Matrix Computations, volume 3. JHU Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gene%20H%20Golub%20and%20Charles%20F%20Van%20Loan%20Matrix%20Computations%20volume%203%20JHU%20Press%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gene%20H%20Golub%20and%20Charles%20F%20Van%20Loan%20Matrix%20Computations%20volume%203%20JHU%20Press%202012"
        },
        {
            "id": "Harju_et+al_1997_a",
            "entry": "A Harju, B Barbiellini, S Siljamaki, Risto M Nieminen, and G Ortiz. Stochastic Gradient Approximation: An Efficient Method to Optimize Many-Body Wave Functions. Physical Review Letters, 79(7):1173, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harju%2C%20A.%20Barbiellini%2C%20B.%20Siljamaki%2C%20S.%20Nieminen%2C%20Risto%20M.%20Stochastic%20Gradient%20Approximation%3A%20An%20Efficient%20Method%20to%20Optimize%20Many-Body%20Wave%20Functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harju%2C%20A.%20Barbiellini%2C%20B.%20Siljamaki%2C%20S.%20Nieminen%2C%20Risto%20M.%20Stochastic%20Gradient%20Approximation%3A%20An%20Efficient%20Method%20to%20Optimize%20Many-Body%20Wave%20Functions%201997"
        },
        {
            "id": "Hsu_et+al_2012_a",
            "entry": "Daniel Hsu, Sham M Kakade, and Tong Zhang. A Spectral Algorithm for Learning Hidden Markov Models. Journal of Computer and System Sciences, 78:1460\u20131480, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20Spectral%20Algorithm%20for%20Learning%20Hidden%20Markov%20Models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20Spectral%20Algorithm%20for%20Learning%20Hidden%20Markov%20Models%202012"
        },
        {
            "id": "Ionescu_et+al_2015_a",
            "entry": "Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix Backpropagation for Deep Networks with Structured Layers. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2965\u20132973, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ionescu%2C%20Catalin%20Vantzos%2C%20Orestis%20Sminchisescu%2C%20Cristian%20Matrix%20Backpropagation%20for%20Deep%20Networks%20with%20Structured%20Layers%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ionescu%2C%20Catalin%20Vantzos%2C%20Orestis%20Sminchisescu%2C%20Cristian%20Matrix%20Backpropagation%20for%20Deep%20Networks%20with%20Structured%20Layers%202015"
        },
        {
            "id": "Kompella_et+al_2012_a",
            "entry": "Varun Raj Kompella, Matthew Luciw, and Jurgen Schmidhuber. Incremental Slow Feature Analysis: Adaptive Low-Complexity Slow Feature Updating from High-Dimensional Input Streams. Neural Computation, 24(11):2994\u20133024, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kompella%2C%20Varun%20Raj%20Luciw%2C%20Matthew%20Schmidhuber%2C%20Jurgen%20Incremental%20Slow%20Feature%20Analysis%3A%20Adaptive%20Low-Complexity%20Slow%20Feature%20Updating%20from%20High-Dimensional%20Input%20Streams%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kompella%2C%20Varun%20Raj%20Luciw%2C%20Matthew%20Schmidhuber%2C%20Jurgen%20Incremental%20Slow%20Feature%20Analysis%3A%20Adaptive%20Low-Complexity%20Slow%20Feature%20Updating%20from%20High-Dimensional%20Input%20Streams%202012"
        },
        {
            "id": "Loaiza-Ganem_et+al_2017_a",
            "entry": "Gabriel Loaiza-Ganem, Yuanjun Gao, and John P Cunningham. Maximum Entropy Flow Networks. In Proceedings of the 5th International Conference on Learning Representations, 2017. URL https://arxiv.org/abs/1701.03504.",
            "url": "https://arxiv.org/abs/1701.03504",
            "arxiv_url": "https://arxiv.org/pdf/1701.03504"
        },
        {
            "id": "Machado_et+al_2017_a",
            "entry": "Marlos C Machado, Marc G Bellemare, and Michael Bowling. A Laplacian Framework for Option Discovery in Reinforcement Learning. Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Bowling%2C%20Michael%20A%20Laplacian%20Framework%20for%20Option%20Discovery%20in%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Bowling%2C%20Michael%20A%20Laplacian%20Framework%20for%20Option%20Discovery%20in%20Reinforcement%20Learning%202017"
        },
        {
            "id": "Machado_et+al_2018_a",
            "entry": "Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption Discovery through the Deep Successor Representation. In Proceedings of the 6th International Conference on Learning Representations, 2018. URL http://arxiv.org/abs/1710.11089.",
            "url": "http://arxiv.org/abs/1710.11089",
            "arxiv_url": "https://arxiv.org/pdf/1710.11089"
        },
        {
            "id": "Mahadevan_2007_a",
            "entry": "Sridhar Mahadevan and Mauro Maggioni. Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes. Journal of Machine Learning Research, 8:2169\u20132231, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahadevan%2C%20Sridhar%20Maggioni%2C%20Mauro%20Proto-value%20Functions%3A%20A%20Laplacian%20Framework%20for%20Learning%20Representation%20and%20Control%20in%20Markov%20Decision%20Processes%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahadevan%2C%20Sridhar%20Maggioni%2C%20Mauro%20Proto-value%20Functions%3A%20A%20Laplacian%20Framework%20for%20Learning%20Representation%20and%20Control%20in%20Markov%20Decision%20Processes%202007"
        },
        {
            "id": "Murray_2016_a",
            "entry": "Iain Murray. Differentiation of the Cholesky decomposition. arXiv preprint arXiv:1602.07527, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07527"
        },
        {
            "id": "Ng_et+al_2002_a",
            "entry": "Andrew Y Ng, Michael I Jordan, and Yair Weiss. On Spectral Clustering: Analysis and an Algorithm. In Advances in Neural Information Processing Systems, pp. 849\u2013856, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20Spectral%20Clustering%3A%20Analysis%20and%20an%20Algorithm%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20Spectral%20Clustering%3A%20Analysis%20and%20an%20Algorithm%202002"
        },
        {
            "id": "Oja_1982_a",
            "entry": "Erkki Oja. Simplified Neuron Model as a Principal Component Analyzer. Journal of Mathematical Biology, 15(3):267\u2013273, Nov 1982. ISSN 0303-6812. doi: 10.1007/BF00275687. URL http://link.springer.com/10.1007/BF00275687.",
            "crossref": "https://dx.doi.org/10.1007/BF00275687",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/BF00275687"
        },
        {
            "id": "Pan_et+al_1998_a",
            "entry": "Victor Y Pan, Z Chen, Ailong Zheng, et al. The Complexity of the Algebraic Eigenproblem. Mathematical Sciences Research Institute, Berkeley, pp. 1998\u201371, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Victor%20Y.%20Chen%2C%20Z.%20Zheng%2C%20Ailong%20The%20Complexity%20of%20the%20Algebraic%20Eigenproblem%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Victor%20Y.%20Chen%2C%20Z.%20Zheng%2C%20Ailong%20The%20Complexity%20of%20the%20Algebraic%20Eigenproblem%201998"
        },
        {
            "id": "Pfau_2016_a",
            "entry": "David Pfau and Oriol Vinyals. Connecting Generative Adversarial Networks and Actor-Critic Methods. NIPS Workshop on Adversarial Training, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pfau%2C%20David%20Vinyals%2C%20Oriol%20Connecting%20Generative%20Adversarial%20Networks%20and%20Actor-Critic%20Methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pfau%2C%20David%20Vinyals%2C%20Oriol%20Connecting%20Generative%20Adversarial%20Networks%20and%20Actor-Critic%20Methods%202016"
        },
        {
            "id": "Roweis_2000_a",
            "entry": "S. T. Roweis and L K Saul. Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science, 290(5500):2323\u20132326, Dec 2000. ISSN 00368075. doi: 10.1126/science.290.5500. 2323.",
            "crossref": "https://dx.doi.org/10.1126/science.290.5500.2323",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1126/science.290.5500.2323"
        },
        {
            "id": "Shi_2000_a",
            "entry": "Jianbo Shi and Jitendra Malik. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Jianbo%20Malik%2C%20Jitendra%20Normalized%20Cuts%20and%20Image%20Segmentation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jianbo%20Malik%2C%20Jitendra%20Normalized%20Cuts%20and%20Image%20Segmentation%202000"
        },
        {
            "id": "Shi_et+al_2018_a",
            "entry": "Jiaxin Shi, Shengyang Sun, and Jun Zhu. A Spectral Approach to Gradient Estimation for Implicit Distributions. Proceedings of the 35th International Conference of Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20Spectral%20Approach%20to%20Gradient%20Estimation%20for%20Implicit%20Distributions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20Spectral%20Approach%20to%20Gradient%20Estimation%20for%20Implicit%20Distributions%202018"
        },
        {
            "id": "Sprekeler_2011_a",
            "entry": "Henning Sprekeler. On the Relation of Slow Feature Analysis and Laplacian Eigenmaps. Neural Computation, 23(12):3287\u20133302, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sprekeler%2C%20Henning%20On%20the%20Relation%20of%20Slow%20Feature%20Analysis%20and%20Laplacian%20Eigenmaps%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sprekeler%2C%20Henning%20On%20the%20Relation%20of%20Slow%20Feature%20Analysis%20and%20Laplacian%20Eigenmaps%202011"
        },
        {
            "id": "Stachenfeld_et+al_2017_a",
            "entry": "Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The Hippocampus as a Predictive Map. Nature Neuroscience, 20:16431653, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stachenfeld%2C%20Kimberly%20L.%20Botvinick%2C%20Matthew%20M.%20Gershman%2C%20Samuel%20J.%20The%20Hippocampus%20as%20a%20Predictive%20Map%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stachenfeld%2C%20Kimberly%20L.%20Botvinick%2C%20Matthew%20M.%20Gershman%2C%20Samuel%20J.%20The%20Hippocampus%20as%20a%20Predictive%20Map%202017"
        },
        {
            "id": "Sun_et+al_2014_a",
            "entry": "Lin Sun, Kui Jia, Tsung-Han Chan, Yuqiang Fang, Gang Wang, and Shuicheng Yan. DL-SFA: Deeply-Learned Slow Feature Analysis for Action Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2625\u20132632, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Lin%20Jia%2C%20Kui%20Chan%2C%20Tsung-Han%20Yuqiang%20Fang%2C%20Gang%20Wang%2C%20and%20Shuicheng%20Yan.%20DL-SFA%3A%20Deeply-Learned%20Slow%20Feature%20Analysis%20for%20Action%20Recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Lin%20Jia%2C%20Kui%20Chan%2C%20Tsung-Han%20Yuqiang%20Fang%2C%20Gang%20Wang%2C%20and%20Shuicheng%20Yan.%20DL-SFA%3A%20Deeply-Learned%20Slow%20Feature%20Analysis%20for%20Action%20Recognition%202014"
        },
        {
            "id": "Tenenbaum_et+al_2000_a",
            "entry": "J. B. Tenenbaum, V de Silva, and J C Langford. A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500):2319\u20132323, Dec 2000. ISSN 00368075. doi: 10.1126/science.290.5500.2319.",
            "crossref": "https://dx.doi.org/10.1126/science.290.5500.2319",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1126/science.290.5500.2319"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the Gradient by a Running",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Tijmen%20Tieleman%20and%20Geoffrey%20Hinton%20Lecture%2065RMSProp%20Divide%20the%20Gradient%20by%20a%20Running"
        },
        {
            "id": "Wiskott_2012_a",
            "entry": "Average of its Recent Magnitude. COURSERA: Neural Networks for Machine Learning, 4(2): 26\u201331, 2012. Laurenz Wiskott and Terrence J Sejnowski. Slow Feature Analysis: Unsupervised Learning of Invariances. Neural Computation, 14(4):715\u2013770, 2002. Reto Wyss, Peter Konig, and Paul FM J Verschure. A Model of the Ventral Visual System Based on Temporal Stability and Local Memory. PLoS Biology, 4(5):e120, 2006. XL Yang, SH Guo, FT Chan, KW Wong, and WY Ching. Analytic Solution of a Two-Dimensional Hydrogen Atom. I. Nonrelativistic Theory. Physical Review A, 43(3):1186, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiskott%20Sejnowski%2C%20Terrence%20J.%20Average%20of%20its%20Recent%20Magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiskott%20Sejnowski%2C%20Terrence%20J.%20Average%20of%20its%20Recent%20Magnitude%202012"
        }
    ]
}
