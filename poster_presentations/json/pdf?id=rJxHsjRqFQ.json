{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HYPERBOLIC ATTENTION NETWORKS",
        "author": "Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, Nando de Freitas DeepMind",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJxHsjRqFQ"
        },
        "abstract": "Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space. We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model. Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry \u2013as opposed to Euclidean geometry\u2013 can encode those objects without having any interference. Our method shows improvements in generalization on neural machine translation on WMT\u201914 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact."
    },
    "keywords": [
        {
            "term": "geometry",
            "url": "https://en.wikipedia.org/wiki/geometry"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "power law distribution",
            "url": "https://en.wikipedia.org/wiki/power_law_distribution"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "hyperbolic geometry",
            "url": "https://en.wikipedia.org/wiki/hyperbolic_geometry"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "complex network",
            "url": "https://en.wikipedia.org/wiki/complex_network"
        }
    ],
    "abbreviations": {
        "RNs": "Relation Nets",
        "RT": "Recursive Transformer",
        "LP": "Link prediction",
        "SPLP": "Shortest path length prediction",
        "H-GAT": "hyperbolic version of GAT"
    },
    "highlights": [
        "The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data, including hierarchy and clustering behaviour",
        "We show that real-world tasks within implicit graph structure such as a diagnostic visual question answering task (Johnson et al, 2017), and neural machine translation, benefit from relying on hyperbolic geometry",
        "Our Relation Nets with hyperbolic attention and sigmoid achieves 95.7% accuracy on the test set at the same capacity level as Relation Nets, whereas the latter reportedly achieves 95.5% accuracy (Santoro et al, 2017).\n5.5",
        "We observe improvements over the Euclidean model by using hyperbolic attention, in particular when coupled with the sigmoid activation function for the attention weights",
        "We have presented a novel way to impose the inductive biases from hyperbolic geometry on the activations of deep neural networks",
        "We have shown improved performance on link prediction and shortest path length prediction in scale free graphs, on two visual question answering datasets, real-world graph transduction tasks and on English to German machine translation"
    ],
    "key_statements": [
        "The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data, including hierarchy and clustering behaviour",
        "Fuelled by the desire of increasing the capacity of neural networks without increasing the number of trainable parameters so as to match the complexity of data, we propose hyperbolic attention networks",
        "We briefly review the definitions of the hyperboloid and Klein models and the relationship between them, in just enough detail to support the presentation in the remainder of the paper",
        "We show that real-world tasks within implicit graph structure such as a diagnostic visual question answering task (Johnson et al, 2017), and neural machine translation, benefit from relying on hyperbolic geometry",
        "We provide experiments with feed-forward networks, the Transformer (Vaswani et al, 2017) and Relation Networks (Santoro et al, 2017) endowed with hyperbolic attention",
        "We report the results in Figure 3",
        "Since we expect hyperbolic attention to be well suited to relational modelling, we investigate our models on the relational variant of the Sort-of-CLEVR dataset (Santoro et al, 2017)",
        "As Figure 4 shows, the attention mechanism computed in the hyperbolic space improves around 20 percent points over the standard Relation Nets, where all the models use only two units of the relational MLP.\n5.3",
        "We report our results in Table 1 and compare against the GAT with the Euclidean attention mechanism",
        "We find that hyperbolic attention with sigmoid consistently outperforms other models",
        "Our Relation Nets with hyperbolic attention and sigmoid achieves 95.7% accuracy on the test set at the same capacity level as Relation Nets, whereas the latter reportedly achieves 95.5% accuracy (Santoro et al, 2017).\n5.5",
        "We observe improvements over the Euclidean model by using hyperbolic attention, in particular when coupled with the sigmoid activation function for the attention weights",
        "We have presented a novel way to impose the inductive biases from hyperbolic geometry on the activations of deep neural networks",
        "We have shown improved performance on link prediction and shortest path length prediction in scale free graphs, on two visual question answering datasets, real-world graph transduction tasks and on English to German machine translation"
    ],
    "summary": [
        "The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data, including hierarchy and clustering behaviour.",
        "We make use of the Klein model, because it admits an efficient expression for the hyperbolic aggregation operation we define in Section 4.2.",
        "Many of these relational reasoning models can be expressed in terms of an attentive read operation.",
        "If one performs an attentive read for each element of the set j the resulting operation corresponds in a natural way to message passing on a graph, where each node i aggregates messages {vij}j from its neighbours along edges of weight f/Z.",
        "In the Transformer model of Vaswani et al (2017) the authors define an all-to-all message passing operation on a set of vectors which they call scaled dot-product attention.",
        "We experiment with both softmax and sigmoid operations for computing the attention weights in our hyperbolic models.",
        "We use the algorithm of von Looz et al (2015) to efficiently generate large scale-free graphs, and define two predictive tasks that test our model\u2019s ability to represent different aspects of the structure of these networks.",
        "For both tasks we train Recursive Transformer (RT) models, using hyperbolic and Euclidean attention.",
        "Our models extend Relation Nets (RNs) with the attention mechanism in hyperbolic space, but otherwise we follow the standard setup-up (Santoro et al, 2017).",
        "As Figure 4 shows, the attention mechanism computed in the hyperbolic space improves around 20 percent points over the standard RN, where all the models use only two units of the relational MLP.",
        "We closely follow the procedure established in (Santoro et al, 2017), both in terms of the model architecture, capacity, or the choice of the hyperparameters, and only differ by the attention mechanism (Euclidean or hyperbolic attention), or sigmoid activations.",
        "We observe improvements over the Euclidean model by using hyperbolic attention, in particular when coupled with the sigmoid activation function for the attention weights.",
        "We implemented our proposed hyperbolic attention mechanism in both Relation Networks and the Transformer and showed that we achieve improved performance on a diverse set of tasks.",
        "We have shown improved performance on link prediction and shortest path length prediction in scale free graphs, on two visual question answering datasets, real-world graph transduction tasks and on English to German machine translation.",
        "As a future work, an interesting potential future direction is to use hyperbolic geometry as an inductive bias for the activation of neural networks in the memory.",
        "The gains are prominent in relatively small models, which confirms our hypothesis that hyperbolic geometry induces more compact representations"
    ],
    "headline": "We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures",
    "reference_links": [
        {
            "id": "Antol_et+al_2015_a",
            "entry": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20Vqa%3A%20Visual%20question%20answering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20Vqa%3A%20Visual%20question%20answering%202015"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202014"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "Ondrej_2014_a",
            "entry": "Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W14/W14-3302.",
            "url": "http://www.aclweb.org/anthology/W/W14/W14-3302",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ondrej%20Bojar%20Christian%20Buck%20Christian%20Federmann%20Barry%20Haddow%20Philipp%20Koehn%20Johannes%20Leveling%20Christof%20Monz%20Pavel%20Pecina%20Matt%20Post%20Herve%20SaintAmand%20Radu%20Soricut%20Lucia%20Specia%20and%20Ale%C5%A1%20Tamchyna%20Findings%20of%20the%202014%20workshop%20on%20statistical%20machine%20translation%20In%20Proceedings%20of%20the%20Ninth%20Workshop%20on%20Statistical%20Machine%20Translation%20pages%201258%20Baltimore%20Maryland%20USA%20June%202014%20Association%20for%20Computational%20Linguistics%20URL%20httpwwwaclweborganthologyWW14W143302"
        },
        {
            "id": "Bronstein_et+al_1982_a",
            "entry": "Gunnar A Borg. Psychophysical bases of perceived exertion. Med sci sports exerc, 14(5):377\u2013381, 1982. Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342, 2017. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. Benjamin Paul Chamberlain, James Clough, and Marc Peter Deisenroth. Neural embeddings of graphs in hyperbolic space. arXiv preprint arXiv:1705.10359, 2017. Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic space. arXiv preprint arXiv:1806.00437, 2018. Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical data. SIAM",
            "arxiv_url": "https://arxiv.org/pdf/1312.6203"
        },
        {
            "id": "Defferrard_2009_a",
            "entry": "review, 51(4):661\u2013703, 2009. Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pages 3844\u20133852, 2016. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20Xavier%20Bresson%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20Xavier%20Bresson%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202009"
        },
        {
            "id": "arXiv_1807_a",
            "entry": "arXiv preprint arXiv:1807.03819, 2018. Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter",
            "arxiv_url": "https://arxiv.org/pdf/1807.03819"
        },
        {
            "id": "Mcgill_et+al_2006_a",
            "entry": "Brian J McGill, Brian J Enquist, Evan Weiher, and Mark Westoby. Rebuilding community ecology from functional traits. Trends in ecology & evolution, 21(4):178\u2013185, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McGill%2C%20Brian%20J.%20Enquist%2C%20Brian%20J.%20Weiher%2C%20Evan%20Westoby%2C%20Mark%20Rebuilding%20community%20ecology%20from%20functional%20traits%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McGill%2C%20Brian%20J.%20Enquist%2C%20Brian%20J.%20Weiher%2C%20Evan%20Westoby%2C%20Mark%20Rebuilding%20community%20ecology%20from%20functional%20traits%202006"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Newman_2005_a",
            "entry": "Mark EJ Newman. Power laws, pareto distributions and zipf\u2019s law. Contemporary physics, 46(5):323\u2013351, 2005. Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. arXiv preprint arXiv:1806.03417, 2018. Maximillian Nickel and Douwe Kiela. Poincar\u00e9 embeddings for learning hierarchical representations. In",
            "arxiv_url": "https://arxiv.org/pdf/1806.03417"
        },
        {
            "id": "Papadopoulos_et+al_2017_a",
            "entry": "Advances in Neural Information Processing Systems, pages 6341\u20136350, 2017. Jorg Ontrup and Helge Ritter. Hyperbolic self-organizing maps for semantic navigation. In Advances in neural information processing systems, pages 1417\u20131424, 2002. Fragkiskos Papadopoulos, Dmitri Krioukov, Mari\u00e1n Bogu\u00f1\u00e1, and Amin Vahdat. Greedy forwarding in dynamic scale-free networks embedded in hyperbolic metric spaces. In INFOCOM, 2010 Proceedings IEEE, pages 1\u20139. IEEE, 2010. Steven T Piantadosi. Zipf\u2019s word frequency law in natural language: A critical review and future directions. Psychonomic bulletin & review, 21(5):1112\u20131130, 2014. David MW Powers. Applications and explanations of zipf\u2019s law. In Proceedings of the joint conferences on new methods in language processing and computational natural language learning, pages 151\u2013160. Association for Computational Linguistics, 1998. Helge Ritter. Self-organizing maps on non-euclidean spaces. In Kohonen maps, pages 97\u2013109.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papadopoulos%2C%20Dmitri%20Krioukov%20Bogu%C3%B1%C3%A1%2C%20Mari%C3%A1n%20Vahdat%2C%20Amin%20Jorg%20Ontrup%20and%20Helge%20Ritter.%20Hyperbolic%20self-organizing%20maps%20for%20semantic%20navigation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papadopoulos%2C%20Dmitri%20Krioukov%20Bogu%C3%B1%C3%A1%2C%20Mari%C3%A1n%20Vahdat%2C%20Amin%20Jorg%20Ontrup%20and%20Helge%20Ritter.%20Hyperbolic%20self-organizing%20maps%20for%20semantic%20navigation%202017"
        },
        {
            "id": "Elsevier_2017_a",
            "entry": "Elsevier, 1999. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pages 4974\u20134983, 2017. Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In International Symposium on Graph Drawing, pages 355\u2013366.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elsevier%201999%20Adam%20Santoro%20David%20Raposo%20David%20G%20Barrett%20Mateusz%20Malinowski%20Razvan%20Pascanu%20Peter%20Battaglia%20and%20Tim%20Lillicrap%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2049744983%202017%20Rik%20Sarkar%20Low%20distortion%20delaunay%20embedding%20of%20trees%20in%20hyperbolic%20plane%20In%20International%20Symposium%20on%20Graph%20Drawing%20pages%20355366",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elsevier%201999%20Adam%20Santoro%20David%20Raposo%20David%20G%20Barrett%20Mateusz%20Malinowski%20Razvan%20Pascanu%20Peter%20Battaglia%20and%20Tim%20Lillicrap%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2049744983%202017%20Rik%20Sarkar%20Low%20distortion%20delaunay%20embedding%20of%20trees%20in%20hyperbolic%20plane%20In%20International%20Symposium%20on%20Graph%20Drawing%20pages%20355366"
        },
        {
            "id": "Springer_2015_a",
            "entry": "Springer, 2015. Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. arXiv preprint arXiv:1711.07971, 2017. Greg Yang and Alexander M. Rush. Lie access neural turing machine. arXiv preprint arXiv:1602.08671, 2016. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995\u20135004, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07971"
        }
    ]
}
