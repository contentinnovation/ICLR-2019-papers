{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH",
        "author": "Hanxiao Liu, CMU hanxiaol@cs.cmu.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1eYHoC5FX"
        },
        "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms."
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "language modeling",
            "url": "https://en.wikipedia.org/wiki/language_modeling"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "PTB": "Penn Treebank"
    },
    "highlights": [
        "Discovering state-of-the-art neural network architectures requires substantial effort of human experts",
        "We show that the architectures learned by DARTS on CIFAR-10 and Penn Treebank are transferable to ImageNet and WikiText-2, respectively",
        "Our experiments on CIFAR-10 and Penn Treebank consist of two stages, architecture search (Sect. 3.1) and architecture evaluation (Sect. 3.2)",
        "We investigate the transferability of the best cells learned on CIFAR-10 and Penn Treebank by evaluating them on ImageNet and WikiText-2 (WT2) respectively.\n3.1",
        "Table 4 shows that the cell identified by DARTS transfers to WT2 better than ENAS, the overall results are less strong than those presented in Table 2 for Penn Treebank",
        "We presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks"
    ],
    "key_statements": [
        "Discovering state-of-the-art neural network architectures requires substantial effort of human experts",
        "On the language modeling task, DARTS efficiently discovers a recurrent cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned LSTM (<a class=\"ref-link\" id=\"cMelis_et+al_2018_a\" href=\"#rMelis_et+al_2018_a\">Melis et al, 2018</a>) and all the existing automatically searched cells based on NAS (Zoph & Le, 2017) and ENAS (Pham et al, 2018b)",
        "Through extensive experiments on image classification and language modeling tasks we show that gradient-based architecture search achieves highly competitive results on CIFAR-10 and outperforms the state of the art on Penn Treebank",
        "We show that the architectures learned by DARTS on CIFAR-10 and Penn Treebank are transferable to ImageNet and WikiText-2, respectively",
        "Our experiments on CIFAR-10 and Penn Treebank consist of two stages, architecture search (Sect. 3.1) and architecture evaluation (Sect. 3.2)",
        "We investigate the transferability of the best cells learned on CIFAR-10 and Penn Treebank by evaluating them on ImageNet and WikiText-2 (WT2) respectively.\n3.1",
        "Table 2 presents the results for recurrent architectures on Penn Treebank, where a cell discovered by DARTS achieved the test perplexity of 55.7",
        "Results in Table 3 show that the cell learned on CIFAR-10 is transferable to ImageNet",
        "Table 4 shows that the cell identified by DARTS transfers to WT2 better than ENAS, the overall results are less strong than those presented in Table 2 for Penn Treebank",
        "We presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks"
    ],
    "summary": [
        "Discovering state-of-the-art neural network architectures requires substantial effort of human experts.",
        "3) we show that DARTS is able to design a convolutional cell that achieves 2.76 \u00b1 0.09% test error on CIFAR-10 for image classification using 3.3M parameters, which is competitive with the state-of-the-art result by regularized evolution (Real et al, 2018) obtained using three orders of magnitude more computation resources.",
        "On the language modeling task, DARTS efficiently discovers a recurrent cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned LSTM (<a class=\"ref-link\" id=\"cMelis_et+al_2018_a\" href=\"#rMelis_et+al_2018_a\">Melis et al, 2018</a>) and all the existing automatically searched cells based on NAS (Zoph & Le, 2017) and ENAS (Pham et al, 2018b).",
        "Through extensive experiments on image classification and language modeling tasks we show that gradient-based architecture search achieves highly competitive results on CIFAR-10 and outperforms the state of the art on PTB.",
        "Analogous to architecture search using RL (Zoph & Le, 2017; Zoph et al, 2018; Pham et al, 2018b) or evolution (<a class=\"ref-link\" id=\"cLiu_et+al_2018_b\" href=\"#rLiu_et+al_2018_b\">Liu et al, 2018b</a>; Real et al, 2018) where the validation set performance is treated as the reward or fitness, DARTS aims to optimize the validation loss, but using gradient descent.",
        "To determine the architecture for final evaluation, we run DARTS four times with different random seeds and pick the best cell based on its validation performance obtained by training from scratch for a short period (100 epochs on CIFAR-10 and 300 epochs on PTB).",
        "DARTS achieved comparable results with the state of the art (Zoph et al, 2018; Real et al, 2018) while using three orders of magnitude less computation resources (i.e. 1.5 or 4 GPU days vs 2000 GPU days for NASNet and 3150 GPU days for AmoebaNet).",
        "Table 2 presents the results for recurrent architectures on PTB, where a cell discovered by DARTS achieved the test perplexity of 55.7.",
        "Results in Table 3 show that the cell learned on CIFAR-10 is transferable to ImageNet. It is worth noticing that DARTS achieves competitive performance with the state-of-the-art RL method (Zoph et al, 2018) while using three orders of magnitude less computation resources.",
        "We presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks.",
        "By searching in a continuous space, DARTS is able to match or outperform the state-of-the-art non-differentiable architecture search methods on image classification and language modeling tasks with remarkable efficiency improvement by several orders of magnitude.",
        "It would be interesting to explore performance-aware architecture derivation schemes based on the one-shot model learned during the search process"
    ],
    "headline": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner",
    "reference_links": [
        {
            "id": "Ahmed_2017_a",
            "entry": "Karim Ahmed and Lorenzo Torresani. Connectivity learning in multi-branch networks. arXiv preprint arXiv:1709.09582, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.09582"
        },
        {
            "id": "Anandalingam_1992_a",
            "entry": "G Anandalingam and TL Friesz. Hierarchical optimization: An introduction. Annals of Operations Research, 34(1):1\u201311, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandalingam%2C%20G.%20Friesz%2C%20T.L.%20Hierarchical%20optimization%3A%20An%20introduction%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandalingam%2C%20G.%20Friesz%2C%20T.L.%20Hierarchical%20optimization%3A%20An%20introduction%201992"
        },
        {
            "id": "Baker_et+al_2017_a",
            "entry": "Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017"
        },
        {
            "id": "Baker_et+al_2018_a",
            "entry": "Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202018"
        },
        {
            "id": "Bender_et+al_2018_a",
            "entry": "Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In International Conference on Machine Learning, pp. 549\u2013558, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018"
        },
        {
            "id": "Brock_et+al_2018_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018"
        },
        {
            "id": "Cai_et+al_2018_a",
            "entry": "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Han%20Chen%2C%20Tianyao%20Zhang%2C%20Weinan%20Yu%2C%20Yong%20Efficient%20architecture%20search%20by%20network%20transformation%202018"
        },
        {
            "id": "Colson_et+al_2007_a",
            "entry": "Beno\u00eet Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of operations research, 153(1):235\u2013256, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Colson%2C%20Beno%C3%AEt%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Colson%2C%20Beno%C3%AEt%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Elsken_et+al_2017_a",
            "entry": "Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for convolutional neural networks. arXiv preprint arXiv:1711.04528, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04528"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pp. 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Franceschi_et+al_2018_a",
            "entry": "Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franceschi%2C%20Luca%20Frasconi%2C%20Paolo%20Salzo%2C%20Saverio%20Pontil%2C%20Massimilano%20Bilevel%20programming%20for%20hyperparameter%20optimization%20and%20meta-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franceschi%2C%20Luca%20Frasconi%2C%20Paolo%20Salzo%2C%20Saverio%20Pontil%2C%20Massimilano%20Bilevel%20programming%20for%20hyperparameter%20optimization%20and%20meta-learning%202018"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In NIPS, pp. 1019\u20131027, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Grave_et+al_2016_a",
            "entry": "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04426"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Inan_et+al_2017_a",
            "entry": "Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Kandasamy_et+al_2018_a",
            "entry": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kandasamy%2C%20Kirthevasan%20Neiswanger%2C%20Willie%20Schneider%2C%20Jeff%20Barnabas%20Poczos%2C%20and%20Eric%20Xing.%20Neural%20architecture%20search%20with%20bayesian%20optimisation%20and%20optimal%20transport%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kandasamy%2C%20Kirthevasan%20Neiswanger%2C%20Willie%20Schneider%2C%20Jeff%20Barnabas%20Poczos%2C%20and%20Eric%20Xing.%20Neural%20architecture%20search%20with%20bayesian%20optimisation%20and%20optimal%20transport%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krause_et+al_2017_a",
            "entry": "Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. arXiv preprint arXiv:1709.07432, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07432"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. ECCV, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018"
        },
        {
            "id": "Liu_et+al_2018_b",
            "entry": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. ICLR, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20representations%20for%20efficient%20architecture%20search%202018"
        },
        {
            "id": "Loshchilov_2016_a",
            "entry": "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.03983"
        },
        {
            "id": "Luketina_et+al_2016_a",
            "entry": "Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In ICML, pp. 2952\u20132960, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luketina%2C%20Jelena%20Berglund%2C%20Mathias%20Greff%2C%20Klaus%20Raiko%2C%20Tapani%20Scalable%20gradient-based%20tuning%20of%20continuous%20regularization%20hyperparameters%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luketina%2C%20Jelena%20Berglund%2C%20Mathias%20Greff%2C%20Klaus%20Raiko%2C%20Tapani%20Scalable%20gradient-based%20tuning%20of%20continuous%20regularization%20hyperparameters%202016"
        },
        {
            "id": "Maclaurin_et+al_2015_a",
            "entry": "Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In ICML, pp. 2113\u20132122, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015"
        },
        {
            "id": "Melis_et+al_2018_a",
            "entry": "G\u00e1bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20lstm%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20lstm%20language%20models%202018"
        },
        {
            "id": "Metz_et+al_2017_a",
            "entry": "Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metz%2C%20Luke%20Poole%2C%20Ben%20Pfau%2C%20David%20Sohl-Dickstein%2C%20Jascha%20Unrolled%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metz%2C%20Luke%20Poole%2C%20Ben%20Pfau%2C%20David%20Sohl-Dickstein%2C%20Jascha%20Unrolled%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Negrinho_2017_a",
            "entry": "Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and training deep architectures. arXiv preprint arXiv:1704.08792, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08792"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Zoph_2018_a",
            "entry": "A large network of 20 cells is trained for 600 epochs with batch size 96. The initial number of channels is increased from 16 to 36 to ensure our model size is comparable with other baselines in the literature (around 3M). Other hyperparameters remain the same as the ones used for architecture search. Following existing works (Pham et al., 2018b; Zoph et al., 2018; Liu et al., 2018a; Real et al., 2018), additional enhancements include cutout (DeVries & Taylor, 2017), path dropout of probability 0.2 and auxiliary towers with weight 0.4. The training takes 1.5 days on a single GPU with our implementation in PyTorch (Paszke et al., 2017). Since the CIFAR results are subject to high variance even with exactly the same setup (Liu et al., 2018b), we report the mean and standard deviation of 10 independent runs for our full model.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%20A%20large%20network%20of%2020%20cells%20is%20trained%20for%20600%20epochs%20with%20batch%20size%2096.%20The%20initial%20number%20of%20channels%20is%20increased%20from%2016%20to%2036%20to%20ensure%20our%20model%20size%20is%20comparable%20with%20other%20baselines%20in%20the%20literature%20%28around%203M%29.%20Other%20hyperparameters%20remain%20the%20same%20as%20the%20ones%20used%20for%20architecture%20search.%20Following%20existing%20works%20%28Pham%20et%20al%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%20A%20large%20network%20of%2020%20cells%20is%20trained%20for%20600%20epochs%20with%20batch%20size%2096.%20The%20initial%20number%20of%20channels%20is%20increased%20from%2016%20to%2036%20to%20ensure%20our%20model%20size%20is%20comparable%20with%20other%20baselines%20in%20the%20literature%20%28around%203M%29.%20Other%20hyperparameters%20remain%20the%20same%20as%20the%20ones%20used%20for%20architecture%20search.%20Following%20existing%20works%20%28Pham%20et%20al%202018"
        },
        {
            "id": "To_2018_a",
            "entry": "To avoid any discrepancy between different implementations or training settings (e.g. the batch sizes), we incorporated the NASNet-A cell (Zoph et al., 2018) and the AmoebaNet-A cell (Real et al., 2018) into our training framework and reported their results under the same settings as our cells.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=To%20avoid%20any%20discrepancy%20between%20different%20implementations%20or%20training%20settings%20eg%20the%20batch%20sizes%20we%20incorporated%20the%20NASNetA%20cell%20Zoph%20et%20al%202018%20and%20the%20AmoebaNetA%20cell%20Real%20et%20al%202018%20into%20our%20training%20framework%20and%20reported%20their%20results%20under%20the%20same%20settings%20as%20our%20cells"
        },
        {
            "id": "A_2018_a",
            "entry": "A network of 14 cells is trained for 250 epochs with batch size 128, weight decay 3 \u00d7 10\u22125 and initial SGD learning rate 0.1 (decayed by a factor of 0.97 after each epoch). Other hyperparameters follow Zoph et al. (2018); Real et al. (2018); Liu et al. (2018a)4. The training takes 12 days on a single GPU.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20network%20of%2014%20cells%20is%20trained%20for%20250%20epochs%20with%20batch%20size%20128%20weight%20decay%203%20%20105%20and%20initial%20SGD%20learning%20rate%2001%20decayed%20by%20a%20factor%20of%20097%20after%20each%20epoch%20Other%20hyperparameters%20follow%20Zoph%20et%20al%202018%20Real%20et%20al%202018%20Liu%20et%20al%202018a4%20The%20training%20takes%2012%20days%20on%20a%20single%20GPU",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20network%20of%2014%20cells%20is%20trained%20for%20250%20epochs%20with%20batch%20size%20128%20weight%20decay%203%20%20105%20and%20initial%20SGD%20learning%20rate%2001%20decayed%20by%20a%20factor%20of%20097%20after%20each%20epoch%20Other%20hyperparameters%20follow%20Zoph%20et%20al%202018%20Real%20et%20al%202018%20Liu%20et%20al%202018a4%20The%20training%20takes%2012%20days%20on%20a%20single%20GPU"
        },
        {
            "id": "DAGs_2018_b",
            "entry": "DAGs without considering graph isomorphism (recall we have 7 non-zero ops, 2 input nodes, 4 intermediate nodes with 2 predecessors each). Since we are jointly learning both normal and reduction cells, the total number of architectures is approximately (109)2 = 1018. This is greater than the 5.6 \u00d7 1014 of PNAS (Liu et al., 2018a) which learns only a single type of cell.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DAGs%20without%20considering%20graph%20isomorphism%20recall%20we%20have%207%20nonzero%20ops%202%20input%20nodes%204%20intermediate%20nodes%20with%202%20predecessors%20each%20Since%20we%20are%20jointly%20learning%20both%20normal%20and%20reduction%20cells%20the%20total%20number%20of%20architectures%20is%20approximately%201092%20%201018%20This%20is%20greater%20than%20the%2056%20%201014%20of%20PNAS%20Liu%20et%20al%202018a%20which%20learns%20only%20a%20single%20type%20of%20cell",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DAGs%20without%20considering%20graph%20isomorphism%20recall%20we%20have%207%20nonzero%20ops%202%20input%20nodes%204%20intermediate%20nodes%20with%202%20predecessors%20each%20Since%20we%20are%20jointly%20learning%20both%20normal%20and%20reduction%20cells%20the%20total%20number%20of%20architectures%20is%20approximately%201092%20%201018%20This%20is%20greater%20than%20the%2056%20%201014%20of%20PNAS%20Liu%20et%20al%202018a%20which%20learns%20only%20a%20single%20type%20of%20cell"
        }
    ]
}
