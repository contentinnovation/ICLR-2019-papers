{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EMERGENT COORDINATION THROUGH COMPETITION",
        "author": "Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, Thore Graepel DeepMind London, United Kingdom {liusiqi,guylever,jsmerel,stunya,heess,thore}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkG8sjR5Km"
        },
        "abstract": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, populationbased training with co-play can lead to a progression in agents\u2019 behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "PBT": "Population Based Training",
        "MARL": "multi-agent reinforcement learning problem",
        "S0": "state distribution P0"
    },
    "highlights": [
        "Competitive games have been grand challenges for artificial intelligence research since at least the 1950s (<a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\"><a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\">Samuel, 1959</a></a>; <a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\"><a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\">Tesauro, 1995</a></a>; Campbell et al, 2002; Vinyals et al, 2017)",
        "We study a framework for continuous multi-agent reinforcement learning based on decentralized population-based training (PBT) of independent reinforcement learning learners (<a class=\"ref-link\" id=\"cJaderberg_et+al_2017_a\" href=\"#rJaderberg_et+al_2017_a\">Jaderberg et al, 2017</a>; 2018), where individual agents learn off-policy with recurrent memory and decomposed shaping reward channels",
        "On the other hand, shaping rewards tend to induce sub-optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al, 1999</a>; <a class=\"ref-link\" id=\"cPopov_et+al_2017_a\" href=\"#rPopov_et+al_2017_a\">Popov et al, 2017</a>); We show in Figure 5 that this is mitigated by coupling training with hyper-parameter evolution which adaptively adjusts the importance of shaping rewards",
        "We have introduced a new 2v2 soccer domain with simulated physics for continuous multi-agent reinforcement learning research, and used competition between agents in this simple domain to train teams of independent reinforcement learning agents, demonstrating coordinated behavior, including repeated passing motifs",
        "We demonstrated that a framework of distributed population-based-training with continuous control, combined with automatic optimization of shaping reward channels, can learn in this environment end-to-end",
        "Our environment can serve as a platform for multiagent research with continuous physical worlds, and can be scaled to more agents and more complex bodies, which we leave for future research"
    ],
    "key_statements": [
        "Competitive games have been grand challenges for artificial intelligence research since at least the 1950s (<a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\"><a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\">Samuel, 1959</a></a>; <a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\"><a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\">Tesauro, 1995</a></a>; Campbell et al, 2002; Vinyals et al, 2017)",
        "Competitive games possess a natural curriculum property, as observed in <a class=\"ref-link\" id=\"cBansal_et+al_2017_a\" href=\"#rBansal_et+al_2017_a\">Bansal et al (2017</a>), where complex behaviors have the potential to emerge in simple environments as a result of competition between agents, rather than due to increasing difficulty of manually designed tasks",
        "One longstanding challenge in AI has been robot soccer (<a class=\"ref-link\" id=\"cKitano_et+al_1997_a\" href=\"#rKitano_et+al_1997_a\">Kitano et al, 1997</a>), including simulated leagues, which has been tackled with machine learning techniques (<a class=\"ref-link\" id=\"cRiedmiller_et+al_2009_a\" href=\"#rRiedmiller_et+al_2009_a\">Riedmiller et al, 2009</a>; <a class=\"ref-link\" id=\"cMacalpine_2018_a\" href=\"#rMacalpine_2018_a\">MacAlpine & Stone, 2018</a>) but not yet mastered by end-to-end reinforcement learning",
        "We introduce a challenging multi-agent soccer environment, using MuJoCo (<a class=\"ref-link\" id=\"cTodorov_et+al_2012_a\" href=\"#rTodorov_et+al_2012_a\">Todorov et al, 2012</a>) which embeds soccer in a wider universe of possible environments with consistent simulated physics, already used extensively in the machine learning research community (<a class=\"ref-link\" id=\"cHeess_et+al_2016_a\" href=\"#rHeess_et+al_2016_a\">Heess et al, 2016</a>; 2017; <a class=\"ref-link\" id=\"cBansal_et+al_2017_a\" href=\"#rBansal_et+al_2017_a\">Bansal et al, 2017</a>; <a class=\"ref-link\" id=\"cBrockman_et+al_2016_a\" href=\"#rBrockman_et+al_2016_a\">Brockman et al, 2016</a>; <a class=\"ref-link\" id=\"cTassa_et+al_2018_a\" href=\"#rTassa_et+al_2018_a\">Tassa et al, 2018</a>; <a class=\"ref-link\" id=\"cRiedmiller_et+al_2018_a\" href=\"#rRiedmiller_et+al_2018_a\">Riedmiller et al, 2018</a>)",
        "We study a framework for continuous multi-agent reinforcement learning based on decentralized population-based training (PBT) of independent reinforcement learning learners (<a class=\"ref-link\" id=\"cJaderberg_et+al_2017_a\" href=\"#rJaderberg_et+al_2017_a\">Jaderberg et al, 2017</a>; 2018), where individual agents learn off-policy with recurrent memory and decomposed shaping reward channels",
        "Behaviors (e.g. <a class=\"ref-link\" id=\"cLowe_et+al_2017_a\" href=\"#rLowe_et+al_2017_a\">Lowe et al, 2017</a>; <a class=\"ref-link\" id=\"cFoerster_et+al_2016_a\" href=\"#rFoerster_et+al_2016_a\">Foerster et al, 2016</a>), we demonstrate that end-to-end Population Based Training can lead to emergent cooperative behaviors in our soccer domain",
        "We demonstrate that Population Based Training is able to evolve agents\u2019 shaping rewards from myopically optimizing dense individual shaping rewards through to focusing relatively more on long-horizon game rewards, i.e. individual agent\u2019s rewards automatically align more with the team objective over time",
        "On the other hand, shaping rewards tend to induce sub-optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al, 1999</a>; <a class=\"ref-link\" id=\"cPopov_et+al_2017_a\" href=\"#rPopov_et+al_2017_a\">Popov et al, 2017</a>); We show in Figure 5 that this is mitigated by coupling training with hyper-parameter evolution which adaptively adjusts the importance of shaping rewards",
        "The population-based training we use here was introduced by <a class=\"ref-link\" id=\"cJaderberg_et+al_2018_a\" href=\"#rJaderberg_et+al_2018_a\">Jaderberg et al (2018</a>) for the capturethe-flag domain, whereas our implementation is for continuous control in simulated physics which is less visually rich but arguably more open-ended, with potential for sophisticated behaviors generally and allows us to focus on complex multi-agent interactions, which may often be physically observable and interpretable",
        "We have introduced a new 2v2 soccer domain with simulated physics for continuous multi-agent reinforcement learning research, and used competition between agents in this simple domain to train teams of independent reinforcement learning agents, demonstrating coordinated behavior, including repeated passing motifs",
        "We demonstrated that a framework of distributed population-based-training with continuous control, combined with automatic optimization of shaping reward channels, can learn in this environment end-to-end",
        "We introduced the idea of automatically optimizing separate discount factors for the shaping rewards, to facilitate the transition from myopically optimizing shaping rewards towards alignment with the sparse long-horizon team rewards and corresponding cooperative behavior",
        "Our environment can serve as a platform for multiagent research with continuous physical worlds, and can be scaled to more agents and more complex bodies, which we leave for future research"
    ],
    "summary": [
        "Competitive games have been grand challenges for artificial intelligence research since at least the 1950s (<a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\"><a class=\"ref-link\" id=\"cSamuel_1959_a\" href=\"#rSamuel_1959_a\">Samuel, 1959</a></a>; <a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\"><a class=\"ref-link\" id=\"cTesauro_1995_a\" href=\"#rTesauro_1995_a\">Tesauro, 1995</a></a>; Campbell et al, 2002; Vinyals et al, 2017).",
        "We focus here on multi-agent interaction by using relatively simple bodies with a 3-dimensional action space.1 We use this environment to examine continuous multiagent reinforcement learning and some of its challenges including coordination, use of shaping rewards, exploitability and evaluation.",
        "We study a framework for continuous multi-agent RL based on decentralized population-based training (PBT) of independent RL learners (<a class=\"ref-link\" id=\"cJaderberg_et+al_2017_a\" href=\"#rJaderberg_et+al_2017_a\">Jaderberg et al, 2017</a>; 2018), where individual agents learn off-policy with recurrent memory and decomposed shaping reward channels.",
        "We seek a method of training agents which addresses the exploitability issues of competitive games, arising from overfitting to a single opponents policy, and provides a method of automatically optimizing hyperparameters and shaping rewards online, which are otherwise hard to tune.",
        "Team rewards can be difficult to co-optimize due to complex credit assignment, and can result in degenerate behavior where one agent learns a reasonable policy before its teammate, discouraging exploration which could interfere with the first agent\u2019s behavior as observed by <a class=\"ref-link\" id=\"cHausknecht_2016_a\" href=\"#rHausknecht_2016_a\">Hausknecht (2016</a>).",
        "We further show that population-based training with co-play and reward shaping induces a progression from random to simple ball chasing and coordinated behaviors.",
        "We annotate a number of algorithmic components as follows: ff: feedforward policy and action-value estimator; evo: population-based training with agents evolving within the population; rwd shp: providing dense shaping rewards on top of sparse environment scoring/conceding rewards; lstm: recurrent policy with recurrent action-value estimator; lstm q: feedforward policy with recurrent action-value estimator; channels: decomposed action-value estimation for each reward component; each with its own, individually evolving discount factor.",
        "The population-based training we use here was introduced by <a class=\"ref-link\" id=\"cJaderberg_et+al_2018_a\" href=\"#rJaderberg_et+al_2018_a\">Jaderberg et al (2018</a>) for the capturethe-flag domain, whereas our implementation is for continuous control in simulated physics which is less visually rich but arguably more open-ended, with potential for sophisticated behaviors generally and allows us to focus on complex multi-agent interactions, which may often be physically observable and interpretable.",
        "We have introduced a new 2v2 soccer domain with simulated physics for continuous multi-agent reinforcement learning research, and used competition between agents in this simple domain to train teams of independent RL agents, demonstrating coordinated behavior, including repeated passing motifs.",
        "We demonstrated that a framework of distributed population-based-training with continuous control, combined with automatic optimization of shaping reward channels, can learn in this environment end-to-end.",
        "Our environment can serve as a platform for multiagent research with continuous physical worlds, and can be scaled to more agents and more complex bodies, which we leave for future research"
    ],
    "headline": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics",
    "reference_links": [
        {
            "id": "Al-Shedivat_et+al_2017_a",
            "entry": "Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. arXiv preprint arXiv:1710.03641, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03641"
        },
        {
            "id": "Bagnell_2005_a",
            "entry": "J. Andrew Bagnell and Andrew Y. Ng. On local rewards and scaling distributed reinforcement learning. In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], pp. 91\u201398, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagnell%2C%20J.Andrew%20Ng%2C%20Andrew%20Y.%20On%20local%20rewards%20and%20scaling%20distributed%20reinforcement%20learning%202005-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bagnell%2C%20J.Andrew%20Ng%2C%20Andrew%20Y.%20On%20local%20rewards%20and%20scaling%20distributed%20reinforcement%20learning%202005-12-05"
        },
        {
            "id": "Balduzzi_et+al_2018_a",
            "entry": "David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. arXiv preprint arXiv:1806.02643, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02643"
        },
        {
            "id": "Bansal_et+al_2017_a",
            "entry": "Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03748"
        },
        {
            "id": "Brafman_2001_a",
            "entry": "Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for nearoptimal reinforcement learning. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp. 953\u2013958, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brafman%2C%20Ronen%20I.%20Tennenholtz%2C%20Moshe%20R-MAX%20-%20A%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202001-08-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brafman%2C%20Ronen%20I.%20Tennenholtz%2C%20Moshe%20R-MAX%20-%20A%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202001-08-04"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Murray_2002_a",
            "entry": "Murray Campbell, A. Joseph Hoane Jr., and Feng-hsiung Hsu. Deep blue. Artif. Intell., 134(1-2): 57\u201383, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murray%20Campbell%2C%20A.%20Joseph%20Hoane%20Jr.%20Hsu%2C%20Feng-hsiung%20Deep%20blue%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murray%20Campbell%2C%20A.%20Joseph%20Hoane%20Jr.%20Hsu%2C%20Feng-hsiung%20Deep%20blue%202002"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). CoRR, abs/1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Elo_1978_a",
            "entry": "Arpad E. Elo. The rating of chessplayers, past and present. Arco Pub., New York, 1978. ISBN 0668047216 9780668047210. URL http://www.amazon.com/ Rating-Chess-Players-Past-Present/dp/0668047216.",
            "url": "http://www.amazon.com/Rating-Chess-Players-Past-Present/dp/0668047216"
        },
        {
            "id": "Foerster_et+al_2016_a",
            "entry": "Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2137\u20132145, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Assael%2C%20Yannis%20M.%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Assael%2C%20Yannis%20M.%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016-12-05"
        },
        {
            "id": "Foerster_et+al_2017_a",
            "entry": "Jakob N. Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip H. S. Torr, Pushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1146\u20131155, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Nardelli%2C%20Nantas%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Stabilising%20experience%20replay%20for%20deep%20multi-agent%20reinforcement%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Nardelli%2C%20Nantas%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Stabilising%20experience%20replay%20for%20deep%20multi-agent%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Foerster_et+al_2018_a",
            "entry": "Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018-02-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018-02-02"
        },
        {
            "id": "Hausknecht_2016_a",
            "entry": "Matthew John Hausknecht. Cooperation and communication in multiagent deep reinforcement learning. PhD thesis, University of Texas at Austin, Austin, USA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausknecht%2C%20Matthew%20John%20Cooperation%20and%20communication%20in%20multiagent%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "Heess_et+al_0000_a",
            "entry": "Nicolas Heess, Jonathan J. Hunt, Timothy P. Lillicrap, and David Silver. Memory-based control with recurrent neural networks. CoRR, abs/1512.04455, 2015a.",
            "arxiv_url": "https://arxiv.org/pdf/1512.04455"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Heess_et+al_2016_a",
            "entry": "Nicolas Heess, Gregory Wayne, Yuval Tassa, Timothy P. Lillicrap, Martin A. Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. CoRR, abs/1610.05182, 2016. URL http://arxiv.org/abs/1610.05182.",
            "url": "http://arxiv.org/abs/1610.05182",
            "arxiv_url": "https://arxiv.org/pdf/1610.05182"
        },
        {
            "id": "Heess_et+al_2017_a",
            "entry": "Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017. URL http://arxiv.org/abs/1707.02286.",
            "url": "http://arxiv.org/abs/1707.02286",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Heinrich_2016_a",
            "entry": "Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfectinformation games. CoRR, abs/1603.01121, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01121"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "Jaderberg_et+al_2018_a",
            "entry": "Max Jaderberg, Wojciech Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garc\u0131a Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. CoRR, abs/1807.01281, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01281"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kitano_et+al_1997_a",
            "entry": "Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, and Eiichi Osawa. Robocup: The robot world cup initiative. In Agents, pp. 340\u2013347, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitano%2C%20Hiroaki%20Asada%2C%20Minoru%20Kuniyoshi%2C%20Yasuo%20Noda%2C%20Itsuki%20Robocup%3A%20The%20robot%20world%20cup%20initiative%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitano%2C%20Hiroaki%20Asada%2C%20Minoru%20Kuniyoshi%2C%20Yasuo%20Noda%2C%20Itsuki%20Robocup%3A%20The%20robot%20world%20cup%20initiative%201997"
        },
        {
            "id": "Lanctot_et+al_2017_a",
            "entry": "Marc Lanctot, Vin\u0131cius Flores Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 4193\u20134206, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Zambaldi%2C%20Vin%C4%B1cius%20Flores%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20Marc%20Zambaldi%2C%20Vin%C4%B1cius%20Flores%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017-12"
        },
        {
            "id": "Littman_1994_a",
            "entry": "Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In In Proceedings of the Eleventh International Conference on Machine Learning, pp. 157\u2013163. Morgan Kaufmann, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "Liu_et+al_2012_a",
            "entry": "Bingyao Liu, Satinder P. Singh, Richard L. Lewis, and Shiyin Qin. Optimal rewards in multiagent teams. In 2012 IEEE International Conference on Development and Learning and Epigenetic Robotics, ICDL-EPIROB 2012, San Diego, CA, USA, November 7-9, 2012, pp. 1\u20138, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Bingyao%20Singh%2C%20Satinder%20P.%20Lewis%2C%20Richard%20L.%20Qin%2C%20Shiyin%20Optimal%20rewards%20in%20multiagent%20teams%202012-11-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Bingyao%20Singh%2C%20Satinder%20P.%20Lewis%2C%20Richard%20L.%20Qin%2C%20Shiyin%20Optimal%20rewards%20in%20multiagent%20teams%202012-11-07"
        },
        {
            "id": "Lowe_et+al_2017_a",
            "entry": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379\u20136390, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "Macalpine_2018_a",
            "entry": "Patrick MacAlpine and Peter Stone. Overlapping layered learning. Artif. Intell., 254:21\u201343, 2018. doi: 10.1016/j.artint.2017.09.001. URL https://doi.org/10.1016/j.artint.2017.09.001.",
            "crossref": "https://dx.doi.org/10.1016/j.artint.2017.09.001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.artint.2017.09.001"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Moravk_et+al_2017_a",
            "entry": "Matej Moravk, Martin Schmid, Neil Burch, Viliam Lis, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in no-limit poker. 356, 01 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matej%20Moravk%20Martin%20Schmid%20Neil%20Burch%20Viliam%20Lis%20Dustin%20Morrill%20Nolan%20Bard%20Trevor%20Davis%20Kevin%20Waugh%20Michael%20Johanson%20and%20Michael%20Bowling%20Deepstack%20Expertlevel%20artificial%20intelligence%20in%20nolimit%20poker%20356%2001%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matej%20Moravk%20Martin%20Schmid%20Neil%20Burch%20Viliam%20Lis%20Dustin%20Morrill%20Nolan%20Bard%20Trevor%20Davis%20Kevin%20Waugh%20Michael%20Johanson%20and%20Michael%20Bowling%20Deepstack%20Expertlevel%20artificial%20intelligence%20in%20nolimit%20poker%20356%2001%202017"
        },
        {
            "id": "Mordatch_2018_a",
            "entry": "Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018-02-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018-02-02"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999), Bled, Slovenia, June 27 - 30, 1999, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999-06-27"
        },
        {
            "id": "Popov_et+al_2017_a",
            "entry": "Ivaylo Popov, Nicolas Heess, Timothy P. Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin A. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. CoRR, abs/1704.03073, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03073"
        },
        {
            "id": "Riedmiller_et+al_2018_a",
            "entry": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playingsolving sparse reward tasks from scratch. arXiv preprint arXiv:1802.10567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10567"
        },
        {
            "id": "Riedmiller_et+al_2009_a",
            "entry": "Martin A. Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange. Reinforcement learning for robot soccer. Auton. Robots, 27(1):55\u201373, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riedmiller%2C%20Martin%20A.%20Gabel%2C%20Thomas%20Hafner%2C%20Roland%20Lange%2C%20Sascha%20Reinforcement%20learning%20for%20robot%20soccer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riedmiller%2C%20Martin%20A.%20Gabel%2C%20Thomas%20Hafner%2C%20Roland%20Lange%2C%20Sascha%20Reinforcement%20learning%20for%20robot%20soccer%202009"
        },
        {
            "id": "Samuel_1959_a",
            "entry": "A. L. Samuel. Some studies in machine learning using the game of checkers. IBM J. Res. Dev., 3 (3):210\u2013229, July 1959. ISSN 0018-8646.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samuel%2C%20A.L.%20Some%20studies%20in%20machine%20learning%20using%20the%20game%20of%20checkers%201959-07-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samuel%2C%20A.L.%20Some%20studies%20in%20machine%20learning%20using%20the%20game%20of%20checkers%201959-07-03"
        },
        {
            "id": "Shapley_1953_a",
            "entry": "L. S. Shapley. Stochastic games. Proceedings of the National Academy of Sciences of the United States of America, 39(10):1095\u20131100, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapley%2C%20L.S.%20Stochastic%20games%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapley%2C%20L.S.%20Stochastic%20games%201953"
        },
        {
            "id": "Silver_et+al_2014_a",
            "entry": "David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 387\u2013395, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014-06-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014-06-21"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Sukhbaatar_et+al_2016_a",
            "entry": "Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2244\u20132252, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016-12-05"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.%20Barto%2C%20A.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "Tassa_et+al_2018_a",
            "entry": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        },
        {
            "id": "Tesauro_1995_a",
            "entry": "G. Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38(3):58\u201368, March 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tesauro%2C%20G.%20Temporal%20difference%20learning%20and%20td-gammon%201995-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tesauro%2C%20G.%20Temporal%20difference%20learning%20and%20td-gammon%201995-03"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026\u20135033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012-10"
        },
        {
            "id": "Tuyls_et+al_2018_a",
            "entry": "Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A generalised method for empirical game theoretic analysis. arXiv preprint arXiv:1803.06376, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06376"
        },
        {
            "id": "Vinyals_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04782"
        },
        {
            "id": "The_2015_a",
            "entry": "The Stochastic Value Gradients (SVG0) algorithm used throughout this work is a special case of the family of policy gradient algorithms provided by Heess et al. (2015b) in which the gradient of a value function used to compute the policy gradient, and is closely related to the Deterministic Policy Gradient algorithm (DPG) (Silver et al., 2014), which is itself a special case of SVG0. For clarity we provide the specific derivation of SVG0 here.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20Stochastic%20Value%20Gradients%20SVG0%20algorithm%20used%20throughout%20this%20work%20is%20a%20special%20case%20of%20the%20family%20of%20policy%20gradient%20algorithms%20provided%20by%20Heess%20et%20al%202015b%20in%20which%20the%20gradient%20of%20a%20value%20function%20used%20to%20compute%20the%20policy%20gradient%20and%20is%20closely%20related%20to%20the%20Deterministic%20Policy%20Gradient%20algorithm%20DPG%20Silver%20et%20al%202014%20which%20is%20itself%20a%20special%20case%20of%20SVG0%20For%20clarity%20we%20provide%20the%20specific%20derivation%20of%20SVG0%20here",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20Stochastic%20Value%20Gradients%20SVG0%20algorithm%20used%20throughout%20this%20work%20is%20a%20special%20case%20of%20the%20family%20of%20policy%20gradient%20algorithms%20provided%20by%20Heess%20et%20al%202015b%20in%20which%20the%20gradient%20of%20a%20value%20function%20used%20to%20compute%20the%20policy%20gradient%20and%20is%20closely%20related%20to%20the%20Deterministic%20Policy%20Gradient%20algorithm%20DPG%20Silver%20et%20al%202014%20which%20is%20itself%20a%20special%20case%20of%20SVG0%20For%20clarity%20we%20provide%20the%20specific%20derivation%20of%20SVG0%20here"
        },
        {
            "id": "Using_2015_b",
            "entry": "Using the reparametrization method of Heess et al. (2015b) we write a stochastic policy \u03c0\u03b8(\u00b7|s) as a deterministic policy \u03bc\u03b8: S \u00d7 R \u2192 A further conditioned on a random variable \u03b7 \u2208 Rp, so that a \u223c \u03c0\u03b8(\u00b7|s) is equivalent to a \u223c \u03bc\u03b8(s, \u03b7), where \u03b7 \u223c \u03c1 for some distribution \u03c1. Then, Q\u03c0\u03b8 (s, a) = r(s, a) + \u03b3Es \u223cP (\u00b7|s,a) Ea \u223c\u03c0(\u00b7|s ) [Q\u03c0\u03b8 (s, a )]",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Using%20the%20reparametrization%20method%20of%20Heess%20et%20al%202015b%20we%20write%20a%20stochastic%20policy%20%CF%80%CE%B8s%20as%20a%20deterministic%20policy%20%CE%BC%CE%B8%20S%20%20R%20%20A%20further%20conditioned%20on%20a%20random%20variable%20%CE%B7%20%20Rp%20so%20that%20a%20%20%CF%80%CE%B8s%20is%20equivalent%20to%20a%20%20%CE%BC%CE%B8s%20%CE%B7%20where%20%CE%B7%20%20%CF%81%20for%20some%20distribution%20%CF%81%20Then%20Q%CF%80%CE%B8%20s%20a%20%20rs%20a%20%20%CE%B3Es%20P%20sa%20Ea%20%CF%80s%20%20Q%CF%80%CE%B8%20s%20a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Using%20the%20reparametrization%20method%20of%20Heess%20et%20al%202015b%20we%20write%20a%20stochastic%20policy%20%CF%80%CE%B8s%20as%20a%20deterministic%20policy%20%CE%BC%CE%B8%20S%20%20R%20%20A%20further%20conditioned%20on%20a%20random%20variable%20%CE%B7%20%20Rp%20so%20that%20a%20%20%CF%80%CE%B8s%20is%20equivalent%20to%20a%20%20%CE%BC%CE%B8s%20%CE%B7%20where%20%CE%B7%20%20%CF%81%20for%20some%20distribution%20%CF%81%20Then%20Q%CF%80%CE%B8%20s%20a%20%20rs%20a%20%20%CE%B3Es%20P%20sa%20Ea%20%CF%80s%20%20Q%CF%80%CE%B8%20s%20a"
        },
        {
            "id": "at_2015_c",
            "entry": "at timestep t following the policy. Typically \u03b3 is replaced with 1 in the definition of \u03b6 to avoid discounting terms depending on future states in the gradient too severely. This suggests Algorithm 3 given in Heess et al. (2015b). For details on recurrent policies see Heess et al. (2015a).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=at%20timestep%20t%20following%20the%20policy%20Typically%20%CE%B3%20is%20replaced%20with%201%20in%20the%20definition%20of%20%CE%B6%20to%20avoid%20discounting%20terms%20depending%20on%20future%20states%20in%20the%20gradient%20too%20severely%20This%20suggests%20Algorithm%203%20given%20in%20Heess%20et%20al%202015b%20For%20details%20on%20recurrent%20policies%20see%20Heess%20et%20al%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=at%20timestep%20t%20following%20the%20policy%20Typically%20%CE%B3%20is%20replaced%20with%201%20in%20the%20definition%20of%20%CE%B6%20to%20avoid%20discounting%20terms%20depending%20on%20future%20states%20in%20the%20gradient%20too%20severely%20This%20suggests%20Algorithm%203%20given%20in%20Heess%20et%20al%202015b%20For%20details%20on%20recurrent%20policies%20see%20Heess%20et%20al%202015a"
        },
        {
            "id": "Algorithm_2015_d",
            "entry": "Algorithm 2 Off-policy SVG0 algorithm (Heess et al., 2015b).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Algorithm%202%20Offpolicy%20SVG0%20algorithm%20Heess%20et%20al%202015b"
        },
        {
            "id": "Where_2015_a",
            "entry": "where, for stability, Q(\u00b7, \u00b7; \u03c8): X \u00d7A \u2192 R and \u03c0are target network and policies (Mnih et al., 2015)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=where%2C%20for%20stability%20Q.%20%C2%B7%3B%20%CF%88%29%3A%20X%20%C3%97A%20%E2%86%92%20R%20and%20%CF%80are%20target%20network%20and%20policies%202015"
        },
        {
            "id": "1",
            "entry": "1. In our soccer experiments k = 40. Though we use off-policy corrections, the replay buffer has a threshold, to ensure that data is relatively recent. We use Elo rating (Elo (1978)), introduced to evaluate the strength of human chess players, to measure an agent\u2019s performance within the population of learning agents and determine eligibility for evolution. Elo is updated from pairwise match results and can be used to predict expected win rates against the other members of the population. For a given pair of agents i, j (or a pair of agent teams), selo estimates the expected win rate of agent i playing against agent j. We show in Algorithm 3 the update rule for a two player competitive game for simplicity, for a team of multiple players, we use their average Elo score instead.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20our%20soccer%20experiments%20k%20%2040%20Though%20we%20use%20offpolicy%20corrections%20the%20replay%20buffer%20has%20a%20threshold%20to%20ensure%20that%20data%20is%20relatively%20recent%20We%20use%20Elo%20rating%20Elo%201978%20introduced%20to%20evaluate%20the%20strength%20of%20human%20chess%20players%20to%20measure%20an%20agents%20performance%20within%20the%20population%20of%20learning%20agents%20and%20determine%20eligibility%20for%20evolution%20Elo%20is%20updated%20from%20pairwise%20match%20results%20and%20can%20be%20used%20to%20predict%20expected%20win%20rates%20against%20the%20other%20members%20of%20the%20population%20For%20a%20given%20pair%20of%20agents%20i%20j%20or%20a%20pair%20of%20agent%20teams%20selo%20estimates%20the%20expected%20win%20rate%20of%20agent%20i%20playing%20against%20agent%20j%20We%20show%20in%20Algorithm%203%20the%20update%20rule%20for%20a%20two%20player%20competitive%20game%20for%20simplicity%20for%20a%20team%20of%20multiple%20players%20we%20use%20their%20average%20Elo%20score%20instead",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20our%20soccer%20experiments%20k%20%2040%20Though%20we%20use%20offpolicy%20corrections%20the%20replay%20buffer%20has%20a%20threshold%20to%20ensure%20that%20data%20is%20relatively%20recent%20We%20use%20Elo%20rating%20Elo%201978%20introduced%20to%20evaluate%20the%20strength%20of%20human%20chess%20players%20to%20measure%20an%20agents%20performance%20within%20the%20population%20of%20learning%20agents%20and%20determine%20eligibility%20for%20evolution%20Elo%20is%20updated%20from%20pairwise%20match%20results%20and%20can%20be%20used%20to%20predict%20expected%20win%20rates%20against%20the%20other%20members%20of%20the%20population%20For%20a%20given%20pair%20of%20agents%20i%20j%20or%20a%20pair%20of%20agent%20teams%20selo%20estimates%20the%20expected%20win%20rate%20of%20agent%20i%20playing%20against%20agent%20j%20We%20show%20in%20Algorithm%203%20the%20update%20rule%20for%20a%20two%20player%20competitive%20game%20for%20simplicity%20for%20a%20team%20of%20multiple%20players%20we%20use%20their%20average%20Elo%20score%20instead"
        },
        {
            "id": "To_2017_a",
            "entry": "To limit the frequency of evolution and prevent premature convergence of the population, we adopted the same eligibility criteria introduced in Jaderberg et al. (2017). In particular, we consider an agent i eligible for evolution if it has: 1. processed 2 \u00d7 109 frames for learning since the beginning of training; and 2. processed 4 \u00d7 108 frames for learning since the last time it became eligible for evolution.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=To%20limit%20the%20frequency%20of%20evolution%20and%20prevent%20premature%20convergence%20of%20the%20population%20we%20adopted%20the%20same%20eligibility%20criteria%20introduced%20in%20Jaderberg%20et%20al%202017%20In%20particular%20we%20consider%20an%20agent%20i%20eligible%20for%20evolution%20if%20it%20has%201%20processed%202%20%20109%20frames%20for%20learning%20since%20the%20beginning%20of%20training%20and%202%20processed%204%20%20108%20frames%20for%20learning%20since%20the%20last%20time%20it%20became%20eligible%20for%20evolution",
            "oa_query": "https://api.scholarcy.com/oa_version?query=To%20limit%20the%20frequency%20of%20evolution%20and%20prevent%20premature%20convergence%20of%20the%20population%20we%20adopted%20the%20same%20eligibility%20criteria%20introduced%20in%20Jaderberg%20et%20al%202017%20In%20particular%20we%20consider%20an%20agent%20i%20eligible%20for%20evolution%20if%20it%20has%201%20processed%202%20%20109%20frames%20for%20learning%20since%20the%20beginning%20of%20training%20and%202%20processed%204%20%20108%20frames%20for%20learning%20since%20the%20last%20time%20it%20became%20eligible%20for%20evolution"
        }
    ]
}
