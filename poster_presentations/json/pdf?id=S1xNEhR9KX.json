{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS",
        "author": "Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, Ruitong Huang Borealis AI Canada",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1xNEhR9KX"
        },
        "abstract": "Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "bMNIST": "binarized MNIST dataset",
        "PGD": "projected gradient descent"
    },
    "highlights": [
        "Neural networks have been demonstrated to be vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>; <a class=\"ref-link\" id=\"cBiggio_et+al_2013_a\" href=\"#rBiggio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBiggio_et+al_2013_a\" href=\"#rBiggio_et+al_2013_a\">Biggio et al, 2013</a></a>)",
        "Our experiments shows that the learned model achieves 3.00% provably robust error on binarized MNIST dataset test data, while maintaining 97.65% clean accuracy",
        "We evaluate the classification performance using the test accuracy of standardly trained models on clean unperturbed examples, and the robustness using the robust accuracy of projected gradient descent trained model, which is the accuracy on adversarially perturbed examples",
        "Given adversarial robustness\u2019 sensitivity to input distribution, we further demonstrate two practical implications: 1) Robust accuracy could be sensitive to image acquisition condition and preprocessing",
        "In this paper we provided theoretical analyses to show the significance of input data distribution in adversarial robustness, which further motivated our systematic experiments on MNIST and CIFAR10 variants",
        "We demonstrated the practical implications of our finding that the existence of such sensitivity questions the reliability in evaluating robust learning algorithms on particular datasets"
    ],
    "key_statements": [
        "Neural networks have been demonstrated to be vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cBiggio_et+al_2013_a\" href=\"#rBiggio_et+al_2013_a\">Biggio et al, 2013</a>)",
        "Since the first discovery of adversarial examples, great progress has been made in constructing stronger adversarial attacks (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>; <a class=\"ref-link\" id=\"cMoosavi-Dezfooli_et+al_2016_a\" href=\"#rMoosavi-Dezfooli_et+al_2016_a\">Moosavi-Dezfooli et al, 2016</a>; <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>; <a class=\"ref-link\" id=\"cCarlini_2017_a\" href=\"#rCarlini_2017_a\">Carlini and Wagner, 2017</a>)",
        "We illustrate the practical implications in Section 4 with two practical examples: 1) the robust accuracy of projected gradient descent trained model is sensitive to gamma values of gamma-corrected CIFAR10 images",
        "We examine the performance of a verifiable defense method on binarized MNIST, and the result suggests the exact opposite: provable adversarial robustness on a MNISTlike dataset is achievable",
        "Our experiments shows that the learned model achieves 3.00% provably robust error on binarized MNIST dataset test data, while maintaining 97.65% clean accuracy",
        "We evaluate the classification performance using the test accuracy of standardly trained models on clean unperturbed examples, and the robustness using the robust accuracy of projected gradient descent trained model, which is the accuracy on adversarially perturbed examples",
        "Not directly indicating robustness, we report the clean accuracy on projected gradient descent trained models to indicate the tradeoff between being accurate and robust",
        "To understand whether low robust accuracy is due to low clean accuracy or vulnerability of model, we report robustness w.r.t. predictions, where the attack is used to perturb against the model\u2019s clean prediction, instead of the true label",
        "Given adversarial robustness\u2019 sensitivity to input distribution, we further demonstrate two practical implications: 1) Robust accuracy could be sensitive to image acquisition condition and preprocessing",
        "FMNIST\u2019s richer semantics makes it better resembles natural images\u2019 pixel value distribution, which could lead to increased difficulty in achieving fMNIST: accuracy, standard training, 92.7% accuracy, projected gradient descent training, 81.2%",
        "We found that the expected difference is not observed, which serves as evidence that differences in perturbable volume are not causing the differences in robustness on the tested MNIST and CIFAR10 variants.\n5.2",
        "In this paper we provided theoretical analyses to show the significance of input data distribution in adversarial robustness, which further motivated our systematic experiments on MNIST and CIFAR10 variants",
        "We demonstrated the practical implications of our finding that the existence of such sensitivity questions the reliability in evaluating robust learning algorithms on particular datasets"
    ],
    "summary": [
        "Neural networks have been demonstrated to be vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>; <a class=\"ref-link\" id=\"cBiggio_et+al_2013_a\" href=\"#rBiggio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBiggio_et+al_2013_a\" href=\"#rBiggio_et+al_2013_a\">Biggio et al, 2013</a></a>).",
        "We illustrate the practical implications in Section 4 with two practical examples: 1) the robust accuracy of PGD trained model is sensitive to gamma values of gamma-corrected CIFAR10 images.",
        "Our experiments shows that the learned model achieves 3.00% provably robust error on bMNIST test data, while maintaining 97.65% clean accuracy.",
        "The above MNIST experiment and Example 2 suggest the essential role of the data distribution in achieving good robust and clean accuracies.",
        "Our experiments reveal an unexpected phenomenon that while standard learning methods manage to achieve stable clean accuracies across different data distributions under \u201csemantic-lossless\u201d shifts, adversarial training, arguably the most popular method to achieve robust models, loses this",
        "We use the smoothing and saturation operations to manipulate the data distributions of MNIST and CIFAR10, and show empirical results on how data distributions affects robust accuracies of neural networks trained on them.",
        "3.3 SENSITIVITY OF ROBUST ACCURACY TO DATA TRANSFORMATIONS Results on MNIST variants are presented in Figure 2a 6.",
        "Note that for binarized MNIST with adversarial training, the clean accuracy and the robust accuracy are almost the same.",
        "We can conclude that robust accuracy under PGD training is much more sensitive than clean accuracy under standard training to the differences in input data distribution.",
        "For the same adversarial training setting, the robustness measure might change drastically between image datasets with different \u201cexposures\u201d.",
        "FMNIST\u2019s richer semantics makes it better resembles natural images\u2019 pixel value distribution, which could lead to increased difficulty in achieving fMNIST: accuracy, standard training, 92.7% accuracy, PGD training, 81.2%",
        "We find counter examples where datasets having the same inter-class distance exhibit different robust accuracies.",
        "This indicates the complexity of the problem, such that a simple measure like inter-class distance cannot fully characterize robustness property of datasets, at least on the variants of MNIST.",
        "It is unclear how far robust accuracy of PGD trained model is from adversarial Bayes error RR\u2217 for the given data distribution.",
        "These together suggest that having more training data and training a larger model could potentially improve the robust accuracies on CIFAR10 variants.",
        "One interesting phenomenon is that binarized MNIST and \u221e-saturated CIFAR10 has different sample complexity property, despite both being \u201ccornered\u201d datasets.",
        "In this paper we provided theoretical analyses to show the significance of input data distribution in adversarial robustness, which further motivated our systematic experiments on MNIST and CIFAR10 variants."
    ],
    "headline": "We demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution",
    "reference_links": [
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Athalye, A., Carlini, N., and Wagner, D. (2018). Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, pages 274\u2013283.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athalye%2C%20A.%20Carlini%2C%20N.%20Wagner%2C%20D.%20Obfuscated%20gradients%20give%20a%20false%20sense%20of%20security%3A%20Circumventing%20defenses%20to%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athalye%2C%20A.%20Carlini%2C%20N.%20Wagner%2C%20D.%20Obfuscated%20gradients%20give%20a%20false%20sense%20of%20security%3A%20Circumventing%20defenses%20to%20adversarial%20examples%202018"
        },
        {
            "id": "Athalye_et+al_2017_a",
            "entry": "Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. (2017). Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "Ball_1997_a",
            "entry": "Ball, K. (1997). An elementary introduction to modern convex geometry. Flavors of geometry, 31:1\u201358.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ball%2C%20K.%20An%20elementary%20introduction%20to%20modern%20convex%20geometry%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ball%2C%20K.%20An%20elementary%20introduction%20to%20modern%20convex%20geometry%201997"
        },
        {
            "id": "Biggio_et+al_2013_a",
            "entry": "Biggio, B., Corona, I., Maiorca, D., Nelson, B., \u0160rndic, N., Laskov, P., Giacinto, G., and Roli, F. (2013). Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pages 387\u2013402. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20B.%20Corona%2C%20I.%20Maiorca%2C%20D.%20Nelson%2C%20B.%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20B.%20Corona%2C%20I.%20Maiorca%2C%20D.%20Nelson%2C%20B.%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%202013"
        },
        {
            "id": "Canny_1986_a",
            "entry": "Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679\u2013698.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Canny%2C%20J.%20A%20computational%20approach%20to%20edge%20detection.%20IEEE%20Transactions%20on%20pattern%20analysis%20and%20machine%20intelligence%201986"
        },
        {
            "id": "Carlini_2016_a",
            "entry": "Carlini, N. and Wagner, D. (2016). Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04311"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Carlini, N. and Wagner, D. (2017). Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39\u201357. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Ding_et+al_2019_a",
            "entry": "Ding, G. W., Wang, L., and Jin, X. (2019). AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623.",
            "arxiv_url": "https://arxiv.org/pdf/1902.07623"
        },
        {
            "id": "Erraqabi_et+al_2018_a",
            "entry": "Erraqabi, A., Baratin, A., Bengio, Y., and Lacoste-Julien, S. (2018). A3t: Adversarially augmented adversarial training. arXiv preprint arXiv:1801.04055.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04055"
        },
        {
            "id": "Fawzi_et+al_2015_a",
            "entry": "Fawzi, A., Fawzi, O., and Frossard, P. (2015). Analysis of classifiers\u2019 robustness to adversarial perturbations. arXiv preprint arXiv:1502.02590.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02590"
        },
        {
            "id": "Gastaldi_2017_a",
            "entry": "Gastaldi, X. (2017). Shake-shake regularization. arXiv preprint arXiv:1705.07485.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07485"
        },
        {
            "id": "Gilmer_et+al_2018_a",
            "entry": "Gilmer, J., Metz, L., Faghri, F., Schoenholz, S. S., Raghu, M., Wattenberg, M., and Goodfellow, I. (2018). Adversarial spheres. arXiv preprint arXiv:1801.02774.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Huang_et+al_2015_a",
            "entry": "Huang, R., Xu, B., Schuurmans, D., and Szepesv\u00e1ri, C. (2015). Learning with a strong adversary. arXiv preprint arXiv:1511.03034.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03034"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Kurakin, A., Goodfellow, I., and Bengio, S. (2016). Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "Ledoux_2005_a",
            "entry": "Ledoux, M. (2005). The concentration of measure phenomenon. Number 89. American Mathematical Soc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20M.%20The%20concentration%20of%20measure%20phenomenon.%20Number%2089%202005"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Moosavi-Dezfooli_et+al_2016_a",
            "entry": "Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574\u20132582.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Raghunathan, A., Steinhardt, J., and Liang, P. (2018). Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "Schmidt_et+al_2018_a",
            "entry": "Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A. (2018). Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "Sinha_et+al_2017_a",
            "entry": "Sinha, A., Namkoong, H., and Duchi, J. (2017). Certifiable distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. (2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Szeliski_2010_a",
            "entry": "Szeliski, R. (2010). Computer vision: algorithms and applications. Springer Science & Business Media.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szeliski%2C%20R.%20Computer%20vision%3A%20algorithms%20and%20applications%202010"
        },
        {
            "id": "Tsipras_et+al_2019_a",
            "entry": "Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. (2019). Robustness may be at odds with accuracy. In International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsipras%2C%20D.%20Santurkar%2C%20S.%20Engstrom%2C%20L.%20Turner%2C%20A.%20Robustness%20may%20be%20at%20odds%20with%20accuracy%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsipras%2C%20D.%20Santurkar%2C%20S.%20Engstrom%2C%20L.%20Turner%2C%20A.%20Robustness%20may%20be%20at%20odds%20with%20accuracy%202019"
        },
        {
            "id": "Warde-Farley_2016_a",
            "entry": "Warde-Farley, D. and Goodfellow, I. (2016). 11 adversarial perturbations of deep neural networks. page 311.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Warde-Farley%2C%20D.%20Goodfellow%2C%20I.%2011%20adversarial%20perturbations%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "Wong_2018_a",
            "entry": "Wong, E. and Kolter, Z. (2018). Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5283\u20135292.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20E.%20Kolter%2C%20Z.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20E.%20Kolter%2C%20Z.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "Xu_et+al_2009_a",
            "entry": "Xu, H., Caramanis, C., and Mannor, S. (2009). Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10(Jul):1485\u20131510.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20H.%20Caramanis%2C%20C.%20Mannor%2C%20S.%20Robustness%20and%20regularization%20of%20support%20vector%20machines%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20H.%20Caramanis%2C%20C.%20Mannor%2C%20S.%20Robustness%20and%20regularization%20of%20support%20vector%20machines%202009"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Zagoruyko, S. and Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        }
    ]
}
