{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "REGULARIZED LEARNING FOR DOMAIN ADAPTATION UNDER LABEL SHIFTS",
        "author": "Kamyar Azizzadenesheli University of California, Irvine kazizzad@uci.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJl0r3R9KX"
        },
        "abstract": "We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain. We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples. We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class. To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available. Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights. Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods."
    },
    "keywords": [
        {
            "term": "sample selection bias",
            "url": "https://en.wikipedia.org/wiki/sample_selection_bias"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        }
    ],
    "abbreviations": {
        "RLLS": "Regularized Learning of Label Shift",
        "BBSL": "black box shift learning method",
        "MSE": "mean square estimation error"
    },
    "highlights": [
        "When machine learning models are employed \u201cin the wild\u201d, the distribution of the data of interest can be significantly shifted compared to the distribution of the data on which the model was trained",
        "If p(x) = q(x) p(y) = q(y) we denote input variables as x and output variables as y, we p(y|x) = q(y|x) p(x|y) = q(x|y) consider the two following settings: (i) Covariate shift, which assumes that the conditional output distribution is invariant: p(y|x) = q(y|x) between source and target distributions, but the source distribution p(x) changes. Label shift, where the conditional input distribution is invariant: p(x|y) = q(x|y) and p(y) changes from source to target",
        "In order to demonstrate the benefit of the proposed method for practical situations, we empirically study the performance of Regularized Learning of Label Shift and show weight estimation as well as prediction accuracy comparison for a variety of shifts, sample sizes and regularization parameters on the CIFAR-10 and MNIST datasets",
        "Among the various methods to correct for covariate shift, the majority uses the concept of importance weights q(x)/p(x) (Zadrozny, 2004; <a class=\"ref-link\" id=\"cCortes_et+al_2010_a\" href=\"#rCortes_et+al_2010_a\">Cortes et al, 2010</a>; <a class=\"ref-link\" id=\"cCortes_2014_a\" href=\"#rCortes_2014_a\">Cortes & Mohri, 2014</a>; <a class=\"ref-link\" id=\"cShimodaira_2000_a\" href=\"#rShimodaira_2000_a\">Shimodaira, 2000</a>), which are unknown but can be estimated for example via kernel embeddings (<a class=\"ref-link\" id=\"cHuang_et+al_2007_a\" href=\"#rHuang_et+al_2007_a\">Huang et al, 2007</a>; <a class=\"ref-link\" id=\"cGretton_et+al_2009_a\" href=\"#rGretton_et+al_2009_a\">Gretton et al, 2009</a>; 2012; Zhang et al, 2013; Zaremba et al, 2013) or by learning a binary discriminative classifier between source and target (<a class=\"ref-link\" id=\"cLopez-Paz_2016_a\" href=\"#rLopez-Paz_2016_a\">Lopez-Paz & Oquab, 2016</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>)",
        "Regularized Learning of Label Shift is inspired by black box shift learning method, it leads to a more robust importance weight estimator as well as generalization guarantees in particular for the small sample regime, which black box shift learning method does not allow for"
    ],
    "key_statements": [
        "When machine learning models are employed \u201cin the wild\u201d, the distribution of the data of interest can be significantly shifted compared to the distribution of the data on which the model was trained",
        "If p(x) = q(x) p(y) = q(y) we denote input variables as x and output variables as y, we p(y|x) = q(y|x) p(x|y) = q(x|y) consider the two following settings: (i) Covariate shift, which assumes that the conditional output distribution is invariant: p(y|x) = q(y|x) between source and target distributions, but the source distribution p(x) changes. Label shift, where the conditional input distribution is invariant: p(x|y) = q(x|y) and p(y) changes from source to target",
        "In order to demonstrate the benefit of the proposed method for practical situations, we empirically study the performance of Regularized Learning of Label Shift and show weight estimation as well as prediction accuracy comparison for a variety of shifts, sample sizes and regularization parameters on the CIFAR-10 and MNIST datasets",
        "Among the various methods to correct for covariate shift, the majority uses the concept of importance weights q(x)/p(x) (Zadrozny, 2004; <a class=\"ref-link\" id=\"cCortes_et+al_2010_a\" href=\"#rCortes_et+al_2010_a\">Cortes et al, 2010</a>; <a class=\"ref-link\" id=\"cCortes_2014_a\" href=\"#rCortes_2014_a\">Cortes & Mohri, 2014</a>; <a class=\"ref-link\" id=\"cShimodaira_2000_a\" href=\"#rShimodaira_2000_a\">Shimodaira, 2000</a>), which are unknown but can be estimated for example via kernel embeddings (<a class=\"ref-link\" id=\"cHuang_et+al_2007_a\" href=\"#rHuang_et+al_2007_a\">Huang et al, 2007</a>; <a class=\"ref-link\" id=\"cGretton_et+al_2009_a\" href=\"#rGretton_et+al_2009_a\">Gretton et al, 2009</a>; 2012; Zhang et al, 2013; Zaremba et al, 2013) or by learning a binary discriminative classifier between source and target (<a class=\"ref-link\" id=\"cLopez-Paz_2016_a\" href=\"#rLopez-Paz_2016_a\">Lopez-Paz & Oquab, 2016</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>)",
        "Regularized Learning of Label Shift is inspired by black box shift learning method, it leads to a more robust importance weight estimator as well as generalization guarantees in particular for the small sample regime, which black box shift learning method does not allow for"
    ],
    "summary": [
        "When machine learning models are employed \u201cin the wild\u201d, the distribution of the data of interest can be significantly shifted compared to the distribution of the data on which the model was trained.",
        "We propose a novel regularization method to compensate for the high estimation error of the importance weights in low target sample settings.",
        "We derive a dimension-independent generalization bound for the final Regularized Learning under Label Shift (RLLS) classifier based on our weight estimator.",
        "In order to demonstrate the benefit of the proposed method for practical situations, we empirically study the performance of RLLS and show weight estimation as well as prediction accuracy comparison for a variety of shifts, sample sizes and regularization parameters on the CIFAR-10 and MNIST datasets.",
        "When a few samples from the target set are available or the label shift is mild, the estimated weights might be too uncertain to be applied.",
        "We can state a generalization bound for the classifier hw in a general hypothesis class H, which is trained on source data with the estimated weights defined in equation (4).",
        "After artificially shifting the label distribution in one of the source and target sets, we follow algorithm 1, where we choose the black box predictor h0 to be a two-layer fully connected neural network trained on source dataset.",
        "In this set of experiments on the CIFAR10 dataset, we illustrate our weight estimation and prediction performance for Tweak-One source shifts and compare it with BBSL.",
        "The different plots in Figure 4 correspond to black-box predictors h0 for weight estimation which are trained on more or less corrupted data, i.e. have a better or worse conditioned confusion matrix.",
        "Our extensive experiments for many different shifts, black box classifiers and sample sizes do not allow for a final conclusive statement about how weighting samples using our estimator affects predictive results for real-world data in general, as it usually does not fulfill the label-shift assumptions.",
        "Among the various methods to correct for covariate shift, the majority uses the concept of importance weights q(x)/p(x) (Zadrozny, 2004; <a class=\"ref-link\" id=\"cCortes_et+al_2010_a\" href=\"#rCortes_et+al_2010_a\">Cortes et al, 2010</a>; <a class=\"ref-link\" id=\"cCortes_2014_a\" href=\"#rCortes_2014_a\">Cortes & Mohri, 2014</a>; <a class=\"ref-link\" id=\"cShimodaira_2000_a\" href=\"#rShimodaira_2000_a\">Shimodaira, 2000</a>), which are unknown but can be estimated for example via kernel embeddings (<a class=\"ref-link\" id=\"cHuang_et+al_2007_a\" href=\"#rHuang_et+al_2007_a\">Huang et al, 2007</a>; <a class=\"ref-link\" id=\"cGretton_et+al_2009_a\" href=\"#rGretton_et+al_2009_a\">Gretton et al, 2009</a>; 2012; Zhang et al, 2013; Zaremba et al, 2013) or by learning a binary discriminative classifier between source and target (<a class=\"ref-link\" id=\"cLopez-Paz_2016_a\" href=\"#rLopez-Paz_2016_a\">Lopez-Paz & Oquab, 2016</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>).",
        "We establish the first generalization guarantee for the label shift setting and propose an importance weighting procedure for which no prior knowledge of q(y)/p(y) is required."
    ],
    "headline": "We propose Regularized Learning under Label shifts, a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain",
    "reference_links": [
        {
            "id": "Anandkumar_et+al_2012_a",
            "entry": "Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models and hidden markov models. In Conference on Learning Theory, pp. 33\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20A%20method%20of%20moments%20for%20mixture%20models%20and%20hidden%20markov%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20A%20method%20of%20moments%20for%20mixture%20models%20and%20hidden%20markov%20models%202012"
        },
        {
            "id": "Azizzadenesheli_et+al_2016_a",
            "entry": "Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of pomdps using spectral methods. arXiv preprint arXiv:1602.07764, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07764"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Belkin_et+al_2018_a",
            "entry": "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1812.11118"
        },
        {
            "id": "Ben-David_et+al_2010_a",
            "entry": "Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151\u2013175, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010"
        },
        {
            "id": "Beygelzimer_et+al_2009_a",
            "entry": "Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pp. 49\u201356. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beygelzimer%2C%20Alina%20Dasgupta%2C%20Sanjoy%20Langford%2C%20John%20Importance%20weighted%20active%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beygelzimer%2C%20Alina%20Dasgupta%2C%20Sanjoy%20Langford%2C%20John%20Importance%20weighted%20active%20learning%202009"
        },
        {
            "id": "Blanchard_et+al_2010_a",
            "entry": "Gilles Blanchard, Gyemin Lee, and Clayton Scott. Semi-supervised novelty detection. Journal of Machine Learning Research, 11(Nov):2973\u20133009, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchard%2C%20Gilles%20Lee%2C%20Gyemin%20Scott%2C%20Clayton%20Semi-supervised%20novelty%20detection%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchard%2C%20Gilles%20Lee%2C%20Gyemin%20Scott%2C%20Clayton%20Semi-supervised%20novelty%20detection%202010"
        },
        {
            "id": "Buck_1966_a",
            "entry": "AA Buck, JJ Gart, et al. Comparison of a screening test and a reference test in epidemiologic studies. ii. a probabilistic model for the comparison of diagnostic tests. American Journal of Epidemiology, 83(3):593\u2013602, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buck%2C%20A.A.%20Gart%2C%20J.J.%20Comparison%20of%20a%20screening%20test%20and%20a%20reference%20test%20in%20epidemiologic%20studies.%20ii.%20a%20probabilistic%20model%20for%20the%20comparison%20of%20diagnostic%20tests%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buck%2C%20A.A.%20Gart%2C%20J.J.%20Comparison%20of%20a%20screening%20test%20and%20a%20reference%20test%20in%20epidemiologic%20studies.%20ii.%20a%20probabilistic%20model%20for%20the%20comparison%20of%20diagnostic%20tests%201966"
        },
        {
            "id": "Yee_2005_a",
            "entry": "Yee Seng Chan and Hwee Tou Ng. Word sense disambiguation with distribution estimation. In IJCAI, volume 5, pp. 1010\u20135, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yee%20Seng%20Chan%20and%20Hwee%20Tou%20Ng%20Word%20sense%20disambiguation%20with%20distribution%20estimation%20In%20IJCAI%20volume%205%20pp%2010105%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yee%20Seng%20Chan%20and%20Hwee%20Tou%20Ng%20Word%20sense%20disambiguation%20with%20distribution%20estimation%20In%20IJCAI%20volume%205%20pp%2010105%202005"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D Ziebart. Robust covariate shift regression. In Artificial Intelligence and Statistics, pp. 1270\u20131279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xiangli%20Monfort%2C%20Mathew%20Liu%2C%20Anqi%20Ziebart%2C%20Brian%20D.%20Robust%20covariate%20shift%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xiangli%20Monfort%2C%20Mathew%20Liu%2C%20Anqi%20Ziebart%2C%20Brian%20D.%20Robust%20covariate%20shift%20regression%202016"
        },
        {
            "id": "Cortes_2014_a",
            "entry": "Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103\u2013126, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20and%20sample%20bias%20correction%20theory%20and%20algorithm%20for%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20and%20sample%20bias%20correction%20theory%20and%20algorithm%20for%20regression%202014"
        },
        {
            "id": "Cortes_et+al_2010_a",
            "entry": "Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in neural information processing systems, pp. 442\u2013450, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Learning%20bounds%20for%20importance%20weighting%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Learning%20bounds%20for%20importance%20weighting%202010"
        },
        {
            "id": "Crammer_et+al_2008_a",
            "entry": "Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. Journal of Machine Learning Research, 9(Aug):1757\u20131774, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crammer%2C%20Koby%20Kearns%2C%20Michael%20Wortman%2C%20Jennifer%20Learning%20from%20multiple%20sources%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crammer%2C%20Koby%20Kearns%2C%20Michael%20Wortman%2C%20Jennifer%20Learning%20from%20multiple%20sources%202008"
        },
        {
            "id": "Forman_2008_a",
            "entry": "George Forman. Quantifying counts and costs via classification. Data Mining and Knowledge Discovery, 17(2):164\u2013206, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forman%2C%20George%20Quantifying%20counts%20and%20costs%20via%20classification.%20Data%20Mining%20and%20Knowledge%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Forman%2C%20George%20Quantifying%20counts%20and%20costs%20via%20classification.%20Data%20Mining%20and%20Knowledge%202008"
        },
        {
            "id": "Freedman_1975_a",
            "entry": "David A Freedman. On tail probabilities for martingales. the Annals of Probability, pp. 100\u2013118, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freedman%2C%20David%20A.%20On%20tail%20probabilities%20for%20martingales.%20the%20Annals%20of%20Probability%201975"
        },
        {
            "id": "Gretton_et+al_2009_a",
            "entry": "Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and Bernhard Sch\u00f6lkopf. Covariate shift by kernel mean matching. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Smola%2C%20Alexander%20J.%20Huang%2C%20Jiayuan%20Schmittfull%2C%20Marcel%20Covariate%20shift%20by%20kernel%20mean%20matching%202009"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hsu_et+al_2012_a",
            "entry": "Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460\u20131480, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20spectral%20algorithm%20for%20learning%20hidden%20markov%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20spectral%20algorithm%20for%20learning%20hidden%20markov%20models%202012"
        },
        {
            "id": "Huang_et+al_2007_a",
            "entry": "Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Sch\u00f6lkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pp. 601\u2013608, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Jiayuan%20Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Sch%C3%B6lkopf%2C%20Bernhard%20Correcting%20sample%20selection%20bias%20by%20unlabeled%20data%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Jiayuan%20Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Sch%C3%B6lkopf%2C%20Bernhard%20Correcting%20sample%20selection%20bias%20by%20unlabeled%20data%202007"
        },
        {
            "id": "Iyer_et+al_2014_a",
            "entry": "Arun Iyer, Saketha Nath, and Sunita Sarawagi. Maximum mean discrepancy for class ratio estimation: Convergence bounds and kernel selection. In International Conference on Machine Learning, pp. 530\u2013538, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyer%2C%20Arun%20Nath%2C%20Saketha%20Sarawagi%2C%20Sunita%20Maximum%20mean%20discrepancy%20for%20class%20ratio%20estimation%3A%20Convergence%20bounds%20and%20kernel%20selection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyer%2C%20Arun%20Nath%2C%20Saketha%20Sarawagi%2C%20Sunita%20Maximum%20mean%20discrepancy%20for%20class%20ratio%20estimation%3A%20Convergence%20bounds%20and%20kernel%20selection%202014"
        },
        {
            "id": "Kakade_et+al_2009_a",
            "entry": "Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pp. 793\u2013800, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009"
        },
        {
            "id": "Krishnamurthy_et+al_2017_a",
            "entry": "Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, and John Langford. Active learning for cost-sensitive classification. arXiv preprint arXiv:1703.01014, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01014"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lecun_2010_a",
            "entry": "Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Lipton_et+al_2018_a",
            "entry": "Zachary C Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and correcting for label shift with black box predictors. arXiv preprint arXiv:1802.03916, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03916"
        },
        {
            "id": "Liu_2014_a",
            "entry": "Anqi Liu and Brian Ziebart. Robust classification under sample selection bias. In Advances in neural information processing systems, pp. 37\u201345, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Anqi%20Ziebart%2C%20Brian%20Robust%20classification%20under%20sample%20selection%20bias%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Anqi%20Ziebart%2C%20Brian%20Robust%20classification%20under%20sample%20selection%20bias%202014"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Song Liu, Akiko Takeda, Taiji Suzuki, and Kenji Fukumizu. Trimmed density ratio estimation. In Advances in Neural Information Processing Systems, pp. 4518\u20134528, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Song%20Takeda%2C%20Akiko%20Suzuki%2C%20Taiji%20Fukumizu%2C%20Kenji%20Trimmed%20density%20ratio%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Song%20Takeda%2C%20Akiko%20Suzuki%2C%20Taiji%20Fukumizu%2C%20Kenji%20Trimmed%20density%20ratio%20estimation%202017"
        },
        {
            "id": "Lopez-Paz_2016_a",
            "entry": "David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06545"
        },
        {
            "id": "Mclachlan_2004_a",
            "entry": "Geoffrey McLachlan. Discriminant analysis and statistical pattern recognition, volume 544. John Wiley & Sons, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20Geoffrey%20Discriminant%20analysis%20and%20statistical%20pattern%20recognition%2C%20volume%20544%202004"
        },
        {
            "id": "Pires_2012_a",
            "entry": "Bernardo Avila Pires and Csaba Szepesv\u00e1ri. Statistical linear estimation with penalized estimators: an application to reinforcement learning. arXiv preprint arXiv:1206.6444, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6444"
        },
        {
            "id": "Ramaswamy_et+al_2016_a",
            "entry": "Harish Ramaswamy, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel embeddings of distributions. In International Conference on Machine Learning, pp. 2052\u20132060, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramaswamy%2C%20Harish%20Scott%2C%20Clayton%20Tewari%2C%20Ambuj%20Mixture%20proportion%20estimation%20via%20kernel%20embeddings%20of%20distributions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramaswamy%2C%20Harish%20Scott%2C%20Clayton%20Tewari%2C%20Ambuj%20Mixture%20proportion%20estimation%20via%20kernel%20embeddings%20of%20distributions%202016"
        },
        {
            "id": "Saerens_et+al_2002_a",
            "entry": "Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural computation, 14(1):21\u201341, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saerens%2C%20Marco%20Latinne%2C%20Patrice%20Decaestecker%2C%20Christine%20Adjusting%20the%20outputs%20of%20a%20classifier%20to%20new%20a%20priori%20probabilities%3A%20a%20simple%20procedure%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saerens%2C%20Marco%20Latinne%2C%20Patrice%20Decaestecker%2C%20Christine%20Adjusting%20the%20outputs%20of%20a%20classifier%20to%20new%20a%20priori%20probabilities%3A%20a%20simple%20procedure%202002"
        },
        {
            "id": "Sanderson_2014_a",
            "entry": "Tyler Sanderson and Clayton Scott. Class proportion estimation with application to multiclass anomaly rejection. In Artificial Intelligence and Statistics, pp. 850\u2013858, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sanderson%2C%20Tyler%20Scott%2C%20Clayton%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sanderson%2C%20Tyler%20Scott%2C%20Clayton%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014"
        },
        {
            "id": "Schoelkopf_et+al_2012_a",
            "entry": "Bernhard Sch\u00f6lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6471"
        },
        {
            "id": "Scott_et+al_2013_a",
            "entry": "Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In Conference On Learning Theory, pp. 489\u2013511, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20Clayton%20Blanchard%2C%20Gilles%20Handy%2C%20Gregory%20Classification%20with%20asymmetric%20label%20noise%3A%20Consistency%20and%20maximal%20denoising.%20In%20Conference%20On%20Learning%20Theory%202013"
        },
        {
            "id": "Shimodaira_2000_a",
            "entry": "Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227\u2013244, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20loglikelihood%20function%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20loglikelihood%20function%202000"
        },
        {
            "id": "Storkey_2009_a",
            "entry": "Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset shift in machine learning, pp. 3\u201328, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Storkey%2C%20Amos%20When%20training%20and%20test%20sets%20are%20different%3A%20characterizing%20learning%20transfer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Storkey%2C%20Amos%20When%20training%20and%20test%20sets%20are%20different%3A%20characterizing%20learning%20transfer%202009"
        },
        {
            "id": "Tropp_2012_a",
            "entry": "Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012"
        },
        {
            "id": "Vapnik_1999_a",
            "entry": "Vladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988\u2013999, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20Naumovich%20An%20overview%20of%20statistical%20learning%20theory%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vapnik%2C%20Vladimir%20Naumovich%20An%20overview%20of%20statistical%20learning%20theory%201999"
        },
        {
            "id": "Wainwright_2019_a",
            "entry": "M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20High-dimensional%20statistics%3A%20A%20non-asymptotic%20viewpoint%202019"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Yiming Ying. Mcdiarmid\u2019s inequalities of bernstein and bennett forms. City University of Hong",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Yiming%20Ying%20Mcdiarmids%20inequalities%20of%20bernstein%20and%20bennett%20forms%20City%20University%20of%20Hong"
        },
        {
            "id": "Kong_2004_a",
            "entry": "Kong, 2004. Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning, pp. 114. ACM, 2004. Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko. B-test: A non-parametric, low variance kernel two-sample test. In Advances in neural information processing systems, pp. 755\u2013763, 2013. Kun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In International Conference on Machine Learning, pp. 819\u2013827, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%202004.%20Bianca%20Zadrozny%20Learning%20and%20evaluating%20classifiers%20under%20sample%20selection%20bias%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%202004.%20Bianca%20Zadrozny%20Learning%20and%20evaluating%20classifiers%20under%20sample%20selection%20bias%202004"
        }
    ]
}
