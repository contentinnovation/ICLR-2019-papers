{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Mostafa Dehghani\u2217\u2020 University of Amsterdam dehghani@uva.nl",
        "author": "Stephan Gouws, DeepMind sgouws@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyzdRiR9Y7"
        },
        "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset."
    },
    "keywords": [
        {
            "term": "inductive bias",
            "url": "https://en.wikipedia.org/wiki/inductive_bias"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "receptive field",
            "url": "https://en.wikipedia.org/wiki/receptive_field"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks",
        "UT": "Universal Transformer",
        "ACT": "Adaptive Computation Time",
        "LTE": "LEARNING TO EXECUTE"
    },
    "highlights": [
        "Convolutional and fully-attentional feed-forward architectures like the Transformer have recently emerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence modeling tasks, notably machine translation (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a>)",
        "We introduce the Universal Transformer (UT), a parallel-in-time recurrent self-attentive sequence model which can be cast as a generalization of the Transformer model, yielding increased theoretical capabilities and improved results on a wide range of challenging sequence-to-sequence tasks",
        "This paper introduces the Universal Transformer, a generalization of the Transformer model that extends its theoretical capabilities and produces state-of-the-art results on a wide range of challenging sequence modeling tasks, such as language understanding but a variety of algorithmic tasks, thereby addressing a key shortcoming of the standard Transformer",
        "The Universal Transformer combines the following key properties into one model: Weight sharing: Following intuitions behind weight sharing found in CNNs and recurrent neural networks, we extend the Transformer with a simple form of weight sharing that strikes an effective balance between inductive bias and model expressivity, which we show extensively on both small and large-scale experiments",
        "Conditional computation: In our goal to build a computationally universal machine, we equipped the Universal Transformer with the ability to halt or continue computation through a recently introduced mechanism, which shows stronger results compared to the fixed-depth Universal Transformer"
    ],
    "key_statements": [
        "Convolutional and fully-attentional feed-forward architectures like the Transformer have recently emerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence modeling tasks, notably machine translation (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a>)",
        "These parallel-in-time architectures address a significant shortcoming of recurrent neural networks, namely their inherently sequential computation which prevents parallelization across elements of the input sequence, whilst still addressing the vanishing gradients problem as the sequence length gets longer (Hochreiter et al, 2003)",
        "We introduce the Universal Transformer (UT), a parallel-in-time recurrent self-attentive sequence model which can be cast as a generalization of the Transformer model, yielding increased theoretical capabilities and improved results on a wide range of challenging sequence-to-sequence tasks",
        "We use one of two different transition functions: either a separable convolution (<a class=\"ref-link\" id=\"cChollet_2016_a\" href=\"#rChollet_2016_a\">Chollet, 2016</a>) or a fully-connected neural network that consists of a single rectified-linear activation function between two affine transformations, applied position-wise, i.e. individually to each row of At",
        "As the per-symbol recurrent transition functions can be applied any number of times, another and possibly more informative way of characterizing the Universal Transformer is as a block of parallel recurrent neural networks evolving per-symbol hidden states concurrently, generated at each step by attending to the sequence of hidden states at the previous step",
        "This paper introduces the Universal Transformer, a generalization of the Transformer model that extends its theoretical capabilities and produces state-of-the-art results on a wide range of challenging sequence modeling tasks, such as language understanding but a variety of algorithmic tasks, thereby addressing a key shortcoming of the standard Transformer",
        "The Universal Transformer combines the following key properties into one model: Weight sharing: Following intuitions behind weight sharing found in CNNs and recurrent neural networks, we extend the Transformer with a simple form of weight sharing that strikes an effective balance between inductive bias and model expressivity, which we show extensively on both small and large-scale experiments",
        "Conditional computation: In our goal to build a computationally universal machine, we equipped the Universal Transformer with the ability to halt or continue computation through a recently introduced mechanism, which shows stronger results compared to the fixed-depth Universal Transformer"
    ],
    "summary": [
        "Convolutional and fully-attentional feed-forward architectures like the Transformer have recently emerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence modeling tasks, notably machine translation (<a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a>).",
        "Position of the sequence in parallel, by combining information from different positions using self-attention and applying a recurrent transition function across all time steps 1 \u2264 t \u2264 T .",
        "The Universal Transformer iteratively refines its representations for all symbols in the sequence in parallel using a self-attention mechanism (Parikh et al, 2016; <a class=\"ref-link\" id=\"cLin_et+al_2017_a\" href=\"#rLin_et+al_2017_a\">Lin et al, 2017</a>), followed by a transformation consisting of a depth-wise separable convolution (<a class=\"ref-link\" id=\"cChollet_2016_a\" href=\"#rChollet_2016_a\"><a class=\"ref-link\" id=\"cChollet_2016_a\" href=\"#rChollet_2016_a\">Chollet, 2016</a></a>; <a class=\"ref-link\" id=\"cKaiser_et+al_2017_a\" href=\"#rKaiser_et+al_2017_a\">Kaiser et al, 2017</a>) or a position-wise fully-connected layer.",
        "The UT iteratively computes representations Ht at step t for all m positions in parallel by applying the multi-headed dot-product self-attention mechanism from <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al (2017</a>), followed by a recurrent transition function.",
        "We use one of two different transition functions: either a separable convolution (<a class=\"ref-link\" id=\"cChollet_2016_a\" href=\"#rChollet_2016_a\"><a class=\"ref-link\" id=\"cChollet_2016_a\" href=\"#rChollet_2016_a\">Chollet, 2016</a></a>) or a fully-connected neural network that consists of a single rectified-linear activation function between two affine transformations, applied position-wise, i.e. individually to each row of At. P t \u2208 Rm\u00d7d above are fixed, constant, two-dimensional coordinate embeddings, obtained by computing the sinusoidal position embedding vectors as defined in (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a>) for the positions 1 \u2264 i \u2264 m and the time-step 1 \u2264 t \u2264 T separately for each vector-dimension 1 \u2264 j \u2264 d, and summing: Pit,2j = sin(i/100002j/d)+sin(t/100002j/d)",
        "As the per-symbol recurrent transition functions can be applied any number of times, another and possibly more informative way of characterizing the UT is as a block of parallel RNNs evolving per-symbol hidden states concurrently, generated at each step by attending to the sequence of hidden states at the previous step.",
        "UTs thereby retain the attractive computational efficiency of the original feedforward Transformer model, but with the added recurrent inductive bias of RNNs. using a dynamic halting mechanism, UTs can choose the number of processing steps based on the input data.",
        "In contrast to these models, which only perform well on algorithmic tasks, the Universal Transformer achieves competitive results on realistic natural language tasks such as LAMBADA and machine translation.",
        "This paper introduces the Universal Transformer, a generalization of the Transformer model that extends its theoretical capabilities and produces state-of-the-art results on a wide range of challenging sequence modeling tasks, such as language understanding but a variety of algorithmic tasks, thereby addressing a key shortcoming of the standard Transformer.",
        "By adding computational capacity and recurrence in processing depth, we hope that further improvements beyond the basic Universal Transformer presented here will help us build learning algorithms that are both more powerful, data efficient, and generalize beyond the current state-of-the-art"
    ],
    "headline": "We propose the Universal Transformer, a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues",
    "reference_links": [
        {
            "id": "Ahmed_et+al_2017_a",
            "entry": "Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer network for machine translation. arXiv preprint arXiv:1711.02132, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02132"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.",
            "url": "http://arxiv.org/abs/1607.06450",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409.0473.",
            "url": "http://arxiv.org/abs/1409.0473",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.",
            "url": "http://arxiv.org/abs/1406.1078",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Chollet_2016_a",
            "entry": "Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02357"
        },
        {
            "id": "Chu_et+al_2017_a",
            "entry": "Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pp. 52\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20Zewei%20Wang%2C%20Hai%20Gimpel%2C%20Kevin%20McAllester%2C%20David%20Broad%20context%20language%20modeling%20as%20reading%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20Zewei%20Wang%2C%20Hai%20Gimpel%2C%20Kevin%20McAllester%2C%20David%20Broad%20context%20language%20modeling%20as%20reading%20comprehension%202017"
        },
        {
            "id": "Dhingra_et+al_2017_a",
            "entry": "Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02620"
        },
        {
            "id": "Dhingra_et+al_2018_a",
            "entry": "Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05922"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL http://arxiv.org/abs/1705.03122.",
            "url": "http://arxiv.org/abs/1705.03122",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "Grave_et+al_2016_a",
            "entry": "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04426"
        },
        {
            "id": "Graves_2013_a",
            "entry": "Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850.",
            "url": "http://arxiv.org/abs/1308.0850",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "Graves_2016_a",
            "entry": "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08983"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401.",
            "url": "http://arxiv.org/abs/1410.5401",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Gulcehre_et+al_2018_a",
            "entry": "Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09786"
        },
        {
            "id": "Henaff_et+al_2016_a",
            "entry": "Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03969"
        },
        {
            "id": "Sepp_2003_a",
            "entry": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sepp%20Hochreiter%2C%20Yoshua%20Bengio%2C%20Paolo%20Frasconi%20Schmidhuber%2C%20J%C3%BCrgen%20Gradient%20flow%20in%20recurrent%20nets%3A%20the%20difficulty%20of%20learning%20long-term%20dependencies.%20A%20Field%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sepp%20Hochreiter%2C%20Yoshua%20Bengio%2C%20Paolo%20Frasconi%20Schmidhuber%2C%20J%C3%BCrgen%20Gradient%20flow%20in%20recurrent%20nets%3A%20the%20difficulty%20of%20learning%20long-term%20dependencies.%20A%20Field%202003"
        },
        {
            "id": "Joulin_2015_a",
            "entry": "A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20A.%20Mikolov%2C%20T.%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20A.%20Mikolov%2C%20T.%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015"
        },
        {
            "id": "Kaiser_2016_a",
            "entry": "\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228.",
            "url": "https://arxiv.org/abs/1511.08228",
            "arxiv_url": "https://arxiv.org/pdf/1511.08228"
        },
        {
            "id": "Kaiser_et+al_2017_a",
            "entry": "\u0141ukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. CoRR, abs/1706.03059, 2017. URL http://arxiv.org/abs/1706.03059.",
            "url": "http://arxiv.org/abs/1706.03059",
            "arxiv_url": "https://arxiv.org/pdf/1706.03059"
        },
        {
            "id": "Kumar_et+al_2016_a",
            "entry": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378\u20131387, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03130"
        },
        {
            "id": "Linzen_et+al_2016_a",
            "entry": "Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Association of Computational Linguistics, 4(1):521\u2013535, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20lstms%20to%20learn%20syntax-sensitive%20dependencies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20lstms%20to%20learn%20syntax-sensitive%20dependencies%202016"
        },
        {
            "id": "Paperno_et+al_2016_a",
            "entry": "Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1525\u20131534, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paperno%2C%20Denis%20Kruszewski%2C%20Germ%C3%A1n%20Lazaridou%2C%20Angeliki%20Pham%2C%20Ngoc%20Quan%20The%20lambada%20dataset%3A%20Word%20prediction%20requiring%20a%20broad%20discourse%20context%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paperno%2C%20Denis%20Kruszewski%2C%20Germ%C3%A1n%20Lazaridou%2C%20Angeliki%20Pham%2C%20Ngoc%20Quan%20The%20lambada%20dataset%3A%20Word%20prediction%20requiring%20a%20broad%20discourse%20context%202016"
        },
        {
            "id": "Ankur_2016_a",
            "entry": "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. URL https://arxiv.org/pdf/1606.01933.pdf.",
            "url": "https://arxiv.org/pdf/1606.01933.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1606.01933"
        },
        {
            "id": "Rae_et+al_2016_a",
            "entry": "Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pp. 3621\u20133629, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016"
        },
        {
            "id": "Seo_et+al_2016_a",
            "entry": "Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Query-reduction networks for question answering. arXiv preprint arXiv:1606.04582, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04582"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1): 1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sukhbaatar_et+al_2015_a",
            "entry": "Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2440\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.",
            "url": "http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20arthur%20szlam%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pp. 3104\u20133112, 2014. URL http://arxiv.org/abs/1409.3215.",
            "url": "http://arxiv.org/abs/1409.3215",
            "arxiv_url": "https://arxiv.org/pdf/1409.3215"
        },
        {
            "id": "Tran_et+al_2018_a",
            "entry": "Ke Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling hierarchical structure. In Proceedings of NAACL\u201918, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Ke%20Bisazza%2C%20Arianna%20Monz%2C%20Christof%20The%20importance%20of%20being%20recurrent%20for%20modeling%20hierarchical%20structure%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Ke%20Bisazza%2C%20Arianna%20Monz%2C%20Christof%20The%20importance%20of%20being%20recurrent%20for%20modeling%20hierarchical%20structure%202018"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, 2017. URL http://arxiv.org/abs/1706.03762.",
            "url": "http://arxiv.org/abs/1706.03762",
            "arxiv_url": "https://arxiv.org/pdf/1706.03762"
        },
        {
            "id": "Vaswani_et+al_2018_a",
            "entry": "Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        },
        {
            "id": "Weston_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri\u00ebnboer, Armand Joulin, and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weston%2C%20Jason%20Bordes%2C%20Antoine%20Chopra%2C%20Sumit%20Rush%2C%20Alexander%20M.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weston%2C%20Jason%20Bordes%2C%20Antoine%20Chopra%2C%20Sumit%20Rush%2C%20Alexander%20M.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Mikolov_2015_a",
            "entry": "Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015. Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. Memory architectures in recurrent neural network language models. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SkFqf0lAZ. Wojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2015. URL http://arxiv.org/abs/1410.4615.",
            "url": "https://openreview.net/forum?id=SkFqf0lAZ",
            "arxiv_url": "https://arxiv.org/pdf/1502.05698"
        }
    ]
}
