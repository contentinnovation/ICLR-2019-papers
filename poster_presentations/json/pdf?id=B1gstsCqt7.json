{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SPARSE DICTIONARY LEARNING BY DYNAMICAL NEURAL NETWORKS",
        "author": "Tsung-Han Lin, Ping Tak Peter Tang Intel Corporation Santa Clara, CA {tsung-han.lin,peter.tang}@intel.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1gstsCqt7"
        },
        "abstract": "A dynamical neural network consists of a set of interconnected neurons that interact over time continuously. It can exhibit computational properties in the sense that the dynamical system\u2019s evolution and/or limit points in the associated state space can correspond to numerical solutions to certain mathematical optimization or learning problems. Such a computational system is particularly attractive in that it can be mapped to a massively parallel computer architecture for power and throughput efficiency, especially if each neuron can rely solely on local information (i.e., local memory). Deriving gradients from the dynamical network\u2019s various states while conforming to this last constraint, however, is challenging. We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the 1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons. Using spiking neurons to construct our dynamical network, we present a learning process, its rigorous mathematical analysis, and numerical results on several dictionary learning problems."
    },
    "keywords": [
        {
            "term": "limit point",
            "url": "https://en.wikipedia.org/wiki/limit_point"
        },
        {
            "term": "dictionary learning",
            "url": "https://en.wikipedia.org/wiki/dictionary_learning"
        },
        {
            "term": "boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/boltzmann_machine"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "sparse representation",
            "url": "https://en.wikipedia.org/wiki/sparse_representation"
        }
    ],
    "abbreviations": {
        "PE": "processing element\u2019s",
        "SGD": "stochastic gradient descent"
    },
    "highlights": [
        "A network of simple neural units can form a physical system that exhibits computational properties",
        "We adopt a neuron model whose activation function corresponds to the unbounded ReLU function rather than the bounded sigmoid-like function in Hopfield networks or Boltzmann machines, and a special network topology where connection weights have dependency",
        "We extend the previously established result (<a class=\"ref-link\" id=\"cTang_et+al_2017_a\" href=\"#rTang_et+al_2017_a\">Tang et al, 2017</a>) in several aspects: (1) \u03b3 can be set to any values in [0, 1); all a\u03b3\u2217 are the optimal sparse code, (2) H needs not be F B exactly; H \u2212 F B being small suffices, and (3) as long as t is large enough, a\u03b3(t) solves an approximate sparse coding problem",
        "We have presented a dynamical neural network formulation that can learn dictionaries for sparse representations",
        "We believe there is still much to be explored in dynamical neural networks"
    ],
    "key_statements": [
        "A network of simple neural units can form a physical system that exhibits computational properties",
        "We adopt a neuron model whose activation function corresponds to the unbounded ReLU function rather than the bounded sigmoid-like function in Hopfield networks or Boltzmann machines, and a special network topology where connection weights have dependency",
        "We extend the previously established result (<a class=\"ref-link\" id=\"cTang_et+al_2017_a\" href=\"#rTang_et+al_2017_a\">Tang et al, 2017</a>) in several aspects: (1) \u03b3 can be set to any values in [0, 1); all a\u03b3\u2217 are the optimal sparse code, (2) H needs not be F B exactly; H \u2212 F B being small suffices, and (3) as long as t is large enough, a\u03b3(t) solves an approximate sparse coding problem",
        "We have presented a dynamical neural network formulation that can learn dictionaries for sparse representations",
        "We believe there is still much to be explored in dynamical neural networks"
    ],
    "summary": [
        "A network of simple neural units can form a physical system that exhibits computational properties.",
        "We adopt a neuron model whose activation function corresponds to the unbounded ReLU function rather than the bounded sigmoid-like function in Hopfield networks or Boltzmann machines, and a special network topology where connection weights have dependency.",
        "This architectural model imposes a critical weight locality constraint on learning algorithms for dynamical networks: The connection weights must be adjusted with rules that rely only on locally available information such as connection weights, a neuron\u2019s internal states, and the rate of spikes it receives.",
        "We consider that the feed-forward and feedback weights in Figure 1(b) are initialized to be symmetric and correspond to a global dictionary D, F T = B = D This network can solve the sparse coding problem.",
        "The sparse code and dictionary gradient are computed using the feedforward and feedback weights respectively.",
        "These are standard datasets in image processing (A), machine learning (B), and computational neuroscience (C).4 For each input, the network is ran with \u03b3 = 0 from t = 0 to t = 20 and with \u03b3 = 0.7 from t = 20 to t = 40, both with a discrete time step of 1/32.",
        "Is relatively small and yields a spike rate precision of only 0.05, we observed that it is sufficient for gradient calculation and dictionary learning purpose.",
        "An interesting observation is that as learning proceeds, weight consistency becomes easier to maintain as the dictionary gradually converges.",
        "We have limited theoretical understanding for networks with random initial weights, Figure 3 shows that our learning procedure can automatically discover consistent and symmetric weights with respect to a single global dictionary.",
        "Figure 4 shows that our algorithm converges and can obtain a solution of similar, if not better, objective function values to SGD consistently across the datasets.",
        "From Figure 4 the network can already improve the dictionary before 104 samples, showing that perfectly symmetric weights are not necessary for learning to proceed.",
        "We have presented a dynamical neural network formulation that can learn dictionaries for sparse representations.",
        "Our work represents a significant step forward that it not only provides a link between the well-established dictionary learning problem and dynamical neural networks, but demonstrates the contrastive learning approach to be a fruitful direction.",
        "Learning in such networks respects data locality and has the unique potential, especially with spiking neurons, to enable low-power, high-throughput training with massively parallel architectures."
    ],
    "headline": "We show that by combining ideas of top-down feedback and contrastive learning, a dynamical network for solving the 1-minimizing dictionary learning problem can be constructed, and the true gradients for learning are provably computable by individual neurons",
    "reference_links": [
        {
            "id": "Ackley_et+al_1985_a",
            "entry": "David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for Boltzmann machines. Cognitive science, 9(1):147\u2013169, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ackley%2C%20David%20H.%20Hinton%2C%20Geoffrey%20E.%20Sejnowski%2C%20Terrence%20J.%20A%20learning%20algorithm%20for%20Boltzmann%20machines%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ackley%2C%20David%20H.%20Hinton%2C%20Geoffrey%20E.%20Sejnowski%2C%20Terrence%20J.%20A%20learning%20algorithm%20for%20Boltzmann%20machines%201985"
        },
        {
            "id": "Aharon_2008_a",
            "entry": "Michal Aharon and Michael Elad. Sparse and redundant modeling of image content using an image-signature-dictionary. SIAM Journal on Imaging Sciences, 1(3):228\u2013247, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aharon%2C%20Michal%20Elad%2C%20Michael%20Sparse%20and%20redundant%20modeling%20of%20image%20content%20using%20an%20image-signature-dictionary%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aharon%2C%20Michal%20Elad%2C%20Michael%20Sparse%20and%20redundant%20modeling%20of%20image%20content%20using%20an%20image-signature-dictionary%202008"
        },
        {
            "id": "Beck_2009_a",
            "entry": "Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20Optimization%202004"
        },
        {
            "id": "Brendel_et+al_2017_a",
            "entry": "Wieland Brendel, Ralph Bourdoukan, Pietro Vertechi, Christian K Machens, and Sophie Den\u00e9ve. Learning to represent signals spike by spike. arXiv preprint arXiv:1703.03777, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03777"
        },
        {
            "id": "Brito_2016_a",
            "entry": "Carlos S. N. Brito and Wulfram Gerstner. Nonlinear Hebbian learning as a unifying principle in receptive field formation. PLoS Comput Biol, 12(9):1\u201324, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brito%2C%20Carlos%20S.N.%20Gerstner%2C%20Wulfram%20Nonlinear%20Hebbian%20learning%20as%20a%20unifying%20principle%20in%20receptive%20field%20formation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brito%2C%20Carlos%20S.N.%20Gerstner%2C%20Wulfram%20Nonlinear%20Hebbian%20learning%20as%20a%20unifying%20principle%20in%20receptive%20field%20formation%202016"
        },
        {
            "id": "Bruckstein_et+al_2008_a",
            "entry": "Alfred M Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations. IEEE Transactions on Information Theory, 54(11):4813\u20134820, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Zibulevsky%2C%20Michael%20On%20the%20uniqueness%20of%20nonnegative%20sparse%20solutions%20to%20underdetermined%20systems%20of%20equations%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Zibulevsky%2C%20Michael%20On%20the%20uniqueness%20of%20nonnegative%20sparse%20solutions%20to%20underdetermined%20systems%20of%20equations%202008"
        },
        {
            "id": "Burbank_2015_a",
            "entry": "Kendra S Burbank. Mirrored STDP implements autoencoder learning in a network of spiking neurons. PLoS Comput Biol, 11(12):e1004566, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burbank%2C%20Kendra%20S.%20Mirrored%20STDP%20implements%20autoencoder%20learning%20in%20a%20network%20of%20spiking%20neurons%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burbank%2C%20Kendra%20S.%20Mirrored%20STDP%20implements%20autoencoder%20learning%20in%20a%20network%20of%20spiking%20neurons%202015"
        },
        {
            "id": "Davies_et+al_2018_a",
            "entry": "Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018"
        },
        {
            "id": "Efron_et+al_2004_a",
            "entry": "Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regression. The Annals of statistics, 32(2):407\u2013499, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efron%2C%20Bradley%20Hastie%2C%20Trevor%20Johnstone%2C%20Iain%20Tibshirani%2C%20Robert%20Least%20angle%20regression%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Efron%2C%20Bradley%20Hastie%2C%20Trevor%20Johnstone%2C%20Iain%20Tibshirani%2C%20Robert%20Least%20angle%20regression%202004"
        },
        {
            "id": "Elad_2006_a",
            "entry": "Michael Elad and Michal Aharon. Image denoising via learned dictionaries and sparse representation. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pp. 895\u2013900. IEEE, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elad%2C%20Michael%20Aharon%2C%20Michal%20Image%20denoising%20via%20learned%20dictionaries%20and%20sparse%20representation%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elad%2C%20Michael%20Aharon%2C%20Michal%20Image%20denoising%20via%20learned%20dictionaries%20and%20sparse%20representation%202006"
        },
        {
            "id": "Federer_2018_a",
            "entry": "Callie Federer and Joel Zylberberg. A self-organizing short-term dynamical memory network. Neural Networks, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Federer%2C%20Callie%20Zylberberg%2C%20Joel%20A%20self-organizing%20short-term%20dynamical%20memory%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Federer%2C%20Callie%20Zylberberg%2C%20Joel%20A%20self-organizing%20short-term%20dynamical%20memory%20network%202018"
        },
        {
            "id": "Foeldiak_1990_a",
            "entry": "Peter F\u00f6ldiak. Forming sparse representations by local anti-Hebbian learning. Biological cybernetics, 64(2):165\u2013170, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=F%C3%B6ldiak%2C%20Peter%20Forming%20sparse%20representations%20by%20local%20anti-Hebbian%20learning%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=F%C3%B6ldiak%2C%20Peter%20Forming%20sparse%20representations%20by%20local%20anti-Hebbian%20learning%201990"
        },
        {
            "id": "Freund_1992_a",
            "entry": "Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors using two layer networks. In Advances in neural information processing systems, pp. 912\u2013919, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Haussler%2C%20David%20Unsupervised%20learning%20of%20distributions%20on%20binary%20vectors%20using%20two%20layer%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Haussler%2C%20David%20Unsupervised%20learning%20of%20distributions%20on%20binary%20vectors%20using%20two%20layer%20networks%201992"
        },
        {
            "id": "Guerguiev_2017_a",
            "entry": "Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated dendrites. ELife, 6:e22901, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20and%20Blake%20A%20Richards.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20and%20Blake%20A%20Richards.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017"
        },
        {
            "id": "Hinton_1988_a",
            "entry": "Geoffrey E Hinton and James L McClelland. Learning representations by recirculation. In Neural information processing systems, pp. 358\u2013366, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20McClelland%2C%20James%20L.%20Learning%20representations%20by%20recirculation%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20McClelland%2C%20James%20L.%20Learning%20representations%20by%20recirculation%201988"
        },
        {
            "id": "Hinton_et+al_2006_a",
            "entry": "Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "Hopfield_1982_a",
            "entry": "J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci., 79(8):2554\u20132558, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopfield%2C%20J.J.%20Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hopfield%2C%20J.J.%20Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities%201982"
        },
        {
            "id": "Hopfield_1984_a",
            "entry": "J. J. Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proc. Natl. Acad. Sci., 1:3088\u20133092, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopfield%2C%20J.J.%20Neurons%20with%20graded%20response%20have%20collective%20computational%20properties%20like%20those%20of%20two-state%20neurons%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hopfield%2C%20J.J.%20Neurons%20with%20graded%20response%20have%20collective%20computational%20properties%20like%20those%20of%20two-state%20neurons%201984"
        },
        {
            "id": "Hoyer_2004_a",
            "entry": "Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine learning research, 5(Nov):1457\u20131469, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoyer%2C%20Patrik%20O.%20Non-negative%20matrix%20factorization%20with%20sparseness%20constraints%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoyer%2C%20Patrik%20O.%20Non-negative%20matrix%20factorization%20with%20sparseness%20constraints%202004"
        },
        {
            "id": "Hu_et+al_2014_a",
            "entry": "Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii. A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization. In 2014 48th Asilomar Conference on Signals, Systems and Computers, pp. 613\u2013619. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Tao%20Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20B.%20A%20hebbian/anti-hebbian%20network%20for%20online%20sparse%20dictionary%20learning%20derived%20from%20symmetric%20matrix%20factorization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Tao%20Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20B.%20A%20hebbian/anti-hebbian%20network%20for%20online%20sparse%20dictionary%20learning%20derived%20from%20symmetric%20matrix%20factorization%202014"
        },
        {
            "id": "Huh_2017_a",
            "entry": "Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. arXiv preprint arXiv:1706.04698, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04698"
        },
        {
            "id": "Kung_1982_a",
            "entry": "Hsiang-Tsung Kung. Why systolic architectures? IEEE computer, 15(1):37\u201346, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kung%2C%20Hsiang-Tsung%20Why%20systolic%20architectures%3F%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kung%2C%20Hsiang-Tsung%20Why%20systolic%20architectures%3F%201982"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Qianli_2016_a",
            "entry": "Qianli Liao, Joel Z Leibo, and Tomaso A Poggio. How important is weight symmetry in backpropagation? In AAAI, pp. 1837\u20131844, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qianli%20Liao%2C%20Joel%20Z%20Leibo%2C%20and%20Tomaso%20A%20Poggio.%20How%20important%20is%20weight%20symmetry%20in%20backpropagation%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qianli%20Liao%2C%20Joel%20Z%20Leibo%2C%20and%20Tomaso%20A%20Poggio.%20How%20important%20is%20weight%20symmetry%20in%20backpropagation%3F%202016"
        },
        {
            "id": "Mairal_et+al_2009_a",
            "entry": "Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning, pp. 689\u2013696. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009"
        },
        {
            "id": "Mairal_et+al_2014_a",
            "entry": "Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. Foundations and Trends R in Computer Graphics and Vision, 8(2-3):85\u2013283, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014"
        },
        {
            "id": "Merolla_et+al_6197_a",
            "entry": "Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spikingneuron integrated circuit with a scalable communication network and interface. Science, 345 (6197):668\u2013673, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spikingneuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%206197",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spikingneuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%206197"
        },
        {
            "id": "Movellan_1990_a",
            "entry": "Javier R Movellan. Contrastive Hebbian learning in the continuous hopfield model. In Connectionist models: Proceedings of the 1990 summer school, pp. 10\u201317, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Movellan%2C%20Javier%20R.%20Contrastive%20Hebbian%20learning%20in%20the%20continuous%20hopfield%20model%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Movellan%2C%20Javier%20R.%20Contrastive%20Hebbian%20learning%20in%20the%20continuous%20hopfield%20model%201990"
        },
        {
            "id": "Olshausen_1996_a",
            "entry": "Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381:13, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996"
        },
        {
            "id": "O_1996_a",
            "entry": "Randall C O\u2019Reilly. Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural computation, 8(5):895\u2013938, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Reilly%2C%20Randall%20C.%20Biologically%20plausible%20error-driven%20learning%20using%20local%20activation%20differences%3A%20The%20generalized%20recirculation%20algorithm%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Reilly%2C%20Randall%20C.%20Biologically%20plausible%20error-driven%20learning%20using%20local%20activation%20differences%3A%20The%20generalized%20recirculation%20algorithm%201996"
        },
        {
            "id": "Pehlevan_et+al_2018_a",
            "entry": "Cengiz Pehlevan, Anirvan M Sengupta, and Dmitri B Chklovskii. Why do similarity matching objectives lead to hebbian/anti-hebbian networks? Neural computation, 30(1):84\u2013124, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Sengupta%2C%20Anirvan%20M.%20Chklovskii%2C%20Dmitri%20B.%20Why%20do%20similarity%20matching%20objectives%20lead%20to%20hebbian/anti-hebbian%20networks%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Sengupta%2C%20Anirvan%20M.%20Chklovskii%2C%20Dmitri%20B.%20Why%20do%20similarity%20matching%20objectives%20lead%20to%20hebbian/anti-hebbian%20networks%3F%202018"
        },
        {
            "id": "Ranzato_et+al_2007_a",
            "entry": "Marc\u2019Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In Advances in neural information processing systems, pp. 1137\u20131144, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20learning%20of%20sparse%20representations%20with%20an%20energy-based%20model%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20learning%20of%20sparse%20representations%20with%20an%20energy-based%20model%202007"
        },
        {
            "id": "Rozell_et+al_2008_a",
            "entry": "Christopher J Rozell, Don H Johnson, Richard G Baraniuk, and Bruno A Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural computation, 20(10):2526\u20132563, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rozell%2C%20Christopher%20J.%20Johnson%2C%20Don%20H.%20Baraniuk%2C%20Richard%20G.%20and%20Bruno%20A%20Olshausen.%20Sparse%20coding%20via%20thresholding%20and%20local%20competition%20in%20neural%20circuits%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rozell%2C%20Christopher%20J.%20Johnson%2C%20Don%20H.%20Baraniuk%2C%20Richard%20G.%20and%20Bruno%20A%20Olshausen.%20Sparse%20coding%20via%20thresholding%20and%20local%20competition%20in%20neural%20circuits%202008"
        },
        {
            "id": "Rubinstein_et+al_2010_a",
            "entry": "Ron Rubinstein, Alfred M Bruckstein, and Michael Elad. Dictionaries for sparse representation modeling. Proceedings of the IEEE, 98(6):1045\u20131057, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Ron%20Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Dictionaries%20for%20sparse%20representation%20modeling%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Ron%20Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Dictionaries%20for%20sparse%20representation%20modeling%202010"
        },
        {
            "id": "Sacramento_et+al_2018_a",
            "entry": "Jo\u00e3o Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits approximate the backpropagation algorithm. arXiv preprint arXiv:1810.11393, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.11393"
        },
        {
            "id": "Scellier_2017_a",
            "entry": "Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energybased models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energybased%20models%20and%20backpropagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energybased%20models%20and%20backpropagation%202017"
        },
        {
            "id": "Seung_2017_a",
            "entry": "H Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields computational interpretations of hebbian excitation, anti-hebbian inhibition, and synapse elimination. arXiv preprint arXiv:1704.00646, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00646"
        },
        {
            "id": "Shapero_et+al_2014_a",
            "entry": "Samuel Shapero, Mengchen Zhu, Jennifer Hasler, and Christopher Rozell. Optimal sparse approximation with integrate and fire neurons. International journal of neural systems, 24(05):1440001, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapero%2C%20Samuel%20Zhu%2C%20Mengchen%20Hasler%2C%20Jennifer%20Rozell%2C%20Christopher%20Optimal%20sparse%20approximation%20with%20integrate%20and%20fire%20neurons%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapero%2C%20Samuel%20Zhu%2C%20Mengchen%20Hasler%2C%20Jennifer%20Rozell%2C%20Christopher%20Optimal%20sparse%20approximation%20with%20integrate%20and%20fire%20neurons%202014"
        },
        {
            "id": "Tang_2016_a",
            "entry": "Ping Tak Peter Tang. Convergence of LCA Flows to (C)LASSO Solutions. ArXiv e-prints, March 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Ping%20Tak%20Peter%20Convergence%20of%20LCA%20Flows%20to%20%28C%29LASSO%20Solutions%202016-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Ping%20Tak%20Peter%20Convergence%20of%20LCA%20Flows%20to%20%28C%29LASSO%20Solutions%202016-03"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Ping Tak Peter Tang, Tsung-Han Lin, and Mike Davies. Sparse coding by spiking neural networks: Convergence theory and computational results. ArXiv e-prints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Ping%20Tak%20Peter%20Lin%2C%20Tsung-Han%20Davies%2C%20Mike%20Sparse%20coding%20by%20spiking%20neural%20networks%3A%20Convergence%20theory%20and%20computational%20results.%20ArXiv%20e-prints%202017"
        },
        {
            "id": "Vertechi_et+al_2014_a",
            "entry": "Pietro Vertechi, Wieland Brendel, and Christian K Machens. Unsupervised learning of an efficient short-term memory network. In Advances in Neural Information Processing Systems, pp. 3653\u2013 3661, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vertechi%2C%20Pietro%20Brendel%2C%20Wieland%20Machens%2C%20Christian%20K.%20Unsupervised%20learning%20of%20an%20efficient%20short-term%20memory%20network%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vertechi%2C%20Pietro%20Brendel%2C%20Wieland%20Machens%2C%20Christian%20K.%20Unsupervised%20learning%20of%20an%20efficient%20short-term%20memory%20network%202014"
        },
        {
            "id": "Whittington_2017_a",
            "entry": "James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5): 1229\u20131262, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Whittington%2C%20James%20C.R.%20Bogacz%2C%20Rafal%20An%20approximation%20of%20the%20error%20backpropagation%20algorithm%20in%20a%20predictive%20coding%20network%20with%20local%20hebbian%20synaptic%20plasticity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Whittington%2C%20James%20C.R.%20Bogacz%2C%20Rafal%20An%20approximation%20of%20the%20error%20backpropagation%20algorithm%20in%20a%20predictive%20coding%20network%20with%20local%20hebbian%20synaptic%20plasticity%202017"
        },
        {
            "id": "Xie_2003_a",
            "entry": "Xiaohui Xie and H Sebastian Seung. Equivalence of backpropagation and contrastive Hebbian learning in a layered network. Neural computation, 15(2):441\u2013454, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Xiaohui%20Seung%2C%20H.Sebastian%20Equivalence%20of%20backpropagation%20and%20contrastive%20Hebbian%20learning%20in%20a%20layered%20network%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Xiaohui%20Seung%2C%20H.Sebastian%20Equivalence%20of%20backpropagation%20and%20contrastive%20Hebbian%20learning%20in%20a%20layered%20network%202003"
        },
        {
            "id": "Zylberberg_et+al_2011_a",
            "entry": "Joel Zylberberg, Jason Timothy Murphy, and Michael Robert DeWeese. A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields. PLoS Comput Biol, 7(10):e1002250, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zylberberg%2C%20Joel%20Murphy%2C%20Jason%20Timothy%20DeWeese%2C%20Michael%20Robert%20A%20sparse%20coding%20model%20with%20synaptically%20local%20plasticity%20and%20spiking%20neurons%20can%20account%20for%20the%20diverse%20shapes%20of%20v1%20simple%20cell%20receptive%20fields%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zylberberg%2C%20Joel%20Murphy%2C%20Jason%20Timothy%20DeWeese%2C%20Michael%20Robert%20A%20sparse%20coding%20model%20with%20synaptically%20local%20plasticity%20and%20spiking%20neurons%20can%20account%20for%20the%20diverse%20shapes%20of%20v1%20simple%20cell%20receptive%20fields%202011"
        }
    ]
}
