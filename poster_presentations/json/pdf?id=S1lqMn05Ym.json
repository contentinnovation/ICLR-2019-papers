{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "INFORMATION ASYMMETRY IN KL-REGULARIZED RL",
        "author": "Alexandre Galashov, Siddhant M. Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Yee Whye Teh, Razvan Pascanu, Nicolas Heess DeepMind London, UK {agalashov,sidmj,leonardh,dhruvat,schwarzjn,gdesjardins, lejlot,ywteh,razp,heess}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lqMn05Ym"
        },
        "abstract": "Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning."
    },
    "keywords": [
        {
            "term": "asymmetry",
            "url": "https://en.wikipedia.org/wiki/asymmetry"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "degrees of freedom",
            "url": "https://en.wikipedia.org/wiki/degrees_of_freedom"
        },
        {
            "term": "em algorithm",
            "url": "https://en.wikipedia.org/wiki/em_algorithm"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "information asymmetry",
            "url": "https://en.wikipedia.org/wiki/information_asymmetry"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "DoF": "degrees of freedom",
        "GTT": "going to moving target task"
    },
    "highlights": [
        "For many interesting reinforcement learning tasks, good policies exhibit similar behaviors in different contexts, behaviors that need to be modified only slightly or occasionally to account for the specific task at hand or to respond to information becoming available",
        "To render this approach effective, we introduce an information asymmetry between the default and agent policies, preventing the default policy from accessing certain information in the state",
        "We study the effect of using a learned default policy to regularize the behavior of our agents, across a wide range of environments spanning sparse and dense reward tasks",
        "In this work we studied the influence of learning the default policy in the KL-regularized RL objective",
        "We showed empirically that in the case of sparse-reward tasks with complex walkers, there is a significant speed-up of learning compared to the baseline",
        "We provided evidence that these gains are mostly due to the information asymmetry between the agent and the default policy"
    ],
    "key_statements": [
        "For many interesting reinforcement learning tasks, good policies exhibit similar behaviors in different contexts, behaviors that need to be modified only slightly or occasionally to account for the specific task at hand or to respond to information becoming available",
        "To render this approach effective, we introduce an information asymmetry between the default and agent policies, preventing the default policy from accessing certain information in the state",
        "We study the effect of using a learned default policy to regularize the behavior of our agents, across a wide range of environments spanning sparse and dense reward tasks",
        "Default Policy Transfer We explore the possibility of reusing pretrained default policies to regularize learning on new tasks",
        "We show that a pure uniform default policy is unhelpful when human knowledge is removed from defining the right subset of actions to be uniform over, and the agent under-performs",
        "In this work we studied the influence of learning the default policy in the KL-regularized RL objective",
        "We showed empirically that in the case of sparse-reward tasks with complex walkers, there is a significant speed-up of learning compared to the baseline",
        "We provided evidence that these gains are mostly due to the information asymmetry between the agent and the default policy"
    ],
    "summary": [
        "For many interesting reinforcement learning tasks, good policies exhibit similar behaviors in different contexts, behaviors that need to be modified only slightly or occasionally to account for the specific task at hand or to respond to information becoming available.",
        "We find that even when the agent and default policies are learned at the same time, significant speed-ups can be achieved on a range of tasks.",
        "Re-introducing this into (5) we find that the KL regularized objective in eq (3) can be seen as a lower bound to eq (5), where the agent has a capacity constraint on the channel between goal-directed history information and actions.",
        "We study the effect of using a learned default policy to regularize the behavior of our agents, across a wide range of environments spanning sparse and dense reward tasks.",
        "The main finding of our experiments is that the default policy with limited task information provides considerable speed-up in terms of learner steps for the sparse-reward tasks with complex walkers.",
        "The action space is too simple to require sophisticated exploration provided by the default policy.",
        "Default Policy Transfer We explore the possibility of reusing pretrained default policies to regularize learning on new tasks.",
        "We observe a significant improvement in learning speed transferring the pretrained default policies to the new task.",
        "Recent works on multitask training (<a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al, 2018</a>) in this domain have used a form of batched-A2C with the V-trace algorithm to maximize an approximation of the entropy regularized objective described earlier, where the default policy is a uniform distribution over the actions.",
        "Even in the extreme case when the default policy is not conditioned on any state information, helps recovering which actions are worth exploring and leads to the emergence of a useful action space without any hand engineering.",
        "LSTM default policy on the other hand, while being recurrent as the agent, it observes only the previous action at1 and does not receive any other state information.",
        "The vector default policy performs surprisingly well, highlighting that for DMLab defining a meaningful action space is extremely important for solving the task.",
        "Figure 6, that the entropy of the default policy over learning frames goes down, indicating that the default policy becomes peaky and is quite different from the uniform distribution which the baseline assumes.",
        "We showed empirically that in the case of sparse-reward tasks with complex walkers, there is a significant speed-up of learning compared to the baseline.",
        "Best results are obtained when the default policy sees only a subset of information, allowing it to learn task-agnostic behaviour."
    ],
    "headline": "In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning",
    "reference_links": [
        {
            "id": "Abdolmaleki_et+al_2018_a",
            "entry": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, R\u00e9mi Munos, Nicolas Heess, and Martin A. Riedmiller. Maximum a posteriori policy optimisation. CoRR, abs/1806.06920, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06920"
        },
        {
            "id": "Alemi_et+al_2016_a",
            "entry": "Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.00410.",
            "url": "http://arxiv.org/abs/1612.00410",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "Alemi_et+al_2017_a",
            "entry": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a broken elbo. CoRR, abs/1711.00464, 2017. URL http://arxiv.org/abs/1711.00464.",
            "url": "http://arxiv.org/abs/1711.00464",
            "arxiv_url": "https://arxiv.org/pdf/1711.00464"
        },
        {
            "id": "Beattie_et+al_2016_a",
            "entry": "Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. URL https://arxiv.org/abs/1612.03801.",
            "url": "https://arxiv.org/abs/1612.03801",
            "arxiv_url": "https://arxiv.org/pdf/1612.03801"
        },
        {
            "id": "Chebotar_et+al_2016_a",
            "entry": "Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and Sergey Levine. Path integral guided policy search. CoRR, abs/1610.00529, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00529"
        },
        {
            "id": "Czarnecki_et+al_2018_a",
            "entry": "Wojciech Marian Czarnecki, Siddhant M. Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Nicolas Heess, Simon Osindero, and Razvan Pascanu. Mix & match agent curricula for reinforcement learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Czarnecki%2C%20Wojciech%20Marian%20Jayakumar%2C%20Siddhant%20M.%20Jaderberg%2C%20Max%20Hasenclever%2C%20Leonard%20Mix%20%26%20match%20agent%20curricula%20for%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Czarnecki%2C%20Wojciech%20Marian%20Jayakumar%2C%20Siddhant%20M.%20Jaderberg%2C%20Max%20Hasenclever%2C%20Leonard%20Mix%20%26%20match%20agent%20curricula%20for%20reinforcement%20learning%202018"
        },
        {
            "id": "Dempster_et+al_1977_a",
            "entry": "A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39:1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv:1802.01561, 2018. URL https://arxiv.org/abs/1802.01561.",
            "url": "https://arxiv.org/abs/1802.01561",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "Fox_et+al_2015_a",
            "entry": "Roy Fox, Ari Pakman, and Naftali Tishby. G-learning: Taming the noise in reinforcement learning via soft updates. CoRR, abs/1512.08562, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.08562"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-andconquer reinforcement learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20Dibya%20Singh%2C%20Avi%20Rajeswaran%2C%20Aravind%20Kumar%2C%20Vikash%20Divide-andconquer%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Dibya%20Singh%2C%20Avi%20Rajeswaran%2C%20Aravind%20Kumar%2C%20Vikash%20Divide-andconquer%20reinforcement%20learning%202018"
        },
        {
            "id": "Goyal_et+al_2019_a",
            "entry": "Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo Larochelle, Yoshua Bengio, and Sergey Levine. Infobot: Transfer and exploration via the information bottleneck. ICLR, abs/1901.10902, 2019. URL http://arxiv.org/abs/1901.10902.",
            "url": "http://arxiv.org/abs/1901.10902",
            "arxiv_url": "https://arxiv.org/pdf/1901.10902"
        },
        {
            "id": "Haarnoja_et+al_2017_a",
            "entry": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. CoRR, abs/1702.08165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08165"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290.",
            "url": "http://arxiv.org/abs/1801.01290",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rk07ZXZRb.",
            "url": "https://openreview.net/forum?id=rk07ZXZRb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv:1512.03385, 2015. URL https://arxiv.org/abs/1512.03385.",
            "url": "https://arxiv.org/abs/1512.03385",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Heess_et+al_2017_a",
            "entry": "Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/1503.02531.",
            "url": "http://arxiv.org/abs/1503.02531",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. URL https://arxiv.org/abs/1711.09846.",
            "url": "https://arxiv.org/abs/1711.09846",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "Kappen_et+al_2012_a",
            "entry": "Hilbert J. Kappen, Vicen\u00e7 G\u00f3mez, and Manfred Opper. Optimal control as a graphical model inference problem. Machine Learning, 87(2):159\u2013182, May 2012. ISSN 1573-0565.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kappen%2C%20Hilbert%20J.%20G%C3%B3mez%2C%20Vicen%C3%A7%20Opper%2C%20Manfred%20Optimal%20control%20as%20a%20graphical%20model%20inference%20problem%202012-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kappen%2C%20Hilbert%20J.%20G%C3%B3mez%2C%20Vicen%C3%A7%20Opper%2C%20Manfred%20Optimal%20control%20as%20a%20graphical%20model%20inference%20problem%202012-05"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, pp. 201611835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016"
        },
        {
            "id": "Kool_2018_a",
            "entry": "Wouter Kool and Matthew Botvinick. Mental labour. Nature Human Behaviour, 2018. doi: 10.1038/s41562-018-0401-9.",
            "crossref": "https://dx.doi.org/10.1038/s41562-018-0401-9",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/s41562-018-0401-9"
        },
        {
            "id": "Levine_2013_a",
            "entry": "Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Advances in Neural Information Processing Systems, pp. 207\u2013215, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Variational%20policy%20search%20via%20trajectory%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Variational%20policy%20search%20via%20trajectory%20optimization%202013"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Montgomery_2016_a",
            "entry": "William Montgomery and Sergey Levine. Guided policy search as approximate mirror descent. CoRR, abs/1607.04614, 2016. URL http://arxiv.org/abs/1607.04614.",
            "url": "http://arxiv.org/abs/1607.04614",
            "arxiv_url": "https://arxiv.org/pdf/1607.04614"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1046\u20131054, 2016. URL http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.",
            "url": "http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20G.%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016-12-05"
        },
        {
            "id": "Nachum_et+al_0000_a",
            "entry": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. CoRR, abs/1702.08892, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08892"
        },
        {
            "id": "Nachum_et+al_0000_b",
            "entry": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust region method for continuous control. CoRR, abs/1707.01891, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01891"
        },
        {
            "id": "Neal_1999_a",
            "entry": "Radford M. Neal and Geoffrey E. Hinton. Learning in graphical models. chapter A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants, pp. 355\u2013368. MIT Press, Cambridge, MA, USA, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Hinton%2C%20Geoffrey%20E.%20Learning%20in%20graphical%20models.%20chapter%20A%20View%20of%20the%20EM%20Algorithm%20That%20Justifies%20Incremental%2C%20Sparse%2C%20and%20Other%20Variants%201999"
        },
        {
            "id": "Nocedal_2006_a",
            "entry": "Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA, second edition, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20Jorge%20Wright%2C%20Stephen%20J.%20Numerical%20Optimization%202006"
        },
        {
            "id": "Ortega_2011_a",
            "entry": "Pedro A. Ortega and Daniel A. Braun. Information, utility &amp; bounded rationality. CoRR, abs/1107.5766, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1107.5766"
        },
        {
            "id": "Ortega_2013_a",
            "entry": "Pedro A. Ortega and Daniel A. Braun. Thermodynamics as a theory of decision-making with information-processing costs. Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 469(2153), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ortega%2C%20Pedro%20A.%20Braun%2C%20Daniel%20A.%20Thermodynamics%20as%20a%20theory%20of%20decision-making%20with%20information-processing%20costs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ortega%2C%20Pedro%20A.%20Braun%2C%20Daniel%20A.%20Thermodynamics%20as%20a%20theory%20of%20decision-making%20with%20information-processing%20costs%202013"
        },
        {
            "id": "Parisotto_et+al_2016_a",
            "entry": "Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20Ba%2C%20Lei%20Jimmy%20Salakhutdinov%2C%20Ruslan%20Actor-mimic%3A%20Deep%20multitask%20and%20transfer%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20Ba%2C%20Lei%20Jimmy%20Salakhutdinov%2C%20Ruslan%20Actor-mimic%3A%20Deep%20multitask%20and%20transfer%20reinforcement%20learning%202016"
        },
        {
            "id": "Peters_et+al_2010_a",
            "entry": "Jan Peters, Katharina M\u00fclling, and Yasemin Alt\u00fcn. Relative entropy policy search. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Alt%C3%BCn%2C%20Yasemin%20Relative%20entropy%20policy%20search%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Alt%C3%BCn%2C%20Yasemin%20Relative%20entropy%20policy%20search%202010"
        },
        {
            "id": "Rawlik_et+al_2012_a",
            "entry": "Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. In (R:SS 2012), 2012. Runner Up Best Paper Award.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rawlik%2C%20Konrad%20Toussaint%2C%20Marc%20Vijayakumar%2C%20Sethu%20On%20stochastic%20optimal%20control%20and%20reinforcement%20learning%20by%20approximate%20inference%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rawlik%2C%20Konrad%20Toussaint%2C%20Marc%20Vijayakumar%2C%20Sethu%20On%20stochastic%20optimal%20control%20and%20reinforcement%20learning%20by%20approximate%20inference%202012"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Riedmiller_et+al_2018_a",
            "entry": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing \u2013 solving sparse reward tasks from scratch. arXiv:1802.10567, 2018a. URL https://arxiv.org/abs/1802.10567.",
            "url": "https://arxiv.org/abs/1802.10567",
            "arxiv_url": "https://arxiv.org/pdf/1802.10567"
        },
        {
            "id": "Riedmiller_et+al_2018_b",
            "entry": "Martin A. Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse reward tasks from scratch. CoRR, abs/1802.10567, 2018b. URL http://arxiv.org/abs/1802.10567.",
            "url": "http://arxiv.org/abs/1802.10567",
            "arxiv_url": "https://arxiv.org/pdf/1802.10567"
        },
        {
            "id": "Rubin_et+al_2012_a",
            "entry": "Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading Value and Information in MDPs, pp. 57\u201374. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jonathan%20Rubin%20Ohad%20Shamir%20and%20Naftali%20Tishby%20Trading%20Value%20and%20Information%20in%20MDPs%20pp%205774%20Springer%20Berlin%20Heidelberg%20Berlin%20Heidelberg%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jonathan%20Rubin%20Ohad%20Shamir%20and%20Naftali%20Tishby%20Trading%20Value%20and%20Information%20in%20MDPs%20pp%205774%20Springer%20Berlin%20Heidelberg%20Berlin%20Heidelberg%202012"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.06295.",
            "url": "https://arxiv.org/abs/1511.06295",
            "arxiv_url": "https://arxiv.org/pdf/1511.06295"
        },
        {
            "id": "Schmitt_et+al_2018_a",
            "entry": "Simon Schmitt, Jonathan J. Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M. Czarnecki, Joel Z. Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, and S. M. Ali Eslami. Kickstarting deep reinforcement learning. arXiv:1803.03835, 2018. URL https://arxiv.org/abs/1803.03835.",
            "url": "https://arxiv.org/abs/1803.03835",
            "arxiv_url": "https://arxiv.org/pdf/1803.03835"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_0000_a",
            "entry": "John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06440"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017b. URL http://arxiv.org/abs/1707.06347.",
            "url": "http://arxiv.org/abs/1707.06347",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Simon_1956_a",
            "entry": "H.A. Simon. Rational choice and the structure of the environment. Psychological Review, 63: 129\u2013138, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%2C%20H.A.%20Rational%20choice%20and%20the%20structure%20of%20the%20environment%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simon%2C%20H.A.%20Rational%20choice%20and%20the%20structure%20of%20the%20environment%201956"
        },
        {
            "id": "Still_2012_a",
            "entry": "Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139\u2013148, 2012. doi: 10.1007/s12064-011-0142-z. URL https://doi.org/10.1007/s12064-011-0142-z.",
            "crossref": "https://dx.doi.org/10.1007/s12064-011-0142-z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s12064-011-0142-z"
        },
        {
            "id": "Teh_et+al_2017_a",
            "entry": "Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. arXiv preprint arXiv:1707.04175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04175"
        },
        {
            "id": "Tiomkin_2017_a",
            "entry": "Stas Tiomkin and Naftali Tishby. A unified bellman equation for causal information and value in markov decision processes. CoRR, abs/1703.01585, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01585"
        },
        {
            "id": "Tishby_2011_a",
            "entry": "Naftali Tishby and Daniel Polani. The information theory of decision and action. In Percept. Action Cycle Springer Ser. in Cognitive Neural Syst., volume 19, pp. 601\u2013636, 01 2011. ISBN 978-1-4419-1451-4.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Polani%2C%20Daniel%20The%20information%20theory%20of%20decision%20and%20action.%20In%20Percept.%20Action%20Cycle%20Springer%20Ser%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Polani%2C%20Daniel%20The%20information%20theory%20of%20decision%20and%20action.%20In%20Percept.%20Action%20Cycle%20Springer%20Ser%202011"
        },
        {
            "id": "Todorov_2007_a",
            "entry": "Emanuel Todorov. Linearly-solvable markov decision problems. In B. Sch\u00f6lkopf, J. C. Platt, and T. Hoffman (eds.), Advances in Neural Information Processing Systems 19, pp. 1369\u20131376. MIT Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Linearly-solvable%20markov%20decision%20problems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Linearly-solvable%20markov%20decision%20problems%202007"
        },
        {
            "id": "Toussaint_2009_a",
            "entry": "Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pp. 1049\u20131056, 2009. ISBN 978-1-60558-516-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009"
        },
        {
            "id": "Williams_1991_a",
            "entry": "R. J. Williams and J. Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Peng%2C%20J.%20Function%20optimization%20using%20connectionist%20reinforcement%20learning%20algorithms%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Peng%2C%20J.%20Function%20optimization%20using%20connectionist%20reinforcement%20learning%20algorithms%201991"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Ying Zhang, Tao Xiang, Timothy Hospedales, and Huchuan Lu. Deep mutual learning. In Computer Vision and Pattern Recognition 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Ying%20Xiang%2C%20Tao%20Hospedales%2C%20Timothy%20Lu%2C%20Huchuan%20Deep%20mutual%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Ying%20Xiang%2C%20Tao%20Hospedales%2C%20Timothy%20Lu%2C%20Huchuan%20Deep%20mutual%20learning%202018"
        },
        {
            "id": "Ziebart_2010_a",
            "entry": "Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Machine Learning Department, Carnegie Mellon University, Dec 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Modeling%20Purposeful%20Adaptive%20Behavior%20with%20the%20Principle%20of%20Maximum%20Causal%20Entropy%202010-12"
        },
        {
            "id": "This_2016_a",
            "entry": "This perspective suggests two different interpretations of the KL regularized objective discussed above: We can see the role of the default policy implementing a way of restricting information flow between (past) states and (future) actions. An alternative view, more consistent with the analogy between RL and probabilistic modeling invoked above is that of learning a \u201cdefault\u201d behavior that is independent of some aspect of the state. (Although the information theoretic view has recently gained more hold in the probabilistic modeling literature, too (e.g. Alemi et al., 2016; 2017)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20perspective%20suggests%20two%20different%20interpretations%20of%20the%20KL%20regularized%20objective%20discussed%20above%20We%20can%20see%20the%20role%20of%20the%20default%20policy%20implementing%20a%20way%20of%20restricting%20information%20flow%20between%20past%20states%20and%20future%20actions%20An%20alternative%20view%20more%20consistent%20with%20the%20analogy%20between%20RL%20and%20probabilistic%20modeling%20invoked%20above%20is%20that%20of%20learning%20a%20default%20behavior%20that%20is%20independent%20of%20some%20aspect%20of%20the%20state%20Although%20the%20information%20theoretic%20view%20has%20recently%20gained%20more%20hold%20in%20the%20probabilistic%20modeling%20literature%20too%20eg%20Alemi%20et%20al%202016%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=This%20perspective%20suggests%20two%20different%20interpretations%20of%20the%20KL%20regularized%20objective%20discussed%20above%20We%20can%20see%20the%20role%20of%20the%20default%20policy%20implementing%20a%20way%20of%20restricting%20information%20flow%20between%20past%20states%20and%20future%20actions%20An%20alternative%20view%20more%20consistent%20with%20the%20analogy%20between%20RL%20and%20probabilistic%20modeling%20invoked%20above%20is%20that%20of%20learning%20a%20default%20behavior%20that%20is%20independent%20of%20some%20aspect%20of%20the%20state%20Although%20the%20information%20theoretic%20view%20has%20recently%20gained%20more%20hold%20in%20the%20probabilistic%20modeling%20literature%20too%20eg%20Alemi%20et%20al%202016%202017"
        },
        {
            "id": "We_2018_a",
            "entry": "We use a distributed off-policy setup similar to Riedmiller et al. (2018a). There is one learner and multiple actors. These are essentially the instantiations of the main agent used for different purposes. Each actor is the main agent version which receives the copy of parameters from the learner and unrolls the trajectories in the environment, saving it to the replay buffer of fixed size 1e6. The learner is the agent version which samples a batch of short trajectories windows (window size is defined by unroll length) from the replay buffer, calculates the gradients and updates the parameters. The updated parameters are then communicated to the actors. Such a setup speeds-up learning significantly and makes the final performance of the policy better. We compare the performance of on go to moving target task with 1 and 32 actors. From figure 7, we see that the effect of the default policy does not disappear when the number of actor decreases to 1, but the learning becomes much slower, noisier and weaker.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20use%20a%20distributed%20offpolicy%20setup%20similar%20to%20Riedmiller%20et%20al%202018a%20There%20is%20one%20learner%20and%20multiple%20actors%20These%20are%20essentially%20the%20instantiations%20of%20the%20main%20agent%20used%20for%20different%20purposes%20Each%20actor%20is%20the%20main%20agent%20version%20which%20receives%20the%20copy%20of%20parameters%20from%20the%20learner%20and%20unrolls%20the%20trajectories%20in%20the%20environment%20saving%20it%20to%20the%20replay%20buffer%20of%20fixed%20size%201e6%20The%20learner%20is%20the%20agent%20version%20which%20samples%20a%20batch%20of%20short%20trajectories%20windows%20window%20size%20is%20defined%20by%20unroll%20length%20from%20the%20replay%20buffer%20calculates%20the%20gradients%20and%20updates%20the%20parameters%20The%20updated%20parameters%20are%20then%20communicated%20to%20the%20actors%20Such%20a%20setup%20speedsup%20learning%20significantly%20and%20makes%20the%20final%20performance%20of%20the%20policy%20better%20We%20compare%20the%20performance%20of%20on%20go%20to%20moving%20target%20task%20with%201%20and%2032%20actors%20From%20figure%207%20we%20see%20that%20the%20effect%20of%20the%20default%20policy%20does%20not%20disappear%20when%20the%20number%20of%20actor%20decreases%20to%201%20but%20the%20learning%20becomes%20much%20slower%20noisier%20and%20weaker",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20use%20a%20distributed%20offpolicy%20setup%20similar%20to%20Riedmiller%20et%20al%202018a%20There%20is%20one%20learner%20and%20multiple%20actors%20These%20are%20essentially%20the%20instantiations%20of%20the%20main%20agent%20used%20for%20different%20purposes%20Each%20actor%20is%20the%20main%20agent%20version%20which%20receives%20the%20copy%20of%20parameters%20from%20the%20learner%20and%20unrolls%20the%20trajectories%20in%20the%20environment%20saving%20it%20to%20the%20replay%20buffer%20of%20fixed%20size%201e6%20The%20learner%20is%20the%20agent%20version%20which%20samples%20a%20batch%20of%20short%20trajectories%20windows%20window%20size%20is%20defined%20by%20unroll%20length%20from%20the%20replay%20buffer%20calculates%20the%20gradients%20and%20updates%20the%20parameters%20The%20updated%20parameters%20are%20then%20communicated%20to%20the%20actors%20Such%20a%20setup%20speedsup%20learning%20significantly%20and%20makes%20the%20final%20performance%20of%20the%20policy%20better%20We%20compare%20the%20performance%20of%20on%20go%20to%20moving%20target%20task%20with%201%20and%2032%20actors%20From%20figure%207%20we%20see%20that%20the%20effect%20of%20the%20default%20policy%20does%20not%20disappear%20when%20the%20number%20of%20actor%20decreases%20to%201%20but%20the%20learning%20becomes%20much%20slower%20noisier%20and%20weaker"
        }
    ]
}
