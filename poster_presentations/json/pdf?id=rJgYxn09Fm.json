{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING IMPLICITLY RECURRENT CNNS THROUGH PARAMETER SHARING",
        "author": "Pedro Savarese TTI-Chicago savarese@ttic.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJgYxn09Fm"
        },
        "abstract": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy."
    },
    "keywords": [
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "CNN": "convolutional neural network",
        "CNNs": "convolutional neural networks",
        "NAS": "neural architecture search",
        "RNNs": "recurrent neural networks",
        "LSM": "layer similarity matrix",
        "WRN": "Wide ResNets"
    },
    "highlights": [
        "The architectural details of convolutional neural networks (CNNs) have undergone rapid exploration and improvement via both human hand-design (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>; Zhu et al, 2018) and automated search methods (Zoph & Le, 2017; <a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\">Liu et al, 2018</a></a>)",
        "As we examine an extension of convolutional neural network, our tasks take the form of queries about planar graphs encoded as image input",
        "Results on CIFAR suggest that training networks with few parameter templates k in our soft sharing scheme results in performance comparable to the base models, which have significantly more parameters",
        "We observe that rich structures emerge naturally in networks trained with soft parameter sharing, even without the recurrence regularizer",
        "We take a step toward more modular and compact convolutional neural network by extracting recurrences from feed-forward models where parameters are shared among layers",
        "We observe that parameter sharing often leads to different layers being functionally equivalent after training, enabling us to collapse them into recurrent blocks"
    ],
    "key_statements": [
        "The architectural details of convolutional neural networks (CNNs) have undergone rapid exploration and improvement via both human hand-design (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>; Zhu et al, 2018) and automated search methods (Zoph & Le, 2017; <a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\">Liu et al, 2018</a></a>)",
        "A convolutional neural network is regarded as a fixed-depth feed-forward circuit, with a distinct parameter governing each internal connection",
        "Though we focus solely on loop-like structures, leaving subroutines and dynamic control flow to future work, this simple change suffices to yield substantial quantitative and qualitative benefits over the standard baseline convolutional neural network models",
        "As we examine an extension of convolutional neural network, our tasks take the form of queries about planar graphs encoded as image input",
        "A strict structure is imposed to layers of recurrent neural networks (RNNs), where, in standard models (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>), a single parameter set W is shared among all time steps",
        "Our method outperforms architectures discovered by recent neural architecture search algorithms, such as DARTS (<a class=\"ref-link\" id=\"cLiu_et+al_2019_a\" href=\"#rLiu_et+al_2019_a\">Liu et al, 2019</a>), Table 3: ImageNet classification results: training Wide ResNets 50-2 with soft parameter sharing leads to better performance by itself, without any tuning on the number of templates k",
        "Results on CIFAR suggest that training networks with few parameter templates k in our soft sharing scheme results in performance comparable to the base models, which have significantly more parameters",
        "We observe that rich structures emerge naturally in networks trained with soft parameter sharing, even without the recurrence regularizer",
        "We take a step toward more modular and compact convolutional neural network by extracting recurrences from feed-forward models where parameters are shared among layers",
        "We observe that parameter sharing often leads to different layers being functionally equivalent after training, enabling us to collapse them into recurrent blocks",
        "We gain a more flexible form of behavior typically attributed to recurrent neural networks, as our networks adapt better to out-of-domain examples",
        "As the only requirement for our method is for a network to have groups of layers with matching parameter sizes, it can be applied to a plethora of convolutional neural network model families, making it a general technique with negligible computational cost"
    ],
    "summary": [
        "The architectural details of convolutional neural networks (CNNs) have undergone rapid exploration and improvement via both human hand-design (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>; Zhu et al, 2018) and automated search methods (Zoph & Le, 2017; <a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\">Liu et al, 2018</a></a>).",
        "We observe improvements to both generalization ability and learning speed for our hybrid CNNs, in comparison to standard CNNs or RNNs. Our parameter sharing scheme, by virtue of providing an architectural bias towards networks with loops, appears to assist in learning to emulate traditional algorithms.",
        "A strict structure is imposed to layers of recurrent neural networks (RNNs), where, in standard models (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>), a single parameter set W is shared among all time steps.",
        "Using Wide ResNets (WRN) (Zagoruyko & Komodakis, 2016) as a base model, we train networks with the proposed soft parameter sharing method.",
        "Networks trained with our method yield superior performance in the setting with no parameter reduction: SWRN 28-10 presents 6.5% and 2.5% lower relative test errors on C-10 and C-100, compared to the base WRN 28-10 model.",
        "Our method outperforms architectures discovered by recent NAS algorithms, such as DARTS (<a class=\"ref-link\" id=\"cLiu_et+al_2019_a\" href=\"#rLiu_et+al_2019_a\">Liu et al, 2019</a>), Table 3: ImageNet classification results: training WRN 50-2 with soft parameter sharing leads to better performance by itself, without any tuning on the number of templates k.",
        "Without any change in hyperparameters, the network trained with our method outperforms the base model and deeper models such as DenseNets, and performs close to ResNet-200, a model with four times the number of layers and a similar parameter count.",
        "Results on CIFAR suggest that training networks with few parameter templates k in our soft sharing scheme results in performance comparable to the base models, which have significantly more parameters.",
        "We observe that rich structures emerge naturally in networks trained with soft parameter sharing, even without the recurrence regularizer.",
        "While the propensity of our parameter sharing scheme to encourage learning of recurrent networks is a useful parameter reduction tool, we would like to leverage it for qualitative advantages over standard CNNs. On tasks for which a natural recurrent algorithm exists, does training CNNs with soft parameter sharing lead to better extrapolation?",
        "Parameter sharing yields models with lower error on CIFAR and ImageNet, and can be used for parameter reduction by training in a regime with fewer parameter templates than layers.",
        "As the only requirement for our method is for a network to have groups of layers with matching parameter sizes, it can be applied to a plethora of CNN model families, making it a general technique with negligible computational cost.",
        "Our form of architecture discovery is competitive with neural architecture search (NAS) algorithms, while having a smaller training cost than state-of-the-art gradient-based NAS"
    ],
    "headline": "We introduce a parameter sharing scheme, in which different layers of a convolutional neural network are defined by a learned linear combination of parameter tensors from a global bank of templates",
    "reference_links": [
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Jonathon%20Shin%2C%20Richard%20Song%2C%20Dawn%20Making%20neural%20programming%20architectures%20generalize%20via%20recursion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Jonathon%20Shin%2C%20Richard%20Song%2C%20Dawn%20Making%20neural%20programming%20architectures%20generalize%20via%20recursion%202017"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. arXiv:1606.00915, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00915"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. Journal of Machine Learning Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DeVries%2C%20Terrance%20Taylor%2C%20Graham%20W.%20Improved%20regularization%20of%20convolutional%20neural%20networks%20with%20cutout%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DeVries%2C%20Terrance%20Taylor%2C%20Graham%20W.%20Improved%20regularization%20of%20convolutional%20neural%20networks%20with%20cutout%202017"
        },
        {
            "id": "Donahue_et+al_2015_a",
            "entry": "Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015"
        },
        {
            "id": "Graves_et+al_2013_a",
            "entry": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. ICASSP, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_a",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adria Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "Greff_et+al_2017_a",
            "entry": "K. Greff, R. K. Srivastava, and J. Schmidhuber. Highway and residual networks learn unrolled iterative estimation. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greff%2C%20K.%20Srivastava%2C%20R.K.%20Schmidhuber%2C%20J.%20Highway%20and%20residual%20networks%20learn%20unrolled%20iterative%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greff%2C%20K.%20Srivastava%2C%20R.K.%20Schmidhuber%2C%20J.%20Highway%20and%20residual%20networks%20learn%20unrolled%20iterative%20estimation%202017"
        },
        {
            "id": "Gross_2016_a",
            "entry": "Sam Gross and Martin Wilber. Training and investigating residual nets. https://github.com/facebook/fb.resnet.torch, 2016.",
            "url": "https://github.com/facebook/fb.resnet.torch"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "David Ha, Andrew Dai, and Quoc V. Le. Hypernetworks. arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "Han_et+al_2016_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. CondenseNet: An efficient DenseNet using learned group convolutions. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Shichen%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20CondenseNet%3A%20An%20efficient%20DenseNet%20using%20learned%20group%20convolutions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Shichen%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20CondenseNet%3A%20An%20efficient%20DenseNet%20using%20learned%20group%20convolutions%202018"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size. arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jastrzebski_et+al_2018_a",
            "entry": "Stanislaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jastrzebski%2C%20Stanislaw%20Arpit%2C%20Devansh%20Ballas%2C%20Nicolas%20Verma%2C%20Vikas%20Residual%20connections%20encourage%20iterative%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jastrzebski%2C%20Stanislaw%20Arpit%2C%20Devansh%20Ballas%2C%20Nicolas%20Verma%2C%20Vikas%20Residual%20connections%20encourage%20iterative%20inference%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Koutn_et+al_2014_a",
            "entry": "Jan Koutn\u0131k, Klaus Greff, Faustino Gomez, and Jurgen Schmidhuber. A clockwork RNN. arXiv:1402.3511, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.3511"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Liao_2016_a",
            "entry": "Qianli Liao and Tomaso A. Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv:1604.03640, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.03640"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L. Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018"
        },
        {
            "id": "Liu_et+al_2019_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Yang%2C%20Yiming%20DARTS%3A%20Differentiable%20architecture%20search%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Yang%2C%20Yiming%20DARTS%3A%20Differentiable%20architecture%20search%202019"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Pham_et+al_2018_a",
            "entry": "Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pham%2C%20Hieu%20Guan%2C%20Melody%20Zoph%2C%20Barret%20Le%2C%20Quoc%20Efficient%20neural%20architecture%20search%20via%20parameters%20sharing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pham%2C%20Hieu%20Guan%2C%20Melody%20Zoph%2C%20Barret%20Le%2C%20Quoc%20Efficient%20neural%20architecture%20search%20via%20parameters%20sharing%202018"
        },
        {
            "id": "Pinheiro_2014_a",
            "entry": "Pedro O. Pinheiro and Ronan Collobert. Recurrent convolutional neural networks for scene labeling. ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinheiro%2C%20Pedro%20O.%20Collobert%2C%20Ronan%20Recurrent%20convolutional%20neural%20networks%20for%20scene%20labeling%202014"
        },
        {
            "id": "Prabhu_et+al_2018_a",
            "entry": "Ameya Prabhu, Girish Varma, and Anoop M. Namboodiri. Deep expander networks: Efficient deep networks from graph theory. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prabhu%2C%20Ameya%20Varma%2C%20Girish%20Namboodiri%2C%20Anoop%20M.%20Deep%20expander%20networks%3A%20Efficient%20deep%20networks%20from%20graph%20theory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prabhu%2C%20Ameya%20Varma%2C%20Girish%20Namboodiri%2C%20Anoop%20M.%20Deep%20expander%20networks%3A%20Efficient%20deep%20networks%20from%20graph%20theory%202018"
        },
        {
            "id": "Real_et+al_2018_a",
            "entry": "Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "Reed_2016_a",
            "entry": "Scott E. Reed and Nando de Freitas. Neural programmer-interpreters. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Neural%20programmer-interpreters%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Neural%20programmer-interpreters%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "Shi_et+al_2015_a",
            "entry": "Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Xingjian%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Dit-Yan%20Yeung%2C%20Wai-Kin%20Wong%2C%20and%20Wang-chun%20Woo.%20Convolutional%20LSTM%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Xingjian%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Dit-Yan%20Yeung%2C%20Wai-Kin%20Wong%2C%20and%20Wang-chun%20Woo.%20Convolutional%20LSTM%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Trask_et+al_2018_a",
            "entry": "Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. arXiv:1808.00508, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.00508"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Saining Xie, Ross B. Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20B.%20Dollar%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20B.%20Dollar%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Xie_et+al_2019_a",
            "entry": "Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Sirui%20Zheng%2C%20Hehui%20Liu%2C%20Chunxiao%20Lin%2C%20Liang%20SNAS%3A%20stochastic%20neural%20architecture%20search%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Sirui%20Zheng%2C%20Hehui%20Liu%2C%20Chunxiao%20Lin%2C%20Liang%20SNAS%3A%20stochastic%20neural%20architecture%20search%202019"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20Show%2C%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20Show%2C%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "Yang_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Yibo Yang, Zhisheng Zhong, Tiancheng Shen, and Zhouchen Lin. Convolutional neural networks with alternately updated clique. CVPR, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. BMVC, 2016. Amir Roshan Zamir, Te-Lin Wu, Lin Sun, William B. Shen, Jitendra Malik, and Silvio Savarese.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yibo%20Zhong%2C%20Zhisheng%20Shen%2C%20Tiancheng%20Lin%2C%20Zhouchen%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yibo%20Zhong%2C%20Zhisheng%20Shen%2C%20Tiancheng%20Lin%2C%20Zhouchen%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Zaremba_et+al_2017_a",
            "entry": "Feedback networks. CVPR, 2017. Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple algorithms from examples. ICML, 2016. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. CVPR, 2017. Ligeng Zhu, Ruizhi Deng, Zhiwei Deng, Greg Mori, and Ping Tan. Sparsely aggregated convolutional networks. ECCV, 2018. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaremba%2C%20Tomas%20Mikolov%20Joulin%2C%20Armand%20Fergus%2C%20Rob%20Feedback%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaremba%2C%20Tomas%20Mikolov%20Joulin%2C%20Armand%20Fergus%2C%20Rob%20Feedback%20networks%202017"
        }
    ]
}
