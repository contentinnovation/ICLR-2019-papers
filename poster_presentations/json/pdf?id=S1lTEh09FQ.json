{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "COMBINATORIAL ATTACKS ON BINARIZED NEURAL NETWORKS",
        "author": "Elias B. Khalil College of Computing Georgia Tech lyes@gatech.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lTEh09FQ"
        },
        "abstract": "Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to \u201cattacks\u201d \u2013 tiny adversarial changes in the input \u2013 which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks. The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks. In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. We propose a Mixed Integer Linear Programming (MILP) formulation of the problem. While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow. To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems. Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP."
    },
    "keywords": [
        {
            "term": "linear programming",
            "url": "https://en.wikipedia.org/wiki/linear_programming"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "combinatorial optimization",
            "url": "https://en.wikipedia.org/wiki/combinatorial_optimization"
        },
        {
            "term": "mixed integer programming",
            "url": "https://en.wikipedia.org/wiki/mixed_integer_programming"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        }
    ],
    "abbreviations": {
        "BNNs": "Binarized Neural Networks",
        "MILP": "Mixed Integer Linear Programming",
        "PGD": "Projected Gradient Descent",
        "FGSM": "Fast Gradient Sign Method",
        "ReLU": "rectified linear unit"
    },
    "highlights": [
        "The success of neural networks in vision, text and speech tasks has led to their widespread deployment in commercial systems and devices",
        "A great deal of current research addresses the \u201crobustification\u201d of neural networks using adversarially generated examples (<a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>; <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>), a variant of standard gradient-based training that uses adversarial training examples to defend against possible attacks",
        "We propose IProp (Integer Propagation), an attack algorithm that exploits the discrete structure of a Binarized Neural Networks, as does the Mixed Integer Linear Programming, but is substantially more efficient",
        "We developed combinatorial search methods for generating adversarial examples that fool trained Binarized Neural Networks, based on a Mixed Integer Linear Programming (MILP) model and a target propagation-driven iterative algorithm IProp",
        "The ultimate goal is to \u201cattack to protect\u201d, i.e. to generate perturbations that can be used during adversarial training, resulting in Binarized Neural Networks that are robust to a class of perturbation",
        "Our Mixed Integer Linear Programming model cannot be solved quickly enough to be incorporated into adversarial training"
    ],
    "key_statements": [
        "The success of neural networks in vision, text and speech tasks has led to their widespread deployment in commercial systems and devices",
        "A great deal of current research addresses the \u201crobustification\u201d of neural networks using adversarially generated examples (<a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>; <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>), a variant of standard gradient-based training that uses adversarial training examples to defend against possible attacks",
        "We focus on designing effective attacks against Binarized Neural Networks (BNNs) (<a class=\"ref-link\" id=\"cCourbariaux_et+al_2016_a\" href=\"#rCourbariaux_et+al_2016_a\">Courbariaux et al, 2016</a>)",
        "Towards designing optimal attacks against Binarized Neural Networks, we propose to model the task of generating an adversarial perturbation as a Mixed Integer Linear Program (MILP)",
        "We propose IProp (Integer Propagation), an attack algorithm that exploits the discrete structure of a Binarized Neural Networks, as does the Mixed Integer Linear Programming, but is substantially more efficient",
        "We evaluate the Mixed Integer Linear Programming model, IProp and the Projected Gradient Descent method (PGD) (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>) \u2013 a representative gradient-based attack \u2013 on Binarized Neural Networks models pre-trained on the MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>) and Fashion-MNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al, 2017</a>) datasets",
        "We briefly introduce our Mixed Integer Linear Programming formulation for the Binarized Neural Networks attack problem",
        "As well as the limitations of Mixed Integer Linear Programming solving, we propose IProp, a Binarized Neural Networks attack algorithm that operates directly on the original Binarized Neural Networks, rather than an approximation of it",
        "We developed combinatorial search methods for generating adversarial examples that fool trained Binarized Neural Networks, based on a Mixed Integer Linear Programming (MILP) model and a target propagation-driven iterative algorithm IProp",
        "The ultimate goal is to \u201cattack to protect\u201d, i.e. to generate perturbations that can be used during adversarial training, resulting in Binarized Neural Networks that are robust to a class of perturbation",
        "Our Mixed Integer Linear Programming model cannot be solved quickly enough to be incorporated into adversarial training"
    ],
    "summary": [
        "The success of neural networks in vision, text and speech tasks has led to their widespread deployment in commercial systems and devices.",
        "Towards designing optimal attacks against BNNs, we propose to model the task of generating an adversarial perturbation as a Mixed Integer Linear Program (MILP).",
        "In the following MILP formulation, the constraints essentially implement a forward pass in the BNN, from the perturbed input to the output layer.",
        "After obtaining T1 as a solution to the last optimization problem in the aforementioned sequence, one can search for a perturbation of x that produces T1, by solving the following mixed binary program: r p = arg max I{h1,j = T1(j)} s.t. h1 = sign(W1(x + p )), 0 \u2264 x + p \u2264 1.",
        "Both of the questions we raised effectively relate to the perturbation budget : as IProp decomposes the attack into layer-to-layer problems (9) and (10), it is easy to lose track of the global constraint , which makes many targets Tl impossible to achieve.",
        "When solving problem (9) at the last hidden layer, we restrict the summation in the objective function to a subset of all neurons; this has the effect of only rewarding target satisfaction up to a limit, so as to not produce overly optimistic solutions that will not withstand the bound .",
        "We use the Gurobi Python API to implement and solve our MILP problems, and an implementation of iterated PGD in PyTorch.",
        "For small perturbation budgets and networks, the MILP approach finds optimal attacks within the time cutoff, but as and network size grow, solving the MILP becomes increasingly computationally intensive and only the best-found solution at timeout is returned.",
        "IProp, achieves a success rate close to the optimal MILP performance on small networks and , and scales better than the MILP approach.",
        "With Figure 2, IProp achieves higher values on average than PGD, indicating that the IProp attacks are more effective at modifying the output-layer activations of the networks.",
        "Figure 7 shows that warm starting IProp in this manner has the potential to significantly improve the success rate of the resulting attacks, highlighting the value of finding good initial solutions our method, which is essentially a combinatorial local search approach.",
        "We developed combinatorial search methods for generating adversarial examples that fool trained Binarized Neural Networks, based on a Mixed Integer Linear Programming (MILP) model and a target propagation-driven iterative algorithm IProp.",
        "Our MILP model results show that standard (PGD) attack methods often are suboptimal in generating good adversarial examples when the perturbation budget is limited.",
        "Through extensive experiments we have shown that our iterative algorithm IProp is able to scale-up this solving"
    ],
    "headline": "We study the problem of attacking a Binarized Neural Networks through the lens of combinatorial and integer optimization",
    "reference_links": [
        {
            "id": "Alemdar_et+al_2017_a",
            "entry": "Hande Alemdar, Vincent Leroy, Adrien Prost-Boucle, and Frederic Petrot. Ternary neural networks for resource-efficient ai applications. In Neural Networks (IJCNN), 2017 International Joint Conference on, pp. 2547\u20132554. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemdar%2C%20Hande%20Leroy%2C%20Vincent%20Prost-Boucle%2C%20Adrien%20Petrot%2C%20Frederic%20Ternary%20neural%20networks%20for%20resource-efficient%20ai%20applications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemdar%2C%20Hande%20Leroy%2C%20Vincent%20Prost-Boucle%2C%20Adrien%20Petrot%2C%20Frederic%20Ternary%20neural%20networks%20for%20resource-efficient%20ai%20applications%202017"
        },
        {
            "id": "Anderson_et+al_2018_a",
            "entry": "Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong convex relaxations and mixed-integer programming formulations for trained neural networks. arXiv preprint arXiv:1811.01988, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.01988"
        },
        {
            "id": "Bengio_2014_a",
            "entry": "Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv preprint arXiv:1407.7906, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.7906"
        },
        {
            "id": "Bertsimas_2017_a",
            "entry": "Dimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: Exact scalable algorithms and phase transitions. arXiv preprint arXiv:1709.10029, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10029"
        },
        {
            "id": "Bertsimas_et+al_2017_b",
            "entry": "Dimitris Bertsimas, Jean Pauphilet, and Bart Van Parys. Sparse classification and phase transitions: A discrete optimization perspective. arXiv preprint arXiv:1710.01352, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.01352"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Courbariaux_et+al_2016_a",
            "entry": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "Dai_et+al_2017_a",
            "entry": "Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Hanjun%20Khalil%2C%20Elias%20B.%20Zhang%2C%20Yuyu%20Dilkina%2C%20Bistra%20Learning%20combinatorial%20optimization%20algorithms%20over%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Hanjun%20Khalil%2C%20Elias%20B.%20Zhang%2C%20Yuyu%20Dilkina%2C%20Bistra%20Learning%20combinatorial%20optimization%20algorithms%20over%20graphs%202017"
        },
        {
            "id": "Fischetti_2018_a",
            "entry": "Matteo Fischetti and Jason Jo. Deep neural networks and mixed integer linear optimization. Constraints, 23:296\u2013309, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fischetti%2C%20Matteo%20Jo%2C%20Jason%20Deep%20neural%20networks%20and%20mixed%20integer%20linear%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fischetti%2C%20Matteo%20Jo%2C%20Jason%20Deep%20neural%20networks%20and%20mixed%20integer%20linear%20optimization%202018"
        },
        {
            "id": "Friesen_2017_a",
            "entry": "Abram L Friesen and Pedro Domingos. Deep learning as a mixed convex-combinatorial optimization problem. arXiv preprint arXiv:1710.11573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11573"
        },
        {
            "id": "Galloway_et+al_2017_a",
            "entry": "Angus Galloway, Graham W Taylor, and Medhat Moussa. Attacking binarized neural networks. arXiv preprint arXiv:1711.00449, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00449"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "He_et+al_2014_a",
            "entry": "He He, Hal Daume III, and Jason Eisner. Learning to search in branch-and-bound algorithms. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20He%20Daume%2C%20III%2C%20Hal%20Eisner%2C%20Jason%20Learning%20to%20search%20in%20branch-and-bound%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20He%20Daume%2C%20III%2C%20Hal%20Eisner%2C%20Jason%20Learning%20to%20search%20in%20branch-and-bound%20algorithms%202014"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Khalil_2016_a",
            "entry": "Elias Boutros Khalil, Pierre Le Bodic, Le Song, George L Nemhauser, and Bistra N Dilkina. Learning to branch in mixed integer programming. In AAAI, pp. 724\u2013731, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khalil%2C%20Elias%20Boutros%20Pierre%20Le%20Bodic%2C%20Le%20Song%2C%20George%20L%20Nemhauser%2C%20and%20Bistra%20N%20Dilkina.%20Learning%20to%20branch%20in%20mixed%20integer%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khalil%2C%20Elias%20Boutros%20Pierre%20Le%20Bodic%2C%20Le%20Song%2C%20George%20L%20Nemhauser%2C%20and%20Bistra%20N%20Dilkina.%20Learning%20to%20branch%20in%20mixed%20integer%20programming%202016"
        },
        {
            "id": "Kolter_2017_a",
            "entry": "J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "Kruber_et+al_2016_a",
            "entry": "Markus Kruber, Marco E Lubbecke, and Axel Parmentier. Learning when to use a decomposition. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kruber%2C%20Markus%20Lubbecke%2C%20Marco%20E.%20Parmentier%2C%20Axel%20Learning%20when%20to%20use%20a%20decomposition%202016"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Mcdanel_et+al_2017_a",
            "entry": "Bradley McDanel, Surat Teerapittayanon, and HT Kung. Embedded binarized neural networks. In Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks, pp. 168\u2013173. Junction Publishing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDanel%2C%20Bradley%20Teerapittayanon%2C%20Surat%20Kung%2C%20H.T.%20Embedded%20binarized%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDanel%2C%20Bradley%20Teerapittayanon%2C%20Surat%20Kung%2C%20H.T.%20Embedded%20binarized%20neural%20networks%202017"
        },
        {
            "id": "Dezfooli_et+al_2016_a",
            "entry": "Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Narodytska_et+al_2017_a",
            "entry": "Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv, and Toby Walsh. Verifying properties of binarized deep neural networks. arXiv preprint arXiv:1709.06662, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06662"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "Sinha_et+al_2017_a",
            "entry": "Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "Spall_1992_a",
            "entry": "James C Spall et al. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE transactions on automatic control, 37(3):332\u2013341, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spall%2C%20James%20C.%20Multivariate%20stochastic%20approximation%20using%20a%20simultaneous%20perturbation%20gradient%20approximation%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spall%2C%20James%20C.%20Multivariate%20stochastic%20approximation%20using%20a%20simultaneous%20perturbation%20gradient%20approximation%201992"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tjeng_et+al_2017_a",
            "entry": "Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07356"
        },
        {
            "id": "Uesato_et+al_2018_a",
            "entry": "Jonathan Uesato, Brendan O\u2019Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05666"
        },
        {
            "id": "Umuroglu_et+al_2017_a",
            "entry": "Yaman Umuroglu, Nicholas J Fraser, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, and Kees Vissers. Finn: A framework for fast, scalable binarized neural network inference. In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 65\u201374. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Umuroglu%2C%20Yaman%20Fraser%2C%20Nicholas%20J.%20Gambardella%2C%20Giulio%20Blott%2C%20Michaela%20Finn%3A%20A%20framework%20for%20fast%2C%20scalable%20binarized%20neural%20network%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Umuroglu%2C%20Yaman%20Fraser%2C%20Nicholas%20J.%20Gambardella%2C%20Giulio%20Blott%2C%20Michaela%20Finn%3A%20A%20framework%20for%20fast%2C%20scalable%20binarized%20neural%20network%20inference%202017"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Han%20Rasul%2C%20Kashif%20Vollgraf%2C%20Roland%20Fashion-mnist%3A%20a%20novel%20image%20dataset%20for%20benchmarking%20machine%20learning%20algorithms%202017"
        }
    ]
}
