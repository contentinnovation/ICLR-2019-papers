{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A UNIVERSAL MUSIC TRANSLATION NETWORK",
        "author": "Noam Mor",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJGkisCcKm",
            "doi": "10.1162/COMJ"
        },
        "abstract": "We present a method for translating music across musical instruments and styles. This method is based on unsupervised training of a multi-domain wavenet autoencoder, with a shared encoder and a domain-independent latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the single encoder allows us to translate also from musical domains that were not seen during training. We evaluate our method on a dataset collected from professional musicians, and achieve convincing translations. We also study the properties of the obtained translation and demonstrate translating even from a whistle, potentially enabling the creation of instrumental music by untrained humans."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "musical instrument",
            "url": "https://en.wikipedia.org/wiki/musical_instrument"
        },
        {
            "term": "Amazon Mechanical Turk",
            "url": "https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"
        },
        {
            "term": "Mean Opinion Scores",
            "url": "https://en.wikipedia.org/wiki/Mean_Opinion_Score"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        }
    ],
    "abbreviations": {
        "MMD": "Maximum Mean Discrepancy",
        "MOS": "Mean Opinion Scores",
        "AMT": "Amazon Mechanical Turk"
    },
    "highlights": [
        "Humans have always created music and replicated it \u2013 whether it is by singing, whistling, clapping, or, after some training, playing improvised or standard musical instruments",
        "We impose a greater constraint on the latent space by (a) having a universal encoder, forcing the embeddings of all domains to lie in the same space, yet (b) training a separate reconstructing decoder for each domain, provided that (c) the latent space is domain independent, thereby reducing source-target pathways memorization, which is accomplished by (d) employing augmentation to distort the input signal",
        "Are we doing more than timbral transfer? Is our system equivalent to pitch estimation followed by rendering with a different instrument, or can it capture stylistic musical elements? We demonstrate that our system does more than timbral transfer in two ways",
        "We reduce the size of the latent space to 8 to limit the ability of the original input to be repeated exactly, thereby encouraging the decoders to be more \u201dcreative\u201d than they normally would, with the goal of observing how decoders trained on different training data will use their freedom of expression",
        "Our work demonstrates capabilities in music conversion, which is a high-level task, and could open the door to other high-level tasks, such as composition",
        "We have initial results that we find interesting: by reducing the size of the latent space, the decoders become more \u201ccreative\u201d and produce outputs that are natural yet novel, in the sense that the exact association with the original input is lost"
    ],
    "key_statements": [
        "Humans have always created music and replicated it \u2013 whether it is by singing, whistling, clapping, or, after some training, playing improvised or standard musical instruments",
        "We impose a greater constraint on the latent space by (a) having a universal encoder, forcing the embeddings of all domains to lie in the same space, yet (b) training a separate reconstructing decoder for each domain, provided that (c) the latent space is domain independent, thereby reducing source-target pathways memorization, which is accomplished by (d) employing augmentation to distort the input signal",
        "Are we doing more than timbral transfer? Is our system equivalent to pitch estimation followed by rendering with a different instrument, or can it capture stylistic musical elements? We demonstrate that our system does more than timbral transfer in two ways",
        "We reduce the size of the latent space to 8 to limit the ability of the original input to be repeated exactly, thereby encouraging the decoders to be more \u201dcreative\u201d than they normally would, with the goal of observing how decoders trained on different training data will use their freedom of expression",
        "We employ a second network that is used for a related task of voice conversion and demonstrate that in the case of voice conversion, latent space embedding is clearly superior to converting the audio itself",
        "Our work demonstrates capabilities in music conversion, which is a high-level task, and could open the door to other high-level tasks, such as composition",
        "We have initial results that we find interesting: by reducing the size of the latent space, the decoders become more \u201ccreative\u201d and produce outputs that are natural yet novel, in the sense that the exact association with the original input is lost"
    ],
    "summary": [
        "Humans have always created music and replicated it \u2013 whether it is by singing, whistling, clapping, or, after some training, playing improvised or standard musical instruments.",
        "The method employs a single generator that receives as input the source image as well as the specification of the target domain.",
        "The decoder is based on WaveNet. we impose a greater constraint on the latent space by (a) having a universal encoder, forcing the embeddings of all domains to lie in the same space, yet (b) training a separate reconstructing decoder for each domain, provided that (c) the latent space is domain independent, thereby reducing source-target pathways memorization, which is accomplished by (d) employing augmentation to distort the input signal.",
        "Our overall architecture differs in that multiple decoders and an additional auxiliary network, which is used for disentangling the domain information from the other aspects of the music representation, are trained and by the addition of an important augmentation step.",
        "Our domain translation method is based on training multiple autoencoder pathways, one per musical domain, such that the encoders are shared.",
        "Since there is a trade-off between the fidelity to the original piece and the ability to create audio in the target domain, we present two scores: audio quality of the output piano and a matching score for the translation.",
        "Supplementary S2 presents a clear stylistic difference: one can hear some counterpoint in the Bach sample, whereas the Beethoven output exhibits a more \u201cSturm und Drang\u201d feeling, indicating that the network learns stylistic elements from the training data.",
        "Samples 1 and 2 are compared with the published results of <a class=\"ref-link\" id=\"cColeman_2016_a\" href=\"#rColeman_2016_a\">Coleman (2016</a>), a work comparing several Concatenative Synthesis methods, and uses a violin passage as source audio input.",
        "Universality Note that our network has never observed drums, brass instruments or an entire orchestra during training, and, the results of supplementary S3 serve to demonstrate the versatility of the encoder module resulting from our training procedure.",
        "In order to investigate the option of not using augmentation in a domain where training without it converges, we have applied our method to the task of voice conversion.",
        "We employ a second network that is used for a related task of voice conversion and demonstrate that in the case of voice conversion, latent space embedding is clearly superior to converting the audio itself.",
        "We have initial results that we find interesting: by reducing the size of the latent space, the decoders become more \u201ccreative\u201d and produce outputs that are natural yet novel, in the sense that the exact association with the original input is lost."
    ],
    "headline": "We present a method for translating music across musical instruments and styles",
    "reference_links": [
        {
            "id": "Barry_2018_a",
            "entry": "Shaun Barry and Youngmoo Kim. style transfer for musical audio using multiple time-frequency representations, 2018. URL https://openreview.net/forum?id=BybQ7zWCb.",
            "url": "https://openreview.net/forum?id=BybQ7zWCb"
        },
        {
            "id": "Choi_et+al_2017_a",
            "entry": "Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation. In arXiv preprint 1711.09020, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09020"
        },
        {
            "id": "Clevert_et+al_2017_a",
            "entry": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clevert%2C%20Djork-Arne%20Unterthiner%2C%20Thomas%20Hochreiter%2C%20Sepp%20Fast%20and%20accurate%20deep%20network%20learning%20by%20exponential%20linear%20units%20%28elus%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clevert%2C%20Djork-Arne%20Unterthiner%2C%20Thomas%20Hochreiter%2C%20Sepp%20Fast%20and%20accurate%20deep%20network%20learning%20by%20exponential%20linear%20units%20%28elus%29%202017"
        },
        {
            "id": "Coleman_2016_a",
            "entry": "G Coleman. Descriptor Control of Sound Transformations and Mosaicing Synthesis. PhD thesis, Universitat Pompeu Fabra, Barcelona, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coleman%2C%20G.%20Descriptor%20Control%20of%20Sound%20Transformations%20and%20Mosaicing%20Synthesis%202016"
        },
        {
            "id": "Dieleman_et+al_2018_a",
            "entry": "Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music generation: modelling raw audio at scale. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieleman%2C%20Sander%20van%20den%20Oord%2C%20Aaron%20Simonyan%2C%20Karen%20The%20challenge%20of%20realistic%20music%20generation%3A%20modelling%20raw%20audio%20at%20scale%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieleman%2C%20Sander%20van%20den%20Oord%2C%20Aaron%20Simonyan%2C%20Karen%20The%20challenge%20of%20realistic%20music%20generation%3A%20modelling%20raw%20audio%20at%20scale%202018"
        },
        {
            "id": "Engel_et+al_2017_a",
            "entry": "Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan. Neural audio synthesis of musical notes with WaveNet autoencoders. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017"
        },
        {
            "id": "Ganin_et+al_2016_a",
            "entry": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):2096\u20132030, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "Gatys_et+al_2016_a",
            "entry": "Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "Hadjeres_2017_a",
            "entry": "Gaetan Hadjeres and Francois Pachet. DeepBach: a steerable model for bach chorales generation. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadjeres%2C%20Gaetan%20Pachet%2C%20Francois%20DeepBach%3A%20a%20steerable%20model%20for%20bach%20chorales%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadjeres%2C%20Gaetan%20Pachet%2C%20Francois%20DeepBach%3A%20a%20steerable%20model%20for%20bach%20chorales%20generation%202017"
        },
        {
            "id": "Haque_et+al_2018_a",
            "entry": "Albert Haque, Michelle Guo, and Prateek Verma. Conditional end-to-end audio transforms. In Arxiv preprint 1804.00047, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00047"
        },
        {
            "id": "The_2017_a",
            "entry": "The lj speech dataset, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20lj%20speech%20dataset%202017"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016"
        },
        {
            "id": "Kaneko_2017_a",
            "entry": "Takuhiro Kaneko and Hirokazu Kameoka. Parallel-data-free voice conversion using cycle-consistent adversarial networks. arXiv preprint arXiv:1711.11293, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11293"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to Discover Cross-Domain Relations with Generative Adversarial Networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Taeksoo%20Cha%2C%20Moonsu%20Kim%2C%20Hyunsoo%20Lee%2C%20Jungkwon%20Learning%20to%20Discover%20Cross-Domain%20Relations%20with%20Generative%20Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Taeksoo%20Cha%2C%20Moonsu%20Kim%2C%20Hyunsoo%20Lee%2C%20Jungkwon%20Learning%20to%20Discover%20Cross-Domain%20Relations%20with%20Generative%20Adversarial%20Networks%202017"
        },
        {
            "id": "King_2011_a",
            "entry": "Simon King and Vasilis Karaiskos. The blizzard challenge 2011. In Blizzard Challenge workshop, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%20King%20and%20Vasilis%20Karaiskos%20The%20blizzard%20challenge%202011%20In%20Blizzard%20Challenge%20workshop%202011"
        },
        {
            "id": "King_2013_a",
            "entry": "Simon King and Vasilis Karaiskos. The blizzard challenge 2013. In Blizzard Challenge workshop, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%20King%20and%20Vasilis%20Karaiskos%20The%20blizzard%20challenge%202013%20In%20Blizzard%20Challenge%20workshop%202013"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Stat, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "Lample_et+al_2017_a",
            "entry": "Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, et al. Fader networks: Manipulating images by sliding attributes. In Advances in Neural Information Processing Systems, pp. 5967\u20135976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20Guillaume%20Zeghidour%2C%20Neil%20Usunier%2C%20Nicolas%20Bordes%2C%20Antoine%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20Guillaume%20Zeghidour%2C%20Neil%20Usunier%2C%20Nicolas%20Bordes%2C%20Antoine%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%202017"
        },
        {
            "id": "Liu_2016_a",
            "entry": "Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NIPS, pp. 469\u2013477. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ming-Yu%20Tuzel%2C%20Oncel%20Coupled%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ming-Yu%20Tuzel%2C%20Oncel%20Coupled%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NIPS. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "Louizos_et+al_2015_a",
            "entry": "Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair autoencoder. arXiv preprint arXiv:1511.00830, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.00830"
        },
        {
            "id": "Mcfee_et+al_2015_a",
            "entry": "Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McFee%2C%20Brian%20Raffel%2C%20Colin%20Liang%2C%20Dawen%20Ellis%2C%20Daniel%20P.W.%20Eric%20Battenberg%2C%20and%20Oriol%20Nieto.%20librosa%3A%20Audio%20and%20music%20signal%20analysis%20in%20python%202015"
        },
        {
            "id": "Mor_et+al_2018_a",
            "entry": "Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taigman. A universal music translation network. arXiv preprint arXiv:1805.07848, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07848"
        },
        {
            "id": "Nuanain_et+al_2017_a",
            "entry": "Carthach O Nuanain, Perfecto Herrera, and Sergi Jorda. Rhythmic concatenative synthesis for electronic music: Techniques, implementation, and evaluation. Computer Music Journal, 41(2):21\u2013 37, June 2017. ISSN 0148-9267. doi: 10.1162/COMJ a 00412.",
            "crossref": "https://dx.doi.org/10.1162/COMJ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/COMJ"
        },
        {
            "id": "Ping_et+al_2018_a",
            "entry": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan Omer Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20Omer%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20Omer%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20ICLR%202018"
        },
        {
            "id": "Rethage_et+al_2017_a",
            "entry": "Dario Rethage, Jordi Pons, and Xavier Serra. A wavenet for speech denoising. arXiv preprint arXiv:1706.07162, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.07162"
        },
        {
            "id": "Ribeiro_et+al_2011_a",
            "entry": "Flavio Ribeiro, Dinei Florencio, Cha Zhang, and Michael Seltzer. Crowdmos: An approach for crowdsourcing mean opinion score studies. In Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference, pp. 2416\u20132419. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Flavio%20Florencio%2C%20Dinei%20Zhang%2C%20Cha%20Seltzer%2C%20Michael%20Crowdmos%3A%20An%20approach%20for%20crowdsourcing%20mean%20opinion%20score%20studies%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Flavio%20Florencio%2C%20Dinei%20Zhang%2C%20Cha%20Seltzer%2C%20Michael%20Crowdmos%3A%20An%20approach%20for%20crowdsourcing%20mean%20opinion%20score%20studies%202011"
        },
        {
            "id": "Schwarz_2006_a",
            "entry": "Diemo Schwarz. Concatenative sound synthesis: The early years. Journal of New Music Research, 35(1):3\u201322, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarz%2C%20Diemo%20Concatenative%20sound%20synthesis%3A%20The%20early%20years%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarz%2C%20Diemo%20Concatenative%20sound%20synthesis%3A%20The%20early%20years%202006"
        },
        {
            "id": "Schwarz_2018_a",
            "entry": "Diemo Schwarz. Corpus-based sound synthesis survey, 2018. URL http://imtr.ircam.fr/imtr/Corpus-Based_Sound_Synthesis_Survey.",
            "url": "http://imtr.ircam.fr/imtr/Corpus-Based_Sound_Synthesis_Survey"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, R. J. Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions. In ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20wavenet%20on%20mel%20spectrogram%20predictions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20wavenet%20on%20mel%20spectrogram%20predictions%202018"
        },
        {
            "id": "Simon_et+al_2005_a",
            "entry": "Ian Simon, Sumit Basu, David Salesin, and Maneesh Agrawala. Audio analogies: Creating new music from an existing performance by concatenative synthesis. In Int. Computer Music Conference, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%2C%20Ian%20Basu%2C%20Sumit%20Salesin%2C%20David%20Agrawala%2C%20Maneesh%20Audio%20analogies%3A%20Creating%20new%20music%20from%20an%20existing%20performance%20by%20concatenative%20synthesis%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simon%2C%20Ian%20Basu%2C%20Sumit%20Salesin%2C%20David%20Agrawala%2C%20Maneesh%20Audio%20analogies%3A%20Creating%20new%20music%20from%20an%20existing%20performance%20by%20concatenative%20synthesis%202005"
        },
        {
            "id": "Sturm_2006_a",
            "entry": "Bob L Sturm. Adaptive concatenative sound synthesis and its application to micromontage composition. Computer Music Journal, 30(4):46\u201366, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sturm%2C%20Bob%20L.%20Adaptive%20concatenative%20sound%20synthesis%20and%20its%20application%20to%20micromontage%20composition%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sturm%2C%20Bob%20L.%20Adaptive%20concatenative%20sound%20synthesis%20and%20its%20application%20to%20micromontage%20composition%202006"
        },
        {
            "id": "Thickstun_et+al_2017_a",
            "entry": "John Thickstun, Zaid Harchaoui, and Sham Kakade. Learning Features of Music From Scratch. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thickstun%2C%20John%20Harchaoui%2C%20Zaid%20Kakade%2C%20Sham%20Learning%20Features%20of%20Music%20From%20Scratch%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thickstun%2C%20John%20Harchaoui%2C%20Zaid%20Kakade%2C%20Sham%20Learning%20Features%20of%20Music%20From%20Scratch%202017"
        },
        {
            "id": "Ulyanov_et+al_2016_a",
            "entry": "D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ulyanov%2C%20D.%20Lebedev%2C%20V.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Texture%20networks%3A%20Feed-forward%20synthesis%20of%20textures%20and%20stylized%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ulyanov%2C%20D.%20Lebedev%2C%20V.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Texture%20networks%3A%20Feed-forward%20synthesis%20of%20textures%20and%20stylized%20images%202016"
        },
        {
            "id": "Van_et+al_2017_a",
            "entry": "Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aaron%20van%20den%20Oord%20Oriol%20Vinyals%20and%20Koray%20Kavukcuoglu%20Neural%20Discrete%20Representation%20Learning%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aaron%20van%20den%20Oord%20Oriol%20Vinyals%20and%20Koray%20Kavukcuoglu%20Neural%20Discrete%20Representation%20Learning%20In%20NIPS%202017"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Arxiv preprint 1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Verfaille_2000_a",
            "entry": "V Verfaille and D Arfib. A-dafx: Adaptive digital audio effects. energy, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verfaille%2C%20V.%20Arfib%2C%20D.%20A-dafx%3A%20Adaptive%20digital%20audio%20effects%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verfaille%2C%20V.%20Arfib%2C%20D.%20A-dafx%3A%20Adaptive%20digital%20audio%20effects%202000"
        },
        {
            "id": "Yi_et+al_2017_a",
            "entry": "Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. DualGAN: Unsupervised dual learning for image-to-image translation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20Zili%20Zhang%2C%20Hao%20Tan%2C%20Ping%20Gong%2C%20Minglun%20DualGAN%3A%20Unsupervised%20dual%20learning%20for%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20Zili%20Zhang%2C%20Hao%20Tan%2C%20Ping%20Gong%2C%20Minglun%20DualGAN%3A%20Unsupervised%20dual%20learning%20for%20image-to-image%20translation%202017"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "Zils_2001_a",
            "entry": "Aymeric Zils and Francois Pachet. Musical mosaicing. In Digital Audio Effects (DAFx), volume 2, pp. 135, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zils%2C%20Aymeric%20Pachet%2C%20Francois%20Musical%20mosaicing%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zils%2C%20Aymeric%20Pachet%2C%20Francois%20Musical%20mosaicing%202001"
        }
    ]
}
