{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON SELF MODULATION FOR GENERATIVE ADVERSARIAL NETWORKS",
        "author": "Ting Chen",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hkl5aoR5tm"
        },
        "journal": "Generative Adversarial Networks",
        "abstract": "Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of 5% \u2212 35% in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in 124/144 (86%) of the studied settings. Selfmodulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN.1"
    },
    "keywords": [
        {
            "term": "nash equilibrium",
            "url": "https://en.wikipedia.org/wiki/nash_equilibrium"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "CBN": "conditional batch normalization",
        "FID": "Frechet Inception Distance"
    },
    "highlights": [
        "Generative Adversarial Networks (GANs) are a powerful class of generative models successfully applied to a variety of tasks such as image generation (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; Miyato et al, 2018; <a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\">Karras et al, 2017</a></a></a>), learned compression (<a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\"><a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\">Tschannen et al, 2018</a></a>), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\"><a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a></a>), inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al, 2016</a></a>), and domain transfer (<a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\">Isola et al, 2016</a></a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a></a>).<br/><br/>Training Generative Adversarial Networks is a notoriously challenging task (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\"><a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\">Lucic et al, 2018</a></a>) as one is searching in a high-dimensional parameter space for a Nash equilibrium of a non-convex game",
        "In this work we show that Generative Adversarial Networks benefit from self-modulation layers in the generator",
        "We provide a simple yet effective technique that can added universally to yield better Generative Adversarial Networks",
        "We compare standard label-conditional batch normalization to self-modulation with additional labels, as discussed in Section 2, Equation 3",
        "Initial proposals were based on concatenating this additional feature with the input vector (<a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\">Mirza & Osindero, 2014</a>; <a class=\"ref-link\" id=\"cRadford_et+al_2016_a\" href=\"#rRadford_et+al_2016_a\">Radford et al, 2016</a>; <a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\">Odena et al, 2017</a>)",
        "We present a generator modification that improves the performance of most Generative Adversarial Networks"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) are a powerful class of generative models successfully applied to a variety of tasks such as image generation (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; Miyato et al, 2018; <a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\">Karras et al, 2017</a></a></a>), learned compression (<a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\"><a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\">Tschannen et al, 2018</a></a>), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\"><a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a></a>), inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al, 2016</a></a>), and domain transfer (<a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\">Isola et al, 2016</a></a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a></a>).<br/><br/>Training Generative Adversarial Networks is a notoriously challenging task (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\"><a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\">Lucic et al, 2018</a></a>) as one is searching in a high-dimensional parameter space for a Nash equilibrium of a non-convex game",
        "In this work we show that Generative Adversarial Networks benefit from self-modulation layers in the generator",
        "We provide a simple yet effective technique that can added universally to yield better Generative Adversarial Networks",
        "We compare standard label-conditional batch normalization to self-modulation with additional labels, as discussed in Section 2, Equation 3",
        "Initial proposals were based on concatenating this additional feature with the input vector (<a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\">Mirza & Osindero, 2014</a>; <a class=\"ref-link\" id=\"cRadford_et+al_2016_a\" href=\"#rRadford_et+al_2016_a\">Radford et al, 2016</a>; <a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\">Odena et al, 2017</a>)",
        "The proposed method applies to generators in Generative Adversarial Networks, and it works with both unconditional and conditional settings",
        "We present a generator modification that improves the performance of most Generative Adversarial Networks",
        "This technique is simple to implement and can be applied to all popular Generative Adversarial Networks, we believe that selfmodulation is a useful addition to the Generative Adversarial Networks toolbox",
        "Overall the correlation between Frechet Inception Distance and condition number is smaller for self-modulated models"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) are a powerful class of generative models successfully applied to a variety of tasks such as image generation (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; Miyato et al, 2018; <a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\"><a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\">Karras et al, 2017</a></a></a>), learned compression (<a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\"><a class=\"ref-link\" id=\"cTschannen_et+al_2018_a\" href=\"#rTschannen_et+al_2018_a\">Tschannen et al, 2018</a></a>), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\"><a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a></a>), inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al, 2016</a></a>), and domain transfer (<a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2016_a\" href=\"#rIsola_et+al_2016_a\">Isola et al, 2016</a></a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a></a>).<br/><br/>Training GANs is a notoriously challenging task (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\"><a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\"><a class=\"ref-link\" id=\"cLucic_et+al_2018_a\" href=\"#rLucic_et+al_2018_a\">Lucic et al, 2018</a></a>) as one is searching in a high-dimensional parameter space for a Nash equilibrium of a non-convex game.",
        "In this work we show that GANs benefit from self-modulation layers in the generator.",
        "Several recent works observe that conditioning the generative process on side information leads to improved models (<a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\"><a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\">Mirza & Osindero, 2014</a></a>; <a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\"><a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\">Odena et al, 2017</a></a>; <a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\">Miyato & Koyama, 2018</a></a></a></a>).",
        "We propose self-modulating layers for the generator network.",
        "Other Arbitrary layers Conditional batch normalization (De Vries et al, 2017; <a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\">Miyato & Koyama, 2018</a></a></a></a>) (Unconditional) Self-Modulation",
        "We perform a large-scale study of self-modulation to demonstrate that this method yields robust improvements in a variety of settings.",
        "We run a Cartesian product of the parameters in Section 3.1 which results in 36 settings for each dataset (2 losses, 2 architectures, 3 hyperparameter settings for spectral normalization, and 6 for gradient penalty).",
        "We demonstrate that self-modulation works for label-conditional generation.",
        "We compare two settings: (1) Generator conditioning is applied via label-conditional Batch Norm (De Vries et al, 2017; <a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\">Miyato & Koyama, 2018</a></a></a></a>) with no use of labels in the discriminator (G-COND).",
        "We compare standard label-conditional batch normalization to self-modulation with additional labels, as discussed in Section 2, Equation 3.",
        "We observe that the simple incorporation of self-modulation leads to a significant improvement in performance in the considered settings.",
        "To demonstrate that self-modulation continues to yield improvement after training for longer, we train IMAGENET for 500k generator steps.",
        "Due to the increased computational demand we use a single setting for the unconditional and conditional settings models following Miyato et al (2018) and <a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\">Miyato & Koyama (2018</a>), but using only two discriminator steps per generator.",
        "After 500k steps the baseline unconditional model attains FID 60.4, self-modulation attains 53.7 (11% improvement).",
        "Conditional GANs. Conditioning on side information, such as class labels, has been shown to improve the performance of GANs. Initial proposals were based on concatenating this additional feature with the input vector (<a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\"><a class=\"ref-link\" id=\"cMirza_2014_a\" href=\"#rMirza_2014_a\">Mirza & Osindero, 2014</a></a>; <a class=\"ref-link\" id=\"cRadford_et+al_2016_a\" href=\"#rRadford_et+al_2016_a\">Radford et al, 2016</a>; <a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\"><a class=\"ref-link\" id=\"cOdena_et+al_2017_a\" href=\"#rOdena_et+al_2017_a\">Odena et al, 2017</a></a>).",
        "Such as the projection cGAN (<a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\"><a class=\"ref-link\" id=\"cMiyato_2018_a\" href=\"#rMiyato_2018_a\">Miyato & Koyama, 2018</a></a></a></a>) injects label information into the generator architecture using conditional Batch Norm layers (De Vries et al, 2017).",
        "The proposed method applies to generators in GAN, and it works with both unconditional and conditional settings.",
        "Overall the correlation between FID and condition number is smaller for self-modulated models.",
        "This is surprising, it appears that rather than unilaterally reducing the condition number, self-modulation provides some training stability, yielding models with a small range of generator condition numbers.",
        "We discuss the effects of this method in light of recently proposed diagnostic tools, generator conditioning (<a class=\"ref-link\" id=\"cOdena_et+al_2018_a\" href=\"#rOdena_et+al_2018_a\"><a class=\"ref-link\" id=\"cOdena_et+al_2018_a\" href=\"#rOdena_et+al_2018_a\">Odena et al, 2018</a></a>) and precision/recall for generative models (<a class=\"ref-link\" id=\"cSajjadi_et+al_2018_a\" href=\"#rSajjadi_et+al_2018_a\"><a class=\"ref-link\" id=\"cSajjadi_et+al_2018_a\" href=\"#rSajjadi_et+al_2018_a\">Sajjadi et al, 2018</a></a>)"
    ],
    "headline": "We propose and study an architectural modification, self-modulation, which improves Generative Adversarial Networks performance across different data sets, architectures, losses, regularizers, and hyperparameter settings",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Mart\u0131n Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Mart%C4%B1n%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Mart%C4%B1n%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Barratt_2018_a",
            "entry": "Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "Vries_et+al_2017_a",
            "entry": "Harm De Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vries%2C%20Harm%20De%20Strub%2C%20Florian%20Mary%2C%20Jeremie%20Larochelle%2C%20Hugo%20Modulating%20early%20visual%20processing%20by%20language%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vries%2C%20Harm%20De%20Strub%2C%20Florian%20Mary%2C%20Jeremie%20Larochelle%2C%20Hugo%20Modulating%20early%20visual%20processing%20by%20language%202017"
        },
        {
            "id": "Dumoulin_et+al_2017_a",
            "entry": "Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hu_et+al_2018_a",
            "entry": "Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%202018"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Isola_et+al_2016_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arxiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202016"
        },
        {
            "id": "Karras_et+al_2017_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202017"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Taesup Kim, Inchul Song, and Yoshua Bengio. Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition. In INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Taesup%20Song%2C%20Inchul%20Bengio%2C%20Yoshua%20Dynamic%20layer%20normalization%20for%20adaptive%20neural%20acoustic%20modeling%20in%20speech%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Taesup%20Song%2C%20Inchul%20Bengio%2C%20Yoshua%20Dynamic%20layer%20normalization%20for%20adaptive%20neural%20acoustic%20modeling%20in%20speech%20recognition%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kurach_et+al_2018_a",
            "entry": "Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The GAN Landscape: Losses, Architectures, Regularization, and Normalization. arXiv preprint arXiv:1807.04720, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04720"
        },
        {
            "id": "Ledig_et+al_2017_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Lucic_et+al_2018_a",
            "entry": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs Created Equal? A Large-scale Study. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20Created%20Equal%3F%20A%20Large-scale%20Study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20Created%20Equal%3F%20A%20Large-scale%20Study%202018"
        },
        {
            "id": "Mao_et+al_2016_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. International Conference on Computer Vision (ICCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "Takeru Miyato and Masanori Koyama. cgans with projection discriminator. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cgans%20with%20projection%20discriminator%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cgans%20with%20projection%20discriminator%202018"
        },
        {
            "id": "Miyato_et+al_2018_b",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Odena_et+al_2017_a",
            "entry": "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "Odena_et+al_2018_a",
            "entry": "Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin Raffel, and Ian Goodfellow. Is generator conditioning causally related to gan performance? arXiv preprint arXiv:1802.08768, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08768"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Perez_et+al_2018_a",
            "entry": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perez%2C%20Ethan%20Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Dumoulin%2C%20Vincent%20Film%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perez%2C%20Ethan%20Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Dumoulin%2C%20Vincent%20Film%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Sajjadi_et+al_2018_a",
            "entry": "Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20S.M.%20Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Bousquet%2C%20Olivier%20Assessing%20generative%20models%20via%20precision%20and%20recall%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20S.M.%20Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Bousquet%2C%20Olivier%20Assessing%20generative%20models%20via%20precision%20and%20recall%202018"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Computer Vision and Pattern Recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Tschannen_et+al_2018_a",
            "entry": "Michael Tschannen, Eirikur Agustsson, and Mario Lucic. Deep generative models for distributionpreserving lossy compression. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tschannen%2C%20Michael%20Agustsson%2C%20Eirikur%20Lucic%2C%20Mario%20Deep%20generative%20models%20for%20distributionpreserving%20lossy%20compression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tschannen%2C%20Michael%20Agustsson%2C%20Eirikur%20Lucic%2C%20Mario%20Deep%20generative%20models%20for%20distributionpreserving%20lossy%20compression%202018"
        },
        {
            "id": "Ulyanov_et+al_2016_a",
            "entry": "D Ulyanov, A Vedaldi, and VS Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202017"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Zhiming Zhou, Yuxuan Song, Lantao Yu, and Yong Yu. Understanding the effectiveness of lipschitz constraint in training of gans via gradient analysis. arXiv preprint arXiv:1807.00751, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.00751"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks.%20arXiv%20p%202017"
        }
    ]
}
