{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE DEEP WEIGHT PRIOR",
        "author": "Andrei Atanov, Skolkovo Institute of Science and Technology Samsung-HSE Laboratory, National Research University Higher School of Economics ai.atanow@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByGuynAct7"
        },
        "abstract": "Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (dwp), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations. We define dwp in a form of an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that dwp improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from dwp accelerates training of conventional convolutional neural networks."
    },
    "keywords": [
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "prior distribution",
            "url": "https://en.wikipedia.org/wiki/prior_distribution"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Bayesian inference is a tool that, after observing training data, allows to transforms a prior distribution over parameters of a machine learning model to a posterior distribution",
        "We propose deep weight prior, a framework that approximates the source kernel distribution and incorporates prior knowledge about the structure of convolutional filters into the prior distribution",
        "We propose to use an implicit prior distribution for stochastic variational inference (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a>) and develop a method for variational inference with the specific type of implicit priors",
        "In this work we propose deep weight prior \u2013 a framework for designing a prior distribution for convolutional neural networks, that exploits prior knowledge about the structure of learned convolutional filters",
        "This framework opens a new direction for applications of Bayesian deep learning, in particular to transfer learning",
        "The method was developed only for semiimplicit variational approximations, and only the recent work on doubly semi-implicit variational inference generalized it for implicit prior distributions (<a class=\"ref-link\" id=\"cMolchanov_et+al_2018_a\" href=\"#rMolchanov_et+al_2018_a\">Molchanov et al, 2018</a>). These algorithms might provide a better way for variational inference with a deep weight prior, the topic needs further investigation"
    ],
    "key_statements": [
        "Bayesian inference is a tool that, after observing training data, allows to transforms a prior distribution over parameters of a machine learning model to a posterior distribution",
        "We propose deep weight prior, a framework that approximates the source kernel distribution and incorporates prior knowledge about the structure of convolutional filters into the prior distribution",
        "In experiments (Section 4), we show that variational inference with deep weight prior significantly improves classification performance upon a number of popular prior distributions in the case of limited training data",
        "The stochastic gradient variational Bayes framework has been applied to approximate posterior distributions over parameters of deep neural networks (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a>)",
        "We introduce the deep weight prior \u2013 an expressive prior distribution that is based on generative models",
        "Our approach allows to perform variational inference with an implicit prior distribution, that is based on previously observed convolutional kernels",
        "We propose to use an implicit prior distribution for stochastic variational inference (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a>) and develop a method for variational inference with the specific type of implicit priors",
        "In this work we propose deep weight prior \u2013 a framework for designing a prior distribution for convolutional neural networks, that exploits prior knowledge about the structure of learned convolutional filters",
        "This framework opens a new direction for applications of Bayesian deep learning, in particular to transfer learning",
        "The factorization of deep weight prior does not take into account inter-layer dependencies of the weights",
        "The method was developed only for semiimplicit variational approximations, and only the recent work on doubly semi-implicit variational inference generalized it for implicit prior distributions (<a class=\"ref-link\" id=\"cMolchanov_et+al_2018_a\" href=\"#rMolchanov_et+al_2018_a\">Molchanov et al, 2018</a>). These algorithms might provide a better way for variational inference with a deep weight prior, the topic needs further investigation"
    ],
    "summary": [
        "Bayesian inference is a tool that, after observing training data, allows to transforms a prior distribution over parameters of a machine learning model to a posterior distribution.",
        "We propose a method that estimates the source kernel distribution in an implicit form and allows us to perform variational inference with the specific type of implicit priors.",
        "The stochastic gradient variational Bayes framework has been applied to approximate posterior distributions over parameters of deep neural networks (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a></a></a>).",
        "Where r(z | w; \u03c8l) is an auxiliary inference model for the prior of l-th layer pl(w), The final auxiliary variational lower bound has the following form: Laux(\u03b8, \u03c8) = LD \u2212 DKboLund \u2264 LD \u2212 DKL p(W )) = L(\u03b8)",
        "In the case when q\u03b8(w), p(w | z; \u03c6l) and r(z | w; \u03c8l) are explicit parametric distributions which can be reparametrized, we can perform an unbiased estimation of a gradient of the auxiliary variational lower bound Laux(\u03b8, \u03c8) w.r.t. parameters \u03b8 of the variational approximation q\u03b8(W ) and parameters \u03c8 of the reverse models r(z | w; \u03c8l).",
        "We apply deep weight prior to variational inference, random feature extraction and initialization of convolutional neural networks.",
        "We trained prior distributions on a number of source networks which were learned from different initial points on NotMNIST and CIFAR-100 datasets for MNIST and CIFAR-10 experiments respectively.",
        "We performed variational inference over weights of a discriminative convolutional neural network (Section 3) with three different prior distributions for the weights of the convolutional layers: deep weight prior, standard normal and log-uniform (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a></a></a>).",
        "We use three initializations for weights of convolutional layers: learned kernels, samples from deep weight prior, samples from Xavier distribution (<a class=\"ref-link\" id=\"cGlorot_2010_a\" href=\"#rGlorot_2010_a\">Glorot & Bengio, 2010</a>).",
        "We compare three different initializations of weights of conventional convolutional layers: learned filters, samples from deep weight prior and samples form Xavier distribution.",
        "In contrast to Bayesian techniques (<a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a></a></a>; <a class=\"ref-link\" id=\"cKochurov_et+al_2018_a\" href=\"#rKochurov_et+al_2018_a\">Kochurov et al, 2018</a>), these methods do not allow to obtain a posterior distribution over parameters of the model, and in most cases, they require to store convolutional weights of pre-trained models and careful tuning of hyperparameters.",
        "Our approach allows to perform variational inference with an implicit prior distribution, that is based on previously observed convolutional kernels.",
        "We show how to use this framework to learn a flexible prior distribution over kernels of Bayesian convolutional neural networks.",
        "In this work we propose deep weight prior \u2013 a framework for designing a prior distribution for convolutional neural networks, that exploits prior knowledge about the structure of learned convolutional filters."
    ],
    "headline": "We propose a new type of prior distributions for convolutional neural networks, deep weight prior, that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations",
    "reference_links": [
        {
            "id": "Bishop_2003_a",
            "entry": "Christopher M Bishop and Michael E Tipping. Bayesian regression and classification. Nato Science Series sub Series III Computer And Systems Sciences, 190:267\u2013288, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20Christopher%20M.%20Tipping%2C%20Michael%20E.%20Bayesian%20regression%20and%20classification.%20Nato%20Science%20Series%20sub%20Series%20III%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bishop%2C%20Christopher%20M.%20Tipping%2C%20Michael%20E.%20Bayesian%20regression%20and%20classification.%20Nato%20Science%20Series%20sub%20Series%20III%202003"
        },
        {
            "id": "Bulatov_2011_a",
            "entry": "Yaroslav Bulatov. Notmnist dataset. technical report. 2011. URL http://yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html.",
            "url": "http://yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html"
        },
        {
            "id": "Burda_et+al_2015_a",
            "entry": "Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. CoRR, abs/1509.00519, 2015. URL http://arxiv.org/abs/1509.00519.",
            "url": "http://arxiv.org/abs/1509.00519",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Dikmen_et+al_2015_a",
            "entry": "Onur Dikmen, Zhirong Yang, and Erkki Oja. Learning the information divergence. IEEE transactions on pattern analysis and machine intelligence, 37(7):1442\u20131454, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dikmen%2C%20Onur%20Yang%2C%20Zhirong%20Oja%2C%20Erkki%20Learning%20the%20information%20divergence%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dikmen%2C%20Onur%20Yang%2C%20Zhirong%20Oja%2C%20Erkki%20Learning%20the%20information%20divergence%202015"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. 5th International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20real%20nvp%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20real%20nvp%202017"
        },
        {
            "id": "Federici_et+al_2017_a",
            "entry": "Marco Federici, Karen Ullrich, and Max Welling. Improved bayesian compression. arXiv preprint arXiv:1711.06494, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06494"
        },
        {
            "id": "Figurnov_et+al_2018_a",
            "entry": "Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. arXiv preprint arXiv:1805.08498, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08498"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "He_2016_a",
            "entry": "Kun He, Yan Wang, and John Hopcroft. A powerful generative model using random weights for the deep image representation. In Advances in Neural Information Processing Systems, pp. 631\u2013639, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kun%20Wang%2C%20Yan%20and%20John%20Hopcroft.%20A%20powerful%20generative%20model%20using%20random%20weights%20for%20the%20deep%20image%20representation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kun%20Wang%2C%20Yan%20and%20John%20Hopcroft.%20A%20powerful%20generative%20model%20using%20random%20weights%20for%20the%20deep%20image%20representation%202016"
        },
        {
            "id": "Hoffman_et+al_2013_a",
            "entry": "Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "Jordan_et+al_1999_a",
            "entry": "Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "Karaletsos_et+al_2018_a",
            "entry": "Theofanis Karaletsos, Peter Dayan, and Zoubin Ghahramani. Probabilistic meta-representations of neural networks. arXiv preprint arXiv:1810.00555, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.00555"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2015_a",
            "entry": "Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems 29, pp. 4743\u20134751. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Kochurov_et+al_2018_a",
            "entry": "Max Kochurov, Timur Garipov, Dmitry Podoprikhin, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Bayesian incremental learning for deep neural networks. arXiv preprint arXiv:1802.07329, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07329"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lamb_et+al_2017_a",
            "entry": "Alex M Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron C Courville, and Yoshua Bengio. Gibbsnet: Iterative adversarial inference for deep graphical models. In Advances in Neural Information Processing Systems, pp. 5089\u20135098, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lamb%2C%20Alex%20M.%20Hjelm%2C%20Devon%20Ganin%2C%20Yaroslav%20Cohen%2C%20Joseph%20Paul%20Gibbsnet%3A%20Iterative%20adversarial%20inference%20for%20deep%20graphical%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lamb%2C%20Alex%20M.%20Hjelm%2C%20Devon%20Ganin%2C%20Yaroslav%20Cohen%2C%20Joseph%20Paul%20Gibbsnet%3A%20Iterative%20adversarial%20inference%20for%20deep%20graphical%20models%202017"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Louizos_2017_a",
            "entry": "Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural networks. arXiv preprint arXiv:1703.01961, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01961"
        },
        {
            "id": "Louizos_et+al_2017_b",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pp. 3288\u20133298, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Chao Ma, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Variational implicit processes. arXiv preprint arXiv:1806.02390, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02390"
        },
        {
            "id": "Mackay_1992_a",
            "entry": "David JC MacKay. Bayesian interpolation. Neural computation, 4(3):415\u2013447, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20J.C.%20Bayesian%20interpolation%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20David%20J.C.%20Bayesian%20interpolation%201992"
        },
        {
            "id": "Molchanov_et+al_2017_a",
            "entry": "Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 2498\u2013 2507, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017"
        },
        {
            "id": "Molchanov_et+al_2018_a",
            "entry": "Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov. Doubly semi-implicit variational inference. arXiv preprint arXiv:1810.02789, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.02789"
        },
        {
            "id": "Nagi_et+al_2011_a",
            "entry": "Jawad Nagi, Frederick Ducatelle, Gianni A Di Caro, Dan Ciresan, Ueli Meier, Alessandro Giusti, Farrukh Nagi, Jurgen Schmidhuber, and Luca Maria Gambardella. Max-pooling convolutional neural networks for vision-based hand gesture recognition. In Signal and Image Processing Applications (ICSIPA), 2011 IEEE International Conference on, pp. 342\u2013347. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagi%2C%20Jawad%20Ducatelle%2C%20Frederick%20Caro%2C%20Gianni%20A.Di%20Ciresan%2C%20Dan%20Max-pooling%20convolutional%20neural%20networks%20for%20vision-based%20hand%20gesture%20recognition%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagi%2C%20Jawad%20Ducatelle%2C%20Frederick%20Caro%2C%20Gianni%20A.Di%20Ciresan%2C%20Dan%20Max-pooling%20convolutional%20neural%20networks%20for%20vision-based%20hand%20gesture%20recognition%202011"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Eric_2018_a",
            "entry": "Eric Nalisnick and Padhraic Smyth. Learning priors for invariance. In International Conference on Artificial Intelligence and Statistics, pp. 366\u2013375, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eric%20Nalisnick%20and%20Padhraic%20Smyth%20Learning%20priors%20for%20invariance%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20pp%20366375%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eric%20Nalisnick%20and%20Padhraic%20Smyth%20Learning%20priors%20for%20invariance%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20pp%20366375%202018"
        },
        {
            "id": "Neklyudov_et+al_2017_a",
            "entry": "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. In Advances in Neural Information Processing Systems, pp. 6775\u20136784, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on International Conference on Machine Learning, pp. 1530\u20131538, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pp. 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Salimans_et+al_2015_a",
            "entry": "Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218\u20131226, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Welling%2C%20Max%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Welling%2C%20Max%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015"
        },
        {
            "id": "Saxe_et+al_2011_a",
            "entry": "Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On random weights and unsupervised feature learning. In ICML, pp. 1089\u20131096, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20M.%20Koh%2C%20Pang%20Wei%20Chen%2C%20Zhenghao%20Bhand%2C%20Maneesh%20Bipin%20Suresh%2C%20and%20Andrew%20Y%20Ng.%20On%20random%20weights%20and%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20M.%20Koh%2C%20Pang%20Wei%20Chen%2C%20Zhenghao%20Bhand%2C%20Maneesh%20Bipin%20Suresh%2C%20and%20Andrew%20Y%20Ng.%20On%20random%20weights%20and%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Razavian_et+al_2014_a",
            "entry": "Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features offthe-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 806\u2013813, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20offthe-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20offthe-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014"
        },
        {
            "id": "Silverman_1986_a",
            "entry": "B.W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis, 1986. ISBN 9780412246203. URL https://books.google.ru/books?id=e-xsrjsL7WkC.",
            "url": "https://books.google.ru/books?id=e-xsrjsL7WkC"
        },
        {
            "id": "Tomczak_1705_a",
            "entry": "Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017. Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. arXiv:1711.10925, 2017. Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pp. 4790\u20134798, 2016. Peter M Williams. Bayesian regularization and pruning using a laplace prior. Neural computation, 7(1):117\u2013143, 1995. Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In Proceedings of the 35th International Conference on Machine Learning, pp. 5660\u20135669, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320\u20133328, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07120"
        }
    ]
}
