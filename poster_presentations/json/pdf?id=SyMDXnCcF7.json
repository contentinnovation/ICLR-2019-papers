{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A Mean Field Theory of Batch Normalization",
        "author": "Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz",
        "date": 2019,
        "identifiers": {
            "arxiv": "1902.08129",
            "doi": null,
            "isbn": null,
            "doc_id": null,
            "url": "https://openreview.net/pdf?id=SyMDXnCcF7"
        },
        "abstract": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "symmetry",
            "url": "https://en.wikipedia.org/wiki/symmetry"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "mean field theory",
            "url": "https://en.wikipedia.org/wiki/mean_field_theory"
        },
        {
            "term": "isometry",
            "url": "https://en.wikipedia.org/wiki/isometry"
        },
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        }
    ],
    "abbreviations": {
        "DOS": "Diagonal-Offdiagonal Semidirect",
        "M z": "M z \u22122k f",
        "B\u22121": "B\u03b1\u0393(\u03b1)\u22121 ds s\u03b1\u22121(1 + 2s\u03bc)\u2212"
    },
    "highlights": [
        "Deep neural networks have been enormously successful across a broad range of disciplines",
        "To fulfill its promise of making neural network design less of a black box, these techniques must be applied to neural network architectures that are used in practice",
        "We note that there is a related vein of research that has emerged that leverages the prior over functions induced by random networks to perform exact Bayesian inference (Lee et al, 2017; de G",
        "In this work we have presented a theory for neural networks with batch normalization at initialization",
        "In the process of doing so, we have uncovered a number of counterintuitive aspects of batch normalization and \u2013 in particular \u2013 the fact that at initialization it unavoidably causes gradients to explode with depth",
        "We have introduced several methods to reduce the degree of gradient explosion, enabling the training of significantly deeper networks in the presence of batch normalization"
    ],
    "key_statements": [
        "Deep neural networks have been enormously successful across a broad range of disciplines",
        "To fulfill its promise of making neural network design less of a black box, these techniques must be applied to neural network architectures that are used in practice",
        "We develop a theory of fully-connected networks with batch normalization whose weights and biases are randomly distributed",
        "For most nonlinearities used in practice, batchnorm in a deep, randomly initialized network induces high degree of symmetry in the embeddings of the samples in the batch (Thm 3.4)",
        "Whenever this symmetry takes hold, we show that for any choice of nonlinearity, gradients of fully-connected networks with batch normalization explode exponentially in the depth of the network (Thm 3.9)",
        "It might seem that such gradient explosion ought to lead to learning dynamics that are unfavorable",
        "We show that networks with batch normalization causes the scale of the gradients to naturally equilibrate after a single step of gradient descent",
        "We note that there is a related vein of research that has emerged that leverages the prior over functions induced by random networks to perform exact Bayesian inference (Lee et al, 2017; de G",
        "One of the main results from this section will be to show that fully-connected networks with batch normalization feature exploding gradients for any choice of nonlinearity such that \u03a3l converges to a BSB1 fixed point",
        "Having developed a theory for neural networks with batch normalization at initialization, we explore the relationship between the properties of these random networks and their learning dynamics",
        "In this work we have presented a theory for neural networks with batch normalization at initialization",
        "In the process of doing so, we have uncovered a number of counterintuitive aspects of batch normalization and \u2013 in particular \u2013 the fact that at initialization it unavoidably causes gradients to explode with depth",
        "We have introduced several methods to reduce the degree of gradient explosion, enabling the training of significantly deeper networks in the presence of batch normalization",
        "This work paves the way for future work on more advanced, state-of-the-art, network architectures and topologies",
        "As discussed in the main text, we are interested in several closely related dynamics induced by batchnorm in a fully-connected network with random weights",
        "When studying the convergence rate of the forward dynamics and the gradient explosion of the backward dynamics, we encounter linear operators on matrices that satisfy an abundance of symmetries, called ultrasymmetric operators. We study these operators in Appendix E.5 and contribute several structural results Thms E.61, E.62, E.72 and E.72 on their eigendecomposition that are crucial to deriving the asymptotics of the dynamics",
        "In Appendix J we discuss what happens outside this regime, and in Appendix K we show our current understanding of BSB2 fixed point dynamics"
    ],
    "summary": [
        "Deep neural networks have been enormously successful across a broad range of disciplines.",
        "These integrals can either be performed analytically (Cho & Saul, 2009; <a class=\"ref-link\" id=\"cWilliams_1997_a\" href=\"#rWilliams_1997_a\">Williams, 1997</a>) or efficiently approximated numerically (Lee et al, 2017), and so Eq (2) defines a computationally efficient method for computing the statistics of neural networks after random initialization.",
        "Together these theorems provide analytic recurrence relations for random neural networks with batch normalization over a wide range of activation functions.",
        "The below theorem shows that, as the batch size becomes larger, a deep ReLU-batchnorm network takes more layers to converge to a BSB1 fixed point.",
        "With a mean field theory of the pre-activations of feed-forward networks with batch normalization having been developed, we turn our attention to the backpropagation of gradients.",
        "In contrast to the case of networks without batch normalization, we will see that exploding gradients at initialization are a severe problem here.",
        "One of the main results from this section will be to show that fully-connected networks with batch normalization feature exploding gradients for any choice of nonlinearity such that \u03a3l converges to a BSB1 fixed point.",
        "Having developed a theory for neural networks with batch normalization at initialization, we explore the relationship between the properties of these random networks and their learning dynamics.",
        "As discussed in the theoretical exposition above, batch normalization necessarily features exploding gradients for any nonlinearity that converges to a BSB1 fixed point.",
        "We have introduced several methods to reduce the degree of gradient explosion, enabling the training of significantly deeper networks in the presence of batch normalization.",
        "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.",
        "As discussed in the main text, we are interested in several closely related dynamics induced by batchnorm in a fully-connected network with random weights.",
        "Bspahtcehrenoorfmracdainusn\u221aatuBra. llAy sbseuminitnegrptrheatetdthaes a linear forward dynamics converges to a BSB1 fixed point, one can express VB\u03c6 and VB\u03c6 as spherical integrals, for all measurable \u03c6.",
        "Thms F.5, F.8 and F.13 respectively compute the BSB1 fixed point from the perspectives of spherical integration, the Laplace method, and the Gegenbauer method.",
        "Thm H.3 gives the form of the cross batch fixed point via spherical integration and the Gegenbauer method, and Thm H.6 yields a more specific form for positive homogeneous \u03c6 via the Laplace method.",
        "The correlation between gradients of the two batches is shown to decrease exponentially in Corollary I.6 using the Gegenbauer method, and the decay rate is given succinctly for positive homogeneous \u03c6 in Thm I.3.",
        "This work paves the way for future work on more advanced, state-of-the-art, network architectures and topologies"
    ],
    "headline": "We develop a mean field theory for batch normalization in fully-connected feedforward neural networks",
    "reference_links": [
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450 [cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450.",
            "url": "http://arxiv.org/abs/1607.06450",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Balduzzi_et+al_2017_a",
            "entry": "David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The Shattered Gradients Problem: If resnets are the answer, then what is the question? In PMLR, pp. 342\u2013350, July 2017. URL http://proceedings.mlr.press/v70/balduzzi17b.html.",
            "url": "http://proceedings.mlr.press/v70/balduzzi17b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20David%20Frean%2C%20Marcus%20Lennox%20Leary%2C%20J.P.Lewis%20Ma%2C%20Kurt%20Wan-Duo%20The%20Shattered%20Gradients%20Problem%3A%20If%20resnets%20are%20the%20answer%2C%20then%20what%20is%20the%20question%3F%202017-07"
        },
        {
            "id": "Bjorck_et+al_2018_a",
            "entry": "Johan Bjorck, Carla Gomes, and Bart Selman. Understanding Batch Normalization. June 2018. URL https://arxiv.org/abs/1806.02375.",
            "url": "https://arxiv.org/abs/1806.02375",
            "arxiv_url": "https://arxiv.org/pdf/1806.02375"
        },
        {
            "id": "Pennington_et+al_2017_a",
            "entry": "Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pp. 4785\u20134795, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017"
        },
        {
            "id": "Philipp_2018_a",
            "entry": "George Philipp and Jaime G. Carbonell. The Nonlinearity Coefficient - Predicting Overfitting in Deep Neural Networks. arXiv:1806.00179 [cs, stat], May 2018. URL http://arxiv.org/abs/1806.00179.",
            "url": "http://arxiv.org/abs/1806.00179",
            "arxiv_url": "https://arxiv.org/pdf/1806.00179"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. arXiv:1606.05340 [cond-mat, stat], June 2016. URL http://arxiv.org/abs/1606.05340.",
            "url": "http://arxiv.org/abs/1606.05340",
            "arxiv_url": "https://arxiv.org/pdf/1606.05340"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. February 2016. URL https://arxiv.org/abs/1602.07868.",
            "url": "https://arxiv.org/abs/1602.07868",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "Santurkar_et+al_2018_a",
            "entry": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.11604.",
            "url": "http://arxiv.org/abs/1805.11604",
            "arxiv_url": "https://arxiv.org/pdf/1805.11604"
        },
        {
            "id": "Schoenholz_et+al_2016_a",
            "entry": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/abs/1611.01232.",
            "url": "http://arxiv.org/abs/1611.01232",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "Suetin_0000_a",
            "entry": "P.K. Suetin. Ultraspherical polynomials - Encyclopedia of Mathematics. URL https://www.encyclopediaofmath.org/index.php/Ultraspherical_polynomials.",
            "url": "https://www.encyclopediaofmath.org/index.php/Ultraspherical_polynomials"
        },
        {
            "id": "Weisstein_0000_a",
            "entry": "Eric W. Weisstein. Gegenbauer Polynomial. URL http://mathworld.wolfram.com/ GegenbauerPolynomial.html.",
            "url": "http://mathworld.wolfram.com/GegenbauerPolynomial.html"
        },
        {
            "id": "Williams_1997_a",
            "entry": "Christopher KI Williams. Computing with infinite networks. In Advances in neural information processing systems, pp. 295\u2013301, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.I.%20Computing%20with%20infinite%20networks%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20K.I.%20Computing%20with%20infinite%20networks%201997"
        },
        {
            "id": "Xiao_et+al_2018_a",
            "entry": "Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5393\u20135402, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/xiao18a.html.",
            "url": "http://proceedings.mlr.press/v80/xiao18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lechao%20Bahri%2C%20Yasaman%20Sohl-Dickstein%2C%20Jascha%20Schoenholz%2C%20Samuel%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20CNNs%3A%20How%20to%20train%2010%2C000-layer%20vanilla%20convolutional%20neural%20networks%202018-07"
        },
        {
            "id": "Yang_2019_a",
            "entry": "Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.",
            "arxiv_url": "https://arxiv.org/pdf/1902.04760"
        },
        {
            "id": "Yang_2017_a",
            "entry": "Greg Yang and Samuel S. Schoenholz. Meanfield Residual Network: On the Edge of Chaos. In Advances in neural information processing systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Greg%20Schoenholz%2C%20Samuel%20S.%20Meanfield%20Residual%20Network%3A%20On%20the%20Edge%20of%20Chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Greg%20Schoenholz%2C%20Samuel%20S.%20Meanfield%20Residual%20Network%3A%20On%20the%20Edge%20of%20Chaos%202017"
        },
        {
            "id": "Yang_2018_a",
            "entry": "Greg Yang and Samuel S Schoenholz. Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Greg%20Schoenholz%2C%20Samuel%20S.%20Deep%20mean%20field%20theory%3A%20Layerwise%20variance%20and%20width%20variation%20as%20methods%20to%20control%20gradient%20explosion%202018"
        }
    ]
}
