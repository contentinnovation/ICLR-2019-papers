{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology",
        "author": "Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, and Karsten Borwardt",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByxkijC5FQ"
        },
        "abstract": "While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistencebased stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss."
    },
    "keywords": [
        {
            "term": "homology group",
            "url": "https://en.wikipedia.org/wiki/homology_group"
        },
        {
            "term": "topology",
            "url": "https://en.wikipedia.org/wiki/topology"
        },
        {
            "term": "good practice",
            "url": "https://en.wikipedia.org/wiki/good_practice"
        },
        {
            "term": "persistent homology",
            "url": "https://en.wikipedia.org/wiki/persistent_homology"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "complexity",
            "url": "https://en.wikipedia.org/wiki/complexity"
        },
        {
            "term": "complexity measure",
            "url": "https://en.wikipedia.org/wiki/complexity_measure"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "simplicial complex",
            "url": "https://en.wikipedia.org/wiki/simplicial_complex"
        },
        {
            "term": "betti number",
            "url": "https://en.wikipedia.org/wiki/betti_number"
        },
        {
            "term": "Topological data analysis",
            "url": "https://en.wikipedia.org/wiki/Topological_data_analysis"
        },
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "algebraic topology",
            "url": "https://en.wikipedia.org/wiki/algebraic_topology"
        },
        {
            "term": "early stopping",
            "url": "https://en.wikipedia.org/wiki/early_stopping"
        }
    ],
    "abbreviations": {
        "TDA": "Topological data analysis"
    },
    "highlights": [
        "The practical successes of deep learning in various fields such as image processing (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\"><a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\">Hu et al, 2018</a></a>), biomedicine (Ching et al, 2018; <a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\"><a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\">Rajpurkar et al, 2017</a></a>; <a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\"><a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\">Rajkomar et al, 2018</a></a>), and language translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>) still outpace our theoretical understanding",
        "The information from the dth homology group is summarized in a simple complexity measure, the dth Betti number \u03b2d, which merely counts the number of d-dimensional features: a circle, for example, has Betti numbers (1, 1), i.e. one connected component and one tunnel, while a filled circle has Betti numbers (1, 0), i.e. one connected component but no tunnel",
        "We develop an early stopping criterion based on neural persistence and we compare it to the traditional criterion based on validation loss",
        "We presented neural persistence, a novel topological measure of the structural complexity of deep neural networks",
        "We showed that this measure captures topological information that pertains to deep learning performance",
        "We extended our framework to convolutional neural networks by deriving a closed-form approximation, and observed that an early stopping criterion based on neural persistence for convolutional layers will require additional work"
    ],
    "key_statements": [
        "The practical successes of deep learning in various fields such as image processing (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\"><a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\">Hu et al, 2018</a></a>), biomedicine (Ching et al, 2018; <a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\"><a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\">Rajpurkar et al, 2017</a></a>; <a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\"><a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\">Rajkomar et al, 2018</a></a>), and language translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>) still outpace our theoretical understanding",
        "- We demonstrate the practical utility of neural persistence in two scenarios: i) it correctly captures the benefits of dropout and batch normalization during the training process, and ii) it can be used as a competitive early stopping criterion that does not require validation data",
        "Topological data analysis (TDA) recently emerged as a field that provides computational tools for analysing complex data within a rigorous mathematical framework that is based on algebraic topology",
        "The information from the dth homology group is summarized in a simple complexity measure, the dth Betti number \u03b2d, which merely counts the number of d-dimensional features: a circle, for example, has Betti numbers (1, 1), i.e. one connected component and one tunnel, while a filled circle has Betti numbers (1, 0), i.e. one connected component but no tunnel",
        "We refrain from showing the empirical upper bounds because most weight distributions are highly right-tailed; the bound will not be as tight as the lower bound. These results are in line with a previous analysis (<a class=\"ref-link\" id=\"cSizemore_et+al_2017_a\" href=\"#rSizemore_et+al_2017_a\">Sizemore et al, 2017</a>) of small weighted networks, in which persistent homology is seen to outperform traditional graph-theoretical complexity measures such as the clustering coefficient",
        "We develop an early stopping criterion based on neural persistence and we compare it to the traditional criterion based on validation loss",
        "Neural persistence can be used as an early stopping criterion that does not require a validation data set to prevent overfitting: if the mean normalized neural persistence does not increase by more than \u2206min during a certain number of epochs g, the training process is stopped",
        "The early stopping behaviour of both measures is simulated for each combination of b and g and their performance over all runs is summarized in terms of median test accuracy and median stopping epoch; if a criterion is not triggered for one run, we report the test accuracy at the end of the training and the number of training epochs",
        "We presented neural persistence, a novel topological measure of the structural complexity of deep neural networks",
        "We showed that this measure captures topological information that pertains to deep learning performance",
        "We showed that our measure correctly identifies networks that employ best practices such as dropout and batch normalization",
        "By saving valuable data for training, we managed to boost accuracy, which can be crucial for enabling deep learning in regimes of smaller sample sizes",
        "We extended our framework to convolutional neural networks by deriving a closed-form approximation, and observed that an early stopping criterion based on neural persistence for convolutional layers will require additional work",
        "On a more general level, neural persistence demonstrates the great potential of topological data analysis in machine learning"
    ],
    "summary": [
        "The practical successes of deep learning in various fields such as image processing (<a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan & Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\"><a class=\"ref-link\" id=\"cHu_et+al_2018_a\" href=\"#rHu_et+al_2018_a\">Hu et al, 2018</a></a>), biomedicine (Ching et al, 2018; <a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\"><a class=\"ref-link\" id=\"cRajpurkar_et+al_2017_a\" href=\"#rRajpurkar_et+al_2017_a\">Rajpurkar et al, 2017</a></a>; <a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\"><a class=\"ref-link\" id=\"cRajkomar_et+al_2018_a\" href=\"#rRajkomar_et+al_2018_a\">Rajkomar et al, 2018</a></a>), and language translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>) still outpace our theoretical understanding.",
        "While it would theoretically be possible to include higher-dimensional information about each layer Gk, for example in the form of cliques (<a class=\"ref-link\" id=\"cRieck_et+al_2018_a\" href=\"#rRieck_et+al_2018_a\">Rieck et al, 2018</a>), we focus on zero-dimensional information in this paper, because of the following advantages: i) the resulting values are interpretable as they essentially describe the clustering of the network at multiple weight thresholds, ii) previous research (<a class=\"ref-link\" id=\"cRieck_2016_a\" href=\"#rRieck_2016_a\">Rieck & Leitte, 2016</a>; <a class=\"ref-link\" id=\"cHofer_et+al_2017_a\" href=\"#rHofer_et+al_2017_a\">Hofer et al, 2017</a>) indicates that zero-dimensional topological information is already capturing a large amount of information, and iii) persistent homology calculations are highly efficient in this regime.",
        "We can use the upper bound of Theorem 1 to normalize the neural persistence of a layer, making it possible to compare layers that feature different architectures, i.e. a different number of neurons.",
        "Figure 2 depicts the neural persistence values as well as the lower bounds according to Theorem 2 for different settings.",
        "These results are in line with a previous analysis (<a class=\"ref-link\" id=\"cSizemore_et+al_2017_a\" href=\"#rSizemore_et+al_2017_a\">Sizemore et al, 2017</a>) of small weighted networks, in which persistent homology is seen to outperform traditional graph-theoretical complexity measures such as the clustering coefficient.",
        "Figure 3 shows that the networks designed according to best practices yield higher normalized neural persistence values on the \u2018MNIST\u2019 data set in comparison to an unmodified network.",
        "Neural persistence can be used as an early stopping criterion that does not require a validation data set to prevent overfitting: if the mean normalized neural persistence does not increase by more than \u2206min during a certain number of epochs g, the training process is stopped.",
        "We perform 100 training runs of the same architecture, monitoring validation loss and mean normalized neural persistence every quarter epoch.",
        "To ensure comparability across scenarios, we did not use the validation data as additional training data when stopping with neural persistence; we refer to Section A.7 for additional experiments in data scarcity scenarios.",
        "We presented neural persistence, a novel topological measure of the structural complexity of deep neural networks.",
        "This did not yield an early stopping measure because it was never triggered, thereby suggesting that neural persistence captures salient information that would otherwise be hidden among all the weights of a network.",
        "We extended our framework to convolutional neural networks by deriving a closed-form approximation, and observed that an early stopping criterion based on neural persistence for convolutional layers will require additional work."
    ],
    "headline": "We propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems: Simple, end-to-end, LeNet-5-like convolutional MNIST model example, 2015. URL https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py.",
            "url": "https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py"
        },
        {
            "id": "Achille_2018_a",
            "entry": "Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep representations. Journal of Machine Learning Research, 18:1\u201334, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Emergence%20of%20invariance%20and%20disentanglement%20in%20deep%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Emergence%20of%20invariance%20and%20disentanglement%20in%20deep%20representations%202018"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Bengio_0000_a",
            "entry": "Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In Gr\u00e9goire Montavon, Genevi\u00e8ve B. Orr, and Klaus-Robert M\u00fcller (eds.), Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pp. 437\u2013478.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Practical%20recommendations%20for%20gradient-based%20training%20of%20deep%20architectures",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Practical%20recommendations%20for%20gradient-based%20training%20of%20deep%20architectures"
        },
        {
            "id": "Springer_2012_a",
            "entry": "Springer, Heidelberg, Germany, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%20Heidelberg%20Germany%202012"
        },
        {
            "id": "Bianchini_2014_a",
            "entry": "Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning Systems, 25(8):1553\u20131565, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bianchini%2C%20Monica%20Scarselli%2C%20Franco%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bianchini%2C%20Monica%20Scarselli%2C%20Franco%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014"
        },
        {
            "id": "Bubenik_2015_a",
            "entry": "Peter Bubenik. Statistical topological data analysis using persistence landscapes. Journal of Machine Learning Research, 16:77\u2013102, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubenik%2C%20Peter%20Statistical%20topological%20data%20analysis%20using%20persistence%20landscapes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubenik%2C%20Peter%20Statistical%20topological%20data%20analysis%20using%20persistence%20landscapes%202015"
        },
        {
            "id": "Carlsson_2010_a",
            "entry": "Gunnar Carlsson and Facundo M\u00e9moli. Characterization, stability and convergence of hierarchical clustering methods. Journal of Machine Learning Research, 11:1425\u20131470, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlsson%2C%20Gunnar%20M%C3%A9moli%2C%20Facundo%20Characterization%2C%20stability%20and%20convergence%20of%20hierarchical%20clustering%20methods%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlsson%2C%20Gunnar%20M%C3%A9moli%2C%20Facundo%20Characterization%2C%20stability%20and%20convergence%20of%20hierarchical%20clustering%20methods%202010"
        },
        {
            "id": "Carlsson_et+al_2008_a",
            "entry": "Gunnar Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local behavior of spaces of natural images. International Journal of Computer Vision, 76(1):1\u201312, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlsson%2C%20Gunnar%20Ishkhanov%2C%20Tigran%20de%20Silva%2C%20Vin%20Zomorodian%2C%20Afra%20On%20the%20local%20behavior%20of%20spaces%20of%20natural%20images%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlsson%2C%20Gunnar%20Ishkhanov%2C%20Tigran%20de%20Silva%2C%20Vin%20Zomorodian%2C%20Afra%20On%20the%20local%20behavior%20of%20spaces%20of%20natural%20images%202008"
        },
        {
            "id": "Carstens_2013_a",
            "entry": "Corrie J. Carstens and Kathy J. Horadam. Persistent homology of collaboration networks. Mathematical Problems in Engineering, 2013:815035, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carstens%2C%20Corrie%20J.%20Horadam%2C%20Kathy%20J.%20Persistent%20homology%20of%20collaboration%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carstens%2C%20Corrie%20J.%20Horadam%2C%20Kathy%20J.%20Persistent%20homology%20of%20collaboration%20networks%202013"
        },
        {
            "id": "Ching_et+al_2017_a",
            "entry": "Travers Ching, Daniel S. Himmelstein, Brett K. Beaulieu-Jones, Alexandr A. Kalinin, Brian T. Do, Gregory P. Way, Enrico Ferrero, Paul-Michael Agapow, Michael Zietz, Michael M. Hoffman, Weil Xie, Gail L. Rosen, Benjamin J. Lengerich, Johnny Israeli, Jack Lanchantin, Stephen Woloszynek, Anne E. Carpenter, Avanti Shrikumar, Jinbo Xu, Evan M. Cofer, Christopher A. Lavender, Srinivas C. Turaga, Amr M. Alexandri, Zhiyong Lu, David J. Harris, Dave DeCaprio, Yanjun Qi, Anshul Kundaje, Yifan Peng, Laura K. Wiley, Marwin H.S. Segler, Simina M. Boca, S. Joshua Swamidass, Austin Huang, Anthony Gitter, and Casey S. Greene. Opportunities and obstacles for deep learning in biology and medicine. Journal of The Royal Society Interface, 15 (141):20170387, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ching%2C%20Travers%20Himmelstein%2C%20Daniel%20S.%20Beaulieu-Jones%2C%20Brett%20K.%20Kalinin%2C%20Alexandr%20A.%20Opportunities%20and%20obstacles%20for%20deep%20learning%20in%20biology%20and%20medicine%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ching%2C%20Travers%20Himmelstein%2C%20Daniel%20S.%20Beaulieu-Jones%2C%20Brett%20K.%20Kalinin%2C%20Alexandr%20A.%20Opportunities%20and%20obstacles%20for%20deep%20learning%20in%20biology%20and%20medicine%202017"
        },
        {
            "id": "Chollet_2015_a",
            "entry": "Fran\u00e7ois Chollet et al. Keras. https://keras.io, 2015.",
            "url": "https://keras.io"
        },
        {
            "id": "Cohen-Steiner_et+al_2009_a",
            "entry": "David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Extending persistence using Poincar\u00e9 and Lefschetz duality. Foundations of Computational Mathematics, 9(1):79\u2013103, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen-Steiner%2C%20David%20Edelsbrunner%2C%20Herbert%20Harer%2C%20John%20Extending%20persistence%20using%20Poincar%C3%A9%20and%20Lefschetz%20duality%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen-Steiner%2C%20David%20Edelsbrunner%2C%20Herbert%20Harer%2C%20John%20Extending%20persistence%20using%20Poincar%C3%A9%20and%20Lefschetz%20duality%202009"
        },
        {
            "id": "Cohen-Steiner_et+al_2010_a",
            "entry": "David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz functions have Lp-stable persistence. Foundations of Computational Mathematics, 10(2):127\u2013139, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen-Steiner%2C%20David%20Edelsbrunner%2C%20Herbert%20Harer%2C%20John%20Mileyko%2C%20Yuriy%20Lipschitz%20functions%20have%20Lp-stable%20persistence%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen-Steiner%2C%20David%20Edelsbrunner%2C%20Herbert%20Harer%2C%20John%20Mileyko%2C%20Yuriy%20Lipschitz%20functions%20have%20Lp-stable%20persistence%202010"
        },
        {
            "id": "Cormen_et+al_2009_a",
            "entry": "Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to algorithms. MIT Press, Cambridge, MA, USA, 3rd edition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cormen%2C%20Thomas%20H.%20Leiserson%2C%20Charles%20E.%20Rivest%2C%20Ronald%20L.%20Stein%2C%20Clifford%20Introduction%20to%20algorithms%202009"
        },
        {
            "id": "Edelsbrunner_2010_a",
            "entry": "Herbert Edelsbrunner and John Harer. Computational topology: An introduction. American Mathematical Society, Providence, RI, USA, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edelsbrunner%2C%20Herbert%20Harer%2C%20John%20Computational%20topology%3A%20An%20introduction%202010"
        },
        {
            "id": "Edelsbrunner_et+al_2002_a",
            "entry": "Herbert Edelsbrunner, David Letscher, and Afra J. Zomorodian. Topological persistence and simplification. Discrete & Computational Geometry, 28(4):511\u2013533, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edelsbrunner%2C%20Herbert%20Letscher%2C%20David%20Zomorodian%2C%20Afra%20J.%20Topological%20persistence%20and%20simplification%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edelsbrunner%2C%20Herbert%20Letscher%2C%20David%20Zomorodian%2C%20Afra%20J.%20Topological%20persistence%20and%20simplification%202002"
        },
        {
            "id": "Guss_2018_a",
            "entry": "William H Guss and Ruslan Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology. arXiv preprint arXiv:1802.04443, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04443"
        },
        {
            "id": "Hara_et+al_0000_a",
            "entry": "Kazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno. Analysis of dropout learning regarded as ensemble learning. In Alessandro E.P. Villa, Paolo Masulli, and Antonio Javier Pons Rivero (eds.), Artificial Neural Networks and Machine Learning (ICANN), number 9887 in Lecture Notes in Computer Science, pp. 72\u201379, Cham, Switzerland, 2016. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hara%2C%20Kazuyuki%20Saitoh%2C%20Daisuke%20Shouno%2C%20Hayaru%20Analysis%20of%20dropout%20learning%20regarded%20as%20ensemble%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hara%2C%20Kazuyuki%20Saitoh%2C%20Daisuke%20Shouno%2C%20Hayaru%20Analysis%20of%20dropout%20learning%20regarded%20as%20ensemble%20learning"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hofer_et+al_2017_a",
            "entry": "Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. Deep learning with topological signatures. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1633\u2013 1643, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofer%2C%20Christoph%20Kwitt%2C%20Roland%20Niethammer%2C%20Marc%20Uhl%2C%20Andreas%20Deep%20learning%20with%20topological%20signatures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hofer%2C%20Christoph%20Kwitt%2C%20Roland%20Niethammer%2C%20Marc%20Uhl%2C%20Andreas%20Deep%20learning%20with%20topological%20signatures%202017"
        },
        {
            "id": "Horak_et+al_2009_a",
            "entry": "Danijela Horak, Slobodan Maletic, and Milan Rajkovic. Persistent homology of complex networks. Journal of Statistical Mechanics: Theory and Experiment, 2009(03):P03034, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horak%2C%20Danijela%20Maletic%2C%20Slobodan%20Rajkovic%2C%20Milan%20Persistent%20homology%20of%20complex%20networks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horak%2C%20Danijela%20Maletic%2C%20Slobodan%20Rajkovic%2C%20Milan%20Persistent%20homology%20of%20complex%20networks%202009"
        },
        {
            "id": "Hu_et+al_2018_a",
            "entry": "Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7132\u20137141, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%202018"
        },
        {
            "id": "Bach_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448\u2013456. PMLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Francis%20Bach%20and%20David%20Blei%20eds%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20volume%2037%20of%20Proceedings%20of%20Machine%20Learning%20Research%20pp%20448456%20PMLR%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Francis%20Bach%20and%20David%20Blei%20eds%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20volume%2037%20of%20Proceedings%20of%20Machine%20Learning%20Research%20pp%20448456%20PMLR%202015"
        },
        {
            "id": "Khrulkov_2018_a",
            "entry": "Valentin Khrulkov and Ivan Oseledets. Geometry score: A method for comparing generative adversarial networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2621\u20132629. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khrulkov%2C%20Valentin%20Oseledets%2C%20Ivan%20Geometry%20score%3A%20A%20method%20for%20comparing%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khrulkov%2C%20Valentin%20Oseledets%2C%20Ivan%20Geometry%20score%3A%20A%20method%20for%20comparing%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Lum_et+al_2013_a",
            "entry": "Pek Y. Lum, Gurjeet Singh, Alan Lehman, Tigran Ishkanov, Mikael Vejdemo-Johansson, Muthu Alagappan, John Carlsson, and Gunnar Carlsson. Extracting insights from the shape of complex data using topology. Scientific Reports, 3:1\u20138, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lum%2C%20Pek%20Y.%20Singh%2C%20Gurjeet%20Lehman%2C%20Alan%20Ishkanov%2C%20Tigran%20Extracting%20insights%20from%20the%20shape%20of%20complex%20data%20using%20topology%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lum%2C%20Pek%20Y.%20Singh%2C%20Gurjeet%20Lehman%2C%20Alan%20Ishkanov%2C%20Tigran%20Extracting%20insights%20from%20the%20shape%20of%20complex%20data%20using%20topology%202013"
        },
        {
            "id": "Montavon_et+al_2017_a",
            "entry": "Gr\u00e9goire Montavon, Wojciech Samek, and Klaus-Robert M\u00fcller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1\u201315, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montavon%2C%20Gr%C3%A9goire%20Samek%2C%20Wojciech%20M%C3%BCller%2C%20Klaus-Robert%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montavon%2C%20Gr%C3%A9goire%20Samek%2C%20Wojciech%20M%C3%BCller%2C%20Klaus-Robert%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017"
        },
        {
            "id": "Munkres_1996_a",
            "entry": "James R. Munkres. Elements of algebraic topology. CRC Press, Boca Raton, FL, USA, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munkres%2C%20James%20R.%20Elements%20of%20algebraic%20topology%201996"
        },
        {
            "id": "Raghu_et+al_2017_a",
            "entry": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2847\u20132854. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Rajkomar_et+al_2018_a",
            "entry": "Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, et al. Scalable and accurate deep learning with electronic health records. npj Digital Medicine, 1(1):18, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajkomar%2C%20Alvin%20Oren%2C%20Eyal%20Chen%2C%20Kai%20Dai%2C%20Andrew%20M.%20Scalable%20and%20accurate%20deep%20learning%20with%20electronic%20health%20records%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajkomar%2C%20Alvin%20Oren%2C%20Eyal%20Chen%2C%20Kai%20Dai%2C%20Andrew%20M.%20Scalable%20and%20accurate%20deep%20learning%20with%20electronic%20health%20records%202018"
        },
        {
            "id": "Rajpurkar_et+al_2017_a",
            "entry": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew Lungren, and Andrew Y. Ng. CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05225"
        },
        {
            "id": "Rieck_2016_a",
            "entry": "Bastian Rieck and Heike Leitte. Exploring and comparing clusterings of multivariate data sets using persistent homology. Computer Graphics Forum, 35(3):81\u201390, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rieck%2C%20Bastian%20Leitte%2C%20Heike%20Exploring%20and%20comparing%20clusterings%20of%20multivariate%20data%20sets%20using%20persistent%20homology%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rieck%2C%20Bastian%20Leitte%2C%20Heike%20Exploring%20and%20comparing%20clusterings%20of%20multivariate%20data%20sets%20using%20persistent%20homology%202016"
        },
        {
            "id": "Rieck_et+al_2018_a",
            "entry": "Bastian Rieck, Ulderico Fugacci, Jonas Lukasczyk, and Heike Leitte. Clique community persistence: A topological visual analysis approach for complex networks. IEEE Transactions on Visualization and Computer Graphics, 24(1):822\u2013831, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rieck%2C%20Bastian%20Fugacci%2C%20Ulderico%20Lukasczyk%2C%20Jonas%20Leitte%2C%20Heike%20Clique%20community%20persistence%3A%20A%20topological%20visual%20analysis%20approach%20for%20complex%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rieck%2C%20Bastian%20Fugacci%2C%20Ulderico%20Lukasczyk%2C%20Jonas%20Leitte%2C%20Heike%20Clique%20community%20persistence%3A%20A%20topological%20visual%20analysis%20approach%20for%20complex%20networks%202018"
        },
        {
            "id": "Saxe_et+al_2018_a",
            "entry": "Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20Michael%20Bansal%2C%20Yamini%20Dapello%2C%20Joel%20Advani%2C%20Madhu%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20Michael%20Bansal%2C%20Yamini%20Dapello%2C%20Joel%20Advani%2C%20Madhu%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Sizemore_et+al_2017_a",
            "entry": "Ann Sizemore, Chad Giusti, and Danielle S. Bassett. Classification of weighted networks through mesoscale homological features. Journal of Complex Networks, 5(2):245\u2013273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sizemore%2C%20Ann%20Giusti%2C%20Chad%20Bassett%2C%20Danielle%20S.%20Classification%20of%20weighted%20networks%20through%20mesoscale%20homological%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sizemore%2C%20Ann%20Giusti%2C%20Chad%20Bassett%2C%20Danielle%20S.%20Classification%20of%20weighted%20networks%20through%20mesoscale%20homological%20features%202017"
        },
        {
            "id": "Springenberg_et+al_2015_a",
            "entry": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. In Workshop Track of the International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20Jost%20Tobias%20Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Riedmiller%2C%20Martin%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20Jost%20Tobias%20Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Riedmiller%2C%20Martin%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In IEEE Information Theory Workshop (ITW), pp. 1\u20135, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "Tsang_et+al_2018_a",
            "entry": "Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network weights. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsang%2C%20Michael%20Cheng%2C%20Dehua%20Liu%2C%20Yan%20Detecting%20statistical%20interactions%20from%20neural%20network%20weights%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsang%2C%20Michael%20Cheng%2C%20Dehua%20Liu%2C%20Yan%20Detecting%20statistical%20interactions%20from%20neural%20network%20weights%202018"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "Zeiler_0000_a",
            "entry": "Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), European Conference on Computer Vision (ECCV), volume 8689 of Lecture Notes in Computer Science, pp. 818\u2013833, Cham, Switzerland, 2014. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
