{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING",
        "author": "Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, & Chelsea Finn University of California, Berkeley {nagaban,iclavera,simin.liu}@berkeley.edu {ronf,pabbeel,svlevine,cbfinn}@berkeley.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyztsoC5Y7"
        },
        "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent\u2019s learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.1"
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "adaptive control",
            "url": "https://en.wikipedia.org/wiki/adaptive_control"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Gaussian Processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Processes"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "model predictive control",
            "url": "https://en.wikipedia.org/wiki/model_predictive_control"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "ReBAL": "Recurrence-Based Adaptive Learner",
        "GrBAL": "Gradient-Based Adaptive Learner",
        "GPs": "Gaussian Processes",
        "RNN": "recurrent neural network",
        "MDP": "Markov decision process",
        "MAML": "Model-agnostic meta-learning",
        "MPPI": "model predictive path integral control",
        "MPC": "model predictive control",
        "MB": "model-based RL",
        "MB+DE": "model-based RL with dynamic evaluation",
        "RS": "random shooting",
        "F.A.": "fast adaptation"
    },
    "highlights": [
        "Both model-based and model-free reinforcement learning (RL) methods generally operate in one of two regimes: all training is performed in advance, producing a model or policy that can be used at test-time to make decisions in settings that approximately match those seen during training; or, training is performed online, in which case the agent can slowly modify its behavior as it interacts with the environment",
        "We evaluate two versions of our approach, recurrence-based adaptive learner (ReBAL) and gradient-based adaptive learner (GrBAL) on stochastic and simulated continuous control tasks with complex contact dynamics (Fig. 2)",
        "We present an approach for model-based meta-reinforcement learning that enables fast, online adaptation of large and expressive models in dynamic environments",
        "We show that meta-learning a model for online adaptation results in a method that is able to adapt to unseen situations or sudden and drastic changes in the environment, and is sample efficient to train",
        "We provide two instantiations of our approach (ReBAL and Gradient-Based Adaptive Learner), and we provide a comparison with other prior methods on a range of continuous control tasks",
        "We show that, our approach is practical for real-world applications, and that this capability to adapt quickly is important under complex real-world dynamics"
    ],
    "key_statements": [
        "Both model-based and model-free reinforcement learning (RL) methods generally operate in one of two regimes: all training is performed in advance, producing a model or policy that can be used at test-time to make decisions in settings that approximately match those seen during training; or, training is performed online, in which case the agent can slowly modify its behavior as it interacts with the environment",
        "Humans can rapidly adapt their behavior to unseen physical perturbations and changes in their dynamics (<a class=\"ref-link\" id=\"cBraun_et+al_2009_a\" href=\"#rBraun_et+al_2009_a\">Braun et al, 2009</a>): adults can learn to walk on crutches in just a few seconds, people can adapt almost instantaneously to picking up an object that is unexpectedly heavy, and children that can walk on carpet and grass can quickly figure out how to walk on ice without having to relearn how to walk",
        "Motivated by the ability to tackle real-world applications, we develop a model-based meta-reinforcement learning algorithm",
        "We evaluate two versions of our approach, recurrence-based adaptive learner (ReBAL) and gradient-based adaptive learner (GrBAL) on stochastic and simulated continuous control tasks with complex contact dynamics (Fig. 2)",
        "While we focus on reinforcement learning problems in our experiments, this meta-learning approach could be used for a learning to adapt online in a variety of sequence modeling domains",
        "That we have discussed our approach for enabling online adaptation, we propose how to build upon this idea to develop a model-based meta-reinforcement learning algorithm",
        "Our evaluation aims to answer the following questions: (1) Is adaptation changing the model? (2) Does our approach enable fast adaptation to varying dynamics, tasks, and environments, both inside and outside of the training distribution? (3) How does our method\u2019s performance compare to that of other methods? (4) How do Gradient-Based Adaptive Learner and Recurrence-Based Adaptive Learner compare? (5) How does meta model-based reinforcement learning compare to meta model-free reinforcement learning in terms of sample efficiency and performance for these experiments? (6) Can our method learn to adapt online on a real robot, and if so, how does it perform? We present our set-up and results, motivated by these questions",
        "For all of our environments, we model the transition probabilities as Gaussian random variables with mean parameterized by a neural network model (3 hidden layers of 512 units each and ReLU activations) and fixed variance",
        "TEST-TIME PERFORMANCE: ONLINE ADAPTATION & GENERALIZATION In our second comparative evaluation, we evaluate final test time performance both Gradient-Based Adaptive Learner and Recurrence-Based Adaptive Learner in comparison to the aforementioned methods",
        "Note that all agents were meta-trained on a distribution of tasks/environments, but we evaluate their adaptation ability on unseen environments at test time",
        "We evaluate the fast adaptation (F.A.) component on the HC disabled joint, ant crippled leg, and the HC pier",
        "We present an approach for model-based meta-reinforcement learning that enables fast, online adaptation of large and expressive models in dynamic environments",
        "We show that meta-learning a model for online adaptation results in a method that is able to adapt to unseen situations or sudden and drastic changes in the environment, and is sample efficient to train",
        "We provide two instantiations of our approach (ReBAL and Gradient-Based Adaptive Learner), and we provide a comparison with other prior methods on a range of continuous control tasks",
        "We show that, our approach is practical for real-world applications, and that this capability to adapt quickly is important under complex real-world dynamics"
    ],
    "summary": [
        "Both model-based and model-free reinforcement learning (RL) methods generally operate in one of two regimes: all training is performed in advance, producing a model or policy that can be used at test-time to make decisions in settings that approximately match those seen during training; or, training is performed online, in which case the agent can slowly modify its behavior as it interacts with the environment.",
        "The primary contribution of our work is an efficient meta reinforcement learning approach that achieves online adaptation in dynamic environments.",
        "Our algorithm efficiently trains a global model that is capable to use its recent experiences to quickly adapt, achieving fast online adaptation in dynamic environments.",
        "That we have discussed our approach for enabling online adaptation, we propose how to build upon this idea to develop a model-based meta-reinforcement learning algorithm.",
        "The improvement in sample efficiency from using model-based methods matches prior findings (<a class=\"ref-link\" id=\"cDeisenroth_2011_a\" href=\"#rDeisenroth_2011_a\">Deisenroth and Rasmussen, 2011</a>; Nagabandi et al, 2017a; <a class=\"ref-link\" id=\"cKurutach_et+al_2018_a\" href=\"#rKurutach_et+al_2018_a\">Kurutach et al, 2018</a>); the most important evaluation, which we discuss in more detail is the ability for our method to adapt online to drastic dynamics changes in only a handful of timesteps.",
        "Note that all agents were meta-trained on a distribution of tasks/environments, but we evaluate their adaptation ability on unseen environments at test time.",
        "To test our meta model-based RL method\u2019s sample efficiency, as well as its ability to perform fast and effective online adaptation, we applied GrBAL to a real legged millirobot, comparing it to model-based RL (MB) and model-based RL with dynamic evaluation (MB+DE).",
        "Our first group of results (Table 1) show that, when data from a random policy is used to train a dynamics model, both a model trained with a standard supervised learning objective (MB) and a GrBAL model achieve comparable performance for executing desired trajectories on terrains from the training distribution.",
        "We test the performance of our method on what it is intended for: fast online adaptation of the learned model to enable successful execution of new, changing, or out-of-distribution environments at test time.",
        "The qualitative results of these experiments in Fig. 7 show that the robot is able to use our method to adapt online and effectively follow the target trajectories, even in the presence of new environments and unexpected perturbations at test time.",
        "We present an approach for model-based meta-RL that enables fast, online adaptation of large and expressive models in dynamic environments.",
        "We show that meta-learning a model for online adaptation results in a method that is able to adapt to unseen situations or sudden and drastic changes in the environment, and is sample efficient to train.",
        "We show that, our approach is practical for real-world applications, and that this capability to adapt quickly is important under complex real-world dynamics"
    ],
    "headline": "We demonstrate the agent\u2019s learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.1",
    "reference_links": [
        {
            "id": "Al-Shedivat_et+al_2017_a",
            "entry": "M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and P. Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. CoRR, abs/1710.03641, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03641"
        },
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to learn by gradient descent by gradient descent. CoRR, abs/1606.04474, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04474"
        },
        {
            "id": "Stroem_2013_a",
            "entry": "K. J. \u00c5str\u00f6m and B. Wittenmark. Adaptive control. Courier Corporation, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C3%85str%C3%B6m%2C%20K.J.%20Wittenmark%2C%20B.%20Adaptive%20control%202013"
        },
        {
            "id": "Aswani_et+al_2012_a",
            "entry": "A. Aswani, P. Bouffard, and C. Tomlin. Extensions of learning-based model predictive control for real-time application to a quadrotor helicopter. In American Control Conference (ACC), 2012. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aswani%2C%20A.%20Bouffard%2C%20P.%20Tomlin%2C%20C.%20Extensions%20of%20learning-based%20model%20predictive%20control%20for%20real-time%20application%20to%20a%20quadrotor%20helicopter%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aswani%2C%20A.%20Bouffard%2C%20P.%20Tomlin%2C%20C.%20Extensions%20of%20learning-based%20model%20predictive%20control%20for%20real-time%20application%20to%20a%20quadrotor%20helicopter%202012"
        },
        {
            "id": "Baker_et+al_2016_a",
            "entry": "B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02167"
        },
        {
            "id": "Bengio_et+al_1990_a",
            "entry": "Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Universit\u00e9 de Montr\u00e9al, D\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Bengio%2C%20S.%20Cloutier%2C%20J.%20Learning%20a%20synaptic%20learning%20rule.%20Universit%C3%A9%20de%20Montr%C3%A9al%2C%20D%C3%A9partement%20d%E2%80%99informatique%20et%20de%20recherche%20op%C3%A9rationnelle%201990"
        },
        {
            "id": "Braun_et+al_2009_a",
            "entry": "D. A. Braun, A. Aertsen, D. M. Wolpert, and C. Mehring. Learning optimal adaptation strategies in unpredictable motor tasks. Journal of Neuroscience, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Braun%2C%20D.A.%20Aertsen%2C%20A.%20Wolpert%2C%20D.M.%20Mehring%2C%20C.%20Learning%20optimal%20adaptation%20strategies%20in%20unpredictable%20motor%20tasks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Braun%2C%20D.A.%20Aertsen%2C%20A.%20Wolpert%2C%20D.M.%20Mehring%2C%20C.%20Learning%20optimal%20adaptation%20strategies%20in%20unpredictable%20motor%20tasks%202009"
        },
        {
            "id": "Chua_et+al_2018_a",
            "entry": "K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12114"
        },
        {
            "id": "Deisenroth_2011_a",
            "entry": "M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In International Conference on machine learning (ICML), pages 465\u2013472, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20Pilco%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20Pilco%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "Deisenroth_et+al_2013_a",
            "entry": "M. P. Deisenroth, G. Neumann, J. Peters, et al. A survey on policy search for robotics. Foundations and Trends R in Robotics, 2(1\u20132):1\u2013142, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.P.%20Neumann%2C%20G.%20Peters%2C%20J.%20A%20survey%20on%20policy%20search%20for%20robotics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.P.%20Neumann%2C%20G.%20Peters%2C%20J.%20A%20survey%20on%20policy%20search%20for%20robotics%202013"
        },
        {
            "id": "Doerr_et+al_2017_a",
            "entry": "A. Doerr, D. Nguyen-Tuong, A. Marco, S. Schaal, and S. Trimpe. Model-based policy search for automatic tuning of multivariate PID controllers. CoRR, abs/1703.02899, 2017. URL http://arxiv.org/abs/1703.02899.",
            "url": "http://arxiv.org/abs/1703.02899",
            "arxiv_url": "https://arxiv.org/pdf/1703.02899"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl$\u02c62$: Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Finn_2017_a",
            "entry": "C. Finn and S. Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. CoRR, abs/1710.11622, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11622"
        },
        {
            "id": "Finn_et+al_2017_b",
            "entry": "C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. CoRR, abs/1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "Fortunato_et+al_2017_a",
            "entry": "M. Fortunato, C. Blundell, and O. Vinyals. Bayesian recurrent neural networks. arXiv preprint arXiv:1704.02798, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02798"
        },
        {
            "id": "Fu_et+al_2015_a",
            "entry": "J. Fu, S. Levine, and P. Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. CoRR, abs/1509.06841, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.06841"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829\u20132838, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20q-learning%20with%20model-based%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20q-learning%20with%20model-based%20acceleration%202016"
        },
        {
            "id": "Kelouwani_et+al_2012_a",
            "entry": "S. Kelouwani, K. Adegnon, K. Agbossou, and Y. Dube. Online system identification and adaptive control for pem fuel cell maximum efficiency tracking. IEEE Transactions on Energy Conversion, 27(3):580\u2013592, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelouwani%2C%20S.%20Adegnon%2C%20K.%20Agbossou%2C%20K.%20Dube%2C%20Y.%20Online%20system%20identification%20and%20adaptive%20control%20for%20pem%20fuel%20cell%20maximum%20efficiency%20tracking%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelouwani%2C%20S.%20Adegnon%2C%20K.%20Agbossou%2C%20K.%20Dube%2C%20Y.%20Online%20system%20identification%20and%20adaptive%20control%20for%20pem%20fuel%20cell%20maximum%20efficiency%20tracking%202012"
        },
        {
            "id": "Ko_2009_a",
            "entry": "J. Ko and D. Fox. Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models. Autonomous Robots, 27(1):75\u201390, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ko%2C%20J.%20Fox%2C%20D.%20Gp-bayesfilters%3A%20Bayesian%20filtering%20using%20gaussian%20process%20prediction%20and%20observation%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ko%2C%20J.%20Fox%2C%20D.%20Gp-bayesfilters%3A%20Bayesian%20filtering%20using%20gaussian%20process%20prediction%20and%20observation%20models%202009"
        },
        {
            "id": "Krause_et+al_2016_a",
            "entry": "B. Krause, L. Lu, I. Murray, and S. Renals. Multiplicative lstm for sequence modelling. arXiv preprint arXiv:1609.07959, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07959"
        },
        {
            "id": "Krause_et+al_2017_a",
            "entry": "B. Krause, E. Kahembwe, I. Murray, and S. Renals. Dynamic evaluation of neural sequence models. CoRR, abs/1709.07432, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07432"
        },
        {
            "id": "Kurutach_et+al_2018_a",
            "entry": "T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10592"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Lenz_et+al_2015_a",
            "entry": "I. Lenz, R. A. Knepper, and A. Saxena. Deepmpc: Learning deep latent features for model predictive control. In Robotics: Science and Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenz%2C%20I.%20Knepper%2C%20R.A.%20Saxena%2C%20A.%20Deepmpc%3A%20Learning%20deep%20latent%20features%20for%20model%20predictive%20control%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenz%2C%20I.%20Knepper%2C%20R.A.%20Saxena%2C%20A.%20Deepmpc%3A%20Learning%20deep%20latent%20features%20for%20model%20predictive%20control%202015"
        },
        {
            "id": "Levine_2013_a",
            "entry": "S. Levine and V. Koltun. Guided policy search. In International Conference on Machine Learning, pages 1\u20139, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Koltun%2C%20V.%20Guided%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Koltun%2C%20V.%20Guided%20policy%20search%202013"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research (JMLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "Li_2016_a",
            "entry": "K. Li and J. Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01885"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Manganiello_et+al_2014_a",
            "entry": "P. Manganiello, M. Ricco, G. Petrone, E. Monmasson, and G. Spagnuolo. Optimization of perturbative pv mppt methods through online system identification. IEEE Trans. Industrial Electronics, 61(12):6812\u20136821, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manganiello%2C%20P.%20Ricco%2C%20M.%20Petrone%2C%20G.%20Monmasson%2C%20E.%20Optimization%20of%20perturbative%20pv%20mppt%20methods%20through%20online%20system%20identification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manganiello%2C%20P.%20Ricco%2C%20M.%20Petrone%2C%20G.%20Monmasson%2C%20E.%20Optimization%20of%20perturbative%20pv%20mppt%20methods%20through%20online%20system%20identification%202014"
        },
        {
            "id": "Meier_2016_a",
            "entry": "F. Meier and S. Schaal. Drifting gaussian processes with varying neighborhood sizes for online model learning. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA) 2016. IEEE, May 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meier%2C%20F.%20Schaal%2C%20S.%20Drifting%20gaussian%20processes%20with%20varying%20neighborhood%20sizes%20for%20online%20model%20learning%202016-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meier%2C%20F.%20Schaal%2C%20S.%20Drifting%20gaussian%20processes%20with%20varying%20neighborhood%20sizes%20for%20online%20model%20learning%202016-05"
        },
        {
            "id": "Meier_et+al_2016_b",
            "entry": "F. Meier, D. Kappler, N. Ratliff, and S. Schaal. Towards robust online inverse dynamics learning. In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meier%2C%20F.%20Kappler%2C%20D.%20Ratliff%2C%20N.%20Schaal%2C%20S.%20Towards%20robust%20online%20inverse%20dynamics%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meier%2C%20F.%20Kappler%2C%20D.%20Ratliff%2C%20N.%20Schaal%2C%20S.%20Towards%20robust%20online%20inverse%20dynamics%20learning%202016"
        },
        {
            "id": "Mishra_et+al_2017_a",
            "entry": "N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In NIPS 2017 Workshop on Meta-Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20N.%20Rohaninejad%2C%20M.%20Chen%2C%20X.%20Abbeel%2C%20P.%20A%20simple%20neural%20attentive%20meta-learner%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20N.%20Rohaninejad%2C%20M.%20Chen%2C%20X.%20Abbeel%2C%20P.%20A%20simple%20neural%20attentive%20meta-learner%202017"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "T. Munkhdalai and H. Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00837"
        },
        {
            "id": "Munkhdalai_et+al_2017_b",
            "entry": "T. Munkhdalai, X. Yuan, S. Mehri, T. Wang, and A. Trischler. Learning rapid-temporal adaptations. arXiv preprint arXiv:1712.09926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09926"
        },
        {
            "id": "Nagabandi_et+al_0000_a",
            "entry": "A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. CoRR, abs/1708.02596, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02596"
        },
        {
            "id": "Nagabandi_et+al_0000_b",
            "entry": "A. Nagabandi, G. Yang, T. Asmar, R. Pandya, G. Kahn, S. Levine, and R. S. Fearing. Learning image-conditioned dynamics models for control of under-actuated legged millirobots. arXiv preprint arXiv:1711.05253, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05253"
        },
        {
            "id": "Naik_1992_a",
            "entry": "D. K. Naik and R. Mammone. Meta-neural networks that learn by learning. In Neural Networks, 1992. IJCNN., International Joint Conference on, volume 1, pages 437\u2013442. IEEE, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naik%2C%20D.K.%20Mammone%2C%20R.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naik%2C%20D.K.%20Mammone%2C%20R.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992"
        },
        {
            "id": "Pastor_et+al_2011_a",
            "entry": "P. Pastor, L. Righetti, M. Kalakrishnan, and S. Schaal. Online movement adaptation based on previous sensor experiences. In IEEE International Conference on Intelligent Robots and Systems (IROS), pages 365\u2013371, 9 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pastor%2C%20P.%20Righetti%2C%20L.%20Kalakrishnan%2C%20M.%20Schaal%2C%20S.%20Online%20movement%20adaptation%20based%20on%20previous%20sensor%20experiences%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pastor%2C%20P.%20Righetti%2C%20L.%20Kalakrishnan%2C%20M.%20Schaal%2C%20S.%20Online%20movement%20adaptation%20based%20on%20previous%20sensor%20experiences%202011"
        },
        {
            "id": "Peters_2008_a",
            "entry": "J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20J.%20Schaal%2C%20S.%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients.%20Neural%20networks%202008"
        },
        {
            "id": "Rai_et+al_2017_a",
            "entry": "A. Rai, G. Sutanto, S. Schaal, and F. Meier. Learning feedback terms for reactive planning and control. In Proceedings 2017 IEEE International Conference on Robotics and Automation (ICRA), Piscataway, NJ, USA, May 2017. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rai%2C%20A.%20Sutanto%2C%20G.%20Schaal%2C%20S.%20Meier%2C%20F.%20Learning%20feedback%20terms%20for%20reactive%20planning%20and%20control%202017-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rai%2C%20A.%20Sutanto%2C%20G.%20Schaal%2C%20S.%20Meier%2C%20F.%20Learning%20feedback%20terms%20for%20reactive%20planning%20and%20control%202017-05"
        },
        {
            "id": "Ravi_2018_a",
            "entry": "S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202018"
        },
        {
            "id": "Rei_2015_a",
            "entry": "M. Rei. Online representation learning in recurrent neural language models. CoRR, abs/1508.03854, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.03854"
        },
        {
            "id": "S_et+al_2018_a",
            "entry": "S. S\u00e6mundsson, K. Hofmann, and M. P. Deisenroth. Meta reinforcement learning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07551"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. One-shot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06065"
        },
        {
            "id": "Sastry_1989_a",
            "entry": "S. S. Sastry and A. Isidori. Adaptive control of linearizable systems. IEEE Transactions on Automatic Control, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sastry%2C%20S.S.%20Isidori%2C%20A.%20Adaptive%20control%20of%20linearizable%20systems%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sastry%2C%20S.S.%20Isidori%2C%20A.%20Adaptive%20control%20of%20linearizable%20systems%201989"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992"
        },
        {
            "id": "Schmidhuber_1991_a",
            "entry": "J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20generate%20artificial%20fovea%20trajectories%20for%20target%20detection%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20generate%20artificial%20fovea%20trajectories%20for%20target%20detection%201991"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05477"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "Sung_et+al_2017_a",
            "entry": "F. Sung, L. Zhang, T. Xiang, T. Hospedales, and Y. Yang. Learning to learn: Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09529"
        },
        {
            "id": "Tanaskovic_et+al_2013_a",
            "entry": "M. Tanaskovic, L. Fagiano, R. Smith, P. Goulart, and M. Morari. Adaptive model predictive control for constrained linear systems. In Control Conference (ECC), 2013 European. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tanaskovic%2C%20M.%20Fagiano%2C%20L.%20Smith%2C%20R.%20Goulart%2C%20P.%20Adaptive%20model%20predictive%20control%20for%20constrained%20linear%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tanaskovic%2C%20M.%20Fagiano%2C%20L.%20Smith%2C%20R.%20Goulart%2C%20P.%20Adaptive%20model%20predictive%20control%20for%20constrained%20linear%20systems%202013"
        },
        {
            "id": "Thrun_1998_a",
            "entry": "S. Thrun and L. Pratt. Learning to learn: Introduction and overview. In Learning to learn. Springer, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Pratt%2C%20L.%20Learning%20to%20learn%3A%20Introduction%20and%20overview.%20In%20Learning%20to%20learn%201998"
        },
        {
            "id": "Ieee_2010_a",
            "entry": "IEEE, 2012. S. J. Underwood and I. Husain. Online parameter estimation and adaptive control of permanent-magnet synchronous machines. IEEE Transactions on Industrial Electronics, 57(7):2435\u20132443, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=IEEE%2C%202012.%20S.%20J.%20Underwood%20Husain%2C%20I.%20Online%20parameter%20estimation%20and%20adaptive%20control%20of%20permanent-magnet%20synchronous%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=IEEE%2C%202012.%20S.%20J.%20Underwood%20Husain%2C%20I.%20Online%20parameter%20estimation%20and%20adaptive%20control%20of%20permanent-magnet%20synchronous%20machines%202010"
        },
        {
            "id": "Botvinick_2016_a",
            "entry": "M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. A. Weinstein and M. Botvinick. Structure learning in motor control: A deep reinforcement learning model.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "Corr_2017_a",
            "entry": "CoRR, abs/1706.06827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06827"
        },
        {
            "id": "Williams_et+al_2015_a",
            "entry": "G. Williams, A. Aldrich, and E. Theodorou. Model predictive path integral control using covariance variable importance sampling. CoRR, abs/1509.01149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01149"
        },
        {
            "id": "Williams_et+al_2017_a",
            "entry": "G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou. Information theoretic mpc for model-based reinforcement learning. In International Conference on Robotics and Automation (ICRA), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20G.%20Wagener%2C%20N.%20Goldfain%2C%20B.%20Drews%2C%20P.%20Theodorou.%20Information%20theoretic%20mpc%20for%20model-based%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20G.%20Wagener%2C%20N.%20Goldfain%2C%20B.%20Drews%2C%20P.%20Theodorou.%20Information%20theoretic%20mpc%20for%20model-based%20reinforcement%20learning%202017"
        },
        {
            "id": "Younger_et+al_2001_a",
            "entry": "A. S. Younger, S. Hochreiter, and P. R. Conwell. Meta-learning with backpropagation. In International Joint Conference on Neural Networks. IEEE, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Younger%2C%20A.S.%20Hochreiter%2C%20S.%20Conwell%2C%20P.R.%20Meta-learning%20with%20backpropagation%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Younger%2C%20A.S.%20Hochreiter%2C%20S.%20Conwell%2C%20P.R.%20Meta-learning%20with%20backpropagation%202001"
        }
    ]
}
