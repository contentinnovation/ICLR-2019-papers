{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DETERMINISTIC PAC-BAYESIAN GENERALIZATION",
        "author": "BOUNDS FOR DEEP NETWORKS VIA GENERALIZING",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hygn2o0qKX"
        },
        "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss \u2013 minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either stochastic or compressed. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned \u2013 a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices \u2013 a guarantee that would not have been possible with prior approaches."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent"
    },
    "highlights": [
        "Modern deep neural networks contain millions of parameters and are trained on relatively few samples",
        "Conventional wisdom in machine learning suggests that such models should massively overfit on the training data, as these models have the capacity to memorize even a randomly labeled dataset of similar size (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_a\" href=\"#rNeyshabur_et+al_2015_a\">Neyshabur et al, 2015</a>). These models have achieved state-ofthe-art generalization error on many real-world tasks. This observation has spurred an active line of research (<a class=\"ref-link\" id=\"cSoudry_et+al_2018_a\" href=\"#rSoudry_et+al_2018_a\">Soudry et al, 2018</a>; <a class=\"ref-link\" id=\"cBrutzkus_et+al_2018_a\" href=\"#rBrutzkus_et+al_2018_a\">Brutzkus et al, 2018</a>; <a class=\"ref-link\" id=\"cLi_2018_a\" href=\"#rLi_2018_a\">Li & Liang, 2018</a>) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well",
        "We revisit the PAC-Bayesian analysis of deep networks in <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al (2017</a>; 2018) and provide a general framework that allows one to use noise-resilience of the deep network on training data to provide a bound on the original deterministic and uncompressed network. We achieve this by arguing that if on the training data, the interaction between the \u2018activated weight matrices\u2019 satisfy certain conditions which results in a wide training loss minimum, these conditions themselves generalize to the weight matrix interactions on the test data. After presenting this general PAC-Bayesian framework, we specialize it to the case of deep ReLU networks, showing that we can provide a generalization bound that accomplishes two goals simultaneously: i) it applies to the original network and ii) it does not scale exponentially with depth in terms of the products of the spectral norms of the weight matrices; instead our bound scales with more meaningful terms that capture the interactions between the weight matrices and do not have such a severe dependence on depth in practice",
        "One of the most important aspects of the generalization puzzle that has been studied is that of the flatness/width of the training loss at the minimum found by stochastic gradient descent",
        "We introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of deep neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network",
        "A better understanding of the source of noise-resilience in deep ReLU networks would help in applying our framework more carefully in these settings, leading to tighter guarantees on the original network"
    ],
    "key_statements": [
        "Modern deep neural networks contain millions of parameters and are trained on relatively few samples",
        "Conventional wisdom in machine learning suggests that such models should massively overfit on the training data, as these models have the capacity to memorize even a randomly labeled dataset of similar size (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_a\" href=\"#rNeyshabur_et+al_2015_a\">Neyshabur et al, 2015</a>). These models have achieved state-ofthe-art generalization error on many real-world tasks. This observation has spurred an active line of research (<a class=\"ref-link\" id=\"cSoudry_et+al_2018_a\" href=\"#rSoudry_et+al_2018_a\">Soudry et al, 2018</a>; <a class=\"ref-link\" id=\"cBrutzkus_et+al_2018_a\" href=\"#rBrutzkus_et+al_2018_a\">Brutzkus et al, 2018</a>; <a class=\"ref-link\" id=\"cLi_2018_a\" href=\"#rLi_2018_a\">Li & Liang, 2018</a>) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well",
        "We revisit the PAC-Bayesian analysis of deep networks in <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al (2017</a>; 2018) and provide a general framework that allows one to use noise-resilience of the deep network on training data to provide a bound on the original deterministic and uncompressed network. We achieve this by arguing that if on the training data, the interaction between the \u2018activated weight matrices\u2019 satisfy certain conditions which results in a wide training loss minimum, these conditions themselves generalize to the weight matrix interactions on the test data. After presenting this general PAC-Bayesian framework, we specialize it to the case of deep ReLU networks, showing that we can provide a generalization bound that accomplishes two goals simultaneously: i) it applies to the original network and ii) it does not scale exponentially with depth in terms of the products of the spectral norms of the weight matrices; instead our bound scales with more meaningful terms that capture the interactions between the weight matrices and do not have such a severe dependence on depth in practice",
        "One of the most important aspects of the generalization puzzle that has been studied is that of the flatness/width of the training loss at the minimum found by stochastic gradient descent",
        "We want that for a given input, if the first r \u2212 1 sets of properties approximately satisfy the condition in Equation 1, the properties in the rth set are noise-resilient i.e., under random parameter perturbations, these properties do not suffer much perturbation. This kind of constraint would naturally hold for deep networks if we have chosen the properties carefully e.g., we will show that, for any given input, the perturbation in the pre-activation values of the dth layer is small as long as the absolute pre-activation values in the layers below d \u2212 1 are large, and a few other norm-bounds on the lower layer weights are satisfied",
        "We introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of deep neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network",
        "A better understanding of the source of noise-resilience in deep ReLU networks would help in applying our framework more carefully in these settings, leading to tighter guarantees on the original network"
    ],
    "summary": [
        "Modern deep neural networks contain millions of parameters and are trained on relatively few samples.",
        "With the guarantee that the classifier is noise-resilient both on training and test data, we derive a generalization bound on the test loss of the original network.",
        "The PAC-Bayesian framework yields the following generalization bound on the 0-1 error of the stochastic classifier that holds with probability 1 \u2212 \u03b4 over the draw of the training set S of m samples3: EW [E(x,y)\u223cD[L0(f x; W , y)]] \u2264 EW [E(x,y)\u223cS[L0(f x; W , y)]] + O KL(W P ) m",
        "This kind of constraint would naturally hold for deep networks if we have chosen the properties carefully e.g., we will show that, for any given input, the perturbation in the pre-activation values of the dth layer is small as long as the absolute pre-activation values in the layers below d \u2212 1 are large, and a few other norm-bounds on the lower layer weights are satisfied.",
        "We apply our framework to feedforward fully connected ReLU networks of depth D and width H and derive a generalization bound on the original network that does not scale with the product of spectral norms of the weight matrices.",
        "We consider a setting where the learning algorithm satisfies the following conditions on the training data that make it noise-resilient on training data: a) the 2 norm of the hidden layers are all small, b) the pre-activation values are all sufficiently large in magnitude, c) the Jacobian of any layer with respect to a lower layer, has rows with a small 2 norm, and has a small spectral norm.",
        "As noted before, we emphasize that the dependence of our bound on the pre-activation values is a limitation in how we characterize noise-resilience through our conditions rather than a drawback in our general PAC-Bayesian framework itself.",
        "We introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of deep neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network.",
        "The main philosophy of our approach is to first generalize the noise-resilience from training data to test data using which we convert a PAC-Bayesian bound on a stochastic network to a standard margin-based generalization bound.",
        "We apply our approach to ReLU based networks and derive a bound that scales with terms that capture the interactions between the weight matrices better than the product of spectral norms.",
        "A better understanding of the source of noise-resilience in deep ReLU networks would help in applying our framework more carefully in these settings, leading to tighter guarantees on the original network"
    ],
    "headline": "We present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned \u2013 a network that is deterministic and uncompressed",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In The 35th International Conference on Machine Learning, ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Brutzkus_et+al_2018_a",
            "entry": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20SGD%20learns%20overparameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20SGD%20learns%20overparameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017"
        },
        {
            "id": "Golowich_et+al_2018_a",
            "entry": "Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. Computational Learning Theory, COLT 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "Ralf_2000_a",
            "entry": "Ralf Herbrich and Thore Graepel. A pac-bayesian margin bound for linear classifiers: Why svms work. In Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%2C%20Ralf%20Herbrich%20Thore%20Graepel.%20A%20pac-bayesian%20margin%20bound%20for%20linear%20classifiers%3A%20Why%20svms%20work%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=and%2C%20Ralf%20Herbrich%20Thore%20Graepel.%20A%20pac-bayesian%20margin%20bound%20for%20linear%20classifiers%3A%20Why%20svms%20work%202000"
        },
        {
            "id": "Hinton_1993_a",
            "entry": "Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, COLT, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1), 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Flat%20minima%201997"
        },
        {
            "id": "Keskar_et+al_2017_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "Langford_2001_a",
            "entry": "John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001], 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Caruana%2C%20Rich%20bounding%20the%20true%20error%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Caruana%2C%20Rich%20bounding%20the%20true%20error%202001"
        },
        {
            "id": "Langford_2002_a",
            "entry": "John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Shawe-Taylor%2C%20John%20Pac-bayes%20%26%20margins%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Shawe-Taylor%2C%20John%20Pac-bayes%20%26%20margins%202002"
        },
        {
            "id": "Li_2018_a",
            "entry": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Liang%2C%20Yingyu%20Learning%20overparameterized%20neural%20networks%20via%20stochastic%20gradient%20descent%20on%20structured%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Liang%2C%20Yingyu%20Learning%20overparameterized%20neural%20networks%20via%20stochastic%20gradient%20descent%20on%20structured%20data%202018"
        },
        {
            "id": "London_et+al_2016_a",
            "entry": "Ben London, Bert Huang, and Lise Getoor. Stability and generalization in structured prediction. Journal of Machine Learning Research, 17:222:1\u2013222:52, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=London%2C%20Ben%20Huang%2C%20Bert%20Getoor%2C%20Lise%20Stability%20and%20generalization%20in%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20Ben%20Huang%2C%20Bert%20Getoor%2C%20Lise%20Stability%20and%20generalization%20in%20structured%20prediction%202016"
        },
        {
            "id": "Mcallester_2003_a",
            "entry": "David McAllester. Simplified pac-bayesian margin bounds. In Learning Theory and Kernel Machines. Springer Berlin Heidelberg, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20Simplified%20pac-bayesian%20margin%20bounds.%20In%20Learning%20Theory%20and%20Kernel%20Machines%202003"
        },
        {
            "id": "Mcallester_0000_a",
            "entry": "David A. McAllester. Some pac-bayesian theorems. Machine Learning, 37(3), 1999a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Some%20pac-bayesian%20theorems",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Some%20pac-bayesian%20theorems"
        },
        {
            "id": "Mcallester_1999_a",
            "entry": "David A. McAllester. Pac-bayesian model averaging. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT 1999, 1999b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Pac-bayesian%20model%20averaging%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Pac-bayesian%20model%20averaging%201999"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J. Zico Kolter. Generalization in deep networks: The role of distance from initialization. Deep Learning: Bridging Theory and Practice Workshop in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Generalization%20in%20deep%20networks%3A%20The%20role%20of%20distance%20from%20initialization.%20Deep%20Learning%3A%20Bridging%20Theory%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Generalization%20in%20deep%20networks%3A%20The%20role%20of%20distance%20from%20initialization.%20Deep%20Learning%3A%20Bridging%20Theory%202017"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. International Conference on Learning Representations Workshop Track, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. Advances in Neural Information Processing Systems to appear, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Neyshabur_et+al_2018_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20A%20pac-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20A%20pac-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= HJC2SzZCW.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20Roman%20Bahri%2C%20Yasaman%20Abolafia%2C%20Daniel%20A.%20Pennington%2C%20Jeffrey%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018"
        },
        {
            "id": "Soudry_et+al_2018_a",
            "entry": "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20Daniel%20Hoffer%2C%20Elad%20Srebro%2C%20Nathan%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20Daniel%20Hoffer%2C%20Elad%20Srebro%2C%20Nathan%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018"
        },
        {
            "id": "Tropp_2012_a",
            "entry": "Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices%202012"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
