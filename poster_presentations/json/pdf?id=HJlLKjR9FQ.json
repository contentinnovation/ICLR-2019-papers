{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "TOWARDS UNDERSTANDING REGULARIZATION IN BATCH NORMALIZATION",
        "author": "Ping Luo,\u2217 Xinjiang Wang,\u2217 Wenqi Shao,\u2217 Zhanglin Peng, 1The Chinese University of Hong Kong 2SenseTime Research 3The University of Hong Kong",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJlLKjR9FQ"
        },
        "abstract": "Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "learning rate",
            "url": "https://en.wikipedia.org/wiki/learning_rate"
        },
        {
            "term": "gamma decay",
            "url": "https://en.wikipedia.org/wiki/gamma_decay"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "Dynamics",
            "url": "https://en.wikipedia.org/wiki/Dynamics"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "mechanics",
            "url": "https://en.wikipedia.org/wiki/mechanics"
        },
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        }
    ],
    "abbreviations": {
        "BN": "Batch Normalization",
        "PN": "population normalization",
        "ODEs": "of differential equations",
        "WN": "Weight normalization",
        "LR": "learning rate",
        "SM": "statistical mechanics",
        "IN": "instance normalization",
        "LN": "layer normalization"
    },
    "highlights": [
        "Batch Normalization (BN) is an indispensable component in many deep neural networks (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>)",
        "We show that Batch Normalization converges with large maximum and effective learning rate (LR), where the former one is the largest learning rate when training converged, while the latter one is the actual learning rate during training",
        "With Batch Normalization, we find that both learning rate would be larger than a network trained without Batch Normalization",
        "We find that this optimum value only depends on its corresponding eigenvalue denoted as \u03bbR",
        "This work investigated an explicit regularization form of Batch Normalization, which was decomposed into population normalization and gamma decay where the regularization strengths from \u03bcB and \u03c3B were explored",
        "Experiments in CNNs showed that Batch Normalization in deep networks share the same traits of regularization"
    ],
    "key_statements": [
        "Batch Normalization (BN) is an indispensable component in many deep neural networks (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>)",
        "Batch Normalization has been widely used in various areas such as machine vision, speech and natural language processing",
        "We demonstrate the results empirically in Sec.5, where we observe that CNNs trained with Batch Normalization share similar traits of regularization as discussed above.\n3 OPTIMIZATION WITH REGULARIZATION",
        "We show that Batch Normalization converges with large maximum and effective learning rate (LR), where the former one is the largest learning rate when training converged, while the latter one is the actual learning rate during training",
        "With Batch Normalization, we find that both learning rate would be larger than a network trained without Batch Normalization",
        "We find that this optimum value only depends on its corresponding eigenvalue denoted as \u03bbR",
        "We demonstrate that \u03bbR < 0 if and only if \u03b7max > \u03b7eff , such that the fixed point R0 is stable for all approaches",
        "For ReLU as an example, we find that \u03b7mbnax \u2265 \u03b7m{wanx,sgd} + 2\u03b6",
        "All models are trained by using SGD with momentum, while the initial learning rates are scaled proportionally (<a class=\"ref-link\" id=\"cGoyal_et+al_2017_a\" href=\"#rGoyal_et+al_2017_a\">Goyal et al, 2017</a>) when different batch sizes are presented",
        "Similar trend can be observed by experiment in a down-sampled version of ImageNet",
        "We study the regulation strengths of vanilla SGD, Batch Normalization, Weight normalization, Weight normalization+mean-only Batch Normalization, and Weight normalization+variance-only Batch Normalization",
        "As \u03bcB and \u03c3B are removed in vanilla SGD, it is found that the training loss decreases while the validation loss increases, implying that reduction in regularization makes the network converged to a sharper local minimum that generalizes less well",
        "This work investigated an explicit regularization form of Batch Normalization, which was decomposed into population normalization and gamma decay where the regularization strengths from \u03bcB and \u03c3B were explored",
        "Experiments in CNNs showed that Batch Normalization in deep networks share the same traits of regularization"
    ],
    "summary": [
        "Batch Normalization (BN) is an indispensable component in many deep neural networks (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>).",
        "Experimental studies (<a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a>) suggested that BN improves convergence and generalization by enabling large learning rate and preventing overfitting when training deep networks.",
        "In Eqn(1), y is the output of a neuron, g(\u00b7) denotes an activation function, h and hare hidden values before and after batch normalization, w and x are kernel weight vector and network input respectively.",
        "Training the above single-layer perceptron with BN in Eqn(1) typically involves minimizing a negative log likelihood function with respect to a set of network parameters \u03b8 = {w, \u03b3, \u03b2}.",
        "With SM, a student network is dedicated to learn relationship between a Gaussian input and an output by using a weight vector w as parameters.",
        "The above loss function represents BN by using WN with gamma decay, and it is sufficient to study the learning rates of different approaches.",
        "For vanilla SGD, the student is computed by y = g with g(\u00b7) being either identity or ReLU, and w being the weight vector to optimize, where w has the same dimension as w\u2217.",
        "We provide closed-form solutions of the generalization errors for vanilla SGD with both linear and ReLU student networks.",
        "The solution of generalization error depends on the rank of correlation matrix \u03a3 = xTx. Here we define an effective load \u03b1 = P/N that is the ratio between number of samples P and number of input neurons N.",
        "All models are trained by using SGD with momentum, while the initial learning rates are scaled proportionally (<a class=\"ref-link\" id=\"cGoyal_et+al_2017_a\" href=\"#rGoyal_et+al_2017_a\">Goyal et al, 2017</a>) when different batch sizes are presented.",
        "Similar trend can be observed by experiment in a down-sampled version of ImageNet. We would like to point out that \u2018PN+gamma decay\u2019 is of interest in theoretical analyses, but it is computation-demanding when applied in practice because evaluating \u03bcP , \u03c3P and \u03b6(h) may require sufficiently large number of samples.",
        "As \u03bcB and \u03c3B are removed in vanilla SGD, it is found that the training loss decreases while the validation loss increases, implying that reduction in regularization makes the network converged to a sharper local minimum that generalizes less well.",
        "As PN and gamma decay requires estimating the population statistics that increases computations, we utilize dropout as an alternative to improve regularization of BN.",
        "Optimization and generalization of BN with regularization were derived and compared with vanilla SGD, WN, and WN+gamma decay, showing that BN enables training to converge with large maximum and effective learning rate, as well as leads to better generalization.",
        "Understanding the characteristics of these normalizers should be the first step to analyze some recent best practices such as whitening (<a class=\"ref-link\" id=\"cLuo_2017_b\" href=\"#rLuo_2017_b\"><a class=\"ref-link\" id=\"cLuo_2017_b\" href=\"#rLuo_2017_b\">Luo, 2017b</a></a>;a), switchable normalization (<a class=\"ref-link\" id=\"cLuo_et+al_2019_a\" href=\"#rLuo_et+al_2019_a\"><a class=\"ref-link\" id=\"cLuo_et+al_2019_a\" href=\"#rLuo_et+al_2019_a\">Luo et al, 2019</a></a>; 2018; <a class=\"ref-link\" id=\"cShao_et+al_2019_a\" href=\"#rShao_et+al_2019_a\"><a class=\"ref-link\" id=\"cShao_et+al_2019_a\" href=\"#rShao_et+al_2019_a\">Shao et al, 2019</a></a>), and switchable whitening (<a class=\"ref-link\" id=\"cPan_et+al_2019_a\" href=\"#rPan_et+al_2019_a\"><a class=\"ref-link\" id=\"cPan_et+al_2019_a\" href=\"#rPan_et+al_2019_a\">Pan et al, 2019</a></a>)"
    ],
    "headline": "By following, we find that Batch Normalization induces Gaussian priors for \u03bcB",
    "reference_links": [
        {
            "id": "Advani_2017_a",
            "entry": "Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv:1710.03667 [physics, q-bio, stat], October 2017. URL http://arxiv.org/abs/1710.03667.arXiv:1710.03667.",
            "url": "http://arxiv.org/abs/1710.03667.arXiv:1710.03667",
            "arxiv_url": "https://arxiv.org/pdf/1710.03667"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bishop_1995_a",
            "entry": "Chris M. Bishop. Training with Noise is Equivalent to Tikhonov Regularization. Neural Computation, 7 (1):108\u2013116, January 1995. ISSN 0899-7667, 1530-888X. doi: 10.1162/neco.1995.7.1.108. URL http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108.",
            "crossref": "https://dx.doi.org/10.1162/neco.1995.7.1.108"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017"
        },
        {
            "id": "Bos_1998_a",
            "entry": "Siegfried Bos. Statistical mechanics approach to early stopping and weight decay. Physical Review E, 58(1):833, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bos%2C%20Siegfried%20Statistical%20mechanics%20approach%20to%20early%20stopping%20and%20weight%20decay%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bos%2C%20Siegfried%20Statistical%20mechanics%20approach%20to%20early%20stopping%20and%20weight%20decay%201998"
        },
        {
            "id": "Bs_1998_a",
            "entry": "Siegfried Bs and Manfred Opper. Dynamics of batch training in a perceptron. In Journal of Physics A: Mathematical and General, volume 31(21), pp. 4835, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bs%2C%20Siegfried%20Opper%2C%20Manfred%20Dynamics%20of%20batch%20training%20in%20a%20perceptron%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bs%2C%20Siegfried%20Opper%2C%20Manfred%20Dynamics%20of%20batch%20training%20in%20a%20perceptron%201998"
        },
        {
            "id": "Fagan_2018_a",
            "entry": "Francois Fagan and Garud Iyengar. Robust Implicit Backpropagation. In arXiv:1808.02433, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02433"
        },
        {
            "id": "Gitman_2017_a",
            "entry": "Igor Gitman and Boris Ginsburg. Comparison of Batch Normalization and Weight Normalization Algorithms for the Large-scale Image Classification. arXiv:1709.08145 [cs], September 2017. URL http://arxiv.org/abs/1709.08145.arXiv:1709.08145.",
            "url": "http://arxiv.org/abs/1709.08145.arXiv:1709.08145",
            "arxiv_url": "https://arxiv.org/pdf/1709.08145"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. In Technical Report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images.%20In%20Technical%20Report%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Krogh_1992_a",
            "entry": "Anders Krogh and John A. Hertz. Generalization in a linear perceptron in the presence of noise. Journal of Physics A: Mathematical and General, 25(5):1135, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20Generalization%20in%20a%20linear%20perceptron%20in%20the%20presence%20of%20noise%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20Generalization%20in%20a%20linear%20perceptron%20in%20the%20presence%20of%20noise%201992"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In arXiv:1801.05134, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05134"
        },
        {
            "id": "Loshchilov_2016_a",
            "entry": "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In arXiv:1608.03983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.03983"
        },
        {
            "id": "Luo_2017_a",
            "entry": "Ping Luo. Eigennet: Towards fast and structural learning of deep neural networks. IJCAI, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Eigennet%3A%20Towards%20fast%20and%20structural%20learning%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Luo_2017_b",
            "entry": "Ping Luo. Learning deep architectures via generalized whitened neural networks. ICML, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Learning%20deep%20architectures%20via%20generalized%20whitened%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Ping%20Learning%20deep%20architectures%20via%20generalized%20whitened%20neural%20networks%202017"
        },
        {
            "id": "Luo_et+al_2018_a",
            "entry": "Ping Luo, Zhanglin Peng, Jiamin Ren, and Ruimao Zhang. Do normalization layers in a deep convnet really need to be distinct? arXiv:1811.07727, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.07727"
        },
        {
            "id": "Luo_et+al_2019_a",
            "entry": "Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, and Jingyu Li. Differentiable learning-to-normalize via switchable normalization. ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Ren%2C%20Jiamin%20Peng%2C%20Zhanglin%20Zhang%2C%20Ruimao%20Differentiable%20learning-to-normalize%20via%20switchable%20normalization%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Ping%20Ren%2C%20Jiamin%20Peng%2C%20Zhanglin%20Zhang%2C%20Ruimao%20Differentiable%20learning-to-normalize%20via%20switchable%20normalization%202019"
        },
        {
            "id": "Mandt_et+al_2017_a",
            "entry": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic Gradient Descent as Approximate Bayesian Inference. arXiv:1704.04289 [cs, stat], April 2017. URL http://arxiv.org/abs/1704.04289.arXiv:1704.04289.",
            "url": "http://arxiv.org/abs/1704.04289.arXiv:1704.04289",
            "arxiv_url": "https://arxiv.org/pdf/1704.04289"
        },
        {
            "id": "Mei_et+al_2016_a",
            "entry": "Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. In arXiv:1607.06534, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06534"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20importance%20of%20single%20directions%20for%20generalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20importance%20of%20single%20directions%20for%20generalization%202018"
        },
        {
            "id": "Opper_et+al_1990_a",
            "entry": "M. Opper, W. Kinzel, J. Kleinz, and R. Nehl. On the ability of the optimal perceptron to generalise. In Journal of Physics A: Mathematical and General, volume 23(11), pp. 581, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20M.%20Kinzel%2C%20W.%20Kleinz%2C%20J.%20Nehl%2C%20R.%20On%20the%20ability%20of%20the%20optimal%20perceptron%20to%20generalise%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20M.%20Kinzel%2C%20W.%20Kleinz%2C%20J.%20Nehl%2C%20R.%20On%20the%20ability%20of%20the%20optimal%20perceptron%20to%20generalise%201990"
        },
        {
            "id": "Pan_et+al_2019_a",
            "entry": "Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In arXiv:1904.09739, 2019.",
            "arxiv_url": "https://arxiv.org/pdf/1904.09739"
        },
        {
            "id": "Pennington_2017_a",
            "entry": "Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix theory. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Bahri%2C%20Yasaman%20Geometry%20of%20neural%20network%20loss%20surfaces%20via%20random%20matrix%20theory%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Bahri%2C%20Yasaman%20Geometry%20of%20neural%20network%20loss%20surfaces%20via%20random%20matrix%20theory%202017"
        },
        {
            "id": "Raghu_et+al_2017_a",
            "entry": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the expressive power of deep neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Rifai_et+al_2011_a",
            "entry": "Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model trained with a regularized objective. arXiv:1104.3250 [cs], April 2011. URL http://arxiv.org/abs/1104.3250.arXiv:1104.3250.",
            "url": "http://arxiv.org/abs/1104.3250.arXiv:1104.3250",
            "arxiv_url": "https://arxiv.org/pdf/1104.3250"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In ICJV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Saad_1996_a",
            "entry": "David Saad and Sara A. Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. In NIPS, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saad%2C%20David%20Solla%2C%20Sara%20A.%20Dynamics%20of%20on-line%20gradient%20descent%20learning%20for%20multilayer%20neural%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saad%2C%20David%20Solla%2C%20Sara%20A.%20Dynamics%20of%20on-line%20gradient%20descent%20learning%20for%20multilayer%20neural%20networks%201996"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In arXiv:1602.07868, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "Santurkar_et+al_2018_a",
            "entry": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Normalization Help Optimization? arXiv:1805.11604 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.11604.arXiv:1805.11604.",
            "url": "http://arxiv.org/abs/1805.11604.arXiv:1805.11604",
            "arxiv_url": "https://arxiv.org/pdf/1805.11604"
        },
        {
            "id": "Seung_et+al_1992_a",
            "entry": "H. S. Seung, Haim Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical Review A, 45(8):6056, 1992. URL https://journals.aps.org/pra/abstract/10.1103/ PhysRevA.45.6056.",
            "url": "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.45.6056",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seung%2C%20H.S.%20Sompolinsky%2C%20Haim%20Tishby%2C%20N.%20Statistical%20mechanics%20of%20learning%20from%20examples%201992"
        },
        {
            "id": "Shao_et+al_2019_a",
            "entry": "Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo. Ssn: Learning sparse switchable normalization via sparsestmax. In CVPR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shao%2C%20Wenqi%20Meng%2C%20Tianjian%20Li%2C%20Jingyu%20Zhang%2C%20Ruimao%20Xiaogang%20Wang%2C%20and%20Ping%20Luo.%20Ssn%3A%20Learning%20sparse%20switchable%20normalization%20via%20sparsestmax%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shao%2C%20Wenqi%20Meng%2C%20Tianjian%20Li%2C%20Jingyu%20Zhang%2C%20Ruimao%20Xiaogang%20Wang%2C%20and%20Ping%20Luo.%20Ssn%3A%20Learning%20sparse%20switchable%20normalization%20via%20sparsestmax%202019"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. In Journal of Machine Learning Research, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. arXiv:1512.00567 [cs], December 2015. URL http://arxiv.org/abs/1512.00567.arXiv:1512.00567.",
            "url": "http://arxiv.org/abs/1512.00567.arXiv:1512.00567",
            "arxiv_url": "https://arxiv.org/pdf/1512.00567"
        },
        {
            "id": "Teye_et+al_2018_a",
            "entry": "Mattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian uncertainty estimation for batch normalized deep networks. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teye%2C%20Mattias%20Azizpour%2C%20Hossein%20Smith%2C%20Kevin%20Bayesian%20uncertainty%20estimation%20for%20batch%20normalized%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teye%2C%20Mattias%20Azizpour%2C%20Hossein%20Smith%2C%20Kevin%20Bayesian%20uncertainty%20estimation%20for%20batch%20normalized%20deep%20networks%202018"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017"
        },
        {
            "id": "Ulyanov_et+al_2016_a",
            "entry": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Van_2017_a",
            "entry": "Twan. van Laarhoven. L2 regularization versus batch and weight normalization. In arXiv:1706.05350, 2017. Stefan Wager, Sida Wang, and Percy Liang. Dropout Training as Adaptive Regularization. arXiv:1307.1493 [cs, stat], July 2013. URL http://arxiv.org/abs/1307.1493.arXiv:1307.1493. Yuxin Wu and Kaiming He. Group normalization.arXiv:1803.08494, 2018. Yuki Yoshida, Ryo Karakida, Masato Okada, and Shun ichi Amari. Statistical mechanical analysis of online learning with weight normalization in single layer perceptron. In Journal of the Physical Society of Japan, 2017. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017a. Qiuyi Zhang, Rina Panigrahy, and Sushant. Sachdeva. Electron-proton dynamics in deep learning. In arXiv:1702.00458, 2017b.",
            "url": "http://arxiv.org/abs/1307.1493.arXiv:1307.1493",
            "arxiv_url": "https://arxiv.org/pdf/1706.05350"
        }
    ]
}
