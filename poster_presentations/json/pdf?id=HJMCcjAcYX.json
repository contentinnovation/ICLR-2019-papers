{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING REPRESENTATIONS OF SETS THROUGH OPTIMIZED PERMUTATIONS",
        "author": "Yan Zhang & Adam Prugel-Bennett & Jonathon Hare Department of Electronics and Computer Science University of Southampton {yz5n,apb,jsh,}@ecs.soton.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJMCcjAcYX"
        },
        "abstract": "Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model\u2019s ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the-art results: number sorting, image mosaics, classification from image mosaics, and visual question answering."
    },
    "keywords": [
        {
            "term": "feature vector",
            "url": "https://en.wikipedia.org/wiki/feature_vector"
        },
        {
            "term": "image mosaic",
            "url": "https://en.wikipedia.org/wiki/image_mosaic"
        },
        {
            "term": "mosaics",
            "url": "https://en.wikipedia.org/wiki/mosaics"
        }
    ],
    "abbreviations": {
        "VQA": "visual question answering"
    },
    "highlights": [
        "Consider a task where each input sample is a set of feature vectors with each feature vector describing an object in an image",
        "We believe that we are the first to show the potential of optimisation for processing sets because \u2013 with an appropriate cost function \u2013 it is able to preserve permutation-invariance",
        "The main drawback of our approach is the cubic time complexity in the set size compared to the quadratic complexity of <a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\">Mena et al (2018</a>), which limits our model to tasks where the number of elements is relatively small. While this is acceptable on the real-world dataset that we used \u2013 visual question answering with up to 100 object proposals per image \u2013 with only a 30% increase in computation time, our method does not scale to the much larger set sizes encountered in domains such as point cloud classification"
    ],
    "key_statements": [
        "Consider a task where each input sample is a set of feature vectors with each feature vector describing an object in an image",
        "We believe that we are the first to show the potential of optimisation for processing sets because \u2013 with an appropriate cost function \u2013 it is able to preserve permutation-invariance",
        "The main drawback of our approach is the cubic time complexity in the set size compared to the quadratic complexity of <a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\">Mena et al (2018</a>), which limits our model to tasks where the number of elements is relatively small. While this is acceptable on the real-world dataset that we used \u2013 visual question answering with up to 100 object proposals per image \u2013 with only a 30% increase in computation time, our method does not scale to the much larger set sizes encountered in domains such as point cloud classification"
    ],
    "summary": [
        "Consider a task where each input sample is a set of feature vectors with each feature vector describing an object in an image.",
        "By feeding the resulting sequence into a traditional model such as an LSTM, we can learn a flexible, permutation-invariant representation of the set while avoiding the bottleneck that a simple reduction operator would introduce.",
        "This shows that our PO module is able to learn good permutation-invariant representations of sets using our approach.",
        "This results in a model that considers both row-wise and column-wise pairwise relations when permuting a set of inputs into a grid structure, and more generally, into a lattice structure.",
        "By basing the cost function on pairwise comparisons, it is able to order elements such that local relations in the output are taken into account.",
        "We believe that this is important for learning from permutations implicitly, because networks such as CNNs and RNNs rely on local ordering more than absolute positioning of elements.",
        "The concurrent work in (<a class=\"ref-link\" id=\"cMurphy_et+al_2019_a\" href=\"#rMurphy_et+al_2019_a\">Murphy et al, 2019</a>) discusses various approximations of averaging the output of a neural network over all possible permutations, with our method falling under their categorisation of a learned canonical input ordering.",
        "The model in <a class=\"ref-link\" id=\"cCruz_et+al_2017_a\" href=\"#rCruz_et+al_2017_a\">Cruz et al (2017</a>) is very similar to <a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\"><a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\">Mena et al (2018</a></a>), including use of a Sinkhorn operator, but they perform significantly more processing on images with a large CNN (AlexNet) beforehand with the main goal of learning good representations for that CNN.",
        "We believe that we are the first to show the potential of optimisation for processing sets because \u2013 with an appropriate cost function \u2013 it is able to preserve permutation-invariance.",
        "We require our model to produce soft assignments so that it is differentiable, since the main goal is not the permutation itself, but processing it further to form a representation of the set being permuted.",
        "We use the alternative cost function described in subsection 2.4 to arrange the tiles into a grid rather than a sequence: this lets our model take relations within rows and columns into account.",
        "The main drawback of our approach is the cubic time complexity in the set size compared to the quadratic complexity of <a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\"><a class=\"ref-link\" id=\"cMena_et+al_2018_a\" href=\"#rMena_et+al_2018_a\">Mena et al (2018</a></a>), which limits our model to tasks where the number of elements is relatively small.",
        "The property of permutation invariance lends itself to greater abstraction by allowing data that has no obvious ordering to be processed, and we took a step towards this by learning an ordering that existing neural networks are able to take advantage of."
    ],
    "headline": "We propose a Permutation-Optimisation module that learns how to permute a set end-to-end",
    "reference_links": [
        {
            "id": "Adams_2011_a",
            "entry": "Ryan Prescott Adams and Richard S. Zemel. Ranking via sinkhorn propagation. arXiv:1106.1925, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1106.1925"
        },
        {
            "id": "Amos_2017_a",
            "entry": "Brandon Amos and J. Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amos%2C%20Brandon%20Kolter%2C%20J.Zico%20Optnet%3A%20Differentiable%20optimization%20as%20a%20layer%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amos%2C%20Brandon%20Kolter%2C%20J.Zico%20Optnet%3A%20Differentiable%20optimization%20as%20a%20layer%20in%20neural%20networks%202017"
        },
        {
            "id": "Anderson_et+al_2018_a",
            "entry": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018"
        },
        {
            "id": "Antol_et+al_2015_a",
            "entry": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20VQA%20Visual%20Question%20Answering%20In%20ICCV%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20VQA%20Visual%20Question%20Answering%20In%20ICCV%202015"
        },
        {
            "id": "Belanger_et+al_2017_a",
            "entry": "David Belanger, Bishan Yang, and Andrew. McCallum. End-to-end learning for structured prediction energy networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belanger%2C%20David%20Yang%2C%20Bishan%20McCallum%2C%20Andrew%20End-to-end%20learning%20for%20structured%20prediction%20energy%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belanger%2C%20David%20Yang%2C%20Bishan%20McCallum%2C%20Andrew%20End-to-end%20learning%20for%20structured%20prediction%20energy%20networks%202017"
        },
        {
            "id": "Burges_et+al_2005_a",
            "entry": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient-descent. In ICML, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burges%2C%20Chris%20Shaked%2C%20Tal%20Renshaw%2C%20Erin%20Lazier%2C%20Ari%20Learning%20to%20rank%20using%20gradient-descent%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burges%2C%20Chris%20Shaked%2C%20Tal%20Renshaw%2C%20Erin%20Lazier%2C%20Ari%20Learning%20to%20rank%20using%20gradient-descent%202005"
        },
        {
            "id": "Charon_2007_a",
            "entry": "Irene Charon and Olivier Hudry. A survey on the linear ordering problem for weighted or unweighted tournaments. 4OR, 5(1):5\u201360, mar 2007. doi: 10.1007/s10288-007-0036-6.",
            "crossref": "https://dx.doi.org/10.1007/s10288-007-0036-6",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10288-007-0036-6"
        },
        {
            "id": "Cruz_et+al_2017_a",
            "entry": "Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. DeepPermNet: Visual Permutation Learning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodrigo%20Santa%20Cruz%20Basura%20Fernando%20Anoop%20Cherian%20and%20Stephen%20Gould%20DeepPermNet%20Visual%20Permutation%20Learning%20In%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodrigo%20Santa%20Cruz%20Basura%20Fernando%20Anoop%20Cherian%20and%20Stephen%20Gould%20DeepPermNet%20Visual%20Permutation%20Learning%20In%20CVPR%202017"
        },
        {
            "id": "Domke_2012_a",
            "entry": "Justin Domke. Generic Methods for Optimization-Based Modeling. In AISTATS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Domke%2C%20Justin%20Generic%20Methods%20for%20Optimization-Based%20Modeling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Domke%2C%20Justin%20Generic%20Methods%20for%20Optimization-Based%20Modeling%202012"
        },
        {
            "id": "Fogel_et+al_2013_a",
            "entry": "Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexandre D' Aspremont. Convex relaxations for permutation problems. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20and%20Alexandre%20D%27%20Aspremont.%20Convex%20relaxations%20for%20permutation%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20and%20Alexandre%20D%27%20Aspremont.%20Convex%20relaxations%20for%20permutation%20problems%202013"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20matter%3A%20Elevating%20the%20role%20of%20image%20understanding%20in%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20matter%3A%20Elevating%20the%20role%20of%20image%20understanding%20in%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv:1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In NIPS, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Zhang%2C%20Byoung-Tak%20Bilinear%20attention%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Zhang%2C%20Byoung-Tak%20Bilinear%20attention%20networks%202018"
        },
        {
            "id": "Kipf_2017_a",
            "entry": "Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "Le_2018_a",
            "entry": "Truc Le and Ye Duan. PointGrid: A Deep Network for 3D Shape Understanding. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Truc%20Duan%2C%20Ye%20PointGrid%3A%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Truc%20Duan%2C%20Ye%20PointGrid%3A%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%202018"
        },
        {
            "id": "Mena_et+al_2018_a",
            "entry": "Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning Latent Permutations with Gumbel-Sinkhorn Networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mena%2C%20Gonzalo%20Belanger%2C%20David%20Linderman%2C%20Scott%20Snoek%2C%20Jasper%20Learning%20Latent%20Permutations%20with%20Gumbel-Sinkhorn%20Networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mena%2C%20Gonzalo%20Belanger%2C%20David%20Linderman%2C%20Scott%20Snoek%2C%20Jasper%20Learning%20Latent%20Permutations%20with%20Gumbel-Sinkhorn%20Networks%202018"
        },
        {
            "id": "Murphy_et+al_2019_a",
            "entry": "Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murphy%2C%20Ryan%20L.%20Srinivasan%2C%20Balasubramaniam%20Rao%2C%20Vinayak%20Ribeiro%2C%20Bruno%20Janossy%20pooling%3A%20Learning%20deep%20permutation-invariant%20functions%20for%20variable-size%20inputs%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murphy%2C%20Ryan%20L.%20Srinivasan%2C%20Balasubramaniam%20Rao%2C%20Vinayak%20Ribeiro%2C%20Bruno%20Janossy%20pooling%3A%20Learning%20deep%20permutation-invariant%20functions%20for%20variable-size%20inputs%202019"
        },
        {
            "id": "Noroozi_2016_a",
            "entry": "Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noroozi%2C%20Mehdi%20Favaro%2C%20Paolo%20Unsupervised%20learning%20of%20visual%20representations%20by%20solving%20jigsaw%20puzzles%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noroozi%2C%20Mehdi%20Favaro%2C%20Paolo%20Unsupervised%20learning%20of%20visual%20representations%20by%20solving%20jigsaw%20puzzles%202016"
        },
        {
            "id": "Pardalos_1991_a",
            "entry": "Panos M. Pardalos and Stephen A. Vavasis. Quadratic programming with one negative eigenvalue is NP-hard. J. Global Optimization, 1(1):15\u201322, 1991. doi: 10.1007/BF00120662.",
            "crossref": "https://dx.doi.org/10.1007/BF00120662",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/BF00120662"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Qi_et+al_2017_a",
            "entry": "Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20Charles%20R.%20Su%2C%20Hao%20Mo%2C%20Kaichun%20Guibas%2C%20Leonidas%20J.%20PointNet%3A%20Deep%20Learning%20on%20Point%20Sets%20for%203D%20Classification%20and%20Segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20Charles%20R.%20Su%2C%20Hao%20Mo%2C%20Kaichun%20Guibas%2C%20Leonidas%20J.%20PointNet%3A%20Deep%20Learning%20on%20Point%20Sets%20for%203D%20Classification%20and%20Segmentation%202017"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In NIPS. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning.%20In%20NIPS%202017"
        },
        {
            "id": "Severyn_2015_a",
            "entry": "Aliaksei Severyn and Alessandro Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Severyn%2C%20Aliaksei%20Moschitti%2C%20Alessandro%20Learning%20to%20rank%20short%20text%20pairs%20with%20convolutional%20deep%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Severyn%2C%20Aliaksei%20Moschitti%2C%20Alessandro%20Learning%20to%20rank%20short%20text%20pairs%20with%20convolutional%20deep%20neural%20networks%202015"
        },
        {
            "id": "Sinkhorn_0000_a",
            "entry": "Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. Ann. Math. Statist., 35(2):876\u2013879, 06 1964. doi: 10.1214/aoms/1177703591.",
            "crossref": "https://dx.doi.org/10.1214/aoms/1177703591",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/aoms/1177703591"
        },
        {
            "id": "Stoyanov_et+al_2011_a",
            "entry": "Veselin Stoyanov, Alexander Ropson, and Jason Eisner. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structureeneric methods for optimization-based modelin. In AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stoyanov%2C%20Veselin%20Ropson%2C%20Alexander%20Eisner%2C%20Jason%20Empirical%20risk%20minimization%20of%20graphical%20model%20parameters%20given%20approximate%20inference%2C%20decoding%2C%20and%20model%20structureeneric%20methods%20for%20optimization-based%20modelin%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stoyanov%2C%20Veselin%20Ropson%2C%20Alexander%20Eisner%2C%20Jason%20Empirical%20risk%20minimization%20of%20graphical%20model%20parameters%20given%20approximate%20inference%2C%20decoding%2C%20and%20model%20structureeneric%20methods%20for%20optimization-based%20modelin%202011"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order Matters: Sequence to sequence for sets. In ICLR, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Bengio%2C%20Samy%20Kudlur%2C%20Manjunath%20Order%20Matters%3A%20Sequence%20to%20sequence%20for%20sets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Bengio%2C%20Samy%20Kudlur%2C%20Manjunath%20Order%20Matters%3A%20Sequence%20to%20sequence%20for%20sets%202015"
        },
        {
            "id": "Vinyals_et+al_2015_b",
            "entry": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS. 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oriol%20Vinyals%20Meire%20Fortunato%20and%20Navdeep%20Jaitly%20Pointer%20networks%20In%20NIPS%202015b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oriol%20Vinyals%20Meire%20Fortunato%20and%20Navdeep%20Jaitly%20Pointer%20networks%20In%20NIPS%202015b"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep Sets. In NIPS. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20Sets%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20Sets%20In%20NIPS%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. Learning to count objects in natural images for visual question answering. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Prugel-Bennett%2C%20Adam%20Learning%20to%20count%20objects%20in%20natural%20images%20for%20visual%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Prugel-Bennett%2C%20Adam%20Learning%20to%20count%20objects%20in%20natural%20images%20for%20visual%20question%20answering%202018"
        },
        {
            "id": "The_2011_a",
            "entry": "The Sinkhorn operator S as defined in Adams & Zemel (2011) is: Tr(X)ij = Xij / Xik",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20Sinkhorn%20operator%20S%20as%20defined%20in%20Adams%20%20Zemel%202011%20is%20TrXij%20%20Xij%20%20Xik",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20Sinkhorn%20operator%20S%20as%20defined%20in%20Adams%20%20Zemel%202011%20is%20TrXij%20%20Xij%20%20Xik"
        },
        {
            "id": "Tr_2018_a",
            "entry": "Tr normalises each row, Tc normalises each column of a square matrix X to sum to one. This formulation is different from the normal Sinkhorn operator by Sinkhorn (1964) by exponentiating all entries first and running for a fixed number of steps L instead of for steps approaching infinity. Mena et al. (2018) include a temperature parameter on the exponentiation, which acts analogously to temperature in the softmax function. In this paper, we fix L to 4.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tr%20normalises%20each%20row%20Tc%20normalises%20each%20column%20of%20a%20square%20matrix%20X%20to%20sum%20to%20one%20This%20formulation%20is%20different%20from%20the%20normal%20Sinkhorn%20operator%20by%20Sinkhorn%201964%20by%20exponentiating%20all%20entries%20first%20and%20running%20for%20a%20fixed%20number%20of%20steps%20L%20instead%20of%20for%20steps%20approaching%20infinity%20Mena%20et%20al%202018%20include%20a%20temperature%20parameter%20on%20the%20exponentiation%20which%20acts%20analogously%20to%20temperature%20in%20the%20softmax%20function%20In%20this%20paper%20we%20fix%20L%20to%204",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tr%20normalises%20each%20row%20Tc%20normalises%20each%20column%20of%20a%20square%20matrix%20X%20to%20sum%20to%20one%20This%20formulation%20is%20different%20from%20the%20normal%20Sinkhorn%20operator%20by%20Sinkhorn%201964%20by%20exponentiating%20all%20entries%20first%20and%20running%20for%20a%20fixed%20number%20of%20steps%20L%20instead%20of%20for%20steps%20approaching%20infinity%20Mena%20et%20al%202018%20include%20a%20temperature%20parameter%20on%20the%20exponentiation%20which%20acts%20analogously%20to%20temperature%20in%20the%20softmax%20function%20In%20this%20paper%20we%20fix%20L%20to%204"
        }
    ]
}
