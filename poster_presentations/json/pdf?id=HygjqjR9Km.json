{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "IMPROVING MMD-GAN TRAINING WITH REPULSIVE LOSS FUNCTION",
        "author": "Wei Wang, University of Melbourne",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HygjqjR9Km"
        },
        "abstract": "Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization. 1"
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "radial basis function",
            "url": "https://en.wikipedia.org/wiki/radial_basis_function"
        },
        {
            "term": "total variation",
            "url": "https://en.wikipedia.org/wiki/total_variation"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial nets",
        "MMD": "maximum mean discrepancy",
        "CNN": "convolutional neural networks",
        "TV": "total variation",
        "RBF": "radial basis function",
        "BN": "batch normalization",
        "TTUR": "two-timescale update rule",
        "IS": "Inception score",
        "FID": "Frechet Inception distance",
        "MS-SSIM": "multi-scale structural similarity"
    },
    "highlights": [
        "Generative adversarial nets (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al (2014</a></a>)) are a branch of generative models that learns to mimic the real data generating process",
        "This study focuses on a loss function called maximum mean discrepancy (MMD), which is well known as the distance metric between two probability distributions and widely applied in kernel two-sample test (<a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\">Gretton et al (2012</a>))",
        "We propose two approaches to stabilize the training of maximum mean discrepancy-Generative adversarial nets: 1) a bounded kernel to avoid the saturation issue caused by an over-confident discriminator; and 2) a generalized power iteration method to estimate the spectral norm of a convolutional kernel, which was used in spectral normalization on the discriminator in all experiments in this study unless specified otherwise.\n4.1",
        "We observed that: 1) maximum mean discrepancy-rep and maximum mean discrepancy-rep-b performed significantly better than maximum mean discrepancy-rbf and maximum mean discrepancy-rbf-b respectively, showing the proposed repulsive loss LrDep (Eq 4) greatly improved over the attractive loss LaDtt (Eq 3); 2) Using a single kernel, maximum mean discrepancy-rbf-b performed better than maximum mean discrepancy-rbf and maximum mean discrepancy-rq which used a linear combination of five kernels, indicating that the kernel saturation may be an issue that slows down maximum mean discrepancy-Generative adversarial nets training; 3) maximum mean discrepancy-rep-b performed comparable or better than maximum mean discrepancy-rep on benchmark datasets where we found the radial basis function-B kernel managed to stabilize maximum mean discrepancy-Generative adversarial nets training using repulsive loss.\n4) maximum mean discrepancy-rep and maximum mean discrepancy-rep-b performed significantly better than the non-saturating and hinge losses, showing the efficacy of the proposed repulsive loss",
        "This study extends the previous work on maximum mean discrepancy-Generative adversarial nets (<a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al (2017a</a>)) with two contributions",
        "We interpreted the optimization of maximum mean discrepancy loss as a combination of attraction and repulsion processes, and proposed a repulsive loss for the discriminator that actively learns the difference among real data"
    ],
    "key_statements": [
        "Generative adversarial nets (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al (2014</a></a>)) are a branch of generative models that learns to mimic the real data generating process",
        "This study focuses on a loss function called maximum mean discrepancy (MMD), which is well known as the distance metric between two probability distributions and widely applied in kernel two-sample test (<a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\">Gretton et al (2012</a>))",
        "We argue that the existing maximum mean discrepancy loss may discourage the learning of fine details in data, as the discriminator attempts to minimize the within-group variance of its outputs for the real data",
        "This study focuses on image generation tasks using convolutional neural networks (CNN) for both G and D",
        "In Appendix A, we demonstrate that the maximum mean discrepancy-Generative adversarial nets trained by gradient descent is locally exponentially stable near equilibrium.\n3 REPULSIVE LOSS FUNCTION",
        "In Appendix A, we demonstrate that Generative adversarial nets trained using gradient descent and the repulsive maximum mean discrepancy loss (LDrep, LmGmd) is locally exponentially stable near equilibrium",
        "We propose two approaches to stabilize the training of maximum mean discrepancy-Generative adversarial nets: 1) a bounded kernel to avoid the saturation issue caused by an over-confident discriminator; and 2) a generalized power iteration method to estimate the spectral norm of a convolutional kernel, which was used in spectral normalization on the discriminator in all experiments in this study unless specified otherwise.\n4.1",
        "Inspired by the hinge loss, we propose a bounded radial basis function (RBF-B) kernel for the discriminator",
        "radial basis function-B kernel is among many methods to address the saturation issue and stabilize maximum mean discrepancy-Generative adversarial nets training",
        "We propose a generalized power iteration method to directly estimate the spectral norm of a convolution kernel and applied spectral normalization to the discriminator in all experiments",
        "We observed that: 1) maximum mean discrepancy-rep and maximum mean discrepancy-rep-b performed significantly better than maximum mean discrepancy-rbf and maximum mean discrepancy-rbf-b respectively, showing the proposed repulsive loss LrDep (Eq 4) greatly improved over the attractive loss LaDtt (Eq 3); 2) Using a single kernel, maximum mean discrepancy-rbf-b performed better than maximum mean discrepancy-rbf and maximum mean discrepancy-rq which used a linear combination of five kernels, indicating that the kernel saturation may be an issue that slows down maximum mean discrepancy-Generative adversarial nets training; 3) maximum mean discrepancy-rep-b performed comparable or better than maximum mean discrepancy-rep on benchmark datasets where we found the radial basis function-B kernel managed to stabilize maximum mean discrepancy-Generative adversarial nets training using repulsive loss.\n4) maximum mean discrepancy-rep and maximum mean discrepancy-rep-b performed significantly better than the non-saturating and hinge losses, showing the efficacy of the proposed repulsive loss",
        "We observed that: 1) the model performed well using repulsive loss (i.e., \u03bb \u2265 0), with \u03bb = 0.5, 1 slightly better than \u03bb = \u22120.5, 0, 2; 2) the maximum mean discrepancy-rbf model can be significantly improved by increasing \u03bb from \u22121 to \u22120.5, which reduces the attraction of discriminator on real sample scores; 3) larger \u03bb may lead to more diverged models, possibly because the discriminator focuses more on expanding the real sample scores over adversarial learning; note when \u03bb 1, the model would learn to expand all real sample scores and pull the generated sample scores to real samples\u2019, which is a divergent process; 4) the radial basis function-B kernel managed to stabilize maximum mean discrepancy-rep for most diverged cases but may occasionally cause the Frechet Inception distance score to rise up",
        "In Appendix C.3, we showed the proposed generalized power iteration (Section 4.2) imposes a stronger Lipschitz constraint than the method in Miyato et al (2018), and benefited maximum mean discrepancy-Generative adversarial nets training using the repulsive loss",
        "This study extends the previous work on maximum mean discrepancy-Generative adversarial nets (<a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al (2017a</a>)) with two contributions",
        "We interpreted the optimization of maximum mean discrepancy loss as a combination of attraction and repulsion processes, and proposed a repulsive loss for the discriminator that actively learns the difference among real data",
        "We observed that the repulsive loss may result in unstable training, due to factors including initialization (Appendix A.2), learning rate (Fig. 3b) and Lipschitz constraints on the discriminator (Appendix C.3)"
    ],
    "summary": [
        "Generative adversarial nets (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al (2014</a></a>)) are a branch of generative models that learns to mimic the real data generating process.",
        "We interpret the training of MMD-GAN as a combination of attraction and repulsion processes, and propose a novel repulsive loss function for the discriminator by rearranging the components in LDatt.",
        "We propose two approaches to stabilize the training of MMD-GAN: 1) a bounded kernel to avoid the saturation issue caused by an over-confident discriminator; and 2) a generalized power iteration method to estimate the spectral norm of a convolutional kernel, which was used in spectral normalization on the discriminator in all experiments in this study unless specified otherwise.",
        "RBF-B kernel is among many methods to address the saturation issue and stabilize MMD-GAN training.",
        "We empirically evaluate the proposed 1) repulsive loss LDrep (Eq 4) on unsupervised training of GAN for image generation tasks; and 2) RBF-B kernel to stabilize MMD-GAN",
        "We observed that: 1) MMD-rep and MMD-rep-b performed significantly better than MMD-rbf and MMD-rbf-b respectively, showing the proposed repulsive loss LrDep (Eq 4) greatly improved over the attractive loss LaDtt (Eq 3); 2) Using a single kernel, MMD-rbf-b performed better than MMD-rbf and MMD-rq which used a linear combination of five kernels, indicating that the kernel saturation may be an issue that slows down MMD-GAN training; 3) MMD-rep-b performed comparable or better than MMD-rep on benchmark datasets where we found the RBF-B kernel managed to stabilize MMD-GAN training using repulsive loss.",
        "We observed that: 1) the model performed well using repulsive loss (i.e., \u03bb \u2265 0), with \u03bb = 0.5, 1 slightly better than \u03bb = \u22120.5, 0, 2; 2) the MMD-rbf model can be significantly improved by increasing \u03bb from \u22121 to \u22120.5, which reduces the attraction of discriminator on real sample scores; 3) larger \u03bb may lead to more diverged models, possibly because the discriminator focuses more on expanding the real sample scores over adversarial learning; note when \u03bb 1, the model would learn to expand all real sample scores and pull the generated sample scores to real samples\u2019, which is a divergent process; 4) the RBF-B kernel managed to stabilize MMD-rep for most diverged cases but may occasionally cause the FID score to rise up.",
        "In Appendix C.3, we showed the proposed generalized power iteration (Section 4.2) imposes a stronger Lipschitz constraint than the method in Miyato et al (2018), and benefited MMD-GAN training using the repulsive loss.",
        "We interpreted the optimization of MMD loss as a combination of attraction and repulsion processes, and proposed a repulsive loss for the discriminator that actively learns the difference among real data."
    ],
    "headline": "We argue that the existing maximum mean discrepancy loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data",
    "reference_links": [
        {
            "id": "Arbel_et+al_2018_a",
            "entry": "Michael Arbel, Dougal J. Sutherland, Miko\u0142aj Binkowski, and Arthur Gretton. On gradient regularizers for MMD GANs. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arbel%2C%20Michael%20Sutherland%2C%20Dougal%20J.%20Binkowski%2C%20Miko%C5%82aj%20Gretton%2C%20Arthur%20On%20gradient%20regularizers%20for%20MMD%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arbel%2C%20Michael%20Sutherland%2C%20Dougal%20J.%20Binkowski%2C%20Miko%C5%82aj%20Gretton%2C%20Arthur%20On%20gradient%20regularizers%20for%20MMD%20GANs%202018"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In ICML, volume 70 of PMLR, pp. 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Binkowski_et+al_2018_a",
            "entry": "Miko\u0142aj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miko%C5%82aj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miko%C5%82aj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, volume 15 of PMLR, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Dumoulin_2016_a",
            "entry": "Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning, 2016. arxiv:1603.07285.",
            "arxiv_url": "https://arxiv.org/pdf/1603.07285"
        },
        {
            "id": "Dziugaite_et+al_2015_a",
            "entry": "Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, pp. 258\u2013267, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015"
        },
        {
            "id": "Genton_2002_a",
            "entry": "Marc G. Genton. Classes of kernels for machine learning: A statistics perspective. J. Mach. Learn. Res., 2:299\u2013312, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genton%2C%20Marc%20G.%20Classes%20of%20kernels%20for%20machine%20learning%3A%20A%20statistics%20perspective%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genton%2C%20Marc%20G.%20Classes%20of%20kernels%20for%20machine%20learning%3A%20A%20statistics%20perspective%202002"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. J. Mach. Learn. Res., 13:723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "Grinblat_et+al_2017_a",
            "entry": "Guillermo L. Grinblat, Lucas C. Uzal, and Pablo M. Granitto. Class-splitting generative adversarial networks, 2017. arXiv:1709.07359.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07359"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. In NIPS, pp. 5767\u20135777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NIPS, pp. 6626\u20136637, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Xun Huang, Yixuan Li, Omid Poursaeed, John E. Hopcroft, and Serge J. Belongie. Stacked generative adversarial networks. CVPR, pp. 1866\u20131875, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Li%2C%20Yixuan%20Poursaeed%2C%20Omid%20Hopcroft%2C%20John%20E.%20Stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Li%2C%20Yixuan%20Poursaeed%2C%20Omid%20Hopcroft%2C%20John%20E.%20Stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lai_et+al_2017_a",
            "entry": "Wei-Sheng Lai, Jia-Bin Huang, and Ming-Hsuan Yang. Semi-supervised learning for optical flow with generative adversarial networks. In NIPS, pp. 354\u2013364, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Wei-Sheng%20Huang%2C%20Jia-Bin%20Yang%2C%20Ming-Hsuan%20Semi-supervised%20learning%20for%20optical%20flow%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Wei-Sheng%20Huang%2C%20Jia-Bin%20Yang%2C%20Ming-Hsuan%20Semi-supervised%20learning%20for%20optical%20flow%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN: Towards deeper understanding of moment matching network. In NIPS, pp. 2203\u20132213, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20MMD%20GAN%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20MMD%20GAN%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "Li_et+al_2017_b",
            "entry": "Jiwei Li, Will Monroe, Tianlin Shi, Sebastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. In EMNLP, pp. 2157\u20132169, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jiwei%20Monroe%2C%20Will%20Shi%2C%20Tianlin%20Jean%2C%20Sebastien%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jiwei%20Monroe%2C%20Will%20Shi%2C%20Tianlin%20Jean%2C%20Sebastien%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In ICML, volume 37, pp. 1718\u20131727, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In NIPS, pp. 5545\u20135553, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, pp. 3730\u20133738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cGANs%20with%20projection%20discriminator%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cGANs%20with%20projection%20discriminator%202018"
        },
        {
            "id": "Miyato_et+al_2018_b",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent GAN optimization is locally stable. In NIPS, pp. 5585\u20135595, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "Nguyen_et+al_2009_a",
            "entry": "XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. On surrogate loss functions and f-divergences. Ann. Stat., 37(2):876\u2013904, 04 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20XuanLong%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20On%20surrogate%20loss%20functions%20and%20f-divergences%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20XuanLong%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20On%20surrogate%20loss%20functions%20and%20f-divergences%202009"
        },
        {
            "id": "Odena_et+al_2017_a",
            "entry": "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In ICML, pp. 2642\u20132651, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Sedghi_et+al_2019_a",
            "entry": "Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sedghi%2C%20Hanie%20Gupta%2C%20Vineet%20Long%2C%20Philip%20M.%20The%20singular%20values%20of%20convolutional%20layers%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sedghi%2C%20Hanie%20Gupta%2C%20Vineet%20Long%2C%20Philip%20M.%20The%20singular%20values%20of%20convolutional%20layers%202019"
        },
        {
            "id": "S_et+al_2017_a",
            "entry": "Casper K. S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised map inference for image super-resolution. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%B8nderby%2C%20Casper%20K.%20Caballero%2C%20Jose%20Theis%2C%20Lucas%20Shi%2C%20Wenzhe%20Amortised%20map%20inference%20for%20image%20super-resolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%B8nderby%2C%20Casper%20K.%20Caballero%2C%20Jose%20Theis%2C%20Lucas%20Shi%2C%20Wenzhe%20Amortised%20map%20inference%20for%20image%20super-resolution%202017"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, pp. 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jonathon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jonathon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Tran_et+al_2017_a",
            "entry": "Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free variational inference, 2017. arXiv:1702.08896.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08896"
        },
        {
            "id": "Tsuzuku_et+al_2018_a",
            "entry": "Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsuzuku%2C%20Yusuke%20Sato%2C%20Issei%20Sugiyama%2C%20Masashi%20Lipschitz-margin%20training%3A%20scalable%20certification%20of%20perturbation%20invariance%20for%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsuzuku%2C%20Yusuke%20Sato%2C%20Issei%20Sugiyama%2C%20Masashi%20Lipschitz-margin%20training%3A%20scalable%20certification%20of%20perturbation%20invariance%20for%20deep%20neural%20networks%202018"
        },
        {
            "id": "Unterthiner_et+al_2018_a",
            "entry": "Thomas Unterthiner, Bernhard Nessler, Calvin Seward, Gunter Klambauer, Martin Heusel, Hubert Ramsauer, and Sepp Hochreiter. Coulomb GANs: Provably optimal Nash equilibria via potential fields. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Seward%2C%20Calvin%20Klambauer%2C%20Gunter%20Coulomb%20GANs%3A%20Provably%20optimal%20Nash%20equilibria%20via%20potential%20fields%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Seward%2C%20Calvin%20Klambauer%2C%20Gunter%20Coulomb%20GANs%3A%20Provably%20optimal%20Nash%20equilibria%20via%20potential%20fields%202018"
        },
        {
            "id": "Van_2014_a",
            "entry": "Laurens van der Maaten. Accelerating t-SNE using tree-based algorithms. J. Mach. Learn. Res., 15: 3221\u20133245, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Accelerating%20t-SNE%20using%20tree-based%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Accelerating%20t-SNE%20using%20tree-based%20algorithms%202014"
        },
        {
            "id": "Virmaux_2018_a",
            "entry": "Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and efficient estimation. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Virmaux%2C%20Aladin%20Scaman%2C%20Kevin%20Lipschitz%20regularity%20of%20deep%20neural%20networks%3A%20analysis%20and%20efficient%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Virmaux%2C%20Aladin%20Scaman%2C%20Kevin%20Lipschitz%20regularity%20of%20deep%20neural%20networks%3A%20analysis%20and%20efficient%20estimation%202018"
        },
        {
            "id": "Wang_et+al_2003_a",
            "entry": "Zhou Wang, Eero. P. Simoncelli, and Alan. C. Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems & Computers, volume 2, pp. 1398\u20131402, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhou%20Simoncelli%2C%20Eero%20P.%20Bovik%2C%20Alan%20C.%20Multiscale%20structural%20similarity%20for%20image%20quality%20assessment%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhou%20Simoncelli%2C%20Eero%20P.%20Bovik%2C%20Alan%20C.%20Multiscale%20structural%20similarity%20for%20image%20quality%20assessment%202003"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. 2015. arXiv:1506.03365.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks, 2018. arXiv:1805.08318.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu. Activation maximization generative adversarial nets. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Zhiming%20Cai%2C%20Han%20Rong%2C%20Shu%20Song%2C%20Yuxuan%20Wang%2C%20and%20Yong%20Yu.%20Activation%20maximization%20generative%20adversarial%20nets%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Zhiming%20Cai%2C%20Han%20Rong%2C%20Shu%20Song%2C%20Yuxuan%20Wang%2C%20and%20Yong%20Yu.%20Activation%20maximization%20generative%20adversarial%20nets%202018-06"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan. Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pp. 2242\u20132251, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        }
    ]
}
