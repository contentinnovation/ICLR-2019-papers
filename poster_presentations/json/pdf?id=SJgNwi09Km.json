{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING LATENT SUPERSTRUCTURES IN VARIATIONAL AUTOENCODERS FOR DEEP MULTIDIMENSIONAL CLUSTERING",
        "author": "Xiaopeng Li, Zhourong Chen, Leonard K. M. Poon, and Nevin L. Zhang, 1 Department of Computer Science and Engineering The Hong Kong University of Science and Technology 2 Department of Mathematics & Information Technology The Education University of Hong Kong {xlibo,zchenbb,lzhang}@cse.ust.hk, kmpoon@eduhk.hk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJgNwi09Km"
        },
        "abstract": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning methods for clustering produce only one partition of data, LTVAE produces multiple partitions of data, each being given by one super latent variable. This is desirable because high dimensional data usually have many different natural facets and can be meaningfully partitioned in multiple ways."
    },
    "keywords": [
        {
            "term": "node deletion",
            "url": "https://en.wikipedia.org/wiki/node_deletion"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "spectral clustering",
            "url": "https://en.wikipedia.org/wiki/spectral_clustering"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "Gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/Gaussian_mixture_model"
        },
        {
            "term": "cluster analysis",
            "url": "https://en.wikipedia.org/wiki/cluster_analysis"
        },
        {
            "term": "restricted Boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/restricted_Boltzmann_machine"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        }
    ],
    "abbreviations": {
        "LTVAE": "latent tree variational autoencoder",
        "RBM": "restricted Boltzmann machine",
        "VAE": "variational autoencoders",
        "HDP": "Hierarchical Dirichlet Process",
        "DCN": "Deep clustering network",
        "GMM": "Gaussian mixture model",
        "ELBO": "evidence lower bound",
        "SGD": "stochastic gradient descent",
        "NI": "node introduction",
        "ND": "node deletion",
        "SI": "state introduction",
        "SD": "state deletion",
        "NR": "node relocation",
        "HHAR": "Heterogeneity Human Activity Recognition",
        "IWAE": "importance weighted autoencoders",
        "MADE": "masked autoencoder distribution estimator"
    },
    "highlights": [
        "Clustering is a fundamental task in unsupervised machine learning, and it is central to many datadriven application domains",
        "We evaluate the proposed latent tree variational autoencoder model on two image datasets and two other datasets, and compare it against other deep learning based clustering algorithms, including two-stage methods, AE+Gaussian mixture model and variational autoencoders+Gaussian mixture model, which first learn AE/variational autoencoders (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a>) models construct a Gaussian mixture model on top of them, and joint learning methods, DEC (<a class=\"ref-link\" id=\"cXie_et+al_2016_a\" href=\"#rXie_et+al_2016_a\">Xie et al, 2016</a>) and Deep clustering network (<a class=\"ref-link\" id=\"cYang_et+al_2017_a\" href=\"#rYang_et+al_2017_a\">Yang et al, 2017</a>)",
        "By using the marginal loglikelihood defined by the latent tree model as the prior, latent tree variational autoencoder better fits the data than conventional variational autoencoders and importance weighted autoencoders (IWAE)",
        "We propose an unsupervised learning method, latent tree variational autoencoder (LTVAE), which simultaneously performs representation learning and multidimensional clustering",
        "Different from previous deep learning based clustering methods, latent tree variational autoencoder learns latent embeddings from data and discovers multi-facet clustering structure based on subsets of latent features rather than one partition over data",
        "Experiments show that the proposed method achieves state-of-the-art clustering performance and reals reasonable multifacet structures of the data"
    ],
    "key_statements": [
        "Clustering is a fundamental task in unsupervised machine learning, and it is central to many datadriven application domains",
        "Many clustering methods have been proposed in the literature (<a class=\"ref-link\" id=\"cAggarwal_2013_a\" href=\"#rAggarwal_2013_a\">Aggarwal & Reddy, 2013</a>), such as k-means (MacQueen et al, 1967), Gaussian mixture models (<a class=\"ref-link\" id=\"cChristopher_2016_a\" href=\"#rChristopher_2016_a\">Christopher, 2016</a>) and spectral clustering (Von Luxburg, 2007)",
        "To resolve the above issues, we propose an unsupervised learning method, latent tree variational autoencoder (LTVAE) to learn latent superstructures in variational autoencoders, and simultaneously perform representation learning and structure learning",
        "We propose efficient learning algorithms for latent tree variational autoencoder with gradient descent and Stepwise EM through message passing",
        "Variational deep embedding (<a class=\"ref-link\" id=\"cJiang_et+al_2017_a\" href=\"#rJiang_et+al_2017_a\">Jiang et al, 2017</a>) is a generative method that models the data generative process using a Gaussian mixture model combined with a variational autoencoders, and performs joint learning of representations and clustering",
        "Different from it, our work focuses on multifacets of clustering, for example, the model could make one partition based on identity of subjects, while another partition based on pose.\n3 THE PROPOSED METHOD",
        "In order to learn the latent structure of z, for example multidimensional cluster structure, we introduce a set of latent variables Y1, ..., Yl on top of z",
        "In order to jointly learn the parameters of the latent tree \u0398, we propose Stepwise EM algorithm based on mini-batch of data",
        "We evaluate the proposed latent tree variational autoencoder model on two image datasets and two other datasets, and compare it against other deep learning based clustering algorithms, including two-stage methods, AE+Gaussian mixture model and variational autoencoders+Gaussian mixture model, which first learn AE/variational autoencoders (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a>) models construct a Gaussian mixture model on top of them, and joint learning methods, DEC (<a class=\"ref-link\" id=\"cXie_et+al_2016_a\" href=\"#rXie_et+al_2016_a\">Xie et al, 2016</a>) and Deep clustering network (<a class=\"ref-link\" id=\"cYang_et+al_2017_a\" href=\"#rYang_et+al_2017_a\">Yang et al, 2017</a>)",
        "By using the marginal loglikelihood defined by the latent tree model as the prior, latent tree variational autoencoder better fits the data than conventional variational autoencoders and importance weighted autoencoders (IWAE)",
        "We propose an unsupervised learning method, latent tree variational autoencoder (LTVAE), which simultaneously performs representation learning and multidimensional clustering",
        "Different from previous deep learning based clustering methods, latent tree variational autoencoder learns latent embeddings from data and discovers multi-facet clustering structure based on subsets of latent features rather than one partition over data",
        "Experiments show that the proposed method achieves state-of-the-art clustering performance and reals reasonable multifacet structures of the data"
    ],
    "summary": [
        "Clustering is a fundamental task in unsupervised machine learning, and it is central to many datadriven application domains.",
        "Variational deep embedding (<a class=\"ref-link\" id=\"cJiang_et+al_2017_a\" href=\"#rJiang_et+al_2017_a\">Jiang et al, 2017</a>) is a generative method that models the data generative process using a Gaussian mixture model combined with a VAE, and performs joint learning of representations and clustering.",
        "We present the proposed latent tree variational autoencoder and the learning algorithms for joint representation learning and structure learning for multidimensional clustering.",
        "We propose efficient learning algorithms for LTVAE through gradient descent and stepwise EM with message passing.",
        "In order to learn the model, it is important to efficiently compute the gradient of the marginal loglikelihood log pS (z; \u0398) from the latent tree model, the third term in Eq 4.",
        "In order to jointly learn the parameters of the latent tree \u0398, we propose Stepwise EM algorithm based on mini-batch of data.",
        "Each iteration of update of LTVAE is composed of one iteration of gradient descent update for the neural network parameters and one iteration of Stepwise EM update for the latent tree model parameters with a mini-batch of data.",
        "Starting from a pretrained model, we iteratively improve the structure and parameters of latent tree model while learning the representations of data through neural network in a greedy manner.",
        "By using the marginal loglikelihood defined by the latent tree model as the prior, LTVAE better fits the data than conventional VAE and importance weighted autoencoders (IWAE)",
        "While alternative quantitative criteria have been proposed (<a class=\"ref-link\" id=\"cBounliphone_et+al_2016_a\" href=\"#rBounliphone_et+al_2016_a\">Bounliphone et al, 2016</a>; <a class=\"ref-link\" id=\"cIm_et+al_2016_a\" href=\"#rIm_et+al_2016_a\">Im et al, 2016</a>; <a class=\"ref-link\" id=\"cSalimans_et+al_2016_a\" href=\"#rSalimans_et+al_2016_a\">Salimans et al, 2016</a>) for generative models, log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a>; <a class=\"ref-link\" id=\"cBurda_et+al_2016_a\" href=\"#rBurda_et+al_2016_a\">Burda et al, 2016</a>; <a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\">Wu et al, 2017</a>; <a class=\"ref-link\" id=\"cGoyal_et+al_2017_a\" href=\"#rGoyal_et+al_2017_a\">Goyal et al, 2017</a>).",
        "Using a faithful inference network structure as in (<a class=\"ref-link\" id=\"cWebb_et+al_2018_a\" href=\"#rWebb_et+al_2018_a\">Webb et al, 2018</a>) to incorporate the dependencies among latent variables in the posterior, for example one parameterized with masked autoencoder distribution estimator (MADE) model (<a class=\"ref-link\" id=\"cGermain_et+al_2015_a\" href=\"#rGermain_et+al_2015_a\">Germain et al, 2015</a>), could have a significant improvement in learning.",
        "We propose an unsupervised learning method, latent tree variational autoencoder (LTVAE), which simultaneously performs representation learning and multidimensional clustering.",
        "Different from previous deep learning based clustering methods, LTVAE learns latent embeddings from data and discovers multi-facet clustering structure based on subsets of latent features rather than one partition over data.",
        "Experiments show that the proposed method achieves state-of-the-art clustering performance and reals reasonable multifacet structures of the data."
    ],
    "headline": "We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features",
    "reference_links": [
        {
            "id": "Aggarwal_2013_a",
            "entry": "Charu C Aggarwal and Chandan K Reddy. Data clustering: algorithms and applications. CRC press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aggarwal%2C%20Charu%20C.%20Reddy%2C%20Chandan%20K.%20Data%20clustering%3A%20algorithms%20and%20applications%202013"
        },
        {
            "id": "Blei_2006_a",
            "entry": "David M Blei, Michael I Jordan, et al. Variational inference for dirichlet process mixtures. Bayesian analysis, 1(1):121\u2013144, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Jordan%2C%20Michael%20I.%20Variational%20inference%20for%20dirichlet%20process%20mixtures%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Jordan%2C%20Michael%20I.%20Variational%20inference%20for%20dirichlet%20process%20mixtures%202006"
        },
        {
            "id": "Bounliphone_et+al_2016_a",
            "entry": "Wacha Bounliphone, Eugene Belilovsky, Matthew B Blaschko, Ioannis Antonoglou, and Arthur Gretton. A test of relative similarity for model selection in generative models. In Proceedings of the International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bounliphone%2C%20Wacha%20Belilovsky%2C%20Eugene%20Blaschko%2C%20Matthew%20B.%20Antonoglou%2C%20Ioannis%20A%20test%20of%20relative%20similarity%20for%20model%20selection%20in%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bounliphone%2C%20Wacha%20Belilovsky%2C%20Eugene%20Blaschko%2C%20Matthew%20B.%20Antonoglou%2C%20Ioannis%20A%20test%20of%20relative%20similarity%20for%20model%20selection%20in%20generative%20models%202016"
        },
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Proceedings of the International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016"
        },
        {
            "id": "Chen_et+al_2012_a",
            "entry": "Tao Chen, Nevin L Zhang, Tengfei Liu, Kin Man Poon, and Yi Wang. Model-based multidimensional clustering of categorical data. Artificial Intelligence, 176(1):2246\u20132269, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tao%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20Poon%2C%20Kin%20Man%20Model-based%20multidimensional%20clustering%20of%20categorical%20data%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tao%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20Poon%2C%20Kin%20Man%20Model-based%20multidimensional%20clustering%20of%20categorical%20data%202012"
        },
        {
            "id": "Christopher_2016_a",
            "entry": "M Bishop Christopher. Pattern Recognition and Machine Learning. Springer-Verlag New York, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%2C%20M.Bishop%20Pattern%20Recognition%20and%20Machine%20Learning%202016"
        },
        {
            "id": "Cremer_et+al_2018_a",
            "entry": "Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cremer%2C%20Chris%20Li%2C%20Xuechen%20Duvenaud%2C%20David%20Inference%20suboptimality%20in%20variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cremer%2C%20Chris%20Li%2C%20Xuechen%20Duvenaud%2C%20David%20Inference%20suboptimality%20in%20variational%20autoencoders%202018"
        },
        {
            "id": "Dilokthanakul_et+al_2016_a",
            "entry": "Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02648"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pp. 881\u2013889, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Made%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Made%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric Xing. Nonparametric variational auto-encoders for hierarchical representation learning. In Proceedings of the IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Prasoon%20Hu%2C%20Zhiting%20Liang%2C%20Xiaodan%20Chenyu%20Wang%2C%20and%20Eric%20Xing.%20Nonparametric%20variational%20auto-encoders%20for%20hierarchical%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Prasoon%20Hu%2C%20Zhiting%20Liang%2C%20Xiaodan%20Chenyu%20Wang%2C%20and%20Eric%20Xing.%20Nonparametric%20variational%20auto-encoders%20for%20hierarchical%20representation%20learning%202017"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with local structure preservation. In International Joint Conference on Artificial Intelligence (IJCAI17), pp. 1753\u20131759, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Xifeng%20Gao%2C%20Long%20Liu%2C%20Xinwang%20Yin%2C%20Jianping%20Improved%20deep%20embedded%20clustering%20with%20local%20structure%20preservation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Xifeng%20Gao%2C%20Long%20Liu%2C%20Xinwang%20Yin%2C%20Jianping%20Improved%20deep%20embedded%20clustering%20with%20local%20structure%20preservation%202017"
        },
        {
            "id": "Hinton_2006_a",
            "entry": "Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "Hinton_et+al_2006_b",
            "entry": "Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "Im_et+al_2016_a",
            "entry": "Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05110"
        },
        {
            "id": "Jiang_et+al_2017_a",
            "entry": "Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. In International Joint Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Zhuxi%20Zheng%2C%20Yin%20Tan%2C%20Huachun%20Tang%2C%20Bangsheng%20Variational%20deep%20embedding%3A%20An%20unsupervised%20and%20generative%20approach%20to%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Zhuxi%20Zheng%2C%20Yin%20Tan%2C%20Huachun%20Tang%2C%20Bangsheng%20Variational%20deep%20embedding%3A%20An%20unsupervised%20and%20generative%20approach%20to%20clustering%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "Koller_2009_a",
            "entry": "Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koller%2C%20Daphne%20Friedman%2C%20Nir%20Probabilistic%20graphical%20models%3A%20principles%20and%20techniques%202009"
        },
        {
            "id": "Macqueen_0000_a",
            "entry": "James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pp. 281\u2013297.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacQueen%2C%20James%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacQueen%2C%20James%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations"
        },
        {
            "id": "Oakland_1967_a",
            "entry": "Oakland, CA, USA, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oakland%20CA%20USA%201967"
        },
        {
            "id": "Mourad_et+al_2013_a",
            "entry": "Raphael Mourad, Christine Sinoquet, Nevin L Zhang, Tengfei Liu, and Philippe Leray. A survey on latent tree models and applications. Journal of Artificial Intelligence Research, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mourad%2C%20Raphael%20Sinoquet%2C%20Christine%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20and%20Philippe%20Leray.%20A%20survey%20on%20latent%20tree%20models%20and%20applications%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mourad%2C%20Raphael%20Sinoquet%2C%20Christine%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20and%20Philippe%20Leray.%20A%20survey%20on%20latent%20tree%20models%20and%20applications%202013"
        },
        {
            "id": "Pearl_2014_a",
            "entry": "Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20Judea%20Probabilistic%20reasoning%20in%20intelligent%20systems%3A%20networks%20of%20plausible%20inference%202014"
        },
        {
            "id": "Poon_et+al_2010_a",
            "entry": "Leonard Poon, Nevin L Zhang, Tao Chen, and Yi Wang. Variable selection in model-based clustering: To do or to facilitate. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 887\u2013894, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poon%2C%20Leonard%20Zhang%2C%20Nevin%20L.%20Chen%2C%20Tao%20Wang%2C%20Yi%20Variable%20selection%20in%20model-based%20clustering%3A%20To%20do%20or%20to%20facilitate%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poon%2C%20Leonard%20Zhang%2C%20Nevin%20L.%20Chen%2C%20Tao%20Wang%2C%20Yi%20Variable%20selection%20in%20model-based%20clustering%3A%20To%20do%20or%20to%20facilitate%202010"
        },
        {
            "id": "Poon_et+al_2013_a",
            "entry": "Leonard KM Poon, Nevin L Zhang, Tengfei Liu, and April H Liu. Model-based clustering of high-dimensional data: Variable selection versus facet determination. International Journal of Approximate Reasoning, 54(1):196\u2013215, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poon%2C%20Leonard%20K.M.%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20Liu%2C%20April%20H.%20Model-based%20clustering%20of%20high-dimensional%20data%3A%20Variable%20selection%20versus%20facet%20determination%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poon%2C%20Leonard%20K.M.%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20Liu%2C%20April%20H.%20Model-based%20clustering%20of%20high-dimensional%20data%3A%20Variable%20selection%20versus%20facet%20determination%202013"
        },
        {
            "id": "Rainforth_et+al_2018_a",
            "entry": "Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood, and Yee Whye Teh. Tighter variational bounds are not necessarily better. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rainforth%2C%20Tom%20Kosiorek%2C%20Adam%20R.%20Le%2C%20Tuan%20Anh%20Maddison%2C%20Chris%20J.%20Tighter%20variational%20bounds%20are%20not%20necessarily%20better%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rainforth%2C%20Tom%20Kosiorek%2C%20Adam%20R.%20Le%2C%20Tuan%20Anh%20Maddison%2C%20Chris%20J.%20Tighter%20variational%20bounds%20are%20not%20necessarily%20better%202018"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Schwarz_1978_a",
            "entry": "Gideon Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):461\u2013464, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarz%2C%20Gideon%20Estimating%20the%20dimension%20of%20a%20model%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarz%2C%20Gideon%20Estimating%20the%20dimension%20of%20a%20model%201978"
        },
        {
            "id": "Teh_et+al_2006_a",
            "entry": "Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(1):1566\u20131581, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Yee%20Whye%20Jordan%2C%20Michael%20I.%20Beal%2C%20Matthew%20J.%20Blei%2C%20David%20M.%20Hierarchical%20dirichlet%20processes%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Yee%20Whye%20Jordan%2C%20Michael%20I.%20Beal%2C%20Matthew%20J.%20Blei%2C%20David%20M.%20Hierarchical%20dirichlet%20processes%202006"
        },
        {
            "id": "Vincent_et+al_2008_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "Vincent_et+al_2010_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "Luxburg_2007_a",
            "entry": "Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395\u2013416, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luxburg%2C%20Ulrike%20Von%20A%20tutorial%20on%20spectral%20clustering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luxburg%2C%20Ulrike%20Von%20A%20tutorial%20on%20spectral%20clustering%202007"
        },
        {
            "id": "Webb_et+al_2018_a",
            "entry": "Stefan Webb, Adam Golinski, Robert Zinkov, N Siddharth, Tom Rainforth, Yee Whye Teh, and Frank Wood. Faithful inversion of generative models for effective amortized inference. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Webb%2C%20Stefan%20Golinski%2C%20Adam%20Robert%20Zinkov%2C%20N.Siddharth%20Rainforth%2C%20Tom%20Faithful%20inversion%20of%20generative%20models%20for%20effective%20amortized%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Webb%2C%20Stefan%20Golinski%2C%20Adam%20Robert%20Zinkov%2C%20N.Siddharth%20Rainforth%2C%20Tom%20Faithful%20inversion%20of%20generative%20models%20for%20effective%20amortized%20inference%202018"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. In Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        },
        {
            "id": "Xie_et+al_2016_a",
            "entry": "Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In International Conference on Machine Learning, pp. 478\u2013487, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Junyuan%20Girshick%2C%20Ross%20Farhadi%2C%20Ali%20Unsupervised%20deep%20embedding%20for%20clustering%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Junyuan%20Girshick%2C%20Ross%20Farhadi%2C%20Ali%20Unsupervised%20deep%20embedding%20for%20clustering%20analysis%202016"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Bo%20Fu%2C%20Xiao%20Sidiropoulos%2C%20Nicholas%20D.%20Hong%2C%20Mingyi%20Towards%20k-means-friendly%20spaces%3A%20Simultaneous%20deep%20learning%20and%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Bo%20Fu%2C%20Xiao%20Sidiropoulos%2C%20Nicholas%20D.%20Hong%2C%20Mingyi%20Towards%20k-means-friendly%20spaces%3A%20Simultaneous%20deep%20learning%20and%20clustering%202017"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5147\u20135156, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Jianwei%20Parikh%2C%20Devi%20Batra%2C%20Dhruv%20Joint%20unsupervised%20learning%20of%20deep%20representations%20and%20image%20clusters%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Jianwei%20Parikh%2C%20Devi%20Batra%2C%20Dhruv%20Joint%20unsupervised%20learning%20of%20deep%20representations%20and%20image%20clusters%202016"
        },
        {
            "id": "Yang_et+al_2010_a",
            "entry": "Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering using local discriminant models and global integration. IEEE Transactions on Image Processing, 19(10): 2761\u20132773, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yi%20Xu%2C%20Dong%20Nie%2C%20Feiping%20Yan%2C%20Shuicheng%20Image%20clustering%20using%20local%20discriminant%20models%20and%20global%20integration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yi%20Xu%2C%20Dong%20Nie%2C%20Feiping%20Yan%2C%20Shuicheng%20Image%20clustering%20using%20local%20discriminant%20models%20and%20global%20integration%202010"
        },
        {
            "id": "Zhang_2004_a",
            "entry": "Nevin L Zhang. Hierarchical latent class models for cluster analysis. Journal of Machine Learning Research, 5(6):697\u2013723, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Nevin%20L.%20Hierarchical%20latent%20class%20models%20for%20cluster%20analysis%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Nevin%20L.%20Hierarchical%20latent%20class%20models%20for%20cluster%20analysis%202004"
        },
        {
            "id": "Zhang_2017_a",
            "entry": "Nevin L Zhang and Leonard KM Poon. Latent tree analysis. In AAAI, pp. 4891\u20134898, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Nevin%20L.%20Poon%2C%20Leonard%20K.M.%20Latent%20tree%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Nevin%20L.%20Poon%2C%20Leonard%20K.M.%20Latent%20tree%20analysis%202017"
        }
    ]
}
