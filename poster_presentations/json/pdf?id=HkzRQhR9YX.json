{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "TREE-STRUCTURED RECURRENT SWITCHING LINEAR DYNAMICAL SYSTEMS FOR MULTI-SCALE MODELING",
        "author": "Josue Nassar Department of Electrical & Computer Engineering Stony Brook University Stony Brook, NY 11794 josue.nassar@stonybrook.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkzRQhR9YX"
        },
        "abstract": "Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models1. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using P\u00f3lya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "state space model",
            "url": "https://en.wikipedia.org/wiki/State_Space_Model"
        }
    ],
    "abbreviations": {
        "SLDS": "Switching linear dynamical systems",
        "rSLDS": "Recurrent switching linear dynamical systems",
        "MNIW": "Matrix Normal Inverse Wishart"
    },
    "highlights": [
        "Complex systems can often be described at multiple levels of abstraction",
        "The key contributions are two-fold2: first, we introduce a new form of tree-structured stick breaking for multinomial models that strictly generalizes the sequential stick breaking of the original Recurrent switching linear dynamical systems, while still permitting P\u00f3lya-gamma data augmentation (Polson et al, 2013) for efficient posterior inference; second, we develop a hierarchical prior that links dynamics parameters across levels of the tree, thereby providing descriptions that vary smoothly with depth",
        "Recurrent switching linear dynamical systems (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a>), known as augmented Switching linear dynamical systems (<a class=\"ref-link\" id=\"cBarber_2006_a\" href=\"#rBarber_2006_a\">Barber, 2006</a>), are an extension of Switching linear dynamical systems where the transition density of the discrete latent state depends on the previous location in the continuous latent space zt|xt\u22121, {R, r} \u223c \u03c0SB , (5)",
        "Building upon the Recurrent switching linear dynamical systems, we propose the tree-structured recurrent switching linear dynamical system (TrSLDS)",
        "We propose tree-structured recurrent switching linear dynamical systems (TrSLDS) which is an extension of Recurrent switching linear dynamical systems (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a>)",
        "The two synthetic experiments show that TrSLDS can recover a multi-scale view of the system, where the resolution of the system increase as we delve deeper into the tree"
    ],
    "key_statements": [
        "Complex systems can often be described at multiple levels of abstraction",
        "The key contributions are two-fold2: first, we introduce a new form of tree-structured stick breaking for multinomial models that strictly generalizes the sequential stick breaking of the original Recurrent switching linear dynamical systems, while still permitting P\u00f3lya-gamma data augmentation (Polson et al, 2013) for efficient posterior inference; second, we develop a hierarchical prior that links dynamics parameters across levels of the tree, thereby providing descriptions that vary smoothly with depth",
        "While there exist methods that perform smoothing to obtain an estimate of x0:T (<a class=\"ref-link\" id=\"cBarber_2006_a\" href=\"#rBarber_2006_a\">Barber, 2006</a>; <a class=\"ref-link\" id=\"cFox_et+al_2009_a\" href=\"#rFox_et+al_2009_a\">Fox et al, 2009</a>; <a class=\"ref-link\" id=\"cDjuric_2006_a\" href=\"#rDjuric_2006_a\">Djuric & Bugallo, 2006</a>), we are often interested in not only obtaining an estimate of the continuous latent states but in learning the dynamics f (\u00b7; \u0398) that govern the dynamics of the system",
        "Recurrent switching linear dynamical systems (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a>), known as augmented Switching linear dynamical systems (<a class=\"ref-link\" id=\"cBarber_2006_a\" href=\"#rBarber_2006_a\">Barber, 2006</a>), are an extension of Switching linear dynamical systems where the transition density of the discrete latent state depends on the previous location in the continuous latent space zt|xt\u22121, {R, r} \u223c \u03c0SB , (5)",
        "Building upon the Recurrent switching linear dynamical systems, we propose the tree-structured recurrent switching linear dynamical system (TrSLDS)",
        "We show an alternate view of TrSLDS which we will refer to as the residual model in which internal nodes do contribute to the dynamics",
        "We fit TrSLDS with K = 4 leaf nodes and 3-dimensional continuous latent space; the sampler was run for 500 samples where the last sample was used to produce the results shown in Fig. 4",
        "We propose tree-structured recurrent switching linear dynamical systems (TrSLDS) which is an extension of Recurrent switching linear dynamical systems (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a>)",
        "We developed a fully Bayesian sampler, which leverages the P\u00f3lya-Gamma augmentation, to learn the parameters of the model and infer latent states",
        "The two synthetic experiments show that TrSLDS can recover a multi-scale view of the system, where the resolution of the system increase as we delve deeper into the tree"
    ],
    "summary": [
        "Complex systems can often be described at multiple levels of abstraction.",
        "We call it the tree-structured recurrent switching linear dynamical system, or TrSLDS.",
        "Section 3 presents our tree-structured model and Section 4 derives an efficient fullyBayesian inference algorithm for the latent states and dynamics parameters.",
        "Recurrent switching linear dynamical systems (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\"><a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a></a>), known as augmented SLDS (<a class=\"ref-link\" id=\"cBarber_2006_a\" href=\"#rBarber_2006_a\">Barber, 2006</a>), are an extension of SLDS where the transition density of the discrete latent state depends on the previous location in the continuous latent space zt|xt\u22121, {R, r} \u223c \u03c0SB , (5)",
        "Building upon the rSLDS, we propose the tree-structured recurrent switching linear dynamical system (TrSLDS).",
        "It is through this hierarchical tree-structured prior that TrSLDS obtains a multi-scale view of the system.",
        "The biggest differences are that our tree-structured model has stochastic choices and the internal nodes contribute to smoothing across partitions through the corresponding hierarchical prior.",
        "While the tree-structured stick-breaking used in TrSLDS is a hierarchy of discrete latent variables, the model proposed in <a class=\"ref-link\" id=\"cStanculescu_et+al_2014_a\" href=\"#rStanculescu_et+al_2014_a\">Stanculescu et al (2014</a>) has no hierarchy of dynamics, preventing it from obtaining a multi-scale view of the dynamics.",
        "The linear dynamic parameters (Ak, bk) and state variance Qk of a leaf node k are conjugate with a Matrix Normal Inverse Wishart (MNIW) prior",
        "We set the number of leaf nodes to be 4 and ran Gibbs for 1,000 samples; the last 50 samples were kept and we choose the sample that produced the highest log likelihood to produce Fig. 2 where the vector fields were produced using the mode of the conditional posteriors of the dynamics.",
        "The observation model was a projection onto 10 dimensional space with Gaussian noise.We set the number of leaf nodes to be 4 and ran Gibbs for 1,000 samples; the last 50 samples were kept and we choose the sample that produced the highest log-likelihood to produce Fig. 3.",
        "We fit TrSLDS with K = 4 leaf nodes and 3-dimensional continuous latent space; the sampler was run for 500 samples where the last sample was used to produce the results shown in Fig. 4.",
        "We propose tree-structured recurrent switching linear dynamical systems (TrSLDS) which is an extension of rSLDS (<a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\"><a class=\"ref-link\" id=\"cLinderman_et+al_2017_a\" href=\"#rLinderman_et+al_2017_a\">Linderman et al, 2017</a></a>).",
        "We developed a fully Bayesian sampler, which leverages the P\u00f3lya-Gamma augmentation, to learn the parameters of the model and infer latent states.",
        "The analysis on the real neural data verifies that TrSLDS can find a multi-scale structure"
    ],
    "headline": "We develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet more accurate models1",
    "reference_links": [
        {
            "id": "Guy_1970_a",
            "entry": "Guy A Ackerson and King-Sun Fu. On state estimation in switching environments. IEEE Transactions on Automatic Control, 15(1):10\u201317, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guy%20A%20Ackerson%20and%20KingSun%20Fu%20On%20state%20estimation%20in%20switching%20environments%20IEEE%20Transactions%20on%20Automatic%20Control%201511017%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guy%20A%20Ackerson%20and%20KingSun%20Fu%20On%20state%20estimation%20in%20switching%20environments%20IEEE%20Transactions%20on%20Automatic%20Control%201511017%201970"
        },
        {
            "id": "Adams_et+al_2010_a",
            "entry": "Ryan P Adams, Zoubin Ghahramani, and Michael I Jordan. Tree-Structured Stick Breaking for Hierarchical Data. In J D Lafferty, C K I Williams, J Shawe-Taylor, R S Zemel, and A Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 19\u201327. Curran Associates, Inc., 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adams%2C%20Ryan%20P.%20Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Tree-Structured%20Stick%20Breaking%20for%20Hierarchical%20Data%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adams%2C%20Ryan%20P.%20Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Tree-Structured%20Stick%20Breaking%20for%20Hierarchical%20Data%202010"
        },
        {
            "id": "Barber_2006_a",
            "entry": "David Barber. Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems. Technical report, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barber%2C%20David%20Expectation%20Correction%20for%20Smoothed%20Inference%20in%20Switching%20Linear%20Dynamical%20Systems%202006"
        },
        {
            "id": "David_2011_a",
            "entry": "David Barber, A Taylan Cemgil, and Silvia Chiappa. Bayesian time series models. Cambridge University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Barber%2C%20A.Taylan%20Cemgil%20Chiappa%2C%20Silvia%20Bayesian%20time%20series%20models%202011"
        },
        {
            "id": "Brooks_et+al_2011_a",
            "entry": "Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo. CRC press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brooks%2C%20Steve%20Gelman%2C%20Andrew%20Jones%2C%20Galin%20Meng%2C%20Xiao-Li%20Handbook%20of%20Markov%20Chain%20Monte%20Carlo%202011"
        },
        {
            "id": "Chaw-Bing_1978_a",
            "entry": "Chaw-Bing Chang and Michael Athans. State estimation for discrete systems with switching parameters. IEEE Transactions on Aerospace and Electronic Systems, (3):418\u2013425, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ChawBing%20Chang%20and%20Michael%20Athans%20State%20estimation%20for%20discrete%20systems%20with%20switching%20parameters%20IEEE%20Transactions%20on%20Aerospace%20and%20Electronic%20Systems%203418425%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ChawBing%20Chang%20and%20Michael%20Athans%20State%20estimation%20for%20discrete%20systems%20with%20switching%20parameters%20IEEE%20Transactions%20on%20Aerospace%20and%20Electronic%20Systems%203418425%201978"
        },
        {
            "id": "Chung_et+al_2015_a",
            "entry": "Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980\u20132988, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015"
        },
        {
            "id": "Djuric_2006_a",
            "entry": "Petar Djuric and M\u00f3nica Bugallo. Cost-Reference Particle Filtering for Dynamic Systems with Nonlinear and Conditionally Linear States, 9 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Djuric%2C%20Petar%20Bugallo%2C%20M%C3%B3nica%20Cost-Reference%20Particle%20Filtering%20for%20Dynamic%20Systems%20with%20Nonlinear%20and%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Djuric%2C%20Petar%20Bugallo%2C%20M%C3%B3nica%20Cost-Reference%20Particle%20Filtering%20for%20Dynamic%20Systems%20with%20Nonlinear%20and%202006"
        },
        {
            "id": "Doucet_et+al_2001_a",
            "entry": "Arnaud Doucet, Nando Freitas, and Neil Gordon. An Introduction to Sequential Monte Carlo Methods. In Sequential Monte Carlo Methods in Practice, pp. 3\u201314. Springer New York, New York, NY, 2001. doi: 10.1007/978-1-4757-3437-9{\\_}1.",
            "crossref": "https://dx.doi.org/10.1007/978-1-4757-3437-9{\\_}1"
        },
        {
            "id": "Erosheva_2017_a",
            "entry": "Elena A. Erosheva and S. McKay Curtis. Dealing with Reflection Invariance in Bayesian Factor Analysis. Psychometrika, 82(2):295\u2013307, 6 2017. ISSN 0033-3123. doi: 10.1007/s11336-017-9564-y.",
            "crossref": "https://dx.doi.org/10.1007/s11336-017-9564-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11336-017-9564-y"
        },
        {
            "id": "Fox_et+al_2009_a",
            "entry": "Emily Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. Nonparametric Bayesian Learning of Switching Linear Dynamical Systems. In D Koller, D Schuurmans, Y Bengio, and L Bottou (eds.), Advances in Neural Information Processing Systems 21, pp. 457\u2013464. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fox%2C%20Emily%20Sudderth%2C%20Erik%20B.%20Jordan%2C%20Michael%20I.%20Willsky%2C%20Alan%20S.%20Nonparametric%20Bayesian%20Learning%20of%20Switching%20Linear%20Dynamical%20Systems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fox%2C%20Emily%20Sudderth%2C%20Erik%20B.%20Jordan%2C%20Michael%20I.%20Willsky%2C%20Alan%20S.%20Nonparametric%20Bayesian%20Learning%20of%20Switching%20Linear%20Dynamical%20Systems%202009"
        },
        {
            "id": "Frigola_et+al_2014_a",
            "entry": "Roger Frigola, Yutian Chen, and Carl Edward Rasmussen. Variational Gaussian Process State-Space Models. In Z Ghahramani, M Welling, C Cortes, N D Lawrence, and K Q Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3680\u20133688. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roger%20Frigola%20Yutian%20Chen%20and%20Carl%20Edward%20Rasmussen%20Variational%20Gaussian%20Process%20StateSpace%20Models%20In%20Z%20Ghahramani%20M%20Welling%20C%20Cortes%20N%20D%20Lawrence%20and%20K%20Q%20Weinberger%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2027%20pp%2036803688%20Curran%20Associates%20Inc%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roger%20Frigola%20Yutian%20Chen%20and%20Carl%20Edward%20Rasmussen%20Variational%20Gaussian%20Process%20StateSpace%20Models%20In%20Z%20Ghahramani%20M%20Welling%20C%20Cortes%20N%20D%20Lawrence%20and%20K%20Q%20Weinberger%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2027%20pp%2036803688%20Curran%20Associates%20Inc%202014"
        },
        {
            "id": "Gao_et+al_2016_a",
            "entry": "Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. Linear dynamical neural population models through nonlinear embeddings. In Advances in neural information processing systems, pp. 163\u2013171, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Yuanjun%20Archer%2C%20Evan%20W.%20Paninski%2C%20Liam%20Cunningham%2C%20John%20P.%20Linear%20dynamical%20neural%20population%20models%20through%20nonlinear%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Yuanjun%20Archer%2C%20Evan%20W.%20Paninski%2C%20Liam%20Cunningham%2C%20John%20P.%20Linear%20dynamical%20neural%20population%20models%20through%20nonlinear%20embeddings%202016"
        },
        {
            "id": "Geweke_1996_a",
            "entry": "John Geweke and Guofu Zhou. Measuring the Pricing Error of the Arbitrage Pricing Theory. Review of Financial Studies, 9(2):557\u2013587, 4 1996. ISSN 0893-9454. doi: 10.1093/rfs/9.2.557.",
            "crossref": "https://dx.doi.org/10.1093/rfs/9.2.557",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1093/rfs/9.2.557"
        },
        {
            "id": "Ghahramani_1996_a",
            "entry": "Zoubin Ghahramani and Geoffrey E Hinton. Switching state-space models. Technical report, University of Toronto, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Hinton%2C%20Geoffrey%20E.%20Switching%20state-space%20models%201996"
        },
        {
            "id": "Graf_et+al_2011_a",
            "entry": "Arnulf B. Graf, Adam Kohn, Mehrdad Jazayeri, and J. Anthony Movshon. Decoding the activity of neuronal populations in macaque primary visual cortex. Nature neuroscience, 14(2):239\u2013245, February 2011. ISSN 1546-1726. doi: 10.1038/nn.2733.",
            "crossref": "https://dx.doi.org/10.1038/nn.2733",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nn.2733"
        },
        {
            "id": "Hamilton_1990_a",
            "entry": "James D Hamilton. Analysis of time series subject to changes in regime. Journal of econometrics, 45 (1):39\u201370, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20James%20D.%20Analysis%20of%20time%20series%20subject%20to%20changes%20in%20regime%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20James%20D.%20Analysis%20of%20time%20series%20subject%20to%20changes%20in%20regime%201990"
        },
        {
            "id": "Haykin_2001_a",
            "entry": "Simon S Haykin. Kalman Filtering and Neural Networks. John Wiley &amp; Sons, Inc., New York, NY, USA, 2001. ISBN 0471369985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haykin%2C%20Simon%20S.%20Kalman%20Filtering%20and%20Neural%20Networks%202001"
        },
        {
            "id": "Izhikevich_2007_a",
            "entry": "Eugene M Izhikevich. Dynamical systems in neuroscience. MIT press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Izhikevich%2C%20Eugene%20M.%20Dynamical%20systems%20in%20neuroscience%202007"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. In Advances in neural information processing systems, pp. 2946\u20132954, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Matthew%20Duvenaud%2C%20David%20K.%20Wiltschko%2C%20Alex%20Adams%2C%20Ryan%20P.%20Composing%20graphical%20models%20with%20neural%20networks%20for%20structured%20representations%20and%20fast%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Matthew%20Duvenaud%2C%20David%20K.%20Wiltschko%2C%20Alex%20Adams%2C%20Ryan%20P.%20Composing%20graphical%20models%20with%20neural%20networks%20for%20structured%20representations%20and%20fast%20inference%202016"
        },
        {
            "id": "Krishnan_et+al_2017_a",
            "entry": "Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Rahul%20G.%20Shalit%2C%20Uri%20Sontag%2C%20David%20Structured%20inference%20networks%20for%20nonlinear%20state%20space%20models%202017"
        },
        {
            "id": "Lakshminarayanan_2016_a",
            "entry": "Balaji Lakshminarayanan. Decision Trees and Forests: A Probabilistic Perspective. Technical report, UCL (University College London), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20Balaji%20Decision%20Trees%20and%20Forests%3A%20A%20Probabilistic%20Perspective%202016"
        },
        {
            "id": "Linderman_et+al_2015_a",
            "entry": "Scott Linderman, Matthew Johnson, and Ryan P Adams. Dependent Multinomial Models Made Easy: Stick-Breaking with the Polya-gamma Augmentation. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3456\u20133464. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linderman%2C%20Scott%20Johnson%2C%20Matthew%20Adams%2C%20Ryan%20P.%20Dependent%20Multinomial%20Models%20Made%20Easy%3A%20Stick-Breaking%20with%20the%20Polya-gamma%20Augmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linderman%2C%20Scott%20Johnson%2C%20Matthew%20Adams%2C%20Ryan%20P.%20Dependent%20Multinomial%20Models%20Made%20Easy%3A%20Stick-Breaking%20with%20the%20Polya-gamma%20Augmentation%202015"
        },
        {
            "id": "Linderman_et+al_2017_a",
            "entry": "Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 914\u2013922, Fort Lauderdale, FL, USA, 9 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linderman%2C%20Scott%20Johnson%2C%20Matthew%20Miller%2C%20Andrew%20Adams%2C%20Ryan%20Bayesian%20Learning%20and%20Inference%20in%20Recurrent%20Switching%20Linear%20Dynamical%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linderman%2C%20Scott%20Johnson%2C%20Matthew%20Miller%2C%20Andrew%20Adams%2C%20Ryan%20Bayesian%20Learning%20and%20Inference%20in%20Recurrent%20Switching%20Linear%20Dynamical%20Systems%202017"
        },
        {
            "id": "Murphy_1998_a",
            "entry": "Kevin P Murphy. Switching Kalman filters. Technical report, Compaq Cambridge Research, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murphy%2C%20Kevin%20P.%20Switching%20Kalman%20filters%201998"
        },
        {
            "id": "Pandarinath_et+al_2018_a",
            "entry": "Chethan Pandarinath, Daniel J O\u2019Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, pp. 1, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pandarinath%2C%20Chethan%20O%E2%80%99Shea%2C%20Daniel%20J.%20Collins%2C%20Jasmine%20Jozefowicz%2C%20Rafal%20Inferring%20single-trial%20neural%20population%20dynamics%20using%20sequential%20auto-encoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pandarinath%2C%20Chethan%20O%E2%80%99Shea%2C%20Daniel%20J.%20Collins%2C%20Jasmine%20Jozefowicz%2C%20Rafal%20Inferring%20single-trial%20neural%20population%20dynamics%20using%20sequential%20auto-encoders%202018"
        },
        {
            "id": "Polson_et+al_0000_a",
            "entry": "Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian Inference for Logistic Models Using P\u00f3lya\u2013Gamma Latent Variables. Journal of the American Statistical Association, 108(504):1339\u2013 1349, 2013. doi: 10.1080/01621459.2013.829001.",
            "crossref": "https://dx.doi.org/10.1080/01621459.2013.829001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/01621459.2013.829001"
        },
        {
            "id": "Saerkkae_2013_a",
            "entry": "Simo S\u00e4rkk\u00e4. Bayesian filtering and smoothing, volume 3. Cambridge University Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A4rkk%C3%A4%2C%20Simo%20Bayesian%20filtering%20and%20smoothing%2C%20volume%203%202013"
        },
        {
            "id": "Stanculescu_et+al_2014_a",
            "entry": "Ioan Stanculescu, Christopher KI Williams, and Yvonne Freer. A hierarchical switching linear dynamical system applied to the detection of sepsis in neonatal condition monitoring. In UAI, pp. 752\u2013761, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanculescu%2C%20Ioan%20Williams%2C%20Christopher%20K.I.%20Freer%2C%20Yvonne%20A%20hierarchical%20switching%20linear%20dynamical%20system%20applied%20to%20the%20detection%20of%20sepsis%20in%20neonatal%20condition%20monitoring%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanculescu%2C%20Ioan%20Williams%2C%20Christopher%20K.I.%20Freer%2C%20Yvonne%20A%20hierarchical%20switching%20linear%20dynamical%20system%20applied%20to%20the%20detection%20of%20sepsis%20in%20neonatal%20condition%20monitoring%202014"
        },
        {
            "id": "David_2016_a",
            "entry": "David Sussillo, Rafal J\u00f3zefowicz, L. F Abbott, and Chethan Pandarinath. LFADS - Latent Factor Analysis via Dynamical Systems. CoRR, abs/1608.06315, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06315"
        },
        {
            "id": "Zhao_2016_a",
            "entry": "Yuan Zhao and Il Memming Park. Interpretable nonlinear dynamic modeling of neural trajectories. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Yuan%20Park%2C%20Il%20Memming%20Interpretable%20nonlinear%20dynamic%20modeling%20of%20neural%20trajectories%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Yuan%20Park%2C%20Il%20Memming%20Interpretable%20nonlinear%20dynamic%20modeling%20of%20neural%20trajectories%202016"
        },
        {
            "id": "Zhao_2017_a",
            "entry": "Yuan Zhao and Il Memming Park. Variational Latent Gaussian Process for Recovering SingleTrial Dynamics from Population Spike Trains. Neural Computation, 29(5), May 2017. doi: 10.1162/NECO_a_00953.",
            "crossref": "https://dx.doi.org/10.1162/NECO_a_00953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/NECO_a_00953"
        },
        {
            "id": "Zhao_2018_a",
            "entry": "Yuan Zhao and Il Memming Park. Variational joint filtering. arXiv, abs/1707.09049, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09049"
        },
        {
            "id": "Zoeter_2003_a",
            "entry": "Onno Zoeter and Tom Heskes. Hierarchical visualization of time-series data using switching linear dynamical systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10): 1202\u20131214, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoeter%2C%20Onno%20Heskes%2C%20Tom%20Hierarchical%20visualization%20of%20time-series%20data%20using%20switching%20linear%20dynamical%20systems%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoeter%2C%20Onno%20Heskes%2C%20Tom%20Hierarchical%20visualization%20of%20time-series%20data%20using%20switching%20linear%20dynamical%20systems%202003"
        },
        {
            "id": "The_2013_a",
            "entry": "The observation is now effectively Gaussian and can be incorporated into the message passing for x1:T. The emission parameters are also conjugate with the augmented observation potential given a Matrix Normal prior. The conditional posterior on the axillary PG variables \u03b7n,t also follows a PG distribution i.e. \u03b7n,t|(cn, dn), xt \u223c PG(1, \u03c5n,t). Note that this augmentation scheme can also work for negative binomial, binomial, and multinomial observations (Polson et al., 2013; Linderman et al., 2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20observation%20is%20now%20effectively%20Gaussian%20and%20can%20be%20incorporated%20into%20the%20message%20passing%20for%20x1T%20The%20emission%20parameters%20are%20also%20conjugate%20with%20the%20augmented%20observation%20potential%20given%20a%20Matrix%20Normal%20prior%20The%20conditional%20posterior%20on%20the%20axillary%20PG%20variables%20%CE%B7nt%20also%20follows%20a%20PG%20distribution%20ie%20%CE%B7ntcn%20dn%20xt%20%20PG1%20%CF%85nt%20Note%20that%20this%20augmentation%20scheme%20can%20also%20work%20for%20negative%20binomial%20binomial%20and%20multinomial%20observations%20Polson%20et%20al%202013%20Linderman%20et%20al%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20observation%20is%20now%20effectively%20Gaussian%20and%20can%20be%20incorporated%20into%20the%20message%20passing%20for%20x1T%20The%20emission%20parameters%20are%20also%20conjugate%20with%20the%20augmented%20observation%20potential%20given%20a%20Matrix%20Normal%20prior%20The%20conditional%20posterior%20on%20the%20axillary%20PG%20variables%20%CE%B7nt%20also%20follows%20a%20PG%20distribution%20ie%20%CE%B7ntcn%20dn%20xt%20%20PG1%20%CF%85nt%20Note%20that%20this%20augmentation%20scheme%20can%20also%20work%20for%20negative%20binomial%20binomial%20and%20multinomial%20observations%20Polson%20et%20al%202013%20Linderman%20et%20al%202015"
        },
        {
            "id": "A_2017_a",
            "entry": "A well known problem with these types of model is it\u2019s susceptibility to rotational and scaling transformation, thus we can only learn the dynamics up to an affine transformation Erosheva & Curtis (2017). During Gibbs sampling the parameters will continuously rotate and scale, which can slow down the mixing of the chains. One possible solution to the issue is if we constrained C to have some special structure which would make the model identifiable; this would require sampling from manifolds which is usually inefficient. Similar to Geweke & Zhou (1996), we use the following procedure to prevent the samples from continuously rotating and scaling:",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20well%20known%20problem%20with%20these%20types%20of%20model%20is%20its%20susceptibility%20to%20rotational%20and%20scaling%20transformation%20thus%20we%20can%20only%20learn%20the%20dynamics%20up%20to%20an%20affine%20transformation%20Erosheva%20%20Curtis%202017%20During%20Gibbs%20sampling%20the%20parameters%20will%20continuously%20rotate%20and%20scale%20which%20can%20slow%20down%20the%20mixing%20of%20the%20chains%20One%20possible%20solution%20to%20the%20issue%20is%20if%20we%20constrained%20C%20to%20have%20some%20special%20structure%20which%20would%20make%20the%20model%20identifiable%20this%20would%20require%20sampling%20from%20manifolds%20which%20is%20usually%20inefficient%20Similar%20to%20Geweke%20%20Zhou%201996%20we%20use%20the%20following%20procedure%20to%20prevent%20the%20samples%20from%20continuously%20rotating%20and%20scaling",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20well%20known%20problem%20with%20these%20types%20of%20model%20is%20its%20susceptibility%20to%20rotational%20and%20scaling%20transformation%20thus%20we%20can%20only%20learn%20the%20dynamics%20up%20to%20an%20affine%20transformation%20Erosheva%20%20Curtis%202017%20During%20Gibbs%20sampling%20the%20parameters%20will%20continuously%20rotate%20and%20scale%20which%20can%20slow%20down%20the%20mixing%20of%20the%20chains%20One%20possible%20solution%20to%20the%20issue%20is%20if%20we%20constrained%20C%20to%20have%20some%20special%20structure%20which%20would%20make%20the%20model%20identifiable%20this%20would%20require%20sampling%20from%20manifolds%20which%20is%20usually%20inefficient%20Similar%20to%20Geweke%20%20Zhou%201996%20we%20use%20the%20following%20procedure%20to%20prevent%20the%20samples%20from%20continuously%20rotating%20and%20scaling"
        }
    ]
}
