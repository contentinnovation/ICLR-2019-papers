{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADAPTIVE INPUT REPRESENTATIONS FOR NEURAL LANGUAGE MODELING",
        "author": "Alexei Baevski & Michael Auli Facebook AI Research, Menlo Park, CA, USA",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByxZX20qFQ"
        },
        "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.1"
    },
    "keywords": [
        {
            "term": "language modeling",
            "url": "https://en.wikipedia.org/wiki/language_modeling"
        },
        {
            "term": "WORD",
            "url": "https://en.wikipedia.org/wiki/WORD"
        },
        {
            "term": "statistical machine translation",
            "url": "https://en.wikipedia.org/wiki/statistical_machine_translation"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "BPE": "byte-pair encoding",
        "ASM": "adaptive softmax",
        "ADP": "adaptive softmax",
        "ADP-T": "and output layers"
    },
    "highlights": [
        "Language modeling is a basic task in natural language processing, with many applications such as speech recognition (<a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\"><a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\">Arisoy et al, 2012</a></a>) and statistical machine translation (<a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\"><a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\">Schwenk et al, 2012</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\">Vaswani et al, 2013</a></a>; <a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\"><a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\">Baltescu & Blunsom, 2015</a></a>)",
        "We introduce adaptive input embeddings which extend the adaptive softmax to input word representations",
        "Table 4 shows that adaptive input representations perform well on BILLION WORD compared to other factorizations",
        "Adaptive input embeddings vary the size of input word embeddings which can improve accuracy while drastically reducing the number of model parameters",
        "We presented a comparison between different input and output layer factorizations including word inputs, character inputs and sub-word units in both the input and output",
        "Our experiments show that models with adaptive input embeddings train faster compared to character input CNNs while achieving higher accuracy"
    ],
    "key_statements": [
        "Language modeling is a basic task in natural language processing, with many applications such as speech recognition (<a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\"><a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\">Arisoy et al, 2012</a></a>) and statistical machine translation (<a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\"><a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\">Schwenk et al, 2012</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\">Vaswani et al, 2013</a></a>; <a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\"><a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\">Baltescu & Blunsom, 2015</a></a>)",
        "We introduce adaptive input embeddings which extend the adaptive softmax to input word representations",
        "For a competitive setup on the BILLION WORD benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23% while achieving higher accuracy over fixed size embeddings",
        "Table 4 shows that adaptive input representations perform well on BILLION WORD compared to other factorizations",
        "We found that adaptive softmax can benefit from additional regularization of rare words",
        "Adaptive input embeddings vary the size of input word embeddings which can improve accuracy while drastically reducing the number of model parameters",
        "We presented a comparison between different input and output layer factorizations including word inputs, character inputs and sub-word units in both the input and output",
        "Our experiments show that models with adaptive input embeddings train faster compared to character input CNNs while achieving higher accuracy"
    ],
    "summary": [
        "Language modeling is a basic task in natural language processing, with many applications such as speech recognition (<a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\"><a class=\"ref-link\" id=\"cArisoy_et+al_2012_a\" href=\"#rArisoy_et+al_2012_a\">Arisoy et al, 2012</a></a>) and statistical machine translation (<a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\"><a class=\"ref-link\" id=\"cSchwenk_et+al_2012_a\" href=\"#rSchwenk_et+al_2012_a\">Schwenk et al, 2012</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2013_a\" href=\"#rVaswani_et+al_2013_a\">Vaswani et al, 2013</a></a>; <a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\"><a class=\"ref-link\" id=\"cBaltescu_2015_a\" href=\"#rBaltescu_2015_a\">Baltescu & Blunsom, 2015</a></a>).",
        "For a competitive setup on the BILLION WORD benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23% while achieving higher accuracy over fixed size embeddings.",
        "For BILLION WORD models we use B = 2048 and typically train on 32 GPUs, giving an effective batch size of 65K tokens.",
        "For fixed size word input layers and softmax output layers we generally use embeddings of size 512 for WIKITEXT-103.",
        "When we use an adaptive softmax in the output and fixed size word embeddings for the input, we use dimension 256 for the input embeddings for BILLION WORD and 64 for WIKITEXT-103.",
        "We use an adaptive softmax output layer to train models with large word-based vocabularies.",
        "For adaptive word inputs and adaptive softmax, we use embeddings of size d = 1024 for the head and reduce the size of subsequent clusters by a factor of k = 4.",
        "For the main results on BILLION WORD, we doubled the batch size by training on 64 GPUs instead of 32 GPUs. We consider two larger setups, one where we added four more blocks (N = 20) and increased the FFN dimension to eff = 6144, and another where we add another four blocks (N = 24) with eff = 8192 and e = 1536.",
        "We consider a word-based setup with fixed size word input embeddings and a standard word softmax (SM) where embeddings have either dimension 512 (WIKITEXT-103) or 64 (BILLION WORD).",
        "We consider replacing the fixed size output representations by an adaptive softmax (ASM) and characters as input (CNN).",
        "Sub-word units are fast to train and perform better than word models with fixed sized embeddings.",
        "For ASM, we found that reducing the dimension of the input word embeddings to 64 on WIKITEXT-103 results in better accuracy (Appendix A).",
        "Table 4 shows that adaptive input representations perform well on BILLION WORD compared to other factorizations.",
        "Adaptive softmax first projects the model output to the dimension of a particular cluster and computes a dot product with the respective word embeddings.",
        "This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on WIKITEXT-103 (Table 6).",
        "Adaptive input embeddings vary the size of input word embeddings which can improve accuracy while drastically reducing the number of model parameters.",
        "Our experiments show that models with adaptive input embeddings train faster compared to character input CNNs while achieving higher accuracy.",
        "We will apply variable sized input embeddings to other tasks"
    ],
    "headline": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. to input representations of variable capacity",
    "reference_links": [
        {
            "id": "Al-Rfou_et+al_2018_a",
            "entry": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. arXiv, abs/1808.04444, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04444"
        },
        {
            "id": "Arisoy_et+al_2012_a",
            "entry": "Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. Deep Neural Network Language Models. In NAACL-HLT Workshop on the Future of Language Modeling for HLT, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebru%20Arisoy%20Tara%20N%20Sainath%20Brian%20Kingsbury%20and%20Bhuvana%20Ramabhadran%20Deep%20Neural%20Network%20Language%20Models%20In%20NAACLHLT%20Workshop%20on%20the%20Future%20of%20Language%20Modeling%20for%20HLT%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ebru%20Arisoy%20Tara%20N%20Sainath%20Brian%20Kingsbury%20and%20Bhuvana%20Ramabhadran%20Deep%20Neural%20Network%20Language%20Models%20In%20NAACLHLT%20Workshop%20on%20the%20Future%20of%20Language%20Modeling%20for%20HLT%202012"
        },
        {
            "id": "Baltescu_2015_a",
            "entry": "Paul Baltescu and Phil Blunsom. Pragmatic neural language modelling in machine translation. In Proc. of ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baltescu%2C%20Paul%20Blunsom%2C%20Phil%20Pragmatic%20neural%20language%20modelling%20in%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baltescu%2C%20Paul%20Blunsom%2C%20Phil%20Pragmatic%20neural%20language%20modelling%20in%20machine%20translation%202015"
        },
        {
            "id": "Bengio_et+al_2003_a",
            "entry": "Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A Neural Probabilistic Language Model. JMLR, 3:1137\u20131155, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Ducharme%2C%20Rejean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20Neural%20Probabilistic%20Language%20Model%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Ducharme%2C%20Rejean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20Neural%20Probabilistic%20Language%20Model%202003"
        },
        {
            "id": "Buckman_2018_a",
            "entry": "Jacob Buckman and Graham Neubig. Neural lattice language models. TACL, 6:529\u2013541, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buckman%2C%20Jacob%20Neubig%2C%20Graham%20Neural%20lattice%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buckman%2C%20Jacob%20Neubig%2C%20Graham%20Neural%20lattice%20language%20models%202018"
        },
        {
            "id": "Chelba_et+al_2013_a",
            "entry": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chelba%2C%20Ciprian%20Mikolov%2C%20Tomas%20Schuster%2C%20Mike%20Ge%2C%20Qi%20One%20billion%20word%20benchmark%20for%20measuring%20progress%20in%20statistical%20language%20modeling%202013"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Wenlin Chen, David Grangier, and Michael Auli. Strategies for training large vocabulary neural language models. In Proc. of ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Wenlin%20Grangier%2C%20David%20Auli%2C%20Michael%20Strategies%20for%20training%20large%20vocabulary%20neural%20language%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wenlin%20Grangier%2C%20David%20Auli%2C%20Michael%20Strategies%20for%20training%20large%20vocabulary%20neural%20language%20models%202016"
        },
        {
            "id": "Dauphin_et+al_2017_a",
            "entry": "Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proc. of ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017"
        },
        {
            "id": "Goodman_2001_a",
            "entry": "Joshua Goodman. Classes for Fast Maximum Entropy Training. In Proc. of ICASSP, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20Joshua%20Classes%20for%20Fast%20Maximum%20Entropy%20Training%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodman%2C%20Joshua%20Classes%20for%20Fast%20Maximum%20Entropy%20Training%202001"
        },
        {
            "id": "Grave_et+al_2016_a",
            "entry": "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv, abs/1612.04426, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04426"
        },
        {
            "id": "Grave_et+al_2017_a",
            "entry": "Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efficient softmax approximation for gpus. In Proc. of ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Cisse%2C%20Moustapha%20Grangier%2C%20David%20Efficient%20softmax%20approximation%20for%20gpus%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Cisse%2C%20Moustapha%20Grangier%2C%20David%20Efficient%20softmax%20approximation%20for%20gpus%202017"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proc. of CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202015"
        },
        {
            "id": "Inan_et+al_2016_a",
            "entry": "Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv, abs/1611.01462, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01462"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv, abs/1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "Kim_et+al_2015_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. arXiv, abs/1508.06615, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.06615"
        },
        {
            "id": "Kim_et+al_2016_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In AAAI, pp. 2741\u20132749, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016"
        },
        {
            "id": "Loshchilov_2016_a",
            "entry": "Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. arXiv, abs/1608.03983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.03983"
        },
        {
            "id": "Merity_et+al_2016_a",
            "entry": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv, abs/1609.07843, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07843"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple scales. arXiv, abs/1803.08240, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08240"
        },
        {
            "id": "Mielke_2018_a",
            "entry": "Sebastian J. Mielke and Jason Eisner. Spell once, summon anywhere: A two-level open-vocabulary language model. arXiv, abs/1804.08205, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08205"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tomas Mikolov, Karafiat Martin, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent Neural Network based Language Model. In Proc. of Interspeech, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomas%20Mikolov%20Karafiat%20Martin%20Lukas%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Recurrent%20Neural%20Network%20based%20Language%20Model%20In%20Proc%20of%20Interspeech%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomas%20Mikolov%20Karafiat%20Martin%20Lukas%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Recurrent%20Neural%20Network%20based%20Language%20Model%20In%20Proc%20of%20Interspeech%202010"
        },
        {
            "id": "Mikolov_et+al_2011_a",
            "entry": "Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of Recurrent Neural Network Language Model. In Proc. of ICASSP, pp. 5528\u20135531, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomas%20Mikolov%20Stefan%20Kombrink%20Lukas%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Extensions%20of%20Recurrent%20Neural%20Network%20Language%20Model%20In%20Proc%20of%20ICASSP%20pp%2055285531%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomas%20Mikolov%20Stefan%20Kombrink%20Lukas%20Burget%20Jan%20Cernocky%20and%20Sanjeev%20Khudanpur%20Extensions%20of%20Recurrent%20Neural%20Network%20Language%20Model%20In%20Proc%20of%20ICASSP%20pp%2055285531%202011"
        },
        {
            "id": "Morin_2005_a",
            "entry": "Frederic Morin and Yoshua Bengio. Hierarchical Probabilistic Neural Network Language Model. In Proc. of AISTATS, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frederic%20Morin%20and%20Yoshua%20Bengio%20Hierarchical%20Probabilistic%20Neural%20Network%20Language%20Model%20In%20Proc%20of%20AISTATS%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frederic%20Morin%20and%20Yoshua%20Bengio%20Hierarchical%20Probabilistic%20Neural%20Network%20Language%20Model%20In%20Proc%20of%20AISTATS%202005"
        },
        {
            "id": "Ott_et+al_2018_a",
            "entry": "Myle Ott, Michael Auli, David Grangier, and MarcAurelio Ranzato. Analyzing uncertainty in neural machine translation. In Proc. of ICML, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ott%2C%20Myle%20Auli%2C%20Michael%20Grangier%2C%20David%20Ranzato%2C%20MarcAurelio%20Analyzing%20uncertainty%20in%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ott%2C%20Myle%20Auli%2C%20Michael%20Grangier%2C%20David%20Ranzato%2C%20MarcAurelio%20Analyzing%20uncertainty%20in%20neural%20machine%20translation%202018"
        },
        {
            "id": "Ott_et+al_2018_b",
            "entry": "Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proc. of WMT, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ott%2C%20Myle%20Edunov%2C%20Sergey%20Grangier%2C%20David%20Auli%2C%20Michael%20Scaling%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ott%2C%20Myle%20Edunov%2C%20Sergey%20Grangier%2C%20David%20Auli%2C%20Michael%20Scaling%20neural%20machine%20translation%202018"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proc. of ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Press_2017_a",
            "entry": "Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Proc. of EACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Press%2C%20Ofir%20Wolf%2C%20Lior%20Using%20the%20output%20embedding%20to%20improve%20language%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Press%2C%20Ofir%20Wolf%2C%20Lior%20Using%20the%20output%20embedding%20to%20improve%20language%20models%202017"
        },
        {
            "id": "Rae_et+al_2018_a",
            "entry": "Jack W. Rae, Chris Dyer, Peter Dayan, and Timothy P. Lillicrap. Fast parametric learning with activation memorization. arXiv, abs/1803.10049, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10049"
        },
        {
            "id": "Schwenk_et+al_2012_a",
            "entry": "Holger Schwenk, Anthony Rousseau, and Mohammed Attik. Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation. In NAACL-HLT Workshop on the Future of Language Modeling for HLT, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwenk%2C%20Holger%20Rousseau%2C%20Anthony%20Large%2C%20Mohammed%20Attik%20Pruned%20or%20Continuous%20Space%20Language%20Models%20on%20a%20GPU%20for%20Statistical%20Machine%20Translation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwenk%2C%20Holger%20Rousseau%2C%20Anthony%20Large%2C%20Mohammed%20Attik%20Pruned%20or%20Continuous%20Space%20Language%20Models%20on%20a%20GPU%20for%20Statistical%20Machine%20Translation%202012"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016"
        },
        {
            "id": "Shazeer_et+al_2016_a",
            "entry": "Noam Shazeer, Joris Pelemans, and Ciprian Chelba. Sparse non-negative matrix language modeling. In Proc. of Interspeech, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shazeer%2C%20Noam%20Pelemans%2C%20Joris%20Chelba%2C%20Ciprian%20Sparse%20non-negative%20matrix%20language%20modeling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shazeer%2C%20Noam%20Pelemans%2C%20Joris%20Chelba%2C%20Ciprian%20Sparse%20non-negative%20matrix%20language%20modeling%202016"
        },
        {
            "id": "Shazeer_et+al_2017_a",
            "entry": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv, abs/1701.06538, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06538"
        },
        {
            "id": "Sutskever_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proc. of ICML, 2013. Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with Large-scale",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20E.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20E.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Vaswani_et+al_2013_a",
            "entry": "Neural Language Models improves Translation. In Proc. of EMNLP, October 2013. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Proc. of NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaswani%2C%20Noam%20Shazeer%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20Jones%2C%20Llion%20Neural%20Language%20Models%20improves%20Translation.%20In%20Proc.%20of%20EMNLP%202013-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaswani%2C%20Noam%20Shazeer%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20Jones%2C%20Llion%20Neural%20Language%20Models%20improves%20Translation.%20In%20Proc.%20of%20EMNLP%202013-10"
        }
    ]
}
