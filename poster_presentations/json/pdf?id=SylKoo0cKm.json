{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HOW IMPORTANT IS A NEURON?",
        "author": "Kedar Dhamdhere Google AI kedar@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SylKoo0cKm"
        },
        "abstract": "The problem of attributing a deep network\u2019s prediction to its input/base features is well-studied (cf. Simonyan et al. (2013)). We introduce the notion of conductance to extend the notion of attribution to understanding the importance of hidden units."
    },
    "keywords": [
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "cost sharing",
            "url": "https://en.wikipedia.org/wiki/cost_sharing"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "<strong>BACKGROUND AND MOTIVATION</strong><br/><br/>The problem of attributing a deep network\u2019s prediction to its input is well-studied; <a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\"><a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\">Simonyan et al (2013</a></a>); <a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\"><a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\">Shrikumar et al (2017</a></a>); <a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\"><a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\">Binder et al (2016</a></a>); <a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\"><a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\">Springenberg et al (2014</a></a>); <a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\"><a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a></a>); <a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\"><a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\">Sundararajan et al (2017</a></a>))",
        "Attributions help determine the influence of base features on the input",
        "We introduce our approach to the problem of neuron importance",
        "We find that other measures of neuron importance, like activation values and gradients, ignore one or more of these terms and this results in intuitively undesirable attributions",
        "We show that neuron importances that satisfy these very natural axioms must be derived from a \u201cpath method\u201d, i.e., as a path integral of the gradients from the output to the hidden layer (Theorem 1)",
        "We provided a partial axiomatization of neuron importance"
    ],
    "key_statements": [
        "<strong>BACKGROUND AND MOTIVATION</strong><br/><br/>The problem of attributing a deep network\u2019s prediction to its input is well-studied; <a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\"><a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\">Simonyan et al (2013</a></a>); <a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\"><a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\">Shrikumar et al (2017</a></a>); <a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\"><a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\">Binder et al (2016</a></a>); <a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\"><a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\">Springenberg et al (2014</a></a>); <a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\"><a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a></a>); <a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\"><a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\">Sundararajan et al (2017</a></a>))",
        "Attributions help determine the influence of base features on the input",
        "We introduce our approach to the problem of neuron importance",
        "We find that other measures of neuron importance, like activation values and gradients, ignore one or more of these terms and this results in intuitively undesirable attributions",
        "We show that neuron importances that satisfy these very natural axioms must be derived from a \u201cpath method\u201d, i.e., as a path integral of the gradients from the output to the hidden layer (Theorem 1)",
        "Conductance naturally couples the path at the base features with the path at the hidden layer and we observe that it satisfies all of these axioms (Theorem 3)",
        "Theorem 2 shows that a naive approach that treats that hidden layer as the input, for instance applying Integrated Gradients directly at the hidden layer, causes the hidden layer and the base features to have inconsistent attributions",
        "Theorem 1 uses Axiom 1-3 to prove that any attribution method for neuron importance must be a path method",
        "We provided a partial axiomatization of neuron importance"
    ],
    "summary": [
        "<strong>BACKGROUND AND MOTIVATION</strong><br/><br/>The problem of attributing a deep network\u2019s prediction to its input is well-studied; <a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\"><a class=\"ref-link\" id=\"cSimonyan_et+al_2013_a\" href=\"#rSimonyan_et+al_2013_a\">Simonyan et al (2013</a></a>); <a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\"><a class=\"ref-link\" id=\"cShrikumar_et+al_2017_a\" href=\"#rShrikumar_et+al_2017_a\">Shrikumar et al (2017</a></a>); <a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\"><a class=\"ref-link\" id=\"cBinder_et+al_2016_a\" href=\"#rBinder_et+al_2016_a\">Binder et al (2016</a></a>); <a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\"><a class=\"ref-link\" id=\"cSpringenberg_et+al_2014_a\" href=\"#rSpringenberg_et+al_2014_a\">Springenberg et al (2014</a></a>); <a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\"><a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a></a>); <a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\"><a class=\"ref-link\" id=\"cSundararajan_et+al_2017_a\" href=\"#rSundararajan_et+al_2017_a\">Sundararajan et al (2017</a></a>)).",
        "We can use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs.",
        "We show that neuron importances that satisfy these very natural axioms must be derived from a \u201cpath method\u201d, i.e., as a path integral of the gradients from the output to the hidden layer (Theorem 1).",
        "Conductance naturally couples the path at the base features with the path at the hidden layer and we observe that it satisfies all of these axioms (Theorem 3).",
        "We use the importance scores computed via conductance and the other methods within a feature selection task.",
        "Theorem 2 shows that a naive approach that treats that hidden layer as the input, for instance applying Integrated Gradients directly at the hidden layer, causes the hidden layer and the base features to have inconsistent attributions.",
        "Completeness for Hidden Features: The attributions for the hidden layer add up to the difference of the function values at the input and the baseline F (x) \u2212 F (x ).",
        "No oblivious hidden-layer single path method satisfies partition consistency when the base layer attributions are computed by integrated gradients.",
        "Fix an oblivious hidden-layer path method \u03b3 that satisfies partition consistency.",
        "Theorem 1 uses Axiom 1-3 to prove that any attribution method for neuron importance must be a path method.",
        "We compare conductance to other neuron importance methods on a feature selection task (Section 5.2).",
        "We use conductance as a measure to identify influential filters in hidden layers in the Inception network.",
        "For the pre-softmax score for this label, we compute the conductance for each of the filters in each of the hidden layers using Equation 3.",
        "We compare conductance with other methods of neuron importance on a feature selection task.",
        "In Figure 4 and 3, we present exemplar images outside the training set that have high conductance for two filters that were identified as important for the labels meerkat and cheeseburger respectively.",
        "In Section 6.2, we compare conductance to other neuron importance methods by doing feature selection for a text classification task.",
        "To show that this is not the case, we run the network over around 4000 inputs from the Stanford Sentiment Treebank, noting examples that have a high conductance for any of these three filters.",
        "Interpretation For image networks, hidden-layer neurons correspond to higher-level features."
    ],
    "headline": "We introduce the notion of conductance to extend the notion of attribution to understanding the importance of hidden units",
    "reference_links": [
        {
            "id": "Alain_2016_a",
            "entry": "Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. https://arxiv.org/abs/1610.01644, 2016.",
            "url": "https://arxiv.org/abs/1610.01644",
            "arxiv_url": "https://arxiv.org/pdf/1610.01644"
        },
        {
            "id": "Bach_et+al_2015_a",
            "entry": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "Baehrens_et+al_2010_a",
            "entry": "David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert M\u00fcller. How to explain individual classification decisions. Journal of Machine Learning Research, pp. 1803\u20131831, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010"
        },
        {
            "id": "Binder_et+al_2016_a",
            "entry": "Alexander Binder, Gr\u00e9goire Montavon, Sebastian Bach, Klaus-Robert M\u00fcller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. CoRR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Bach%2C%20Sebastian%20M%C3%BCller%2C%20Klaus-Robert%20Layer-wise%20relevance%20propagation%20for%20neural%20networks%20with%20local%20renormalization%20layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Bach%2C%20Sebastian%20M%C3%BCller%2C%20Klaus-Robert%20Layer-wise%20relevance%20propagation%20for%20neural%20networks%20with%20local%20renormalization%20layers%202016"
        },
        {
            "id": "Datta_et+al_2018_a",
            "entry": "Anupam Datta, Matt Fredrikson, Klas Leino, Linyi Li, and Shayak Sen. Influence-directed explanations for deep convolutional networks, 2018. URL https://openreview.net/forum?id=SJPpHzW0-.",
            "url": "https://openreview.net/forum?id=SJPpHzW0-"
        },
        {
            "id": "Dosovitskiy_2015_a",
            "entry": "Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Inverting%20visual%20representations%20with%20convolutional%20networks%202015"
        },
        {
            "id": "Friedman_2004_a",
            "entry": "Eric J. Friedman. Paths and consistency in additive cost sharing. International Journal of Game Theory, 32(4):501\u2013518, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Eric%20J.%20Paths%20and%20consistency%20in%20additive%20cost%20sharing%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Eric%20J.%20Paths%20and%20consistency%20in%20additive%20cost%20sharing%202004"
        },
        {
            "id": "Garfinkel_1972_a",
            "entry": "R S Garfinkel and G L Nemhauser. Integer Programming. John Wiley, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garfinkel%2C%20R.S.%20Nemhauser%2C%20G.L.%20Integer%20Programming%201972"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Been Kim, Justin Gilmer, Martin Wattenberg, and Fernanda Vi\u00e9gas. TCAV: Relative concept importance testing with linear concept activation vectors, 2018. URL https://openreview.net/forum?id=S1viikbCW.",
            "url": "https://openreview.net/forum?id=S1viikbCW"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Yoon Kim. Convolutional neural networks for sentence classification. In ACL, 2014a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "Kim_2014_b",
            "entry": "Yoon Kim. Convolutional neural networks for sentence classification. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1746\u20131751. ACL, 2014b. ISBN 978-1-93728496-1. URL http://aclweb.org/anthology/D/D14/D14-1181.pdf.",
            "url": "http://aclweb.org/anthology/D/D14/D14-1181.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014-10-25"
        },
        {
            "id": "Le_et+al_2012_a",
            "entry": "Quoc V. Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML\u201912, pp. 507\u2013514, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Monga%2C%20Rajat%20Devin%2C%20Matthieu%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Monga%2C%20Rajat%20Devin%2C%20Matthieu%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202012"
        },
        {
            "id": "Lundberg_2017_a",
            "entry": "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 4768\u20134777. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "Mahendran_2015_a",
            "entry": "Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5188\u20135196, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015"
        },
        {
            "id": "Mordvintse_et+al_2015_a",
            "entry": "Alexander Mordvintse, Christopher Olah, and Mike Tyka. Inceptionism: Going Deeper into Neural Networks. https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html, 2015.",
            "url": "https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"
        },
        {
            "id": "Mordvintse_et+al_2017_a",
            "entry": "Alexander Mordvintse, Christopher Olah, and Ludwig Schubert. Feature Visualization. https://distill.pub/2017/feature-visualization/, 2017.",
            "url": "https://distill.pub/2017/feature-visualization/"
        },
        {
            "id": "Pasupat_2015_a",
            "entry": "Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. ACL, 2015. URL https://arxiv.org/pdf/1508.00305.pdf. See also https://github.com/ppasupat/WikiTableQuestions.",
            "url": "https://arxiv.org/pdf/1508.00305.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1508.00305"
        },
        {
            "id": "Doina_2017_a",
            "entry": "Doina Precup and Yee Whye Teh (eds.). Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, 2017. PMLR. URL http://jmlr.org/proceedings/papers/v70/.",
            "url": "http://jmlr.org/proceedings/papers/v70/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doina%20Precup%20and%20Yee%20Whye%20Teh%20eds%20Proceedings%20of%20the%2034th%20International%20Conference%20on%20Machine%20Learning%20ICML%202017%20Sydney%20NSW%20Australia%20611%20August%202017%20volume%2070%20of%20Proceedings%20of%20Machine%20Learning%20Research%202017%20PMLR%20URL%20httpjmlrorgproceedingspapersv70"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), pp. 211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015"
        },
        {
            "id": "Samek_et+al_2015_a",
            "entry": "Wojciech Samek, Alexander Binder, Gr\u00e9goire Montavon, Sebastian Bach, and Klaus-Robert M\u00fcller. Evaluating the visualization of what a deep neural network has learned. CoRR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samek%2C%20Wojciech%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Bach%2C%20Sebastian%20Evaluating%20the%20visualization%20of%20what%20a%20deep%20neural%20network%20has%20learned%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samek%2C%20Wojciech%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Bach%2C%20Sebastian%20Evaluating%20the%20visualization%20of%20what%20a%20deep%20neural%20network%20has%20learned%202015"
        },
        {
            "id": "Shrikumar_et+al_2017_a",
            "entry": "Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Precup & Teh (2017), pp. 3145\u20133153. URL http://proceedings.mlr.press/v70/shrikumar17a.html.",
            "url": "http://proceedings.mlr.press/v70/shrikumar17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Kundaje%2C%20Anshul%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017"
        },
        {
            "id": "Shrikumar_et+al_2018_a",
            "entry": "Avanti Shrikumar, Jocelin Su, and Anshul Knterundaje. Computationally efficient measures of internal neuron importance. CoRR, 2018. URL https://arxiv.org/abs/1807.09946.",
            "url": "https://arxiv.org/abs/1807.09946",
            "arxiv_url": "https://arxiv.org/pdf/1807.09946"
        },
        {
            "id": "Simonyan_et+al_2013_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202013"
        },
        {
            "id": "Springenberg_et+al_2014_a",
            "entry": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. CoRR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20Jost%20Tobias%20Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Riedmiller%2C%20Martin%20A.%20Striving%20for%20simplicity%202014"
        },
        {
            "id": "Sprumont_1999_a",
            "entry": "Yves Sprumont. Coherent Cost-Sharing Rules. Cahiers de recherche 9902, Universite de Montreal, Departement de sciences economiques, 1999. URL https://ideas.repec.org/p/mtl/montde/9902.html.",
            "url": "https://ideas.repec.org/p/mtl/montde/9902.html"
        },
        {
            "id": "Sundararajan_et+al_2017_a",
            "entry": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Precup & Teh (2017), pp. 3319\u20133328. URL http://proceedings.mlr.press/v70/sundararajan17a.html.",
            "url": "http://proceedings.mlr.press/v70/sundararajan17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017"
        },
        {
            "id": "Szegedy_et+al_2014_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CoRR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202014"
        },
        {
            "id": "Wang_1999_a",
            "entry": "YunTong Wang. The additivity and dummy axioms in the discrete cost sharing model. Economics Letters, 64(2):187 \u2013 192, 1999. ISSN 0165-1765. doi: https://doi.org/10.1016/S0165-1765(99)00080-4. URL http://www.sciencedirect.com/science/article/pii/S0165176599000804.",
            "crossref": "https://dx.doi.org/10.1016/S0165-1765(99)00080-4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/S0165-1765%2899%2900080-4"
        },
        {
            "id": "Yosinski_et+al_2015_a",
            "entry": "Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. CoRR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Mai%20Fuchs%2C%20Thomas%20Understanding%20neural%20networks%20through%20deep%20visualization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Mai%20Fuchs%2C%20Thomas%20Understanding%20neural%20networks%20through%20deep%20visualization%202015"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. ISSN 01628828. doi: 10.1109/TPAMI.2018.2858759.",
            "crossref": "https://dx.doi.org/10.1109/TPAMI.2018.2858759",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TPAMI.2018.2858759"
        }
    ]
}
