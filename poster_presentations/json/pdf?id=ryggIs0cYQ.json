{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DIFFERENTIABLE LEARNING-TO-NORMALIZE VIA SWITCHABLE NORMALIZATION",
        "author": "Ping Luo,\u2217 Jiamin Ren,\u2217 Zhanglin Peng, Ruimao Zhang, Jingyu Li, 1The Chinese University of Hong Kong 2SenseTime Research 3The University of Hong Kong",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryggIs0cYQ"
        },
        "abstract": "We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been released in https://github.com/switchablenorms/."
    },
    "keywords": [
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "automatic differentiation",
            "url": "https://en.wikipedia.org/wiki/automatic_differentiation"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        }
    ],
    "abbreviations": {
        "SN": "Switchable Normalization",
        "BN": "Batch Normalization",
        "IN": "Instance Normalization",
        "LN": "Layer Normalization",
        "GN": "group normalization",
        "RNNs": "recurrent neural networks",
        "WN": "weight normalization",
        "AD": "automatic differentiation",
        "BRN": "Batch Renormalization",
        "BKN": "Batch Kalman Normalization",
        "FPN": "Feature Pyramid Network",
        "I3D": "Inflated 3D"
    },
    "highlights": [
        "Normalization techniques are effective components in deep learning, advancing many research fields such as natural language processing, computer vision, and machine learning",
        "To address the above issues, we propose Switchable Normalization (SN), which combines three types of statistics estimated channel-wise, layer-wise, and minibatch-wise by using Instance Normalization, Layer Normalization, and Batch Normalization respectively",
        "(1) We introduce Switchable Normalization (SN), which is applicable in both CNNs and recurrent neural networks/LSTMs, and improves the other normalization techniques on many challenging benchmarks and tasks including image recognition in ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>), object detection in COCO (Lin et al, 2014), scene parsing in Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>) and ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>), artistic image stylization (<a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\">Johnson et al, 2016</a>), neural architecture search (<a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a>), and video recognition in Kinetics (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>)",
        "We present the main results of Switchable Normalization in multiple challenging problems and benchmarks, such as ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>), COCO (Lin et al, 2014), Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>), ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>), and Kinetics (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>), where the effectiveness of Switchable Normalization is demonstrated by comparing with existing normalization techniques.\n4.1",
        "Unlike (8, 4) that uses 8 GPUs, Batch Normalization achieves 76.5% in (1, 32), which is the best-performing result of Batch Normalization, the batch size to compute the gradients is as small as 32",
        "For small minibatch, Batch Kalman Normalization reported 76.1% that was evaluated in a micro-batch setting where 256 samples are used to compute gradients and 4 samples to estimate the statistics"
    ],
    "key_statements": [
        "Normalization techniques are effective components in deep learning, advancing many research fields such as natural language processing, computer vision, and machine learning",
        "Despite their great successes, existing practices often employed the same normalizer in all normalization layers of an entire network, rendering suboptimal performance",
        "To address the above issues, we propose Switchable Normalization (SN), which combines three types of statistics estimated channel-wise, layer-wise, and minibatch-wise by using Instance Normalization, Layer Normalization, and Batch Normalization respectively",
        "(1) We introduce Switchable Normalization (SN), which is applicable in both CNNs and recurrent neural networks/LSTMs, and improves the other normalization techniques on many challenging benchmarks and tasks including image recognition in ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>), object detection in COCO (Lin et al, 2014), scene parsing in Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>) and ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>), artistic image stylization (<a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\">Johnson et al, 2016</a>), neural architecture search (<a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a>), and video recognition in Kinetics (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>)",
        "We present the main results of Switchable Normalization in multiple challenging problems and benchmarks, such as ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>), COCO (Lin et al, 2014), Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>), ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>), and Kinetics (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>), where the effectiveness of Switchable Normalization is demonstrated by comparing with existing normalization techniques.\n4.1",
        "Unlike (8, 4) that uses 8 GPUs, Batch Normalization achieves 76.5% in (1, 32), which is the best-performing result of Batch Normalization, the batch size to compute the gradients is as small as 32",
        "For small minibatch, Batch Kalman Normalization reported 76.1% that was evaluated in a micro-batch setting where 256 samples are used to compute gradients and 4 samples to estimate the statistics",
        "We investigate using Switchable Normalization and group normalization in both the backbone and head",
        "We find that group normalization improves Batch Normalization\u2020+Switchable Normalization by only a small margin of 0.2",
        "We investigate Switchable Normalization in semantic image segmentation in ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>) and Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>)",
        "Fig.9 in Appendix compares the importance weights of Switchable Normalization in ResNet50 trained on both ADE20K and Cityscapes, showing that different datasets would choose different normalizers when the models and tasks are the same.\n4.4",
        "We evaluate video recognition in Kinetics dataset (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>), which has 400 action categories"
    ],
    "summary": [
        "Normalization techniques are effective components in deep learning, advancing many research fields such as natural language processing, computer vision, and machine learning.",
        "Using Eqn(4), the computational complexity of SN is O(N CHW ), which is comparable to previous work.",
        "Remark 1 simplifies SN in Eqn(3), enabling us to compare different normalizers geometrically by formulating them with respect to WN.",
        "Top-1 accuracies of ResNet50 on ImageNet by using batch average with 50k and all training samples are 76.90% and 76.92% respectively.",
        "We see that SN possesses comparable numbers of parameters and computations, as well as rich statistics.",
        "SN has richer statistics, the computational complexity to estimate them is comparable to previous methods, as shown in the second portion of Table 1.",
        "We present the main results of SN in multiple challenging problems and benchmarks, such as ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>), COCO (Lin et al, 2014), Cityscapes (<a class=\"ref-link\" id=\"cCordts_et+al_2016_a\" href=\"#rCordts_et+al_2016_a\">Cordts et al, 2016</a>), ADE20K (<a class=\"ref-link\" id=\"cZhou_et+al_2017_a\" href=\"#rZhou_et+al_2017_a\">Zhou et al, 2017</a>), and Kinetics (<a class=\"ref-link\" id=\"cKay_et+al_2017_a\" href=\"#rKay_et+al_2017_a\">Kay et al, 2017</a>), where the effectiveness of SN is demonstrated by comparing with existing normalization techniques.",
        "Unlike (8, 4) that uses 8 GPUs, BN achieves 76.5% in (1, 32), which is the best-performing result of BN, the batch size to compute the gradients is as small as 32.",
        "Fig.1 (a) and Fig.4 plot histograms to compare the importance weights of SN with respect to different tasks and batch sizes.",
        "We repeat training of ResNet50 several times in ImageNet, to show that when the network, task, batch setting and data are fixed, the importance weights of SN are not sensitive to the change of training protocols such as solver, parameter initialization, and learning rate decay.",
        "For small minibatch, BKN reported 76.1% that was evaluated in a micro-batch setting where 256 samples are used to compute gradients and 4 samples to estimate the statistics.",
        "As BN is not applicable in small minibatch, previous work (Ren et al, 2015; <a class=\"ref-link\" id=\"cLin_et+al_2016_a\" href=\"#rLin_et+al_2016_a\">Lin et al, 2016</a>; <a class=\"ref-link\" id=\"cHe_et+al_2017_a\" href=\"#rHe_et+al_2017_a\">He et al, 2017</a>) often freeze BN and turns it into a constant linear transformation layer, which performs no normalization.",
        "Fig.9 in Appendix compares the importance weights of SN in ResNet50 trained on both ADE20K and Cityscapes, showing that different datasets would choose different normalizers when the models and tasks are the same.",
        "This work has demonstrated SN in multiple tasks of CV such as recognition, detection, segmentation, image stylization, and neural architecture search, where SN outperforms previous normalizers without bells and whistles."
    ],
    "headline": "We address a learning-to-normalize problem by proposing Switchable Normalization, which learns to select different normalizers for different normalization layers of a deep neural network",
    "reference_links": [
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Carreira_2017_a",
            "entry": "Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. arXiv:1705.07750, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07750"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834\u2013848, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20Deeplab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20crfs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20Deeplab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20crfs%202018"
        },
        {
            "id": "Colson_et+al_2007_a",
            "entry": "Beno?t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of operations research, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Colson%2C%20Beno%3Ft%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Colson%2C%20Beno%3Ft%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007"
        },
        {
            "id": "Cordts_et+al_2016_a",
            "entry": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Desjardins_et+al_2015_a",
            "entry": "Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural networks. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guillaume%20Desjardins%20Karen%20Simonyan%20Razvan%20Pascanu%20and%20Koray%20Kavukcuoglu%20Natural%20neural%20networks%20NIPS%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guillaume%20Desjardins%20Karen%20Simonyan%20Razvan%20Pascanu%20and%20Koray%20Kavukcuoglu%20Natural%20neural%20networks%20NIPS%202015"
        },
        {
            "id": "Girshick_et+al_2018_a",
            "entry": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, and Kaiming He. Detectron. https://github.com/facebookresearch/detectron, 2018.",
            "url": "https://github.com/facebookresearch/detectron"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Priya Goyal, Piotr Dollr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "He_et+al_2017_a",
            "entry": "Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick. Mask r-cnn. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Gkioxari%2C%20Georgia%20Dollr%2C%20Piotr%20Girshick%2C%20Ross%20Mask%20r-cnn%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Gkioxari%2C%20Georgia%20Dollr%2C%20Piotr%20Girshick%2C%20Ross%20Mask%20r-cnn%202017"
        },
        {
            "id": "Huang_2017_a",
            "entry": "Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. arXiv:1703.06868, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06868"
        },
        {
            "id": "Ioffe_2017_a",
            "entry": "Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. arXiv:1702.03275, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03275"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and superresolution. arXiv:1603.08155, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08155"
        },
        {
            "id": "Kay_et+al_2017_a",
            "entry": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv:1705.06950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06950"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lin_et+al_2016_a",
            "entry": "Tsung-Yi Lin, Piotr Dollra, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. arXiv:1612.03144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03144"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv:1806.09055, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09055"
        },
        {
            "id": "Luo_2017_a",
            "entry": "Ping Luo. Eigennet: Towards fast and structural learning of deep neural networks. IJCAI, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Eigennet%3A%20Towards%20fast%20and%20structural%20learning%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Luo_2017_b",
            "entry": "Ping Luo. Learning deep architectures via generalized whitened neural networks. ICML, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Learning%20deep%20architectures%20via%20generalized%20whitened%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Ping%20Learning%20deep%20architectures%20via%20generalized%20whitened%20neural%20networks%202017"
        },
        {
            "id": "Luo_et+al_2018_a",
            "entry": "Ping Luo, Zhanglin Peng, Jiamin Ren, and Ruimao Zhang. Do normalization layers in a deep convnet really need to be distinct? arXiv:1811.07727, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.07727"
        },
        {
            "id": "Luo_et+al_2019_a",
            "entry": "Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization in batch normalization. ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Ping%20Wang%2C%20Xinjiang%20Shao%2C%20Wenqi%20Peng%2C%20Zhanglin%20Towards%20understanding%20regularization%20in%20batch%20normalization%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Ping%20Wang%2C%20Xinjiang%20Shao%2C%20Wenqi%20Peng%2C%20Zhanglin%20Towards%20understanding%20regularization%20in%20batch%20normalization%202019"
        },
        {
            "id": "Pan_et+al_2019_a",
            "entry": "Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In arXiv:1904.09739, 2019.",
            "arxiv_url": "https://arxiv.org/pdf/1904.09739"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large mini-batch object detector. arXiv:1711.07240, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07240"
        },
        {
            "id": "Perez_et+al_2017_a",
            "entry": "Ethan Perez, Harm de Vries, and Florian Strub. Learning visual reasoning without strong priors. In arXiv:1707.03017, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03017"
        },
        {
            "id": "Pham_et+al_2018_a",
            "entry": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. arXiv:1802.03268, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03268"
        },
        {
            "id": "Ren_et+al_2016_a",
            "entry": "Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, and Richard S. Zemel. Normalizing the normalizers: Comparing and extending network normalization schemes. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Mengye%20Liao%2C%20Renjie%20Urtasun%2C%20Raquel%20Sinz%2C%20Fabian%20H.%20Normalizing%20the%20normalizers%3A%20Comparing%20and%20extending%20network%20normalization%20schemes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Mengye%20Liao%2C%20Renjie%20Urtasun%2C%20Raquel%20Sinz%2C%20Fabian%20H.%20Normalizing%20the%20normalizers%3A%20Comparing%20and%20extending%20network%20normalization%20schemes%202016"
        },
        {
            "id": "Ren_et+al_0000_a",
            "entry": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv:1506.01497, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.01497"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv:1602.07868, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "Shao_et+al_2019_a",
            "entry": "Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo. Ssn: Learning sparse switchable normalization via sparsestmax. In CVPR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shao%2C%20Wenqi%20Meng%2C%20Tianjian%20Li%2C%20Jingyu%20Zhang%2C%20Ruimao%20Xiaogang%20Wang%2C%20and%20Ping%20Luo.%20Ssn%3A%20Learning%20sparse%20switchable%20normalization%20via%20sparsestmax%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shao%2C%20Wenqi%20Meng%2C%20Tianjian%20Li%2C%20Jingyu%20Zhang%2C%20Ruimao%20Xiaogang%20Wang%2C%20and%20Ping%20Luo.%20Ssn%3A%20Learning%20sparse%20switchable%20normalization%20via%20sparsestmax%202019"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Ulyanov_et+al_2016_a",
            "entry": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Guangrun Wang, Jiefeng Peng, Ping Luo, Xinjiang Wang, and Liang Lin. Batch kalman normalization: Towards training deep neural networks with micro-batches. NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Guangrun%20Peng%2C%20Jiefeng%20Luo%2C%20Ping%20Wang%2C%20Xinjiang%20Batch%20kalman%20normalization%3A%20Towards%20training%20deep%20neural%20networks%20with%20micro-batches%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Guangrun%20Peng%2C%20Jiefeng%20Luo%2C%20Ping%20Wang%2C%20Xinjiang%20Batch%20kalman%20normalization%3A%20Towards%20training%20deep%20neural%20networks%20with%20micro-batches%202018"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Wu_2018_a",
            "entry": "Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08494"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch, 2017.",
            "url": "https://github.com/jwyang/faster-rcnn.pytorch"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Hengshuang%20Shi%2C%20Jianping%20Qi%2C%20Xiaojuan%20Wang%2C%20Xiaogang%20Pyramid%20scene%20parsing%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Hengshuang%20Shi%2C%20Jianping%20Qi%2C%20Xiaojuan%20Wang%2C%20Xiaogang%20Pyramid%20scene%20parsing%20network%202017"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Zhao%2C%20Hang%20Puig%2C%20Xavier%20Fidler%2C%20Sanja%20Scene%20parsing%20through%20ADE20K%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Zhao%2C%20Hang%20Puig%2C%20Xavier%20Fidler%2C%20Sanja%20Scene%20parsing%20through%20ADE20K%20dataset%202017"
        }
    ]
}
