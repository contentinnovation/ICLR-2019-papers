{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "FROM HARD TO SOFT: UNDERSTANDING DEEP NETWORK NONLINEARITIES VIA VECTOR QUANTIZATION AND STATISTICAL INFERENCE",
        "author": "Randall Balestriero & Richard G. Baraniuk Department of Electrical and Computer Engineering Rice University Houston, TX 77005, USA randallbalestriero@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Syxt2jC5FX"
        },
        "abstract": "Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the role played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as max-affine spline operators (MASOs) that have an elegant link to vector quantization (VQ) and K-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs). We show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural \u201chard\u201d VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding \u201csoft\u201d VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a \u03b2-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a \u03b2-VQ DN nonlinearity is the swish nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "affine transformation",
            "url": "https://en.wikipedia.org/wiki/affine_transformation"
        },
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "hyperbolic tangent",
            "url": "https://en.wikipedia.org/wiki/hyperbolic_tangent"
        },
        {
            "term": "vector quantization",
            "url": "https://en.wikipedia.org/wiki/vector_quantization"
        },
        {
            "term": "absolute value",
            "url": "https://en.wikipedia.org/wiki/absolute_value"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "Deep (neural) networks",
            "url": "https://en.wikipedia.org/wiki/Deep_neural_networks"
        }
    ],
    "abbreviations": {
        "DN": "deep (neural) network",
        "MASOs": "max-affine spline operators",
        "VQ": "vector quantization",
        "GMMs": "Gaussian Mixture Models",
        "DNs": "Deep (neural) networks",
        "SMASO": "Soft MASO",
        "MAP": "maximum a posteriori",
        "HVQ": "Hard VQ",
        "SVQ": "Soft VQ"
    },
    "highlights": [
        "Deep networks (DNs) have recently come to the fore in a wide range of machine learning tasks, from regression to classification and beyond",
        "Progress has been made on understanding the role played by piecewise affine and convex nonlinearities like the ReLU, leaky ReLU, and absolute value activations and downsampling operations like max-, average-, and channel-pooling <a class=\"ref-link\" id=\"cBalestriero_2018_a\" href=\"#rBalestriero_2018_a\">Balestriero & Baraniuk (2018a</a>;b)",
        "Contributions. We address this gap in the deep network theory by developing a new framework that unifies a wide range of deep network nonlinearities and inspires and supports the development of new ones",
        "We summarize our primary contributions as follows: Contribution 1: We leverage the well-understood relationship between vector quantization, K-means, and Gaussian Mixture Models to propose the Soft max-affine spline operators (SMASO) model, a probabilistic Gaussian Mixture Models that extends the concept of a deterministic max-affine spline operators deep network layer",
        "We develop a method for soft maximum a posteriori inference of the vector quantization parameters based on the probability that the layer input belongs to a given vector quantization region",
        "We find a home for the sigmoid, hyperbolic tangent, and softmax in the framework as a new kind of deep network layer where the max-affine spline operators output is the vector quantization probability"
    ],
    "key_statements": [
        "Deep networks (DNs) have recently come to the fore in a wide range of machine learning tasks, from regression to classification and beyond",
        "Progress has been made on understanding the role played by piecewise affine and convex nonlinearities like the ReLU, leaky ReLU, and absolute value activations and downsampling operations like max-, average-, and channel-pooling <a class=\"ref-link\" id=\"cBalestriero_2018_a\" href=\"#rBalestriero_2018_a\">Balestriero & Baraniuk (2018a</a>;b)",
        "Contributions. We address this gap in the deep network theory by developing a new framework that unifies a wide range of deep network nonlinearities and inspires and supports the development of new ones",
        "We summarize our primary contributions as follows: Contribution 1: We leverage the well-understood relationship between vector quantization, K-means, and Gaussian Mixture Models to propose the Soft max-affine spline operators (SMASO) model, a probabilistic Gaussian Mixture Models that extends the concept of a deterministic max-affine spline operators deep network layer",
        "We develop a method for soft maximum a posteriori inference of the vector quantization parameters based on the probability that the layer input belongs to a given vector quantization region",
        "We find a home for the sigmoid, hyperbolic tangent, and softmax in the framework as a new kind of deep network layer where the max-affine spline operators output is the vector quantization probability",
        "We show that the \u03b2-vector quantization version of the hard ReLU activation is the swish nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc through experimentation <a class=\"ref-link\" id=\"cRamachandran_et+al_2017_a\" href=\"#rRamachandran_et+al_2017_a\">Ramachandran et al (2017</a>)",
        "Since the factorial aspect of the new model would make na\u0131ve vector quantization inference exponentially computationally complex, we develop a simple sufficient condition under which a we can achieve efficient, tractable, jointly optimal vector quantization inference",
        "K independent max-affine splines <a class=\"ref-link\" id=\"cMagnani_2009_a\" href=\"#rMagnani_2009_a\">Magnani & Boyd (2009</a>); <a class=\"ref-link\" id=\"cHannah_2013_a\" href=\"#rHannah_2013_a\">Hannah & Dunson (2013</a>), with each spline formed from R piecewise affine and convex mappings",
        "We used the swish activation, which we showed to be a \u03b2-vector quantization nonlinearity in Section 4",
        "Orthogonal deep network filters will enable new analysis techniques and deep network probing methods, since from a signal processing point of view problems such as denoising, reconstruction, compression have been extensively studied in terms of orthogonal filters"
    ],
    "summary": [
        "Deep networks (DNs) have recently come to the fore in a wide range of machine learning tasks, from regression to classification and beyond.",
        "Under the SMASO model, hard maximum a posteriori (MAP) inference of the VQ parameters corresponds to conventional deterministic MASO DN operations that involve piecewise affine and convex functions, such as fully connected and convolution matrix multiplication; ReLU, leaky-ReLU, and absolute value activation; and max-, average-, and channelpooling.",
        "We develop a method for soft MAP inference of the VQ parameters based on the probability that the layer input belongs to a given VQ region.",
        "Switching from HVQ to SVQ MASO inference recovers several classical and powerful nonlinearities and provides an avenue to derive completely new ones.",
        "Given a set of MASO parameters A( ), B( ) for calculating the layer- output of a DN via (1), we can derive two distinctly different DNs: one based on the HVQ inference of (5) and one based on the SVQ inference of (6).",
        "The MASO parameters A( ), B( ) that induce the ReLU activation under HVQ induce the sigmoid gated linear unit <a class=\"ref-link\" id=\"cElfwing_et+al_2018_a\" href=\"#rElfwing_et+al_2018_a\">Elfwing et al (2018</a>) under SVQ.",
        "The MASO parameters A( ), B( ) that induce the max-pooling nonlinearity under HVQ induce softmax-pooling <a class=\"ref-link\" id=\"cBoureau_et+al_2010_a\" href=\"#rBoureau_et+al_2010_a\">Boureau et al (2010</a>) under SVQ.",
        "Consider a new soft DN layer whose unit output [z( )]k is not the piecewise affine spline of (2) but rather the probability [z( )]k = p([t( )]k = 1|z( \u22121)) that the input z( ) falls into each VQ region.",
        "The MASO parameters A( ), B( ) that induce the ReLU activation under HVQ induce the sigmoid activation in the corresponding soft DN layer.3",
        "The MASO parameters A( ), B( ) that induce a fully-connected-pooling layer under HVQ equal to the number of classes C) induce the softmax nonlinearity in the corresponding soft DN layer.",
        "Orthogonality is achieved in a convolution layer composed with activation or pooling) when the rows of C( ) are either non-overlapping or properly apodized; see Appendix E.4 for the details plus the proof of the following result.",
        "Replacing the entropy penalty in the (7) and (8) with a different penalty will create entirely new classes of nonlinearities that inherit the rich analytical properties of MASO DNs. Third, orthogonal DN filters will enable new analysis techniques and DN probing methods, since from a signal processing point of view problems such as denoising, reconstruction, compression have been extensively studied in terms of orthogonal filters."
    ],
    "headline": "This paper extends the max-affine spline operators framework to these and an infinitely large class of new nonlinearities by linking deterministic max-affine spline operators with probabilistic Gaussian Mixture Models",
    "reference_links": [
        {
            "id": "Agarap_2018_a",
            "entry": "A. F. Agarap. Deep learning using rectified linear units (ReLU). arXiv preprint arXiv:1803.08375, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08375"
        },
        {
            "id": "Balestriero_0000_a",
            "entry": "R. Balestriero and R. Baraniuk. Mad max: Affine spline insights into deep learning. arXiv preprint arXiv:1805.06576, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06576"
        },
        {
            "id": "Balestriero_2018_a",
            "entry": "R. Balestriero and R. G. Baraniuk. A spline theory of deep networks. In Proc. Int. Conf. Mach. Learn., volume 80, pp. 374\u2013383, Jul. 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balestriero%2C%20R.%20Baraniuk%2C%20R.G.%20A%20spline%20theory%20of%20deep%20networks%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balestriero%2C%20R.%20Baraniuk%2C%20R.G.%20A%20spline%20theory%20of%20deep%20networks%202018-07"
        },
        {
            "id": "Biernacki_et+al_2000_a",
            "entry": "C. Biernacki, G. Celeux, and G. Govaert. Assessing a mixture model for clustering with the integrated completed likelihood. IEEE Trans. Pattern Anal. Mach. Intell., 22(7):719\u2013725, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biernacki%2C%20C.%20Celeux%2C%20G.%20Govaert%2C%20G.%20Assessing%20a%20mixture%20model%20for%20clustering%20with%20the%20integrated%20completed%20likelihood%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biernacki%2C%20C.%20Celeux%2C%20G.%20Govaert%2C%20G.%20Assessing%20a%20mixture%20model%20for%20clustering%20with%20the%20integrated%20completed%20likelihood%202000"
        },
        {
            "id": "Bishop_2006_a",
            "entry": "C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Pattern%20Recognition%20and%20Machine%20Learning%202006"
        },
        {
            "id": "Boureau_et+al_2010_a",
            "entry": "Y. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis of feature pooling in visual recognition. In Proc. Int. Conf. Mach. Learn., pp. 111\u2013118, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boureau%2C%20Y.%20Ponce%2C%20J.%20LeCun%2C%20Y.%20A%20theoretical%20analysis%20of%20feature%20pooling%20in%20visual%20recognition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boureau%2C%20Y.%20Ponce%2C%20J.%20LeCun%2C%20Y.%20A%20theoretical%20analysis%20of%20feature%20pooling%20in%20visual%20recognition%202010"
        },
        {
            "id": "Brock_et+al_2016_a",
            "entry": "A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07093"
        },
        {
            "id": "Daniel_et+al_1976_a",
            "entry": "J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization. Math. Comput., 30(136):772\u2013795, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20J.W.%20Gragg%2C%20W.B.%20Kaufman%2C%20L.%20Stewart%2C%20G.W.%20Reorthogonalization%20and%20stable%20algorithms%20for%20updating%20the%20Gram-Schmidt%20QR%20factorization%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20J.W.%20Gragg%2C%20W.B.%20Kaufman%2C%20L.%20Stewart%2C%20G.W.%20Reorthogonalization%20and%20stable%20algorithms%20for%20updating%20the%20Gram-Schmidt%20QR%20factorization%201976"
        },
        {
            "id": "Elfwing_et+al_2018_a",
            "entry": "S. Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Netw., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elfwing%2C%20S.%20Uchibe%2C%20E.%20Doya%2C%20K.%20Sigmoid-weighted%20linear%20units%20for%20neural%20network%20function%20approximation%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elfwing%2C%20S.%20Uchibe%2C%20E.%20Doya%2C%20K.%20Sigmoid-weighted%20linear%20units%20for%20neural%20network%20function%20approximation%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "Gersho_2012_a",
            "entry": "A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Springer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gersho%2C%20A.%20Gray%2C%20R.M.%20Vector%20Quantization%20and%20Signal%20Compression%202012"
        },
        {
            "id": "Ghahramani_1995_a",
            "entry": "Zoubin Ghahramani. Factorial learning and the em algorithm. In Advances in neural information processing systems, pp. 617\u2013624, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Factorial%20learning%20and%20the%20em%20algorithm%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Zoubin%20Factorial%20learning%20and%20the%20em%20algorithm%201995"
        },
        {
            "id": "Ghahramani_1996_a",
            "entry": "Zoubin Ghahramani and Michael I Jordan. Factorial hidden Markov models. In Advances in Neural Information Processing Systems, pp. 472\u2013478, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Factorial%20hidden%20Markov%20models%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Factorial%20hidden%20Markov%20models%201996"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th Int. Conf. AI Statist., volume 9, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning, volume 1. MIT Press, 2016. http://www.deeplearningbook.org.",
            "url": "http://www.deeplearningbook.org"
        },
        {
            "id": "Hannah_2013_a",
            "entry": "L. A. Hannah and D. B. Dunson. Multivariate convex regression with adaptive partitioning. J. Mach. Learn. Res., 14(1):3261\u20133294, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hannah%2C%20L.A.%20Dunson%2C%20D.B.%20Multivariate%20convex%20regression%20with%20adaptive%20partitioning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hannah%2C%20L.A.%20Dunson%2C%20D.B.%20Multivariate%20convex%20regression%20with%20adaptive%20partitioning%202013"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "L. Huang, X. Liu, B. Lang, A. W. Yu, Y. Wang, and B. Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv preprint arXiv:1709.06079, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06079"
        },
        {
            "id": "Magnani_2009_a",
            "entry": "A. Magnani and S. P. Boyd. Convex piecewise-linear fitting. Optim. Eng., 10(1):1\u201317, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009"
        },
        {
            "id": "Manning_2003_a",
            "entry": "Christopher Manning and Dan Klein. Optimization, maxent models, and conditional estimation without magic. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials-Volume 5, pp. 8\u20138. Association for Computational Linguistics, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manning%2C%20Christopher%20Klein%2C%20Dan%20Optimization%2C%20maxent%20models%2C%20and%20conditional%20estimation%20without%20magic%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manning%2C%20Christopher%20Klein%2C%20Dan%20Optimization%2C%20maxent%20models%2C%20and%20conditional%20estimation%20without%20magic%202003"
        },
        {
            "id": "Mount_2011_a",
            "entry": "John Mount. The equivalence of logistic regression and maximum entropy models. URL: http://www.winvector.com/dfiles/LogisticRegressionMaxEnt.pdf, 2011.",
            "url": "http://www.winvector.com/dfiles/LogisticRegressionMaxEnt.pdf"
        },
        {
            "id": "Nasrabadi_1988_a",
            "entry": "N. M. Nasrabadi and R. A. King. Image coding using vector quantization: A review. IEEE Trans. Commun., 36(8):957\u2013971, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nasrabadi%2C%20N.M.%20King%2C%20R.A.%20Image%20coding%20using%20vector%20quantization%3A%20A%20review%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nasrabadi%2C%20N.M.%20King%2C%20R.A.%20Image%20coding%20using%20vector%20quantization%3A%20A%20review%201988"
        },
        {
            "id": "Ramachandran_et+al_2017_a",
            "entry": "P. Ramachandran, B. Zoph, and Q. Le. Searching for activation functions. arXiv:1710.05941v2, Oct. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05941v2"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "R. K. Srivastava, J. Masci, F. Gomez, and J. Schmidhuber. Understanding locally competitive networks. arXiv preprint arXiv:1410.1165, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.1165"
        },
        {
            "id": "Trottier_et+al_2017_a",
            "entry": "L. Trottier, P. Gigu, and B. Chaib-draa. Parametric exponential linear unit for deep convolutional neural networks. pp. 207\u2013214. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trottier%2C%20L.%20Gigu%2C%20P.%20Chaib-draa%2C%20B.%20Parametric%20exponential%20linear%20unit%20for%20deep%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "Weisstein_2002_a",
            "entry": "E. W. Weisstein. CRC Concise Encyclopedia of Mathematics. CRC press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weisstein%2C%20E.W.%20CRC%20Concise%20Encyclopedia%20of%20Mathematics%202002"
        },
        {
            "id": "Zemel_1994_a",
            "entry": "Richard S Zemel. A minimum description length framework for unsupervised learning. Citeseer, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20Richard%20S.%20A%20minimum%20description%20length%20framework%20for%20unsupervised%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20Richard%20S.%20A%20minimum%20description%20length%20framework%20for%20unsupervised%20learning%201994"
        }
    ]
}
