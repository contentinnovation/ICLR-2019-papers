{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE COMPARATIVE POWER OF RELU NETWORKS AND POLYNOMIAL KERNELS IN THE PRESENCE OF SPARSE LATENT STRUCTURE",
        "author": "Frederic Koehler, Department of Mathematics Massachusetts Institute of Technology",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJgTTjA9tX"
        },
        "journal": "Email",
        "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worstcase setting: e.g. characterizing the best L1 or L\u221e approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used. However, in typical applications we expect data to be high dimensional, but structured \u2013 so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now. With this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting."
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "rational functions",
            "url": "https://en.wikipedia.org/wiki/rational_functions"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "universal approximator",
            "url": "https://en.wikipedia.org/wiki/universal_approximator"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "The concept of representational power has been always of great interest in machine learning",
        "We prove the following theorem, which shows that small 2-layer ReLU networks can achieve an almost optimal statistical rate",
        "The lower bound of the previous section leaves open the possibility that polynomials of degree O) still do not suffice to perform sparse regression and solve our inference problem; it is a well-known fact) that to approximate a single ReLU to -closeness in infinity norm in [\u22121, 1] requires polynomials of degree poly(1/ ); this follows from standard facts in approximation theory (<a class=\"ref-link\" id=\"cDevore_1993_a\" href=\"#rDevore_1993_a\">DeVore and Lorentz, 1993</a>) since ReLU is not a smooth function. Proceeding with this \u201cworst-case\u201d way of thinking: our upper bound follows by designing a polynomial approximation to ReLU into our neural network construction; since estimates for Y typically accumulate error from estimating each of the m coordinates of Z, to guarantee accurate reconstruction we would need m t\u221ao be small",
        "We considered the problem of providing representation lower and upper bounds for different classes of universal approximators in a natural statistical setup that exhibits sparse latent structure",
        "We hope this will inspire researchers to move beyond the worst-case setup when considering the representational power of different function classes.\n10Small technical remark: our upper bound (Theorem 4.3) is with-high-probability and not in expectation, so we drop the outlier 1 percent of test results which had largest error"
    ],
    "key_statements": [
        "The concept of representational power has been always of great interest in machine learning",
        "We prove the following theorem, which shows that small 2-layer ReLU networks can achieve an almost optimal statistical rate",
        "The lower bound of the previous section leaves open the possibility that polynomials of degree O) still do not suffice to perform sparse regression and solve our inference problem; it is a well-known fact) that to approximate a single ReLU to -closeness in infinity norm in [\u22121, 1] requires polynomials of degree poly(1/ ); this follows from standard facts in approximation theory (<a class=\"ref-link\" id=\"cDevore_1993_a\" href=\"#rDevore_1993_a\">DeVore and Lorentz, 1993</a>) since ReLU is not a smooth function. Proceeding with this \u201cworst-case\u201d way of thinking: our upper bound follows by designing a polynomial approximation to ReLU into our neural network construction; since estimates for Y typically accumulate error from estimating each of the m coordinates of Z, to guarantee accurate reconstruction we would need m t\u221ao be small",
        "We considered the problem of providing representation lower and upper bounds for different classes of universal approximators in a natural statistical setup that exhibits sparse latent structure",
        "We hope this will inspire researchers to move beyond the worst-case setup when considering the representational power of different function classes.\n10Small technical remark: our upper bound (Theorem 4.3) is with-high-probability and not in expectation, so we drop the outlier 1 percent of test results which had largest error"
    ],
    "summary": [
        "The concept of representational power has been always of great interest in machine learning.",
        "We prove the following theorem, which shows that small 2-layer ReLU networks can achieve an almost optimal statistical rate.",
        "Let\u2019s introduce the notation \u03c1\u2297\u03c4 m to denote the map given by applying \u03c1\u03c4 coordinate-wise to a vector in Rm. Consider the following estimator, corresponding to a 2-layer neural network: ZNN := \u03c1\u2297\u03c4 m(A X) YNN := w, ZNN We can prove the following result for the estimator: Theorem 4.1 (2-layer ReLU).",
        "The lower bound of the previous section leaves open the possibility that polynomials of degree O) still do not suffice to perform sparse regression and solve our inference problem; it is a well-known fact) that to approximate a single ReLU to -closeness in infinity norm in [\u22121, 1] requires polynomials of degree poly(1/ ); this follows from standard facts in approximation theory (<a class=\"ref-link\" id=\"cDevore_1993_a\" href=\"#rDevore_1993_a\">DeVore and Lorentz, 1993</a>) since ReLU is not a smooth function.",
        "Proceeding with this \u201cworst-case\u201d way of thinking: our upper bound follows by designing a polynomial approximation to ReLU into our neural network construction; since estimates for Y typically accumulate error from estimating each of the m coordinates of Z, to guarantee accurate reconstruction we would need m t\u221ao be small.",
        "Once we have reduced to considering estimators with decoupled structure, it becomes feasible to analyze the performance of all possible low degree polynomials using a bias-variance calculation in a carefully chosen basis.",
        "Along with the structural Lemma 5.1 to perform a bias-variance tradeoff analysis of any predictor: namely, we show (1) If the Fourier coefficients |f (n)| are large, the estimator will be very sensitive to noise.",
        "Note that when f is sufficiently high-degree, it can effectively take advantage of the difference in scales between the noise and the signal to achieve both low bias and low variance simultaneously: see the following upper bound section for details.",
        "It\u2019s a result from classical approximation theory that no low-degree polynomial is close to the ReLU function on all of [\u22121, 1].",
        "We considered the problem of providing representation lower and upper bounds for different classes of universal approximators in a natural statistical setup that exhibits sparse latent structure.",
        "10Small technical remark: our upper bound (Theorem 4.3) is with-high-probability and not in expectation, so we drop the outlier 1 percent of test results which had largest error."
    ],
    "headline": "We have only proven the lower bound in the case \u03bc = 0: this is the easiest setting for algorithms, so this makes the lower bounds the strongest. 4.1",
    "reference_links": [
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Beck_2009_a",
            "entry": "Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "Chang_et+al_2010_a",
            "entry": "Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research, 11(Apr):1471\u20131490, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Yin-Wen%20Hsieh%2C%20Cho-Jui%20Chang%2C%20Kai-Wei%20Ringgaard%2C%20Michael%20Training%20and%20testing%20low-degree%20polynomial%20data%20mappings%20via%20linear%20svm%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Yin-Wen%20Hsieh%2C%20Cho-Jui%20Chang%2C%20Kai-Wei%20Ringgaard%2C%20Michael%20Training%20and%20testing%20low-degree%20polynomial%20data%20mappings%20via%20linear%20svm%202010"
        },
        {
            "id": "Devore_1993_a",
            "entry": "Ronald A DeVore and George G Lorentz. Constructive approximation, volume 303. Springer Science & Business Media, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronald%20A%20DeVore%20and%20George%20G%20Lorentz%20Constructive%20approximation%20volume%20303%20Springer%20Science%20%20Business%20Media%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronald%20A%20DeVore%20and%20George%20G%20Lorentz%20Constructive%20approximation%20volume%20303%20Springer%20Science%20%20Business%20Media%201993"
        },
        {
            "id": "Diakonikolas_et+al_2010_a",
            "entry": "Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 11\u201320. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20Ilias%20Kane%2C%20Daniel%20M.%20Nelson%2C%20Jelani%20Bounded%20independence%20fools%20degree-2%20threshold%20functions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20Ilias%20Kane%2C%20Daniel%20M.%20Nelson%2C%20Jelani%20Bounded%20independence%20fools%20degree-2%20threshold%20functions%202010"
        },
        {
            "id": "Donoho_2006_a",
            "entry": "David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289\u20131306, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Compressed%20sensing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Compressed%20sensing%202006"
        },
        {
            "id": "Eldan_2016_a",
            "entry": "Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907\u2013940, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225\u20131234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "Livni_et+al_2014_a",
            "entry": "Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855\u2013863, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "Moitra_2018_a",
            "entry": "Ankur Moitra. Algorithmic aspects of machine learning. Preprint. Cambridge University Press (to appear), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moitra%2C%20Ankur%20Algorithmic%20aspects%20of%20machine%20learning%202018"
        },
        {
            "id": "Newman_1964_a",
            "entry": "Donald J Newman et al. Rational approximation to |x|. The Michigan Mathematical Journal, 11(1): 11\u201314, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newman%2C%20Donald%20J.%20Rational%20approximation%20to%20%7Cx%7C%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newman%2C%20Donald%20J.%20Rational%20approximation%20to%20%7Cx%7C%201964"
        },
        {
            "id": "Poggio_et+al_2017_a",
            "entry": "Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503\u2013519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017"
        },
        {
            "id": "Rigollet_2017_a",
            "entry": "Phillippe Rigollet. High-dimensional statistics. Lecture notes (MIT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Phillippe%20High-dimensional%20statistics.%20Lecture%20notes%202017"
        },
        {
            "id": "Safran_2017_a",
            "entry": "Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In International Conference on Machine Learning, pages 2979\u20132987, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Depth-width%20tradeoffs%20in%20approximating%20natural%20functions%20with%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Depth-width%20tradeoffs%20in%20approximating%20natural%20functions%20with%20neural%20networks%202017"
        },
        {
            "id": "Shalev-Shwartz_et+al_2011_a",
            "entry": "Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623\u20131646, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Learning%20kernel-based%20halfspaces%20with%20the%200-1%20loss%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Learning%20kernel-based%20halfspaces%20with%20the%200-1%20loss%202011"
        },
        {
            "id": "Telgarsky_2016_a",
            "entry": "Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, pages 1517\u20131539, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Telgarsky%2C%20Matus%20Benefits%20of%20depth%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Telgarsky%2C%20Matus%20Benefits%20of%20depth%20in%20neural%20networks%202016"
        },
        {
            "id": "Telgarsky_2017_a",
            "entry": "Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine Learning, pages 3387\u20133393, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matus%20Telgarsky%20Neural%20networks%20and%20rational%20functions%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2033873393%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matus%20Telgarsky%20Neural%20networks%20and%20rational%20functions%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2033873393%202017"
        },
        {
            "id": "Vershynin_2018_a",
            "entry": "Roman Vershynin. High-Dimensional Probability. Cambridge University Press (to appear), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20High-Dimensional%20Probability%202018"
        },
        {
            "id": "Yarotsky_2017_a",
            "entry": "Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pages 993\u20131001, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20l1-regularized%20neural%20networks%20are%20improperly%20learnable%20in%20polynomial%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20l1-regularized%20neural%20networks%20are%20improperly%20learnable%20in%20polynomial%20time%202016"
        },
        {
            "id": "We_2017_a",
            "entry": "We will first prove a bound on the error of the soft-thresholding estimator ZNN (Lemma A.2), which corresponds to the hidden layer of the neural network: this is essentially a standard fact in high-dimensional statistics (see reference text (Rigollet, 2017)). The idea is that the soft thresholding will correctly zero-out most of the coordinates in the support while adding only a small additional error to the coordinates outside the support.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20will%20first%20prove%20a%20bound%20on%20the%20error%20of%20the%20softthresholding%20estimator%20ZNN%20Lemma%20A2%20which%20corresponds%20to%20the%20hidden%20layer%20of%20the%20neural%20network%20this%20is%20essentially%20a%20standard%20fact%20in%20highdimensional%20statistics%20see%20reference%20text%20Rigollet%202017%20The%20idea%20is%20that%20the%20soft%20thresholding%20will%20correctly%20zeroout%20most%20of%20the%20coordinates%20in%20the%20support%20while%20adding%20only%20a%20small%20additional%20error%20to%20the%20coordinates%20outside%20the%20support",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20will%20first%20prove%20a%20bound%20on%20the%20error%20of%20the%20softthresholding%20estimator%20ZNN%20Lemma%20A2%20which%20corresponds%20to%20the%20hidden%20layer%20of%20the%20neural%20network%20this%20is%20essentially%20a%20standard%20fact%20in%20highdimensional%20statistics%20see%20reference%20text%20Rigollet%202017%20The%20idea%20is%20that%20the%20soft%20thresholding%20will%20correctly%20zeroout%20most%20of%20the%20coordinates%20in%20the%20support%20while%20adding%20only%20a%20small%20additional%20error%20to%20the%20coordinates%20outside%20the%20support"
        }
    ]
}
