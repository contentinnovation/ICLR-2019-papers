{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "TWO-TIMESCALE NETWORKS FOR NONLINEAR VALUE FUNCTION APPROXIMATION",
        "author": "Wesley Chung, Somjit Nath, Ajin George Joseph and Martha White Department of Computing Science University of Alberta",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJleN20qK7"
        },
        "abstract": "A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control. Many of the algorithms for learning values, however, are designed for linear function approximation\u2014with a fixed basis or fixed representation. Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning. In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates. We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation. We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control."
    },
    "keywords": [
        {
            "term": "nonlinear function",
            "url": "https://en.wikipedia.org/wiki/nonlinear_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "stochastic approximation",
            "url": "https://en.wikipedia.org/wiki/stochastic_approximation"
        },
        {
            "term": "temporal difference",
            "url": "https://en.wikipedia.org/wiki/temporal_difference"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "temporal difference learning",
            "url": "https://en.wikipedia.org/wiki/temporal_difference_learning"
        },
        {
            "term": "linear function",
            "url": "https://en.wikipedia.org/wiki/linear_function"
        },
        {
            "term": "function approximation",
            "url": "https://en.wikipedia.org/wiki/function_approximation"
        }
    ],
    "abbreviations": {
        "TTN": "two-timescale network",
        "TD": "temporal difference",
        "MDP": "Markov Decision Process",
        "TTNs": "Two-timescale Networks",
        "MSPBE": "mean-squared projected Bellman error",
        "MSTDE": "mean-squared TD error",
        "MSBE": "mean-squared Bellman error",
        "MSRE": "mean-squared return error",
        "RMSVE": "root-mean-squared value error",
        "FQI": "fitted Q-iteration"
    },
    "highlights": [
        "Value function approximation\u2014estimating the expected returns from states for a policy\u2014is heavily reliant on the quality of the representation of state",
        "We show that two-timescale network are a promising direction for nonlinear function approximation, allowing us to leverage linear algorithms while retaining the flexibility of nonlinear function approximators",
        "We show empirically that two-timescale network are effective compared to other nonlinear value function approximations and that they can exploit several benefits of linear value approximations algorithms",
        "We demonstrate that two-timescale network can be effective for control with neural networks, enabling use of fitted Q-iteration within two-timescale network as an alternative to target networks",
        "We proposed Two-timescale Networks as a new strategy for policy evaluation with nonlinear function approximation",
        "As opposed to many other algorithms derived for nonlinear value function approximation, two-timescale network are intentionally designed to be simple to promote ease-of-use"
    ],
    "key_statements": [
        "Value function approximation\u2014estimating the expected returns from states for a policy\u2014is heavily reliant on the quality of the representation of state",
        "We show that these Two-timescale Networks (TTNs) converge, because the features change on a sufficiently slow scale, so that they are effectively fixed for the fast linear value function estimator",
        "We show that two-timescale network are a promising direction for nonlinear function approximation, allowing us to leverage linear algorithms while retaining the flexibility of nonlinear function approximators",
        "We show that two-timescale network converge, despite the fact that a linear algorithm is used with a changing representation",
        "We show empirically that two-timescale network are effective compared to other nonlinear value function approximations and that they can exploit several benefits of linear value approximations algorithms",
        "We demonstrate that two-timescale network can be effective for control with neural networks, enabling use of fitted Q-iteration within two-timescale network as an alternative to target networks",
        "We first introduce Two-timescale Networks (TTNs), and describe different surrogate objectives that can be used in two-timescale network",
        "We prove that the two-timescale network converge asymptotically to the stable equilibria of a projected ODE which completely captures the mean dynamics of the algorithm",
        "We investigate the performance of two-timescale network versus a variety of other nonlinear policy evaluation algorithms, as well as the impact of choices within two-timescale network",
        "We aim to answer (a) is it beneficial to optimize the mean-squared projected Bellman error to obtain value estimates, rather than using value estimates from surrogate losses like the mean-squared TD error; (b) do two-timescale network provide gains over other nonlinear policy evaluation algorithms; and (c) can two-timescale network benefit from the variety of options in linear algorithms, including leastsquares approaches, eligibility traces and different policy evaluation algorithms",
        "The results overall indicate that two-timescale network can benefit from the ability to use different linear policy evaluation algorithms and traces, in particular from the use of least-squares methods as shown in Figure 4 for Puddle World and Catcher",
        "We investigated sensitivity to \u03bb, and found that most of the two-timescale network variants benefit from a nonzero \u03bb value and, in many cases, the best setting is high, near 1",
        "We proposed Two-timescale Networks as a new strategy for policy evaluation with nonlinear function approximation",
        "As opposed to many other algorithms derived for nonlinear value function approximation, two-timescale network are intentionally designed to be simple to promote ease-of-use",
        "We highlighted several cases where the decoupled architecture in two-timescale network can improve learning, enabling the use of linear methods\u2014which facilitates use of least-squares methods and eligibility traces",
        "We provided some evidence that, when using stochastic approximation algorithms rather than least-squares algorithms, the addition of traces can have a significant effect within two-timescale network"
    ],
    "summary": [
        "Value function approximation\u2014estimating the expected returns from states for a policy\u2014is heavily reliant on the quality of the representation of state.",
        "Linear algorithms can make use of eligibility traces, which can significantly speed learning (<a class=\"ref-link\" id=\"cSutton_1988_a\" href=\"#rSutton_1988_a\">Sutton, 1988</a>; <a class=\"ref-link\" id=\"cDann_et+al_2014_a\" href=\"#rDann_et+al_2014_a\">Dann et al, 2014</a>; <a class=\"ref-link\" id=\"cWhite_2016_a\" href=\"#rWhite_2016_a\">White and White, 2016</a>), but have not been able to be extended to nonlinear value function approximation.",
        "We show that these Two-timescale Networks (TTNs) converge, because the features change on a sufficiently slow scale, so that they are effectively fixed for the fast linear value function estimator.",
        "We first introduce Two-timescale Networks (TTNs), and describe different surrogate objectives that can be used in TTNs. We discuss why these surrogate objectives within TTNs are useful to drive the representation, but are not good replacements for the MSPBE for learning the value function.",
        "Projected stochastic gradient descent is used to reduce the surrogate loss, Lslow(\u03b8) and a linear policy evaluation algorithm, such as GTD2 or TD(\u03bb), is coupled to the network where the prediction vector w is callibrated proportional to \u2212\u2207wMSPBE\u03b8(w).",
        "We investigate the performance of TTNs versus a variety of other nonlinear policy evaluation algorithms, as well as the impact of choices within TTNs. We aim to answer (a) is it beneficial to optimize the MSPBE to obtain value estimates, rather than using value estimates from surrogate losses like the MSTDE; (b) do TTNs provide gains over other nonlinear policy evaluation algorithms; and (c) can TTNs benefit from the variety of options in linear algorithms, including leastsquares approaches, eligibility traces and different policy evaluation algorithms.",
        "The results overall indicate that TTNs can benefit from the ability to use different linear policy evaluation algorithms and traces, in particular from the use of least-squares methods as shown in Figure 4 for Puddle World and Catcher.",
        "The DQN algorithm (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a>) utilizes two main tricks to stabilize training: experience replay\u2014storing past transitions and replaying them multiple times\u2014and a target network\u2014which keeps the value function in the Q-learning targets fixed, updating the target network infrequently.",
        "TTNs provide a straightforward mechanism to instead directly use FQI, where we can solve for the weights on the entire replay buffer, taking advantage of the closed form solution for linear regression towards the Q-values from the last update.",
        "The algorithm combines a slow learning process for adapting features and a fast process for learning a linear value function, both of which are straightforward to train.",
        "TTNs provide the opportunity to investigate the utility of the many linear value function algorithms, in more complex domains with learned representations."
    ],
    "headline": "We provide a two-timescale network architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale",
    "reference_links": [
        {
            "id": "Aytar_et+al_2018_a",
            "entry": "Y. Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas. Playing hard exploration games by watching youtube. In Advances in Neural Information Processing Systems, pages 2935\u20132945, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aytar%2C%20Y.%20Pfaff%2C%20T.%20Budden%2C%20D.%20Paine%2C%20T.%20Playing%20hard%20exploration%20games%20by%20watching%20youtube%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aytar%2C%20Y.%20Pfaff%2C%20T.%20Budden%2C%20D.%20Paine%2C%20T.%20Playing%20hard%20exploration%20games%20by%20watching%20youtube%202018"
        },
        {
            "id": "Bena_et+al_2005_a",
            "entry": "M. Bena\u00efm, J. Hofbauer, and S. Sorin. Stochastic approximations and differential inclusions. SIAM Journal on Control and Optimization, 44(1):328\u2013348, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bena%C3%AFm%2C%20M.%20Hofbauer%2C%20J.%20Sorin%2C%20S.%20Stochastic%20approximations%20and%20differential%20inclusions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bena%C3%AFm%2C%20M.%20Hofbauer%2C%20J.%20Sorin%2C%20S.%20Stochastic%20approximations%20and%20differential%20inclusions%202005"
        },
        {
            "id": "Bhatnagar_et+al_2013_a",
            "entry": "S. Bhatnagar, V. S. Borkar, and P. K. J. Feature Search in the Grassmanian in Online Reinforcement Learning. J. Sel. Topics Signal Processing, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatnagar%2C%20S.%20Borkar%2C%20V.S.%20J%2C%20P.K.%20Feature%20Search%20in%20the%20Grassmanian%20in%20Online%20Reinforcement%20Learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhatnagar%2C%20S.%20Borkar%2C%20V.S.%20J%2C%20P.K.%20Feature%20Search%20in%20the%20Grassmanian%20in%20Online%20Reinforcement%20Learning%202013"
        },
        {
            "id": "Borkar_1997_a",
            "entry": "V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291\u2013294, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20V.S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borkar%2C%20V.S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997"
        },
        {
            "id": "Borkar_2008_a",
            "entry": "V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20V.S.%20Stochastic%20Approximation%3A%20A%20Dynamical%20Systems%20Viewpoint%202008"
        },
        {
            "id": "Bradtke_1996_a",
            "entry": "S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bradtke%2C%20S.J.%20Barto%2C%20A.G.%20Linear%20least-squares%20algorithms%20for%20temporal%20difference%20learning%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bradtke%2C%20S.J.%20Barto%2C%20A.G.%20Linear%20least-squares%20algorithms%20for%20temporal%20difference%20learning%201996"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Chen_2006_a",
            "entry": "H.-F. Chen. Stochastic approximation and its applications, volume 64. Springer Science & Business Media, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20H.-F.%20Stochastic%20approximation%20and%20its%20applications%2C%20volume%2064%202006"
        },
        {
            "id": "Dai_et+al_2017_a",
            "entry": "B. Dai, A. Shaw, L. Li, L. Xiao, N. He, J. Chen, and L. Song. Smoothed dual embedding control. arXiv preprint arXiv:1712.10285, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.10285"
        },
        {
            "id": "Dann_et+al_2014_a",
            "entry": "C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: a survey and comparison. The Journal of Machine Learning Research, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dann%2C%20C.%20Neumann%2C%20G.%20Peters%2C%20J.%20Policy%20evaluation%20with%20temporal%20differences%3A%20a%20survey%20and%20comparison%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dann%2C%20C.%20Neumann%2C%20G.%20Peters%2C%20J.%20Policy%20evaluation%20with%20temporal%20differences%3A%20a%20survey%20and%20comparison%202014"
        },
        {
            "id": "Castro_2010_a",
            "entry": "D. Di Castro and S. Mannor. Adaptive Bases for Reinforcement Learning. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castro%2C%20D.Di%20Mannor%2C%20S.%20Adaptive%20Bases%20for%20Reinforcement%20Learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castro%2C%20D.Di%20Mannor%2C%20S.%20Adaptive%20Bases%20for%20Reinforcement%20Learning%202010"
        },
        {
            "id": "Ernst_et+al_2005_a",
            "entry": "D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6(Apr):503\u2013556, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20D.%20Geurts%2C%20P.%20Wehenkel%2C%20L.%20Tree-based%20batch%20mode%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20D.%20Geurts%2C%20P.%20Wehenkel%2C%20L.%20Tree-based%20batch%20mode%20reinforcement%20learning%202005"
        },
        {
            "id": "Gidaris_et+al_2018_a",
            "entry": "S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Hessel_et+al_2017_a",
            "entry": "M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. G. Azar, and D. Silver. Rainbow - Combining Improvements in Deep Reinforcement Learning. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hessel%2C%20M.%20Modayil%2C%20J.%20van%20Hasselt%2C%20H.%20Schaul%2C%20T.%20Rainbow%20-%20Combining%20Improvements%20in%20Deep%20Reinforcement%20Learning%202017"
        },
        {
            "id": "P_et+al_2016_a",
            "entry": "P. K. J, S. Bhatnagar, and V. S. Borkar. Actor-Critic Algorithms with Online Feature Adaptation. ACM Trans. Model. Comput. Simul., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%2C%20P.K.%20Bhatnagar%2C%20S.%20Borkar%2C%20V.S.%20Actor-Critic%20Algorithms%20with%20Online%20Feature%20Adaptation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%2C%20P.K.%20Bhatnagar%2C%20S.%20Borkar%2C%20V.S.%20Actor-Critic%20Algorithms%20with%20Online%20Feature%20Adaptation%202016"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "Konidaris_et+al_2011_a",
            "entry": "G. Konidaris, S. Osentoski, and P. S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In International Conference on Machine Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20G.%20Osentoski%2C%20S.%20Thomas%2C%20P.S.%20Value%20function%20approximation%20in%20reinforcement%20learning%20using%20the%20Fourier%20basis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20G.%20Osentoski%2C%20S.%20Thomas%2C%20P.S.%20Value%20function%20approximation%20in%20reinforcement%20learning%20using%20the%20Fourier%20basis%202011"
        },
        {
            "id": "Kushner_2003_a",
            "entry": "H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kushner%2C%20H.%20Yin%2C%20G.G.%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%20applications%2C%20volume%2035%202003"
        },
        {
            "id": "Kushner_2012_a",
            "entry": "H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kushner%2C%20H.J.%20Clark%2C%20D.S.%20Stochastic%20Approximation%20Methods%20for%20Constrained%20and%20Unconstrained%20Systems%202012"
        },
        {
            "id": "Levin_2017_a",
            "entry": "D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20D.A.%20Peres%2C%20Y.%20Markov%20chains%20and%20mixing%20times%2C%20volume%20107%202017"
        },
        {
            "id": "Levine_et+al_2017_a",
            "entry": "N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar, and S. Mannor. Shallow Updates for Deep Reinforcement Learning. In International Conference on Learning Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20N.%20Zahavy%2C%20T.%20Mankowitz%2C%20D.J.%20Tamar%2C%20A.%20Shallow%20Updates%20for%20Deep%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20N.%20Zahavy%2C%20T.%20Mankowitz%2C%20D.J.%20Tamar%2C%20A.%20Shallow%20Updates%20for%20Deep%20Reinforcement%20Learning%202017"
        },
        {
            "id": "Ljung_1977_a",
            "entry": "L. Ljung. Analysis of recursive stochastic algorithms. IEEE transactions on automatic control, 22(4):551\u2013575, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ljung%2C%20L.%20Analysis%20of%20recursive%20stochastic%20algorithms%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ljung%2C%20L.%20Analysis%20of%20recursive%20stochastic%20algorithms%201977"
        },
        {
            "id": "Maei_2011_a",
            "entry": "H. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maei%2C%20H.%20Gradient%20Temporal-Difference%20Learning%20Algorithms%202011"
        },
        {
            "id": "Maei_et+al_2009_a",
            "entry": "H. Maei, C. Szepesv\u00e1ri, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maei%2C%20H.%20Szepesv%C3%A1ri%2C%20C.%20Bhatnagar%2C%20S.%20Precup%2C%20D.%20Convergent%20temporal-difference%20learning%20with%20arbitrary%20smooth%20function%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maei%2C%20H.%20Szepesv%C3%A1ri%2C%20C.%20Bhatnagar%2C%20S.%20Precup%2C%20D.%20Convergent%20temporal-difference%20learning%20with%20arbitrary%20smooth%20function%20approximation%202009"
        },
        {
            "id": "Mahadevan_et+al_2014_a",
            "entry": "S. Mahadevan, B. Liu, P. S. Thomas, W. Dabney, S. Giguere, N. Jacek, I. Gemp, and J. Liu. Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. CoRR abs/1405.6757, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.6757"
        },
        {
            "id": "Mahmood_2015_a",
            "entry": "A. R. Mahmood and R. Sutton. Off-policy learning based on weighted importance sampling with linear computational complexity. In Conference on Uncertainty in Artificial Intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahmood%2C%20A.R.%20Sutton%2C%20R.%20Off-policy%20learning%20based%20on%20weighted%20importance%20sampling%20with%20linear%20computational%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahmood%2C%20A.R.%20Sutton%2C%20R.%20Off-policy%20learning%20based%20on%20weighted%20importance%20sampling%20with%20linear%20computational%20complexity%202015"
        },
        {
            "id": "Mahmood_et+al_2017_a",
            "entry": "A. R. Mahmood, H. Yu, and R. S. Sutton. Multi-step Off-policy Learning Without Importance Sampling Ratios. arXiv:1509.01240v2, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01240v2"
        },
        {
            "id": "Menache_et+al_2005_a",
            "entry": "I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference reinforcement learning. Annals of Operations Research, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menache%2C%20I.%20Mannor%2C%20S.%20Shimkin%2C%20N.%20Basis%20function%20adaptation%20in%20temporal%20difference%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menache%2C%20I.%20Mannor%2C%20S.%20Shimkin%2C%20N.%20Basis%20function%20adaptation%20in%20temporal%20difference%20reinforcement%20learning%202005"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Oh_et+al_2015_a",
            "entry": "J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pages 2863\u20132871, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015"
        },
        {
            "id": "Pan_et+al_2017_a",
            "entry": "Y. Pan, E. S. Azer, and M. White. Effective sketching methods for value function approximation. In Conference on Uncertainty in Artificial Intelligence, Amsterdam, Netherlands, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Y.%20Azer%2C%20E.S.%20White%2C%20M.%20Effective%20sketching%20methods%20for%20value%20function%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Y.%20Azer%2C%20E.S.%20White%2C%20M.%20Effective%20sketching%20methods%20for%20value%20function%20approximation%202017"
        },
        {
            "id": "Ramaswamy_2016_a",
            "entry": "A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an application to the lagrangian dual problem. Stochastics, 88(8):1173\u20131187, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramaswamy%2C%20A.%20Bhatnagar%2C%20S.%20Stochastic%20recursive%20inclusion%20in%20two%20timescales%20with%20an%20application%20to%20the%20lagrangian%20dual%20problem%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramaswamy%2C%20A.%20Bhatnagar%2C%20S.%20Stochastic%20recursive%20inclusion%20in%20two%20timescales%20with%20an%20application%20to%20the%20lagrangian%20dual%20problem%202016"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "S. J. Reddi, S. Kale, and S. Kumar. On the Convergence of Adam and Beyond. In International Conference on Learning Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20Convergence%20of%20Adam%20and%20Beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20Convergence%20of%20Adam%20and%20Beyond%202018"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Song_et+al_2016_a",
            "entry": "Z. Song, R. E. Parr, X. Liao, and L. Carin. Linear feature encoding for reinforcement learning. In Advances in Neural Information Processing Systems, pages 4224\u20134232, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Z.%20Parr%2C%20R.E.%20Liao%2C%20X.%20Carin%2C%20L.%20Linear%20feature%20encoding%20for%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Z.%20Parr%2C%20R.E.%20Liao%2C%20X.%20Carin%2C%20L.%20Linear%20feature%20encoding%20for%20reinforcement%20learning%202016"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "R. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.%20Barto%2C%20A.G.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "Sutton_1988_a",
            "entry": "R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences%201988"
        },
        {
            "id": "Sutton_et+al_2009_a",
            "entry": "R. S. Sutton, H. Maei, D. Precup, and S. Bhatnagar. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In International Conference on Machine Learning, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Maei%2C%20H.%20Precup%2C%20D.%20Bhatnagar%2C%20S.%20Fast%20gradient-descent%20methods%20for%20temporal-difference%20learning%20with%20linear%20function%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Maei%2C%20H.%20Precup%2C%20D.%20Bhatnagar%2C%20S.%20Fast%20gradient-descent%20methods%20for%20temporal-difference%20learning%20with%20linear%20function%20approximation%202009"
        },
        {
            "id": "Sutton_et+al_2016_a",
            "entry": "R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy temporaldifference learning. The Journal of Machine Learning Research, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Mahmood%2C%20A.R.%20White%2C%20M.%20An%20emphatic%20approach%20to%20the%20problem%20of%20off-policy%20temporaldifference%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Mahmood%2C%20A.R.%20White%2C%20M.%20An%20emphatic%20approach%20to%20the%20problem%20of%20off-policy%20temporaldifference%20learning%202016"
        },
        {
            "id": "Szepesvari_2010_a",
            "entry": "C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool Publishers, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesvari%2C%20C.%20Algorithms%20for%20Reinforcement%20Learning%202010"
        },
        {
            "id": "Tasfi_2016_a",
            "entry": "N. Tasfi. Pygame Learning Environment. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tasfi%2C%20N.%20Pygame%20Learning%20Environment%202016"
        },
        {
            "id": "Tsitsiklis_1997_a",
            "entry": "J. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.%20Roy%2C%20B.Van%20An%20analysis%20of%20temporal-difference%20learning%20with%20function%20approximation%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.%20Roy%2C%20B.Van%20An%20analysis%20of%20temporal-difference%20learning%20with%20function%20approximation%201997"
        },
        {
            "id": "Van_et+al_2014_a",
            "entry": "H. van Hasselt, A. R. Mahmood, and R. Sutton. Off-policy TD (\u03bb) with a true online equivalence. In Conference on Uncertainty in Artificial Intelligence, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Hasselt%2C%20H.%20Mahmood%2C%20A.R.%20Sutton%2C%20R.%20Off-policy%20TD%20%28%CE%BB%29%20with%20a%20true%20online%20equivalence%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Hasselt%2C%20H.%20Mahmood%2C%20A.R.%20Sutton%2C%20R.%20Off-policy%20TD%20%28%CE%BB%29%20with%20a%20true%20online%20equivalence%202014"
        },
        {
            "id": "Van_2015_a",
            "entry": "H. van Seijen and R. Sutton. A deeper look at planning as learning from replay. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Seijen%2C%20H.%20Sutton%2C%20R.%20A%20deeper%20look%20at%20planning%20as%20learning%20from%20replay%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Seijen%2C%20H.%20Sutton%2C%20R.%20A%20deeper%20look%20at%20planning%20as%20learning%20from%20replay%202015"
        },
        {
            "id": "Van_2014_b",
            "entry": "H. van Seijen and R. S. Sutton. True online TD(lambda). In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%20van%20Seijen%20and%20R%20S%20Sutton%20True%20online%20TDlambda%20In%20International%20Conference%20on%20Machine%20Learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%20van%20Seijen%20and%20R%20S%20Sutton%20True%20online%20TDlambda%20In%20International%20Conference%20on%20Machine%20Learning%202014"
        },
        {
            "id": "White_2016_a",
            "entry": "A. M. White and M. White. Investigating practical, linear temporal difference learning. In International Conference on Autonomous Agents and Multiagent Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=White%2C%20A.M.%20White%2C%20M.%20Investigating%20practical%2C%20linear%20temporal%20difference%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=White%2C%20A.M.%20White%2C%20M.%20Investigating%20practical%2C%20linear%20temporal%20difference%20learning%202016"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 M. White. Unifying task specification in reinforcement learning. In International Conference on Machine",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20M%20White%20Unifying%20task%20specification%20in%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20M%20White%20Unifying%20task%20specification%20in%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine"
        },
        {
            "id": "Learning_2015_a",
            "entry": "Learning, pages 3742\u20133750, 2017. H. Yu. On convergence of emphatic temporal-difference learning. In Annual Conference on Learning Theory, 2015. H. Yu and D. P. Bertsekas. Basis function adaptation methods for cost approximation in MDP. In IEEE",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Learning%20On%20convergence%20of%20emphatic%20temporal-difference%20learning.%20In%20Annual%20Conference%20on%20Learning%20Theory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Learning%20On%20convergence%20of%20emphatic%20temporal-difference%20learning.%20In%20Annual%20Conference%20on%20Learning%20Theory%202015"
        },
        {
            "id": "Symposium_2009_a",
            "entry": "Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Symposium%20on%20Adaptive%20Dynamic%20Programming%20and%20Reinforcement%20Learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Symposium%20on%20Adaptive%20Dynamic%20Programming%20and%20Reinforcement%20Learning%202009"
        },
        {
            "id": "2",
            "entry": "2. For each x \u2208 Rd, \u2203K \u2208 (0, \u221e) such that supy\u2208h(x) y \u2264 K(1 + x ). Assumption 2 implies that the underlying Markov chain is asymptotically stationary and henceforth it guarantees the existence of a unique steady-state distribution d\u03c0 over the state space S (Levin and Peres, 2017), i.e., limt\u2192\u221e P(St = s) = d\u03c0(s), \u2200s \u2208 S.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20each%20x%20%20Rd%20K%20%200%20%20such%20that%20supyhx%20y%20%20K1%20%20x%20%20Assumption%202%20implies%20that%20the%20underlying%20Markov%20chain%20is%20asymptotically%20stationary%20and%20henceforth%20it%20guarantees%20the%20existence%20of%20a%20unique%20steadystate%20distribution%20d%CF%80%20over%20the%20state%20space%20S%20Levin%20and%20Peres%202017%20ie%20limt%20PSt%20%20s%20%20d%CF%80s%20s%20%20S",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20each%20x%20%20Rd%20K%20%200%20%20such%20that%20supyhx%20y%20%20K1%20%20x%20%20Assumption%202%20implies%20that%20the%20underlying%20Markov%20chain%20is%20asymptotically%20stationary%20and%20henceforth%20it%20guarantees%20the%20existence%20of%20a%20unique%20steadystate%20distribution%20d%CF%80%20over%20the%20state%20space%20S%20Levin%20and%20Peres%202017%20ie%20limt%20PSt%20%20s%20%20d%CF%80s%20s%20%20S"
        }
    ]
}
