{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GENERATING MULTIPLE OBJECTS AT SPATIALLY DISTINCT LOCATIONS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1edIiA9KQ"
        },
        "abstract": "Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MSCOCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background."
    },
    "keywords": [
        {
            "term": "bounding box",
            "url": "https://en.wikipedia.org/wiki/bounding_box"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "high resolution",
            "url": "https://en.wikipedia.org/wiki/high_resolution"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "Dimitris Metaxas",
            "url": "https://en.wikipedia.org/wiki/Dimitris_Metaxas"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "GAN": "generative adversarial network",
        "STN": "Spatial Transformer Network",
        "IoU": "Intersection over Union"
    },
    "highlights": [
        "Understanding how to learn powerful representations from complex distributions is the intriguing goal behind adversarial training on image data",
        "We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background",
        "To summarize our model and contributions: 1) We propose a Generative Adversarial Networks model that enables us to control the layout of a scene without the use of a scene layout.\n2) Through the use of an object pathway which is responsible for learning features of different object categories, we gain control over the identity and location of arbitrarily many objects within a scene.\n3) The discriminator judges not only if the image is realistic and aligned to the natural language description, but whether the specified objects are at the given locations and of the correct object category.\n4) We show that the object pathway does learn relevant features for the different objects, while the global pathway focuses on general image features and the background.\n2 RELATED WORK",
        "The specific changes of the generator compared to standard architectures are the object pathway that generates additional features at specific locations based on provided labels, as well as the layout encoding which is used as additional input to the global pathway",
        "With the goal of understanding how to gain more control over the image generation process in Generative Adversarial Networks, we introduced the concept of an additional object pathway",
        "Our tests on synthetic and real-world data sets suggest that the object pathway is an extension that can be added to common Generative Adversarial Networks architectures without much change to the original architecture and can, along with more fine-grained control over the image layout, lead to better image quality"
    ],
    "key_statements": [
        "Understanding how to learn powerful representations from complex distributions is the intriguing goal behind adversarial training on image data",
        "Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions",
        "The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes",
        "The global pathway focuses on the image background and the general image layout",
        "We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background",
        "While recent advances have enabled us to generate high-resolution images with Generative Adversarial Networks (GANs), currently most Generative Adversarial Networks models still focus on modeling images that either contain only one centralized object, objects (ImageNet), birds (CUB-200), flowers (Oxford-102), etc.) or on images from one specific domain (e.g. LSUN bedrooms, LSUN churches, etc.)",
        "To summarize our model and contributions: 1) We propose a Generative Adversarial Networks model that enables us to control the layout of a scene without the use of a scene layout.\n2) Through the use of an object pathway which is responsible for learning features of different object categories, we gain control over the identity and location of arbitrarily many objects within a scene.\n3) The discriminator judges not only if the image is realistic and aligned to the natural language description, but whether the specified objects are at the given locations and of the correct object category.\n4) We show that the object pathway does learn relevant features for the different objects, while the global pathway focuses on general image features and the background.\n2 RELATED WORK",
        "The generator G gets as input a randomly sampled noise vector z, the location and size of the individual bounding boxes bboxi, a label for each of the bounding boxes encoded as a one-hot vector lonehoti , and, if existent, an image caption embedding \u03c6 obtained with a pretrained char-CNN-RNN network from <a class=\"ref-link\" id=\"cReed_et+al_2016_b\" href=\"#rReed_et+al_2016_b\">Reed et al (2016b</a>)",
        "The specific changes of the generator compared to standard architectures are the object pathway that generates additional features at specific locations based on provided labels, as well as the layout encoding which is used as additional input to the global pathway",
        "To test the impact of our model extensions, i.e. the object pathway in both the generator and the discriminator as well as the layout encoding, we performed ablation studies on the previously created Multi-MNIST data set with three digits at random locations",
        "We extended the AttnGAN by <a class=\"ref-link\" id=\"cXu_et+al_2018_b\" href=\"#rXu_et+al_2018_b\">Xu et al (2018b</a>), the current state-of-the-art model on the MS-COCO data set, with our object pathway to evaluate its impact on",
        "DISCUSSION Our experiments indicate that we do get additional control over the image generation process through the introduction of object pathways in Generative Adversarial Networks. This enables us to control the identity and location of multiple objects within a given image based on bounding boxes and thereby facilitates the generation of more complex scenes",
        "The results further indicate that the focus on global image statistics by the global pathway and the more fine-grained attention to detail of specific objects by the object pathway works well",
        "With the goal of understanding how to gain more control over the image generation process in Generative Adversarial Networks, we introduced the concept of an additional object pathway",
        "The features generated by the object and global pathway are concatenated and are used to generate the final image output",
        "Our tests on synthetic and real-world data sets suggest that the object pathway is an extension that can be added to common Generative Adversarial Networks architectures without much change to the original architecture and can, along with more fine-grained control over the image layout, lead to better image quality"
    ],
    "summary": [
        "Understanding how to learn powerful representations from complex distributions is the intriguing goal behind adversarial training on image data.",
        "In contrast to previous work we do not generate a scene layout of the whole scene but only focus on relevant objects which are placed at the specified locations, while the global consistency of the image is the responsibility of the other part of our model.",
        "The generator G gets as input a randomly sampled noise vector z, the location and size of the individual bounding boxes bboxi, a label for each of the bounding boxes encoded as a one-hot vector lonehoti , and, if existent, an image caption embedding \u03c6 obtained with a pretrained char-CNN-RNN network from <a class=\"ref-link\" id=\"cReed_et+al_2016_b\" href=\"#rReed_et+al_2016_b\">Reed et al (2016b</a>).",
        "The specific changes of the generator compared to standard architectures are the object pathway that generates additional features at specific locations based on provided labels, as well as the layout encoding which is used as additional input to the global pathway.",
        "To the generator we only use one object pathway that is applied to multiple image locations, where the outputs are added onto the empty canvas, summing up overlapping parts and keeping areas outside of the bounding boxes set to zero.",
        "To test the impact of our model extensions, i.e. the object pathway in both the generator and the discriminator as well as the layout encoding, we performed ablation studies on the previously created Multi-MNIST data set with three digits at random locations.",
        "4.4 DISCUSSION Our experiments indicate that we do get additional control over the image generation process through the introduction of object pathways in GANs. This enables us to control the identity and location of multiple objects within a given image based on bounding boxes and thereby facilitates the generation of more complex scenes.",
        "The object pathway, on the other hand, gets as input an object label and uses this to generate features for this object which are placed at the location given by a bounding box The object pathway is applied iteratively for each object at each given location and as such, we obtain a representation of individual objects at individual locations and of the general image layout as a whole.",
        "Our tests on synthetic and real-world data sets suggest that the object pathway is an extension that can be added to common GAN architectures without much change to the original architecture and can, along with more fine-grained control over the image layout, lead to better image quality."
    ],
    "headline": "We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator",
    "reference_links": [
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016"
        },
        {
            "id": "Chen_2017_a",
            "entry": "Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In IEEE International Conference on Computer Vision, pp. 1520\u20131529, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Qifeng%20Koltun%2C%20Vladlen%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Qifeng%20Koltun%2C%20Vladlen%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017"
        },
        {
            "id": "Eslami_et+al_2016_a",
            "entry": "S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, koray kavukcuoglu, and Geoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in Neural Information Processing Systems, pp. 3225\u20133233, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eslami%2C%20S.M.Ali%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Tassa%2C%20Yuval%20Attend%2C%20infer%2C%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eslami%2C%20S.M.Ali%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Tassa%2C%20Yuval%20Attend%2C%20infer%2C%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626\u20136637, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Hong_et+al_2018_a",
            "entry": "Seunghoon Hong, Xinchen Yan, Thomas Huang, and Honglak Lee. Learning hierarchical semantic image manipulation through structured representations. In Advances in Neural Information Processing Systems, pp. 2712\u20132722, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20Seunghoon%20Yan%2C%20Xinchen%20Huang%2C%20Thomas%20Lee%2C%20Honglak%20Learning%20hierarchical%20semantic%20image%20manipulation%20through%20structured%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20Seunghoon%20Yan%2C%20Xinchen%20Huang%2C%20Thomas%20Lee%2C%20Honglak%20Learning%20hierarchical%20semantic%20image%20manipulation%20through%20structured%20representations%202018"
        },
        {
            "id": "Hong_et+al_2018_b",
            "entry": "Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7986\u20137994, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20Seunghoon%20Yang%2C%20Dingdong%20Choi%2C%20Jongwook%20Lee%2C%20Honglak%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20Seunghoon%20Yang%2C%20Dingdong%20Choi%2C%20Jongwook%20Lee%2C%20Honglak%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems, pp. 2017\u20132025, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017"
        },
        {
            "id": "Johnson_et+al_1988_a",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1988\u20131997, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20Clevr%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20Clevr%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%201988"
        },
        {
            "id": "Johnson_et+al_2018_a",
            "entry": "Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1219\u20131228, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Gupta%2C%20Agrim%20Fei-Fei%2C%20Li%20Image%20generation%20from%20scene%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Gupta%2C%20Agrim%20Fei-Fei%2C%20Li%20Image%20generation%20from%20scene%20graphs%202018"
        },
        {
            "id": "Karacan_et+al_2016_a",
            "entry": "Levent Karacan, Zeynep Akata, Aykut Erdem, and Erkut Erdem. Learning to generate images of outdoor scenes from attributes and semantic layouts. arXiv preprint arXiv:1612.00215, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00215"
        },
        {
            "id": "Lin_et+al_2014_a",
            "entry": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pp. 740\u2013755, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Dollar%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20European%20Conference%20on%20Computer%20Vision%20pp%20740755%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Dollar%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20European%20Conference%20on%20Computer%20Vision%20pp%20740755%202014"
        },
        {
            "id": "Mansimov_et+al_2016_a",
            "entry": "Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansimov%2C%20Elman%20Parisotto%2C%20Emilio%20Ba%2C%20Jimmy%20Lei%20Salakhutdinov%2C%20Ruslan%20Generating%20images%20from%20captions%20with%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansimov%2C%20Elman%20Parisotto%2C%20Emilio%20Ba%2C%20Jimmy%20Lei%20Salakhutdinov%2C%20Ruslan%20Generating%20images%20from%20captions%20with%20attention%202016"
        },
        {
            "id": "Nguyen_et+al_2017_a",
            "entry": "Anh Nguyen, Yoshua Bengio, and Alexey Dosovitskiy. Plug & play generative networks: Conditional iterative generation of images in latent space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467\u20134477, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Anh%20Bengio%2C%20Yoshua%20Dosovitskiy%2C%20Alexey%20Plug%20%26%20play%20generative%20networks%3A%20Conditional%20iterative%20generation%20of%20images%20in%20latent%20space%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Anh%20Bengio%2C%20Yoshua%20Dosovitskiy%2C%20Alexey%20Plug%20%26%20play%20generative%20networks%3A%20Conditional%20iterative%20generation%20of%20images%20in%20latent%20space%202017"
        },
        {
            "id": "Raj_et+al_2017_a",
            "entry": "Amit Raj, Cusuh Ham, Huda Alamri, Vincent Cartillier, Stefan Lee, and James Hays. Compositional generation of images. In Advances in Neural Information Processing Systems ViGIL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raj%2C%20Amit%20Ham%2C%20Cusuh%20Alamri%2C%20Huda%20Cartillier%2C%20Vincent%20Compositional%20generation%20of%20images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raj%2C%20Amit%20Ham%2C%20Cusuh%20Alamri%2C%20Huda%20Cartillier%2C%20Vincent%20Compositional%20generation%20of%20images%202017"
        },
        {
            "id": "Redmon_2018_a",
            "entry": "Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02767"
        },
        {
            "id": "Reed_et+al_0000_a",
            "entry": "Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learning what and where to draw. In Advances in Neural Information Processing Systems, pp. 217\u2013225, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Mohan%2C%20Santosh%20Tenka%2C%20Samuel%20Learning%20what%20and%20where%20to%20draw",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Mohan%2C%20Santosh%20Tenka%2C%20Samuel%20Learning%20what%20and%20where%20to%20draw"
        },
        {
            "id": "Reed_et+al_2016_a",
            "entry": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In International Conference on Machine Learning, pp. 1060\u20131069, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Yan%2C%20Xinchen%20Logeswaran%2C%20Lajanugen%20Generative%20adversarial%20text%20to%20image%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Yan%2C%20Xinchen%20Logeswaran%2C%20Lajanugen%20Generative%20adversarial%20text%20to%20image%20synthesis%202016"
        },
        {
            "id": "Reed_et+al_2016_b",
            "entry": "Scott Reed, Aaron van den Oord, Nal Kalchbrenner, Victor Bapst, Matt Botvinick, and Nando de Freitas. Generating interpretable images with controllable structure. 2016c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Bapst%2C%20Victor%20Generating%20interpretable%20images%20with%20controllable%20structure%202016"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Sharma_et+al_2018_a",
            "entry": "Shikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi Kahou, and Yoshua Bengio. Chatpainter: Improving text to image generation using dialogue. In International Conference on Learning Representations Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharma%2C%20Shikhar%20Suhubdy%2C%20Dendi%20Michalski%2C%20Vincent%20Kahou%2C%20Samira%20Ebrahimi%20Chatpainter%3A%20Improving%20text%20to%20image%20generation%20using%20dialogue%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharma%2C%20Shikhar%20Suhubdy%2C%20Dendi%20Michalski%2C%20Vincent%20Kahou%2C%20Samira%20Ebrahimi%20Chatpainter%3A%20Improving%20text%20to%20image%20generation%20using%20dialogue%202018"
        },
        {
            "id": "Tan_et+al_2018_a",
            "entry": "Fuwen Tan, Song Feng, and Vicente Ordonez. Text2scene: Generating abstract scenes from textual descriptions. arXiv preprint arXiv:1809.01110, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.01110"
        },
        {
            "id": "Wah_et+al_2011_a",
            "entry": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20Catherine%20Branson%2C%20Steve%20Welinder%2C%20Peter%20Perona%2C%20Pietro%20The%20caltech-ucsd%20birds-200-2011%202011"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8798\u20138807, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20Highresolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20Highresolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Kun Xu, Haoyu Liang, Jun Zhu, Hang Su, and Bo Zhang. Deep structured generative models. In ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kun%20Liang%2C%20Haoyu%20Zhu%2C%20Jun%20Su%2C%20Hang%20Deep%20structured%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kun%20Liang%2C%20Haoyu%20Zhu%2C%20Jun%20Su%2C%20Hang%20Deep%20structured%20generative%20models%202018"
        },
        {
            "id": "Xu_et+al_2018_b",
            "entry": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Tao%20Zhang%2C%20Pengchuan%20Huang%2C%20Qiuyuan%20Zhang%2C%20Han%20Attngan%3A%20Fine-grained%20text%20to%20image%20generation%20with%20attentional%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Tao%20Zhang%2C%20Pengchuan%20Huang%2C%20Qiuyuan%20Zhang%2C%20Han%20Attngan%3A%20Fine-grained%20text%20to%20image%20generation%20with%20attentional%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision, pp. 5907\u20135915, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u201316, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Shaoting%20Zhang%2C%20Xiaogang%20Wang%2C%20Xiaolei%20Huang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%2B%2B%3A%20Realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Shaoting%20Zhang%2C%20Xiaogang%20Wang%2C%20Xiaolei%20Huang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%2B%2B%3A%20Realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks"
        },
        {
            "id": "Zhang_et+al_0000_b",
            "entry": "Shengyu Zhang, Hao Dong, Wei Hu, Yike Guo, Chao Wu, Di Xie, and Fei Wu. Text-to-image synthesis via visual-memory creative adversarial network. In Pacific Rim Conference on Multimedia, pp. 417\u2013427, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Shengyu%20Dong%2C%20Hao%20Hu%2C%20Wei%20Guo%2C%20Yike%20Text-to-image%20synthesis%20via%20visual-memory%20creative%20adversarial%20network",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Shengyu%20Dong%2C%20Hao%20Hu%2C%20Wei%20Guo%2C%20Yike%20Text-to-image%20synthesis%20via%20visual-memory%20creative%20adversarial%20network"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic text-to-image synthesis with a hierarchicallynested adversarial network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6199\u20136208, 2018c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Zizhao%20Xie%2C%20Yuanpu%20Yang%2C%20Lin%20Photographic%20text-to-image%20synthesis%20with%20a%20hierarchicallynested%20adversarial%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Zizhao%20Xie%2C%20Yuanpu%20Yang%2C%20Lin%20Photographic%20text-to-image%20synthesis%20with%20a%20hierarchicallynested%20adversarial%20network%202018"
        },
        {
            "id": "6",
            "entry": "6 https://github.com/hanzhanggit/StackGAN-Pytorch",
            "url": "https://github.com/hanzhanggit/StackGAN-Pytorch"
        },
        {
            "id": "Downloaded_0000_b",
            "entry": "Downloaded from https://github.com/reedscot/icml2016",
            "url": "https://github.com/reedscot/icml2016"
        },
        {
            "id": "Published_2019_c",
            "entry": "Published as a conference paper at ICLR 2019 Multi-MNIST",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20MultiMNIST"
        }
    ]
}
