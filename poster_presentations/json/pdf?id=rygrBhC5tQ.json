{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "COMPOSING COMPLEX SKILLS BY LEARNING TRANSITION POLICIES",
        "author": "Youngwoon Lee, Shao-Hua Sun, Sriram Somasundaram, Edward S. Hu, Joseph J. Lim University of Southern California {lee504,shaohuas,sriramso,hues,limjj}@usc.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rygrBhC5tQ"
        },
        "abstract": "Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at. We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills. The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments. We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition."
    },
    "keywords": [
        {
            "term": "intrinsic motivation",
            "url": "https://en.wikipedia.org/wiki/intrinsic_motivation"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "TP": "Transition Policies",
        "PPO": "Policy Optimization with dense reward"
    },
    "highlights": [
        "While humans are capable of learning complex tasks by reusing previously learned skills, composing and mastering complex skills are not as trivial as sequentially executing those acquired skills",
        "We propose a modular framework with transition policies that learn to make transition between one policy to the subsequent policy, and can exploit the given primitive skills to compose complex skills",
        "To illustrate the potential of the proposed framework, modular framework with Transition Policies (TP), we designed a set of complex tasks that require agents to utilize diverse primitive skills which are not optimized for smooth composition",
        "We show that transitions with sparse rewards can compete with and even outperform baselines learning from dense rewards",
        "We propose a modular framework with transition policies to empower reinforcement learning agents to learn complex tasks with sparse reward by utilizing prior knowledge",
        "To learn transition polices in a sparse reward setting, we propose a proximity predictor which generates dense reward signals and jointly train transition policies and proximity predictors"
    ],
    "key_statements": [
        "While humans are capable of learning complex tasks by reusing previously learned skills, composing and mastering complex skills are not as trivial as sequentially executing those acquired skills",
        "We propose a proximity predictor which outputs the proximity to the initiation set of the skill and acts as a dense reward function for the transition policy",
        "We propose a modular framework with transition policies that learn to make transition between one policy to the subsequent policy, and can exploit the given primitive skills to compose complex skills",
        "To illustrate the potential of the proposed framework, modular framework with Transition Policies (TP), we designed a set of complex tasks that require agents to utilize diverse primitive skills which are not optimized for smooth composition",
        "We evaluate our method to answer how transition policies benefit complex task learning and how joint training with proximity predictors boosts training of transition policies",
        "We show that transitions with sparse rewards can compete with and even outperform baselines learning from dense rewards",
        "Transition policies trained from task completion reward (TP-Task) and sparse proximity reward (TP-Sparse) learn to connect consecutive primitives slower because sparse reward is hard to learn from due to the credit assignment problem",
        "We propose a modular framework with transition policies to empower reinforcement learning agents to learn complex tasks with sparse reward by utilizing prior knowledge",
        "To learn transition polices in a sparse reward setting, we propose a proximity predictor which generates dense reward signals and jointly train transition policies and proximity predictors",
        "Our experimental results on robotic manipulation and locomotion tasks demonstrate the effectiveness of employing transition policies"
    ],
    "summary": [
        "While humans are capable of learning complex tasks by reusing previously learned skills, composing and mastering complex skills are not as trivial as sequentially executing those acquired skills.",
        "We propose a proximity predictor which outputs the proximity to the initiation set of the skill and acts as a dense reward function for the transition policy.",
        "While those methods assume ground truth trajectories or goal states are given, our method collects both success and failure trajectories online to train proximity predictors which provide rewards for transition policies.",
        "Transition policies are trained to make the execution of the corresponding following primitive policies successful.",
        "We label the states in a transition trajectory as success or failure based on whether the following primitive is successfully executed or not, and add them into the corresponding buffers BS or BF , respectively.",
        "To illustrate the potential of the proposed framework, modular framework with Transition Policies (TP), we designed a set of complex tasks that require agents to utilize diverse primitive skills which are not optimized for smooth composition.",
        "The agent trained with dense rewards is not able to consistently switch directions, whereas our model can utilize previously learned primitives including Balancing to stabilize a reversal in velocity.",
        "The modular framework without transition policies (Without-TP) tends to fail the execution of the second skill since the second skill is not trained to cover ending states of the first skill.",
        "Transition policies trained from task completion reward (TP-Task) and sparse proximity reward (TP-Sparse) learn to connect consecutive primitives slower because sparse reward is hard to learn from due to the credit assignment problem.",
        "To investigate how transition polices learn to solve the tasks, we present the lengths of transition trajectories and the obtained proximity rewards during training in Figure 5.",
        "As these failing initial states with high proximity are collected in the failure buffers, the proximity predictor lowers their proximity and the transition policy learns to avoid them.",
        "Trajectories in the figure show that as the transition policy moves toward states with higher proximity, and ends up with states t0 and t1 which are in the initiation set of the primitive policy.",
        "We propose a modular framework with transition policies to empower reinforcement learning agents to learn complex tasks with sparse reward by utilizing prior knowledge.",
        "To learn transition polices in a sparse reward setting, we propose a proximity predictor which generates dense reward signals and jointly train transition policies and proximity predictors.",
        "The proposed framework solves complex tasks without reward shaping and outperforms baseline RL algorithms and other ablated baselines"
    ],
    "headline": "To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards",
    "reference_links": [
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%20In%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%20In%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%202016"
        },
        {
            "id": "Andreas_et+al_2017_a",
            "entry": "Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017"
        },
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Association for the Advancement of Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Bahdanau_et+al_2019_a",
            "entry": "Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. Learning to understand goal specifications by modelling reward. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Hill%2C%20Felix%20Leike%2C%20Jan%20Hughes%2C%20Edward%20Learning%20to%20understand%20goal%20specifications%20by%20modelling%20reward%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Hill%2C%20Felix%20Leike%2C%20Jan%20Hughes%2C%20Edward%20Learning%20to%20understand%20goal%20specifications%20by%20modelling%20reward%202019"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Co-Reyes_et+al_2018_a",
            "entry": "John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Co-Reyes%2C%20John%20Liu%2C%20YuXuan%20Gupta%2C%20Abhishek%20Eysenbach%2C%20Benjamin%20Self-consistent%20trajectory%20autoencoder%3A%20Hierarchical%20reinforcement%20learning%20with%20trajectory%20embeddings%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Co-Reyes%2C%20John%20Liu%2C%20YuXuan%20Gupta%2C%20Abhishek%20Eysenbach%2C%20Benjamin%20Self-consistent%20trajectory%20autoencoder%3A%20Hierarchical%20reinforcement%20learning%20with%20trajectory%20embeddings%202018"
        },
        {
            "id": "Daniel_et+al_2016_a",
            "entry": "Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Dilokthanakul_et+al_2017_a",
            "entry": "Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, and Murray Shanahan. Feature control as intrinsic motivation for hierarchical reinforcement learning. In IEEE Transactions on Neural Networks and Learning Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dilokthanakul%2C%20Nat%20Kaplanis%2C%20Christos%20Pawlowski%2C%20Nick%20Shanahan%2C%20Murray%20Feature%20control%20as%20intrinsic%20motivation%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dilokthanakul%2C%20Nat%20Kaplanis%2C%20Christos%20Pawlowski%2C%20Nick%20Shanahan%2C%20Murray%20Feature%20control%20as%20intrinsic%20motivation%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Frans_et+al_2018_a",
            "entry": "Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frans%2C%20Kevin%20Ho%2C%20Jonathan%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20Meta%20learning%20shared%20hierarchies%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frans%2C%20Kevin%20Ho%2C%20Jonathan%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20Meta%20learning%20shared%20hierarchies%202018"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide and conquer reinforcement learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20Dibya%20Singh%2C%20Avi%20Rajeswaran%2C%20Aravind%20Kumar%2C%20Vikash%20Divide%20and%20conquer%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Dibya%20Singh%2C%20Avi%20Rajeswaran%2C%20Aravind%20Kumar%2C%20Vikash%20Divide%20and%20conquer%20reinforcement%20learning%202018"
        },
        {
            "id": "Gudimella_et+al_2017_a",
            "entry": "Aditya Gudimella, Ross Story, Matineh Shaker, Ruofan Kong, Matthew Brown, Victor Shnayder, and Marcos Campos. Deep reinforcement learning for dexterous manipulation with concept networks. arXiv preprint arXiv: 1709.06977, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06977"
        },
        {
            "id": "Heess_et+al_2017_a",
            "entry": "Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv: 1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederick%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederick%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kober_et+al_2010_a",
            "entry": "Jens Kober, Katharina Mulling, Oliver Kromer, Christoph H Lampert, Bernhard Scholkopf, and Jan Peters. Movement templates for learning of hitting and batting. In International Conference on Robotics and Automation, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Mulling%2C%20Katharina%20Kromer%2C%20Oliver%20Lampert%2C%20Christoph%20H.%20Movement%20templates%20for%20learning%20of%20hitting%20and%20batting%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Mulling%2C%20Katharina%20Kromer%2C%20Oliver%20Lampert%2C%20Christoph%20H.%20Movement%20templates%20for%20learning%20of%20hitting%20and%20batting%202010"
        },
        {
            "id": "Kulkarni_et+al_2016_a",
            "entry": "Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Le_et+al_2018_a",
            "entry": "Hoang Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, and Hal Daume, III. Hierarchical imitation and reinforcement learning. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Hoang%20Jiang%2C%20Nan%20Agarwal%2C%20Alekh%20Dudik%2C%20Miroslav%20Hierarchical%20imitation%20and%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Hoang%20Jiang%2C%20Nan%20Agarwal%2C%20Alekh%20Dudik%2C%20Miroslav%20Hierarchical%20imitation%20and%20reinforcement%20learning%202018"
        },
        {
            "id": "Levy_et+al_2017_a",
            "entry": "Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00948"
        },
        {
            "id": "Mao_et+al_2018_a",
            "entry": "Jiayuan Mao, Honghua Dong, and Joseph J. Lim. Universal agent for disentangling environments and tasks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Jiayuan%20Dong%2C%20Honghua%20Lim%2C%20Joseph%20J.%20Universal%20agent%20for%20disentangling%20environments%20and%20tasks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Jiayuan%20Dong%2C%20Honghua%20Lim%2C%20Joseph%20J.%20Universal%20agent%20for%20disentangling%20environments%20and%20tasks%202018"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Jarryd_et+al_2017_a",
            "entry": "Jarryd Martin, S Suraj Narayanan, Tom Everitt, and Marcus Hutter. Count-based exploration in feature space for reinforcement learning. In Association for the Advancement of Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jarryd%20Martin%2C%20S.Suraj%20Narayanan%20Everitt%2C%20Tom%20Hutter%2C%20Marcus%20Count-based%20exploration%20in%20feature%20space%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jarryd%20Martin%2C%20S.Suraj%20Narayanan%20Everitt%2C%20Tom%20Hutter%2C%20Marcus%20Count-based%20exploration%20in%20feature%20space%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "Merel_et+al_2017_a",
            "entry": "Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv: 1707.02201, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02201"
        },
        {
            "id": "Mulling_et+al_2013_a",
            "entry": "Katharina Mulling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. International Journal of Robotics Research, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mulling%2C%20Katharina%20Kober%2C%20Jens%20Kroemer%2C%20Oliver%20Peters%2C%20Jan%20Learning%20to%20select%20and%20generalize%20striking%20movements%20in%20robot%20table%20tennis%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mulling%2C%20Katharina%20Kober%2C%20Jens%20Kroemer%2C%20Oliver%20Peters%2C%20Jan%20Learning%20to%20select%20and%20generalize%20striking%20movements%20in%20robot%20table%20tennis%202013"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Kohli%2C%20Pushmeet%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Kohli%2C%20Pushmeet%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Pastor_et+al_2009_a",
            "entry": "Peter Pastor, Heiko Hoffmann, Tamim Asfour, and Stefan Schaal. Learning and generalization of motor skills by learning from demonstration. In International Conference on Robotics and Automation, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pastor%2C%20Peter%20Hoffmann%2C%20Heiko%20Asfour%2C%20Tamim%20Schaal%2C%20Stefan%20Learning%20and%20generalization%20of%20motor%20skills%20by%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pastor%2C%20Peter%20Hoffmann%2C%20Heiko%20Asfour%2C%20Tamim%20Schaal%2C%20Stefan%20Learning%20and%20generalization%20of%20motor%20skills%20by%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Riedmiller_et+al_2018_a",
            "entry": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom van de Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse reward tasks from scratch. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riedmiller%2C%20Martin%20Hafner%2C%20Roland%20Lampe%2C%20Thomas%20Neunert%2C%20Michael%20Learning%20by%20playing%20solving%20sparse%20reward%20tasks%20from%20scratch%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riedmiller%2C%20Martin%20Hafner%2C%20Roland%20Lampe%2C%20Thomas%20Neunert%2C%20Michael%20Learning%20by%20playing%20solving%20sparse%20reward%20tasks%20from%20scratch%202018"
        },
        {
            "id": "Schmidhuber_1990_a",
            "entry": "Jurgen Schmidhuber. Towards compositional learning with dynamic neural networks. 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Towards%20compositional%20learning%20with%20dynamic%20neural%20networks%201990"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Shu_et+al_2018_a",
            "entry": "Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shu%2C%20Tianmin%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Hierarchical%20and%20interpretable%20skill%20acquisition%20in%20multi-task%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shu%2C%20Tianmin%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Hierarchical%20and%20interpretable%20skill%20acquisition%20in%20multi-task%20reinforcement%20learning%202018"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning.%20Artificial%20intelligence%201999"
        },
        {
            "id": "Sutton_1984_a",
            "entry": "Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University of Massachusetts, Amherst, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20Stuart%20Temporal%20credit%20assignment%20in%20reinforcement%20learning%201984"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Alexander%20Sasha%20Vezhnevets%20Simon%20Osindero%20Tom%20Schaul%20Nicolas%20Heess%20Max%20Jaderberg%20David"
        },
        {
            "id": "Silver_2017_a",
            "entry": "Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In International Conference on Machine Learning, 2017. Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, 2017. Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Neural task programming: Learning to generalize across hierarchical tasks. In International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%20Kavukcuoglu%2C%20Koray%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%20Kavukcuoglu%2C%20Koray%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "The_2015_b",
            "entry": "The modular framework proposed in this paper allows a primitive policy to be any of a pre-trained neural network, inverse kinematics module, or hard-coded policy. In this paper, we use neural networks trained with TRPO (Schulman et al., 2015) on dedicated environments as primitive policies (see Section C for the details of environments and reward functions). All policy networks we used consists of 2 layers of 32 hidden units with tanh nonlinearities and predicts the mean and standard deviation of a Gaussian distribution over an action space. We trained all primitive policies until the total return converged (up to 10,000 iterations).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20modular%20framework%20proposed%20in%20this%20paper%20allows%20a%20primitive%20policy%20to%20be%20any%20of%20a%20pretrained%20neural%20network%20inverse%20kinematics%20module%20or%20hardcoded%20policy%20In%20this%20paper%20we%20use%20neural%20networks%20trained%20with%20TRPO%20Schulman%20et%20al%202015%20on%20dedicated%20environments%20as%20primitive%20policies%20see%20Section%20C%20for%20the%20details%20of%20environments%20and%20reward%20functions%20All%20policy%20networks%20we%20used%20consists%20of%202%20layers%20of%2032%20hidden%20units%20with%20tanh%20nonlinearities%20and%20predicts%20the%20mean%20and%20standard%20deviation%20of%20a%20Gaussian%20distribution%20over%20an%20action%20space%20We%20trained%20all%20primitive%20policies%20until%20the%20total%20return%20converged%20up%20to%2010000%20iterations"
        },
        {
            "id": "For_2017_c",
            "entry": "For the TRPO and PPO implementation, we used OpenAI baselines (Dhariwal et al., 2017) with default hyperparameters including learning rate, KL penalty, and entropy coefficients unless specified below.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20TRPO%20and%20PPO%20implementation%20we%20used%20OpenAI%20baselines%20Dhariwal%20et%20al%202017%20with%20default%20hyperparameters%20including%20learning%20rate%20KL%20penalty%20and%20entropy%20coefficients%20unless%20specified%20below"
        }
    ]
}
