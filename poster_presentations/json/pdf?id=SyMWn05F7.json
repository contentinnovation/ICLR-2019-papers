{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING EXPLORATION POLICIES FOR NAVIGATION",
        "author": "Tao Chen, Saurabh Gupta, 1Carnegie Mellon University 2Facebook AI Research",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyMWn05F7"
        },
        "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at: https://sites.google.com/view/exploration-for-nav/."
    },
    "keywords": [
        {
            "term": "geometry",
            "url": "https://en.wikipedia.org/wiki/geometry"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "spatial memory",
            "url": "https://en.wikipedia.org/wiki/spatial_memory"
        },
        {
            "term": "visual navigation",
            "url": "https://en.wikipedia.org/wiki/visual_navigation"
        },
        {
            "term": "system identification",
            "url": "https://en.wikipedia.org/wiki/system_identification"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "POMDPs": "Partially Observable Markov Decision Processes",
        "PPO": "policy optimization"
    },
    "highlights": [
        "SLAM-based approaches, first build a map and use localization and path planning for navigation",
        "Our work on learning exploration policies for navigation in real world scenes is related to active SLAM in classical robotics, and intrinsic rewards based exploration in reinforcement learning",
        "Before we describe our policy architecture, let\u2019s define what are the features that a good exploration policy needs: (a) good exploration of a new environment requires an agent to meaningfully move around by detecting and avoiding obstacles; (b) good policy requires the agent to identify semantic cues such as doors that may facilitate exploration; (c) it requires the agent to keep track of what parts of the environment have or have not been explored, and to estimate how to get to parts of the environment that may not have been explored",
        "We describe the intrinsic reward function that we use to train our exploration policy",
        "We showed how to design and train such policies, and how experience gathered from such policies enables better performance for downstream tasks"
    ],
    "key_statements": [
        "SLAM-based approaches, first build a map and use localization and path planning for navigation",
        "Our work on learning exploration policies for navigation in real world scenes is related to active SLAM in classical robotics, and intrinsic rewards based exploration in reinforcement learning",
        "Before we describe our policy architecture, let\u2019s define what are the features that a good exploration policy needs: (a) good exploration of a new environment requires an agent to meaningfully move around by detecting and avoiding obstacles; (b) good policy requires the agent to identify semantic cues such as doors that may facilitate exploration; (c) it requires the agent to keep track of what parts of the environment have or have not been explored, and to estimate how to get to parts of the environment that may not have been explored",
        "We describe the intrinsic reward function that we use to train our exploration policy",
        "We showed how to design and train such policies, and how experience gathered from such policies enables better performance for downstream tasks"
    ],
    "summary": [
        "SLAM-based approaches, first build a map and use localization and path planning for navigation.",
        "On the other hand for approaches that use learning, most only learn policies for specific tasks (Zhu et al, 2017; <a class=\"ref-link\" id=\"cMirowski_et+al_2017_a\" href=\"#rMirowski_et+al_2017_a\">Mirowski et al, 2017</a>; <a class=\"ref-link\" id=\"cGupta_et+al_2017_a\" href=\"#rGupta_et+al_2017_a\">Gupta et al, 2017</a>), or assume environments have already been explored (<a class=\"ref-link\" id=\"cSavinov_et+al_2018_a\" href=\"#rSavinov_et+al_2018_a\">Savinov et al, 2018</a>).",
        "Another possibility is to learn exploration policies on training environments.",
        "We show how experience gathered from our learned exploration policies improves performance at down-stream navigation tasks.",
        "Our work on learning exploration policies for navigation in real world scenes is related to active SLAM in classical robotics, and intrinsic rewards based exploration in reinforcement learning.",
        "We focus on this problem in the specific context of navigation in complex and realistic 3D environments, and propose specialized policy architectures and intrinsic reward functions.",
        "The design of the policy and the reward function depend on a rudimentary map of the environment that we maintain over time.",
        "While information from the occupancy maps allows the agent to keep track of parts of the environment that have or have not been explored and to plan paths to unexplored regions without bumping into obstacles.",
        "We describe the intrinsic reward function that we use to train our exploration policy.",
        "To overcome this large sample complexity, we pre-train our policy to imitate human demonstrations of how to explore a new environment.",
        "As described in Sec. 3.3, the intrinsic reward of the agent is based upon the map that it constructs as it moves around, and readings from the bump sensor.",
        "Learning allows us to arrive at policies that can explore using only RGB images at test time.",
        "We note that our agent can even be trained when the intrinsic reward estimation itself suffers from state estimation noise: for instance performance with 10% estimation noise is 98.9m2, only a minor degradation from 117.4m2 (10% estimation noise for map construction at test time only).",
        "This evaluation requires a navigation policy \u03c0n, that uses the exploration for the down-stream navexploration experience and the goal image to output actions that igation tasks.",
        "We motivated the need for learning explorations policies for navigation in novel 3D environments.",
        "We think we have just scratched the surface of this problem, and hope this inspires future research towards semantic exploration using more expressive policy architectures, reward functions and training techniques."
    ],
    "headline": "We study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards",
    "reference_links": [
        {
            "id": "Springer_2007_a",
            "entry": "Springer, 2007. 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202007%2014"
        },
        {
            "id": "Anderson_et+al_2018_a",
            "entry": "Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 9, 12",
            "arxiv_url": "https://arxiv.org/pdf/1807.06757"
        },
        {
            "id": "Argall_et+al_2009_a",
            "entry": "Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009. 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Bai_et+al_2016_a",
            "entry": "Shi Bai, Jinkun Wang, Fanfei Chen, and Brendan Englot. Information-theoretic exploration with bayesian optimization. In IROS, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bai%2C%20Shi%20Wang%2C%20Jinkun%20Chen%2C%20Fanfei%20Englot%2C%20Brendan%20Information-theoretic%20exploration%20with%20bayesian%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bai%2C%20Shi%20Wang%2C%20Jinkun%20Chen%2C%20Fanfei%20Englot%2C%20Brendan%20Information-theoretic%20exploration%20with%20bayesian%20optimization%202016"
        },
        {
            "id": "Burda_et+al_2019_a",
            "entry": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. In ICLR, 2019. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Yuri%20Edwards%2C%20Harri%20Pathak%2C%20Deepak%20Storkey%2C%20Amos%20and%20Alexei%20A%20Efros.%20Large-scale%20study%20of%20curiosity-driven%20learning%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Edwards%2C%20Harri%20Pathak%2C%20Deepak%20Storkey%2C%20Amos%20and%20Alexei%20A%20Efros.%20Large-scale%20study%20of%20curiosity-driven%20learning%202019"
        },
        {
            "id": "Cadena_et+al_2016_a",
            "entry": "Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jose Neira, Ian Reid, and John J Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cadena%2C%20Cesar%20Carlone%2C%20Luca%20Carrillo%2C%20Henry%20Latif%2C%20Yasir%20and%20future%20of%20simultaneous%20localization%20and%20mapping%3A%20Toward%20the%20robust-perception%20age%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cadena%2C%20Cesar%20Carlone%2C%20Luca%20Carrillo%2C%20Henry%20Latif%2C%20Yasir%20and%20future%20of%20simultaneous%20localization%20and%20mapping%3A%20Toward%20the%20robust-perception%20age%202016"
        },
        {
            "id": "Carrillo_et+al_2012_a",
            "entry": "Henry Carrillo, Ian Reid, and Jose A Castellanos. On the comparison of uncertainty criteria for active slam. In ICRA, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carrillo%2C%20Henry%20Reid%2C%20Ian%20Castellanos%2C%20Jose%20A.%20On%20the%20comparison%20of%20uncertainty%20criteria%20for%20active%20slam%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carrillo%2C%20Henry%20Reid%2C%20Ian%20Castellanos%2C%20Jose%20A.%20On%20the%20comparison%20of%20uncertainty%20criteria%20for%20active%20slam%202012"
        },
        {
            "id": "Chentanez_et+al_2005_a",
            "entry": "Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement learning. In NIPS, 2005. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "Das_et+al_2018_a",
            "entry": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answering. In CVPR, 2018. 5, 6, 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20Abhishek%20Datta%2C%20Samyak%20Gkioxari%2C%20Georgia%20Lee%2C%20Stefan%20Embodied%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20Abhishek%20Datta%2C%20Samyak%20Gkioxari%2C%20Georgia%20Lee%2C%20Stefan%20Embodied%20Question%20Answering%202018"
        },
        {
            "id": "Dornhege_2013_a",
            "entry": "Christian Dornhege and Alexander Kleiner. A frontier-void-based approach for autonomous exploration in 3D. Advanced Robotics, 2013. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dornhege%2C%20Christian%20Kleiner%2C%20Alexander%20A%20frontier-void-based%20approach%20for%20autonomous%20exploration%20in%203D%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dornhege%2C%20Christian%20Kleiner%2C%20Alexander%20A%20frontier-void-based%20approach%20for%20autonomous%20exploration%20in%203D%202013"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. 3",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, John Co-Reyes, and Sergey Levine. EX2: Exploration with exemplar models for deep reinforcement learning. In NIPS, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Co-Reyes%2C%20John%20Levine%2C%20Sergey%20EX2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Co-Reyes%2C%20John%20Levine%2C%20Sergey%20EX2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Gandhi_et+al_2017_a",
            "entry": "Dhiraj Gandhi, Lerrel Pinto, and Abhinav Gupta. Learning to fly by crashing. In IROS, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gandhi%2C%20Dhiraj%20Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Learning%20to%20fly%20by%20crashing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gandhi%2C%20Dhiraj%20Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Learning%20to%20fly%20by%20crashing%202017"
        },
        {
            "id": "Gupta_et+al_2017_a",
            "entry": "Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In CVPR, 2017. 1, 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Saurabh%20Davidson%2C%20James%20Levine%2C%20Sergey%20Sukthankar%2C%20Rahul%20Cognitive%20mapping%20and%20planning%20for%20visual%20navigation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Saurabh%20Davidson%2C%20James%20Levine%2C%20Sergey%20Sukthankar%2C%20Rahul%20Cognitive%20mapping%20and%20planning%20for%20visual%20navigation%202017"
        },
        {
            "id": "Hartley_2003_a",
            "entry": "Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 1, 2, 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartley%2C%20Richard%20Zisserman%2C%20Andrew%20Multiple%20view%20geometry%20in%20computer%20vision%202003"
        },
        {
            "id": "Henriques_2018_a",
            "entry": "Joao F Henriques and Andrea Vedaldi. MapNet: An allocentric spatial memory for mapping environments. In CVPR, 2018. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henriques%2C%20Joao%20F.%20Vedaldi%2C%20Andrea%20MapNet%3A%20An%20allocentric%20spatial%20memory%20for%20mapping%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henriques%2C%20Joao%20F.%20Vedaldi%2C%20Andrea%20MapNet%3A%20An%20allocentric%20spatial%20memory%20for%20mapping%20environments%202018"
        },
        {
            "id": "Jayaraman_2018_a",
            "entry": "Dinesh Jayaraman and Kristen Grauman. Learning to look around: Intelligently exploring unseen environments for unknown tasks. In CVPR, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayaraman%2C%20Dinesh%20Grauman%2C%20Kristen%20Learning%20to%20look%20around%3A%20Intelligently%20exploring%20unseen%20environments%20for%20unknown%20tasks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayaraman%2C%20Dinesh%20Grauman%2C%20Kristen%20Learning%20to%20look%20around%3A%20Intelligently%20exploring%20unseen%20environments%20for%20unknown%20tasks%202018"
        },
        {
            "id": "Kollar_2008_a",
            "entry": "Thomas Kollar and Nicholas Roy. Trajectory optimization using reinforcement learning for map exploration. IJRR, 2008. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kollar%2C%20Thomas%20Roy%2C%20Nicholas%20Trajectory%20optimization%20using%20reinforcement%20learning%20for%20map%20exploration%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kollar%2C%20Thomas%20Roy%2C%20Nicholas%20Trajectory%20optimization%20using%20reinforcement%20learning%20for%20map%20exploration%202008"
        },
        {
            "id": "Lavalle_2006_a",
            "entry": "Steven M LaValle. Planning algorithms. Cambridge university press, 2006. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LaValle%2C%20Steven%20M.%20Planning%20algorithms%202006"
        },
        {
            "id": "Ljung_1987_a",
            "entry": "Lennart Ljung. System identification: theory for the user. Prentice-hall, 1987. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ljung%2C%20Lennart%20System%20identification%3A%20theory%20for%20the%20user%201987"
        },
        {
            "id": "Lopes_et+al_2012_a",
            "entry": "Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS, 2012. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopes%2C%20Manuel%20Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Oudeyer%2C%20Pierre-Yves%20Exploration%20in%20model-based%20reinforcement%20learning%20by%20empirically%20estimating%20learning%20progress%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopes%2C%20Manuel%20Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Oudeyer%2C%20Pierre-Yves%20Exploration%20in%20model-based%20reinforcement%20learning%20by%20empirically%20estimating%20learning%20progress%202012"
        },
        {
            "id": "Martinez-Cantin_et+al_2009_a",
            "entry": "Ruben Martinez-Cantin, Nando de Freitas, Eric Brochu, Jose Castellanos, and Arnaud Doucet. A bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Autonomous Robots, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martinez-Cantin%2C%20Ruben%20de%20Freitas%2C%20Nando%20Brochu%2C%20Eric%20Castellanos%2C%20Jose%20A%20bayesian%20exploration-exploitation%20approach%20for%20optimal%20online%20sensing%20and%20planning%20with%20a%20visually%20guided%20mobile%20robot%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martinez-Cantin%2C%20Ruben%20de%20Freitas%2C%20Nando%20Brochu%2C%20Eric%20Castellanos%2C%20Jose%20A%20bayesian%20exploration-exploitation%20approach%20for%20optimal%20online%20sensing%20and%20planning%20with%20a%20visually%20guided%20mobile%20robot%202009"
        },
        {
            "id": "Michalski_et+al_1998_a",
            "entry": "Ryszard S Michalski, Ivan Bratko, and Avan Bratko. Machine learning and data mining; methods and applications. John Wiley & Sons, Inc., 1998. 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michalski%2C%20Ryszard%20S.%20Bratko%2C%20Ivan%20Bratko%2C%20Avan%20Machine%20learning%20and%20data%20mining%3B%20methods%20and%20applications%201998"
        },
        {
            "id": "Mirowski_et+al_2017_a",
            "entry": "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. In ICLR, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirowski%2C%20Piotr%20Pascanu%2C%20Razvan%20Viola%2C%20Fabio%20Soyer%2C%20Hubert%20Learning%20to%20navigate%20in%20complex%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirowski%2C%20Piotr%20Pascanu%2C%20Razvan%20Viola%2C%20Fabio%20Soyer%2C%20Hubert%20Learning%20to%20navigate%20in%20complex%20environments%202017"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. In ICLR, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20metalearner%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20metalearner%202018"
        },
        {
            "id": "Parisotto_2018_a",
            "entry": "Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. In ICLR, 2018. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20Salakhutdinov%2C%20Ruslan%20Neural%20map%3A%20Structured%20memory%20for%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20Salakhutdinov%2C%20Ruslan%20Neural%20map%3A%20Structured%20memory%20for%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017. 3, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In ICLR, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Sadeghi_2017_a",
            "entry": "Fereshteh Sadeghi and Sergey Levine. CAD2RL: Real single-image flight without a single real image. In RSS, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sadeghi%2C%20Fereshteh%20Levine%2C%20Sergey%20CAD2RL%3A%20Real%20single-image%20flight%20without%20a%20single%20real%20image%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sadeghi%2C%20Fereshteh%20Levine%2C%20Sergey%20CAD2RL%3A%20Real%20single-image%20flight%20without%20a%20single%20real%20image%202017"
        },
        {
            "id": "Savinov_et+al_2018_a",
            "entry": "Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. In ICLR, 2018. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Savinov%2C%20Nikolay%20Dosovitskiy%2C%20Alexey%20Koltun%2C%20Vladlen%20Semi-parametric%20topological%20memory%20for%20navigation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Savinov%2C%20Nikolay%20Dosovitskiy%2C%20Alexey%20Koltun%2C%20Vladlen%20Semi-parametric%20topological%20memory%20for%20navigation%202018"
        },
        {
            "id": "Schmidhuber_1991_a",
            "entry": "Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In International Conference on Simulation of Adaptive Behavior: From Animals to Animats, 1991. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers.%20In%20International%20Conference%20on%20Simulation%20of%20Adaptive%20Behavior%3A%20From%20Animals%20to%20Animats%201991"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5, 6",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Song_et+al_2017_a",
            "entry": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. In CVPR, 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Shuran%20Yu%2C%20Fisher%20Zeng%2C%20Andy%20Chang%2C%20Angel%20X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Shuran%20Yu%2C%20Fisher%20Zeng%2C%20Andy%20Chang%2C%20Angel%20X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017"
        },
        {
            "id": "Stadie_et+al_2015_a",
            "entry": "Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015. 3",
            "arxiv_url": "https://arxiv.org/pdf/1507.00814"
        },
        {
            "id": "Thrun_et+al_1999_a",
            "entry": "Sebastian Thrun, Maren Bennewitz, Wolfram Burgard, Armin B Cremers, Frank Dellaert, Dieter Fox, Dirk Hahnel, Charles Rosenberg, Nicholas Roy, Jamieson Schulte, et al. MINERVA: A second-generation museum tour-guide robot. In ICRA, 1999. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sebastian%20Thrun%20Maren%20Bennewitz%20Wolfram%20Burgard%20Armin%20B%20Cremers%20Frank%20Dellaert%20Dieter%20Fox%20Dirk%20Hahnel%20Charles%20Rosenberg%20Nicholas%20Roy%20Jamieson%20Schulte%20et%20al%20MINERVA%20A%20secondgeneration%20museum%20tourguide%20robot%20In%20ICRA%201999%201",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sebastian%20Thrun%20Maren%20Bennewitz%20Wolfram%20Burgard%20Armin%20B%20Cremers%20Frank%20Dellaert%20Dieter%20Fox%20Dirk%20Hahnel%20Charles%20Rosenberg%20Nicholas%20Roy%20Jamieson%20Schulte%20et%20al%20MINERVA%20A%20secondgeneration%20museum%20tourguide%20robot%20In%20ICRA%201999%201"
        },
        {
            "id": "Thrun_et+al_2005_a",
            "entry": "Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20Burgard%2C%20Wolfram%20Fox%2C%20Dieter%20Probabilistic%20robotics%202005"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "5",
            "entry": "5 Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018. 5, 13",
            "arxiv_url": "https://arxiv.org/pdf/1801.02209"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Kai Xu, Lintao Zheng, Zihao Yan, Guohang Yan, Eugene Zhang, Matthias Niessner, Oliver Deussen, Daniel Cohen-Or, and Hui Huang. Autonomous reconstruction of unknown indoor scenes guided by time-varying tensor fields. In SIGGRAPH Asia, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kai%20Zheng%2C%20Lintao%20Yan%2C%20Zihao%20Yan%2C%20Guohang%20Autonomous%20reconstruction%20of%20unknown%20indoor%20scenes%20guided%20by%20time-varying%20tensor%20fields%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kai%20Zheng%2C%20Lintao%20Yan%2C%20Zihao%20Yan%2C%20Guohang%20Autonomous%20reconstruction%20of%20unknown%20indoor%20scenes%20guided%20by%20time-varying%20tensor%20fields%202017"
        },
        {
            "id": "3",
            "entry": "3 Brian Yamauchi. A frontier-based approach for autonomous exploration. In Computational Intelligence in Robotics and Automation (CIRA), 1997. 1, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamauchi%2C%20Brian%20A%20frontier-based%20approach%20for%20autonomous%20exploration%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamauchi%2C%20Brian%20A%20frontier-based%20approach%20for%20autonomous%20exploration%201997"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. In RSS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Wenhao%20Jie%20Tan%2C%20C.Karen%20Liu%20Turk%2C%20Greg%20Preparing%20for%20the%20unknown%3A%20Learning%20a%20universal%20policy%20with%20online%20system%20identification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Wenhao%20Jie%20Tan%2C%20C.Karen%20Liu%20Turk%2C%20Greg%20Preparing%20for%20the%20unknown%3A%20Learning%20a%20universal%20policy%20with%20online%20system%20identification%202017"
        },
        {
            "id": "3",
            "entry": "3 Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, and Ming Liu. Neural slam: Learning to explore with external memory. arXiv preprint, 2017. URL http://arxiv.org/abs/1706.09520.",
            "url": "http://arxiv.org/abs/1706.09520",
            "arxiv_url": "https://arxiv.org/pdf/1706.09520"
        },
        {
            "id": "3",
            "entry": "3 Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yuke%20Mottaghi%2C%20Roozbeh%20Kolve%2C%20Eric%20Lim%2C%20Joseph%20J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yuke%20Mottaghi%2C%20Roozbeh%20Kolve%2C%20Eric%20Lim%2C%20Joseph%20J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017"
        }
    ]
}
