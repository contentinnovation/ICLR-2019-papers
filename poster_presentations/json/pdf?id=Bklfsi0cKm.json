{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES",
        "author": "Adri\u00e0 Garriga-Alonso department of Engineering University of Cambridge ag,@cam.ac.uk",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bklfsi0cKm"
        },
        "abstract": "We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike \u201cdeep kernels\u201d, has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GPs with a comparable number of parameters. 1"
    },
    "keywords": [
        {
            "term": "Central Limit Theorem",
            "url": "https://en.wikipedia.org/wiki/Central_Limit_Theorem"
        },
        {
            "term": "Gaussian Processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Processes"
        },
        {
            "term": "large number",
            "url": "https://en.wikipedia.org/wiki/large_number"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "Convolutional Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        }
    ],
    "abbreviations": {
        "CNN": "convolutional neural network",
        "GP": "Gaussian process",
        "CNNs": "Convolutional Neural Networks",
        "GPs": "Gaussian Processes",
        "NNs": "neural networks",
        "CLT": "Central Limit Theorem"
    },
    "highlights": [
        "Convolutional Neural Networks (CNNs) have powerful pattern-recognition capabilities that have recently given dramatic improvements in important tasks such as image classification (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>)",
        "As convolutional neural network are increasingly being applied in real-world, safety-critical domains, their vulnerability to adversarial examples (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>), and their poor uncertainty estimates are becoming increasingly problematic",
        "We show that two properties of the Gaussian process kernel induced by a convolutional neural network allow it to be computed very efficiently",
        "While it is still necessary to compute the variance of the output of a convolutional filter applied at all locations within the image, the specific structure of the kernel induced by the convolutional neural network means that the variance at every location can be computed simultaneously and efficiently as a convolution",
        "We have shown that deep Bayesian convolutional neural network with infinitely many filters are equivalent to a Gaussian process with a recursive kernel"
    ],
    "key_statements": [
        "Convolutional Neural Networks (CNNs) have powerful pattern-recognition capabilities that have recently given dramatic improvements in important tasks such as image classification (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>)",
        "As convolutional neural network are increasingly being applied in real-world, safety-critical domains, their vulnerability to adversarial examples (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>), and their poor uncertainty estimates are becoming increasingly problematic",
        "We show that two properties of the Gaussian process kernel induced by a convolutional neural network allow it to be computed very efficiently",
        "While it is still necessary to compute the variance of the output of a convolutional filter applied at all locations within the image, the specific structure of the kernel induced by the convolutional neural network means that the variance at every location can be computed simultaneously and efficiently as a convolution",
        "We have shown that deep Bayesian convolutional neural network with infinitely many filters are equivalent to a Gaussian process with a recursive kernel"
    ],
    "summary": [
        "Convolutional Neural Networks (CNNs) have powerful pattern-recognition capabilities that have recently given dramatic improvements in important tasks such as image classification (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>).",
        "If each hidden layer has an infinite number of convolutional filters, the network prior is equivalent to a GP.",
        "In previous work it was necessary to compute the covariance matrix for the output of a single convolutional filter applied at all possible locations within a single image, which was prohibitively computationally expensive.",
        "While it is still necessary to compute the variance of the output of a convolutional filter applied at all locations within the image, the specific structure of the kernel induced by the CNN means that the variance at every location can be computed simultaneously and efficiently as a convolution.",
        "It is surprising that we can compute the kernel efficiently because the feature maps, a(i )(X), display rich covariance structure due to the shared convolutional filter.",
        "This is surprising because for many networks, we need to compute the covariance of activations between all pairs of locations in the feature map (i.e. C Ai(,\u03bc+1)(X), Ai(,\u03bc+1)(X ) for \u03bc, \u03bc \u2208 {1, .",
        "The particular form for the kernel implies that the required variances and covariances at all required locations can be computed efficiently as a convolution.",
        "The kernel hyperparameters are: \u03c3b2, \u03c3w2 ; the number of layers; the convolution stride, filter sizes and edge behaviour; the nonlinearity; and the frequency of residual skip connections.",
        "We chose to remove the pooling because computing its output variance requires the off-diagonal elements of the filter covariance, in which case we could not exploit the efficiency gains described in Sec. 3.3.",
        "We found that the despite it not being optimised, the 32-layer ResNet GP outperformed all other comparable architectures (Table 1), including the NNGP in <a class=\"ref-link\" id=\"cLee_et+al_2017_a\" href=\"#rLee_et+al_2017_a\">Lee et al (2017</a>), which is state-ofthe-art for non-convolutional networks, and convolutional GPs. That said, our results have not reached state-of-the-art for methods that incorporate a parametric neural network, such as a standard ResNet (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a>) and a Gaussian process with a deep neural network kernel <a class=\"ref-link\" id=\"cBradshaw_et+al_2017_a\" href=\"#rBradshaw_et+al_2017_a\">Bradshaw et al (2017</a>).",
        "The kernels in this work, directly motivated by the infinite-filter limit of a CNN, only apply something like kp to the corresponding pairs of patches within X and X (Eq 10).",
        "Apart from their very different focus, the key difference to our work is that they compute the variance for a single trainingexample, whereas to obtain the GPs kernel, we need to compute the output covariances for different training/test examples (<a class=\"ref-link\" id=\"cXiao_et+al_2018_a\" href=\"#rXiao_et+al_2018_a\">Xiao et al, 2018</a>).",
        "We hope to apply GP CNNs in domains as widespread as adversarial examples, lifelong learning and k-shot learning, and we hope to improve them by developing efficient multi-layered inducing point approximation schemes"
    ],
    "headline": "We show that the output of a convolutional neural network with an appropriate prior over the weights and biases is a Gaussian process in the limit of infinitely many convolutional filters, extending similar results for dense networks",
    "reference_links": [
        {
            "id": "Jr_1958_a",
            "entry": "JR Blum, H Chernoff, M Rosenblatt, and H Teicher. Central limit theorems for interchangeable processes. Canad. J. Math, 10:222\u2013229, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JR%20Blum%2C%20H%20Chernoff%2C%20M%20Rosenblatt%20Teicher%2C%20H%20Central%20limit%20theorems%20for%20interchangeable%20processes%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=JR%20Blum%2C%20H%20Chernoff%2C%20M%20Rosenblatt%20Teicher%2C%20H%20Central%20limit%20theorems%20for%20interchangeable%20processes%201958"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015"
        },
        {
            "id": "Borovykh_2018_a",
            "entry": "Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. ResearchGate, 05 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borovykh%2C%20Anastasia%20A%20gaussian%20process%20perspective%20on%20convolutional%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borovykh%2C%20Anastasia%20A%20gaussian%20process%20perspective%20on%20convolutional%20neural%20networks%202018"
        },
        {
            "id": "Bradshaw_et+al_2017_a",
            "entry": "John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02476"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07366"
        },
        {
            "id": "Cho_2009_a",
            "entry": "Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342\u2013350, 2009. URL http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf.",
            "url": "http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "Damianou_2013_a",
            "entry": "Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Carlos M. Carvalho and Pradeep Ravikumar (eds.), Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pp. 207\u2013 215, Scottsdale, Arizona, USA, 29 Apr\u201301 May 2013. PMLR. URL http://proceedings.mlr.press/v31/damianou13a.html.",
            "url": "http://proceedings.mlr.press/v31/damianou13a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damianou%2C%20Andreas%20Lawrence%2C%20Neil%20Deep%20gaussian%20processes%202013-04-29"
        },
        {
            "id": "Deisenroth_2011_a",
            "entry": "Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML11), pp. 465\u2013472, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20Marc%20Rasmussen%2C%20Carl%20E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20Marc%20Rasmussen%2C%20Carl%20E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "Gal_2015_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02142"
        },
        {
            "id": "Gal_2018_a",
            "entry": "Yarin Gal and Lewis Smith. Idealised bayesian neural networks cannot have adversarial examples: Theoretical and empirical study. arXiv preprint arXiv:1806.00667, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00667"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016a. URL https://arxiv.org/pdf/1512.03385.pdf.",
            "url": "https://arxiv.org/pdf/1512.03385.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Hern_et+al_2011_a",
            "entry": "Daniel Hern\u00e1ndez-lobato, Jose M. Hern\u00e1ndez-lobato, and Pierre Dupont. Robust multiclass gaussian process classification. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 24, pp. 280\u2013288. 2011. URL http://papers.nips.cc/paper/4241-robust-multi-class-gaussian-process-classification.pdf.",
            "url": "http://papers.nips.cc/paper/4241-robust-multi-class-gaussian-process-classification.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-lobato%2C%20Daniel%20Hern%C3%A1ndez-lobato%2C%20Jose%20M.%20Dupont%2C%20Pierre%20Robust%20multiclass%20gaussian%20process%20classification%202011"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kumar_et+al_2018_a",
            "entry": "Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep gaussian processes with convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01655"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "Lakshminarayanan_et+al_2017_a",
            "entry": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pp. 6402\u20136413, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017"
        },
        {
            "id": "Lecun_1990_a",
            "entry": "Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pp. 396\u2013404, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yann%20LeCun%20Bernhard%20E%20Boser%20John%20S%20Denker%20Donnie%20Henderson%20Richard%20E%20Howard%20Wayne%20E%20Hubbard%20and%20Lawrence%20D%20Jackel%20Handwritten%20digit%20recognition%20with%20a%20backpropagation%20network%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20396404%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yann%20LeCun%20Bernhard%20E%20Boser%20John%20S%20Denker%20Donnie%20Henderson%20Richard%20E%20Howard%20Wayne%20E%20Hubbard%20and%20Lawrence%20D%20Jackel%20Handwritten%20digit%20recognition%20with%20a%20backpropagation%20network%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20396404%201990"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00165"
        },
        {
            "id": "Mandt_et+al_2017_a",
            "entry": "Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873\u20134907, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandt%2C%20Stephan%20Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandt%2C%20Stephan%20Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017"
        },
        {
            "id": "De_et+al_2018_a",
            "entry": "Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018a. URL https://openreview.net/forum?id= H1-nGgWC-.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20G.%20Matthews%2C%20Alexander%20G.%20Hron%2C%20Jiri%20Rowland%2C%20Mark%20Turner%2C%20Richard%20E.%20Gaussian%20process%20behaviour%20in%20wide%20deep%20neural%20networks%202018"
        },
        {
            "id": "De_et+al_0000_a",
            "entry": "Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11271"
        },
        {
            "id": "Matthews_et+al_2017_a",
            "entry": "De G Matthews, G Alexander, Mark Van Der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo Le\u00f3n-Villagr\u00e1, Zoubin Ghahramani, and James Hensman. GPflow: A gaussian process library using TensorFlow. The Journal of Machine Learning Research, 18(1):1299\u20131304, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20De%20G.%20Alexander%2C%20G.%20Wilk%2C%20Mark%20Van%20Der%20Nickson%2C%20Tom%20GPflow%3A%20A%20gaussian%20process%20library%20using%20TensorFlow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20De%20G.%20Alexander%2C%20G.%20Wilk%2C%20Mark%20Van%20Der%20Nickson%2C%20Tom%20GPflow%3A%20A%20gaussian%20process%20library%20using%20TensorFlow%202017"
        },
        {
            "id": "Neal_1996_a",
            "entry": "Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996. ISBN 0387947248.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20Learning%20for%20Neural%20Networks%201996"
        },
        {
            "id": "Rasmussen_2006_a",
            "entry": "Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20processes%20for%20machine%20learning%2C%20volume%201%202006"
        },
        {
            "id": "Schoenholz_et+al_2016_a",
            "entry": "Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "Snoek_et+al_2012_a",
            "entry": "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Van_et+al_2017_a",
            "entry": "Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes. In Advances in Neural Information Processing Systems, pp. 2845\u20132854, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Wilk%2C%20Mark%20Rasmussen%2C%20Carl%20Edward%20Hensman%2C%20James%20Convolutional%20gaussian%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Wilk%2C%20Mark%20Rasmussen%2C%20Carl%20Edward%20Hensman%2C%20James%20Convolutional%20gaussian%20processes%202017"
        },
        {
            "id": "Welling_2011_a",
            "entry": "Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681\u2013688, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011"
        },
        {
            "id": "Wilson_et+al_2016_a",
            "entry": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pp. 370\u2013378, Cadiz, Spain, 09\u201311 May 2016. PMLR. URL http://proceedings.mlr.press/v51/wilson16.html.",
            "url": "http://proceedings.mlr.press/v51/wilson16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20Xing%2C%20Eric%20P.%20Deep%20kernel%20learning%202016-05"
        },
        {
            "id": "Xiao_et+al_2018_a",
            "entry": "Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05393"
        },
        {
            "id": "Yang_2017_a",
            "entry": "Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103\u20137114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Ge%20Schoenholz%2C%20Samuel%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Ge%20Schoenholz%2C%20Samuel%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%202017"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        }
    ]
}
