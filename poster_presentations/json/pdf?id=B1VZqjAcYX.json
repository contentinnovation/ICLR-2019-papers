{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY",
        "author": "Namhoon Lee, Thalaiyasingam Ajanthan & Philip H. S. Torr University of Oxford {namhoon,ajanthan,phst}@robots.ox.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1VZqjAcYX"
        },
        "abstract": "Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task."
    },
    "keywords": [
        {
            "term": "sparse network",
            "url": "https://en.wikipedia.org/wiki/sparse_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "SNIP",
            "url": "https://en.wikipedia.org/wiki/SNIP"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "new approach",
            "url": "https://en.wikipedia.org/wiki/New_Approach"
        },
        {
            "term": "single shot",
            "url": "https://en.wikipedia.org/wiki/single_shot"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Despite the success of deep neural networks in machine learning, they are often found to be highly overparametrized making them computationally expensive with excessive memory requirements",
        "Given a large reference neural network, the goal is to learn a much smaller subnetwork that mimics the performance of the reference network",
        "The majority of existing methods in the literature attempt to find a subset of weights from the pretrained reference network either based on a saliency criterion (<a class=\"ref-link\" id=\"cMozer_1989_a\" href=\"#rMozer_1989_a\">Mozer & Smolensky (1989</a>); <a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\">LeCun et al (1990</a>); <a class=\"ref-link\" id=\"cHan_et+al_2015_a\" href=\"#rHan_et+al_2015_a\">Han et al (2015</a>)) or utilizing sparsity enforcing penalties (<a class=\"ref-link\" id=\"cChauvin_1989_a\" href=\"#rChauvin_1989_a\">Chauvin (1989</a>); Carreira-Perpinan & Idelbayev (2018))",
        "We have shown that our approach can prune a variety of deep neural network architectures for extreme sparsities without losing much on accuracy",
        "We have presented a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of neural network models without modifications",
        "While SNIP results in extremely sparse models, we find that our connection sensitivity measure itself is noteworthy in that it diagnoses important connections in the network from a purely untrained network"
    ],
    "key_statements": [
        "Despite the success of deep neural networks in machine learning, they are often found to be highly overparametrized making them computationally expensive with excessive memory requirements",
        "Given a large reference neural network, the goal is to learn a much smaller subnetwork that mimics the performance of the reference network",
        "The majority of existing methods in the literature attempt to find a subset of weights from the pretrained reference network either based on a saliency criterion (<a class=\"ref-link\" id=\"cMozer_1989_a\" href=\"#rMozer_1989_a\">Mozer & Smolensky (1989</a>); <a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\">LeCun et al (1990</a>); <a class=\"ref-link\" id=\"cHan_et+al_2015_a\" href=\"#rHan_et+al_2015_a\">Han et al (2015</a>)) or utilizing sparsity enforcing penalties (<a class=\"ref-link\" id=\"cChauvin_1989_a\" href=\"#rChauvin_1989_a\">Chauvin (1989</a>); Carreira-Perpinan & Idelbayev (2018))",
        "Since pruning is included as a part of an iterative optimization procedure, all these methods require many expensive prune \u2013 retrain cycles and heuristic design choices with additional hyperparameters, making them non-trivial to extend to new architectures and tasks",
        "We introduce a saliency criterion that identifies connections in the network that are important to the given task in a data-dependent way before training",
        "We discover important connections based on their influence on the loss function at a variance scaling initialization, which we call connection sensitivity",
        "Our method has no additional hyperparameters and once pruned, training of the sparse network is performed in the standard way. Versatility",
        "We evaluate our method on MNIST, CIFAR-10, and Tiny-ImageNet classification datasets with widely varying architectures",
        "We focus on weight pruning that is free from structural constraints and amenable to further compression schemes.\n3 NEURAL NETWORK PRUNING",
        "Since the dataset and the loss function defines the task at hand, by relying on both of them, our saliency criterion discovers the connections in the network that are important to the given task",
        "SNIP, on MNIST, CIFAR-10 and Tiny-ImageNet classification tasks with a variety of network architectures",
        "Our results show that SNIP yields extremely sparse models with minimal or no loss in accuracy across all tested architectures, while being much simpler than other state-of-the-art alternatives",
        "For slightly relaxed sparsities (i.e., 95% for LeNet-300-100 and 98% for LeNet-5-Caffe), the sparse models pruned by SNIP record better performances than the dense reference network",
        "We have shown that our approach can prune a variety of deep neural network architectures for extreme sparsities without losing much on accuracy",
        "We have presented a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of neural network models without modifications",
        "While SNIP results in extremely sparse models, we find that our connection sensitivity measure itself is noteworthy in that it diagnoses important connections in the network from a purely untrained network"
    ],
    "summary": [
        "Despite the success of deep neural networks in machine learning, they are often found to be highly overparametrized making them computationally expensive with excessive memory requirements.",
        "We introduce a saliency criterion that identifies connections in the network that are important to the given task in a data-dependent way before training.",
        "Our method has no additional hyperparameters and once pruned, training of the sparse network is performed in the standard way.",
        "Given a neural network and a dataset, our goal is to design a method that can selectively prune redundant connections for the given task in a data-dependent way even before training.",
        "\u2212\u2202L/\u2202w or \u2212\u2202L/\u2202\u03b1; \u03b1 refers to the connectivity of neurons), depend on the loss value before pruning, which in turn, require the network to be pre-trained and iterative optimization cycles to ensure minimal loss in performance.",
        "Since the dataset and the loss function defines the task at hand, by relying on both of them, our saliency criterion discovers the connections in the network that are important to the given task.",
        "Note that our saliency measure does not require the network to be pre-trained and is computed at random initialization.",
        "For slightly relaxed sparsities (i.e., 95% for LeNet-300-100 and 98% for LeNet-5-Caffe), the sparse models pruned by SNIP record better performances than the dense reference network.",
        "Note that prior art that use a saliency criterion based on the weights would require considerable adjustments in their pruning schedules as per changes in the model.",
        "Even though these methods are designed for RNNs, none of them are able to obtain extreme sparsity without substantial loss in accuracy reflecting the challenges of pruning RNNs. To the best of our knowledge, we are the first to demonstrate on convolutional, residual and recurrent networks for extreme sparsities without requiring additional hyperparameters or modifying the pruning procedure.",
        "Recall that our connection saliency measure depends on the network weights w as well as the given data D (Section 4.2).",
        "Our connection saliency measure depends on a mini-batch of train examples Db. To study the effect of data, we vary the batch size used to compute the saliency (|Db|) and check which connections are being pruned as well as how much performance change this results in on the corresponding sparse network.",
        "We have presented a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of neural network models without modifications."
    ],
    "headline": "We present a new approach that prunes a given network once at initialization prior to training",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018"
        },
        {
            "id": "Breiman_1995_a",
            "entry": "Leo Breiman. Better subset regression using the nonnegative garrote. Technometrics, 1995. Miguel A. Carreira-Perpinan and Yerlan Idelbayev. \u201cLearning-compression\u201d algorithms for neural net pruning. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Better%20subset%20regression%20using%20the%20nonnegative%20garrote%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Better%20subset%20regression%20using%20the%20nonnegative%20garrote%201995"
        },
        {
            "id": "Chauvin_1989_a",
            "entry": "Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. NIPS, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chauvin%2C%20Yves%20A%20back-propagation%20algorithm%20with%20optimal%20use%20of%20hidden%20units%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chauvin%2C%20Yves%20A%20back-propagation%20algorithm%20with%20optimal%20use%20of%20hidden%20units%201989"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Dong_et+al_2017_a",
            "entry": "Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layer-wise%20optimal%20brain%20surgeon%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layer-wise%20optimal%20brain%20surgeon%202017"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Gong_et+al_2014_a",
            "entry": "Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%202016"
        },
        {
            "id": "Guo_et+al_2016_a",
            "entry": "Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016"
        },
        {
            "id": "Gupta_et+al_2015_a",
            "entry": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "Hassibi_et+al_1993_a",
            "entry": "Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. Neural Networks, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Stork%2C%20David%20G.%20Wolff%2C%20Gregory%20J.%20Optimal%20brain%20surgeon%20and%20general%20network%20pruning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Stork%2C%20David%20G.%20Wolff%2C%20Gregory%20J.%20Optimal%20brain%20surgeon%20and%20general%20network%20pruning%201993"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "Hubara_et+al_2016_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubara%2C%20Itay%20Courbariaux%2C%20Matthieu%20Soudry%2C%20Daniel%20El-Yaniv%2C%20Ran%20Binarized%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubara%2C%20Itay%20Courbariaux%2C%20Matthieu%20Soudry%2C%20Daniel%20El-Yaniv%2C%20Ran%20Binarized%20neural%20networks%202016"
        },
        {
            "id": "Ishikawa_1996_a",
            "entry": "Masumi Ishikawa. Structural learning with forgetting. Neural Networks, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishikawa%2C%20Masumi%20Structural%20learning%20with%20forgetting%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishikawa%2C%20Masumi%20Structural%20learning%20with%20forgetting%201996"
        },
        {
            "id": "Jaderberg_et+al_2014_a",
            "entry": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. BMVC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014"
        },
        {
            "id": "Karnin_1990_a",
            "entry": "Ehud D Karnin. A simple procedure for pruning back-propagation trained neural networks. Neural Networks, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karnin%2C%20Ehud%20D.%20A%20simple%20procedure%20for%20pruning%20back-propagation%20trained%20neural%20networks%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karnin%2C%20Ehud%20D.%20A%20simple%20procedure%20for%20pruning%20back-propagation%20trained%20neural%20networks%201990"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Koh_2017_a",
            "entry": "Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20Pang%20Wei%20Liang%2C%20Percy%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20Pang%20Wei%20Liang%2C%20Percy%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Le_et+al_2015_a",
            "entry": "Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. CoRR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Jaitly%2C%20Navdeep%20Hinton%2C%20Geoffrey%20E.%20A%20simple%20way%20to%20initialize%20recurrent%20networks%20of%20rectified%20linear%20units%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Jaitly%2C%20Navdeep%20Hinton%2C%20Geoffrey%20E.%20A%20simple%20way%20to%20initialize%20recurrent%20networks%20of%20rectified%20linear%20units%202015"
        },
        {
            "id": "Lecun_et+al_1990_a",
            "entry": "Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. NIPS, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Efficient backprop. Neural Networks: Tricks of the Trade, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Orr%2C%20Genevieve%20B.%20Muller%2C%20Klaus-Robert%20Efficient%20backprop%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Orr%2C%20Genevieve%20B.%20Muller%2C%20Klaus-Robert%20Efficient%20backprop%201998"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20convnets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20convnets%202017"
        },
        {
            "id": "Louizos_et+al_2018_a",
            "entry": "Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l0 regularization. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Kingma%2C%20Diederik%20P.%20Learning%20sparse%20neural%20networks%20through%20l0%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Kingma%2C%20Diederik%20P.%20Learning%20sparse%20neural%20networks%20through%20l0%20regularization%202018"
        },
        {
            "id": "Mocanu_et+al_2018_a",
            "entry": "Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mocanu%2C%20Decebal%20Constantin%20Mocanu%2C%20Elena%20Stone%2C%20Peter%20Nguyen%2C%20Phuong%20H.%20Scalable%20training%20of%20artificial%20neural%20networks%20with%20adaptive%20sparse%20connectivity%20inspired%20by%20network%20science%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mocanu%2C%20Decebal%20Constantin%20Mocanu%2C%20Elena%20Stone%2C%20Peter%20Nguyen%2C%20Phuong%20H.%20Scalable%20training%20of%20artificial%20neural%20networks%20with%20adaptive%20sparse%20connectivity%20inspired%20by%20network%20science%202018"
        },
        {
            "id": "Molchanov_et+al_2017_a",
            "entry": "Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. ICML, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017"
        },
        {
            "id": "Molchanov_et+al_2017_b",
            "entry": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. ICLR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20Pavlo%20Tyree%2C%20Stephen%20Karras%2C%20Tero%20Aila%2C%20Timo%20Pruning%20convolutional%20neural%20networks%20for%20resource%20efficient%20inference%202017"
        },
        {
            "id": "Mozer_1989_a",
            "entry": "Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. NIPS, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mozer%2C%20Michael%20C.%20Smolensky%2C%20Paul%20Skeletonization%3A%20A%20technique%20for%20trimming%20the%20fat%20from%20a%20network%20via%20relevance%20assessment%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mozer%2C%20Michael%20C.%20Smolensky%2C%20Paul%20Skeletonization%3A%20A%20technique%20for%20trimming%20the%20fat%20from%20a%20network%20via%20relevance%20assessment%201989"
        },
        {
            "id": "Narang_et+al_2017_a",
            "entry": "Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho Sengupta. Exploring sparsity in recurrent neural networks. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narang%2C%20Sharan%20Elsen%2C%20Erich%20Diamos%2C%20Gregory%20Sengupta%2C%20Shubho%20Exploring%20sparsity%20in%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narang%2C%20Sharan%20Elsen%2C%20Erich%20Diamos%2C%20Gregory%20Sengupta%2C%20Shubho%20Exploring%20sparsity%20in%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "Novikov_et+al_2015_a",
            "entry": "Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novikov%2C%20Alexander%20Podoprikhin%2C%20Dmitrii%20Osokin%2C%20Anton%20Vetrov%2C%20Dmitry%20P.%20Tensorizing%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novikov%2C%20Alexander%20Podoprikhin%2C%20Dmitrii%20Osokin%2C%20Anton%20Vetrov%2C%20Dmitry%20P.%20Tensorizing%20neural%20networks%202015"
        },
        {
            "id": "Nowlan_1992_a",
            "entry": "Steven J Nowlan and Geoffrey E Hinton. Simplifying neural networks by soft weight-sharing. Neural Computation, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Simplifying%20neural%20networks%20by%20soft%20weight-sharing%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Simplifying%20neural%20networks%20by%20soft%20weight-sharing%201992"
        },
        {
            "id": "Prabhu_et+al_2018_a",
            "entry": "Ameya Prabhu, Girish Varma, and Anoop Namboodiri. Deep expander networks: Efficient deep networks from graph theory. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prabhu%2C%20Ameya%20Varma%2C%20Girish%20Namboodiri%2C%20Anoop%20Deep%20expander%20networks%3A%20Efficient%20deep%20networks%20from%20graph%20theory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prabhu%2C%20Ameya%20Varma%2C%20Girish%20Namboodiri%2C%20Anoop%20Deep%20expander%20networks%3A%20Efficient%20deep%20networks%20from%20graph%20theory%202018"
        },
        {
            "id": "Reed_1993_a",
            "entry": "Russell Reed. Pruning algorithms-a survey. Neural Networks, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Russell%20Pruning%20algorithms-a%20survey%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Russell%20Pruning%20algorithms-a%20survey%201993"
        },
        {
            "id": "See_et+al_2016_a",
            "entry": "Abigail See, Minh-Thang Luong, and Christopher D Manning. Compression of neural machine translation models via pruning. CoNLL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%2C%20Abigail%20Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Compression%20of%20neural%20machine%20translation%20models%20via%20pruning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=See%2C%20Abigail%20Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Compression%20of%20neural%20machine%20translation%20models%20via%20pruning%202016"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017"
        },
        {
            "id": "Weigend_et+al_1991_a",
            "entry": "Andreas S Weigend, David E Rumelhart, and Bernardo A Huberman. Generalization by weightelimination with application to forecasting. NIPS, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weigend%2C%20Andreas%20S.%20Rumelhart%2C%20David%20E.%20Huberman%2C%20Bernardo%20A.%20Generalization%20by%20weightelimination%20with%20application%20to%20forecasting%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weigend%2C%20Andreas%20S.%20Rumelhart%2C%20David%20E.%20Huberman%2C%20Bernardo%20A.%20Generalization%20by%20weightelimination%20with%20application%20to%20forecasting%201991"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Zagoruyko_2015_a",
            "entry": "Sergey Zagoruyko. 92.45% on cifar-10 in torch. Torch Blog, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%2092.45%25%20on%20cifar-10%20in%20torch%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%2092.45%25%20on%20cifar-10%20in%20torch%202015"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. BMVC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        },
        {
            "id": "Zaremba_et+al_2014_a",
            "entry": "Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
