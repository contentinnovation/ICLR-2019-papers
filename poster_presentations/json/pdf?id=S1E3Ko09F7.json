{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "L-SHAPLEY AND C-SHAPLEY: EFFICIENT MODEL INTERPRETATION FOR STRUCTURED DATA",
        "author": "Jianbo Chen, Le Song,\u00a7 Martin J. Wainwright,\u2021 Michael I. Jordan, UC Berkeley, Georgia Institute of Technology, Ant Financial\u00a7, Voleon Group",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1E3Ko09F7"
        },
        "abstract": "Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graphstructured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation."
    },
    "keywords": [
        {
            "term": "IMDB",
            "url": "https://en.wikipedia.org/wiki/IMDB"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "Amazon Mechanical Turk",
            "url": "https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"
        },
        {
            "term": "shapley value",
            "url": "https://en.wikipedia.org/wiki/shapley_value"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "metrics",
            "url": "https://en.wikipedia.org/wiki/metrics"
        }
    ],
    "abbreviations": {
        "LSTM": "long-short term memory",
        "AMT": "Amazon Mechanical Turk"
    },
    "highlights": [
        "Many black box machine learning models, such as random forests, deep neural networks, and kernel methods, can produce highly accurate prediction in many applications, such prediction often comes at the cost of interpretability",
        "We focus on settings in which a graph structure is appropriate for describing the relations between features in the data, and distant features according to the graph have weak interaction during the computation of Shapley values",
        "Coming back to Example ( ) where we evaluate the importance of \u201cnot,\u201d both the L-Shapley estimate of order larger than two and the exact Shapley value estimate would evaluate the model on the word subset \u201cIt not heartwarming,\u201d which rarely appears in real data and may not make sense to a human or a model trained on real-world data",
        "We study the performance of L-Shapley and C-Shapley on three popular neural models for text classification: word-based CNNs (<a class=\"ref-link\" id=\"cKim_2014_a\" href=\"#rKim_2014_a\">Kim, 2014</a>), characterbased CNNs (<a class=\"ref-link\" id=\"cZhang_et+al_2015_a\" href=\"#rZhang_et+al_2015_a\">Zhang et al, 2015</a>), and long-short term memory (LSTM) recurrent neural networks (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>), with the following three data sets on different scales: (i) IMDB Review with Word-CNN: A simple word-based CNN model is used on the IMDB movie review data set, achieving a test accuracy of 90.1%; AG news with Char-CNN: We implement a character-based CNN on the AG news corpus <a class=\"ref-link\" id=\"cZhang_et+al_2015_a\" href=\"#rZhang_et+al_2015_a\">Zhang et al (2015</a>), achieving a test accuracy of",
        "We have proposed two new algorithms\u2014L-Shapley and C-Shapley\u2014for instancewise feature importance scoring, making use of a graphical representation of the data",
        "We have demonstrated the superior performance of these algorithms compared to other methods on black-box models for instancewise feature importance scoring in both text and image classification with both quantitative metrics and human evaluation"
    ],
    "key_statements": [
        "Many black box machine learning models, such as random forests, deep neural networks, and kernel methods, can produce highly accurate prediction in many applications, such prediction often comes at the cost of interpretability",
        "We study instancewise feature importance scoring as a specific approach to the problem of interpreting the predictions of black-box models",
        "Such a method yields, for each instance to which the model is applied, a vector of importance scores associated with the underlying features",
        "The importance scores can act as an explanation for the specific instance, indicating which features are the key for the model to make its prediction on that instance",
        "We focus on settings in which a graph structure is appropriate for describing the relations between features in the data, and distant features according to the graph have weak interaction during the computation of Shapley values",
        "We propose two methods for instancewise feature importance scoring in this framework, which we term L-Shapley and C-Shapley; here the abbreviations \u201cL\" and \u201cC\" refer to \u201clocal\u201d and \u201cconnected,\u201d respectively",
        "We demonstrate the relationship of these measures with a constrained form of Shapley value, and we relate C-Shapley with another solution concept from cooperative game theory, known as the Myerson value (<a class=\"ref-link\" id=\"cMyerson_1977_a\" href=\"#rMyerson_1977_a\">Myerson, 1977</a>)",
        "In terms of this notation, for a given feature vector x \u2208 X , subset S and fitted model distribution Pm(Y | x), we introduce the importance score vx(S) : = Em",
        "<a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a>) proposed to evaluate the model over randomly sampled subsets and use a weighted linear regression to approximate the Shapley values based on the collected model evaluations",
        "Each feature vector x in sequence data, can be associated with a line graph, where positions too far apart in a sequence may not affect each other in Shapley value computation; each image data is naturally modeled with a grid graph, such that pixels that are far apart may have little effect on each other in the computation of Shapley value",
        "We show that under certain probabilistic assumptions on the marginal distribution over the features, these quantities yield good approximations to the original Shapley values",
        "We propose a second algorithm, C-Shapley, that further reduces the complexity of approximating the Shapley value",
        "Coming back to Example ( ) where we evaluate the importance of \u201cnot,\u201d both the L-Shapley estimate of order larger than two and the exact Shapley value estimate would evaluate the model on the word subset \u201cIt not heartwarming,\u201d which rarely appears in real data and may not make sense to a human or a model trained on real-world data",
        "In order to characterize the relationship between L-Shapley and the Shapley value in terms of some conditional independence assumption between features, we introduce absolute mutual information as a measure of dependence",
        "Regression-based methods <a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a>) proposed to sample feature subsets based on a weighted kernel, and carry out a weighted linear regression to estimate the Shapley value",
        "We compare L-Shapley and C-Shapley with several competitive algorithms for instancewise feature importance scoring on black-box models, including the regressionbased approximation known as KernelSHAP (<a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee, 2017</a>), SampleShapley (\u0160trumbelj & Kononenko, 2010), and the LIME method (<a class=\"ref-link\" id=\"cRibeiro_et+al_2016_a\" href=\"#rRibeiro_et+al_2016_a\">Ribeiro et al, 2016</a>)",
        "We study the performance of L-Shapley and C-Shapley on three popular neural models for text classification: word-based CNNs (<a class=\"ref-link\" id=\"cKim_2014_a\" href=\"#rKim_2014_a\">Kim, 2014</a>), characterbased CNNs (<a class=\"ref-link\" id=\"cZhang_et+al_2015_a\" href=\"#rZhang_et+al_2015_a\">Zhang et al, 2015</a>), and long-short term memory (LSTM) recurrent neural networks (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>), with the following three data sets on different scales: (i) IMDB Review with Word-CNN: A simple word-based CNN model is used on the IMDB movie review data set, achieving a test accuracy of 90.1%; AG news with Char-CNN: We implement a character-based CNN on the AG news corpus <a class=\"ref-link\" id=\"cZhang_et+al_2015_a\" href=\"#rZhang_et+al_2015_a\">Zhang et al (2015</a>), achieving a test accuracy of",
        "We visualize the importance scores produced by different Shapley-based methods on Example ( ), which is part of a negative movie review taken from IMDB",
        "We have proposed two new algorithms\u2014L-Shapley and C-Shapley\u2014for instancewise feature importance scoring, making use of a graphical representation of the data",
        "We have demonstrated the superior performance of these algorithms compared to other methods on black-box models for instancewise feature importance scoring in both text and image classification with both quantitative metrics and human evaluation"
    ],
    "summary": [
        "Many black box machine learning models, such as random forests, deep neural networks, and kernel methods, can produce highly accurate prediction in many applications, such prediction often comes at the cost of interpretability.",
        "Such a method yields, for each instance to which the model is applied, a vector of importance scores associated with the underlying features.",
        "We begin by introducing some background and notation for instancewise feature importance scoring and the Shapley value.",
        "In terms of this notation, for a given feature vector x \u2208 X , subset S and fitted model distribution Pm(Y | x), we introduce the importance score vx(S) : = Em",
        "<a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a>) proposed to evaluate the model over randomly sampled subsets and use a weighted linear regression to approximate the Shapley values based on the collected model evaluations.",
        "As one approximation, we propose the L-Shapley score, which only perturbs the neighboring features of a given feature when evaluating its importance: Definition 1.",
        "Coming back to Example ( ) where we evaluate the importance of \u201cnot,\u201d both the L-Shapley estimate of order larger than two and the exact Shapley value estimate would evaluate the model on the word subset \u201cIt not heartwarming,\u201d which rarely appears in real data and may not make sense to a human or a model trained on real-world data.",
        "If we use a plug-in estimate for conditional probability, the decomposability condition (12) is equivalent to assuming that the influence of disconnected subsets of features are additive at sample x, and C-Shapley of order k = d is exactly the Myerson value over G.",
        "Sampling-based methods An alternative definition of the Shapley value defines the contribution of a feature i as the average of the marginal contribution of i to its preceding features over the set of all permutations of d features.",
        "Regression-based methods <a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee (2017</a>) proposed to sample feature subsets based on a weighted kernel, and carry out a weighted linear regression to estimate the Shapley value.",
        "We compare L-Shapley and C-Shapley with several competitive algorithms for instancewise feature importance scoring on black-box models, including the regressionbased approximation known as KernelSHAP (<a class=\"ref-link\" id=\"cLundberg_2017_a\" href=\"#rLundberg_2017_a\">Lundberg & Lee, 2017</a>), SampleShapley (\u0160trumbelj & Kononenko, 2010), and the LIME method (<a class=\"ref-link\" id=\"cRibeiro_et+al_2016_a\" href=\"#rRibeiro_et+al_2016_a\">Ribeiro et al, 2016</a>).",
        "We visualize the importance scores produced by different Shapley-based methods on Example ( ), which is part of a negative movie review taken from IMDB.",
        "We have demonstrated the superior performance of these algorithms compared to other methods on black-box models for instancewise feature importance scoring in both text and image classification with both quantitative metrics and human evaluation."
    ],
    "headline": "We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graphstructured factorization",
    "reference_links": [
        {
            "id": "Bach_et+al_2015_a",
            "entry": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS One, 10(7):e0130140, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "Baehrens_et+al_2010_a",
            "entry": "David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert M\u00fcller. How to explain individual classification decisions. Journal of Machine Learning Research, 11:1803\u20131831, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. Learning to explain: An information-theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07814"
        },
        {
            "id": "Cover_2012_a",
            "entry": "Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20Information%20Theory%202012"
        },
        {
            "id": "Datta_et+al_2016_a",
            "entry": "Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 598\u2013617. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Datta%2C%20Anupam%20Sen%2C%20Shayak%20Zick%2C%20Yair%20Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Datta%2C%20Anupam%20Sen%2C%20Shayak%20Zick%2C%20Yair%20Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jiao_2018_a",
            "entry": "Yunlong Jiao and Jean-Philippe Vert. The kendall and mallows kernels for permutations. IEEE transactions on pattern analysis and machine intelligence, 40(7):1755\u20131769, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiao%2C%20Yunlong%20Vert%2C%20Jean-Philippe%20The%20kendall%20and%20mallows%20kernels%20for%20permutations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiao%2C%20Yunlong%20Vert%2C%20Jean-Philippe%20The%20kendall%20and%20mallows%20kernels%20for%20permutations%202018"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Kendall_1975_a",
            "entry": "Maurice Kendall. Rank Correlation Methods. Charles Griffin, London, 4 edition, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20Maurice%20Rank%20Correlation%20Methods.%20Charles%20Griffin%2C%20London%201975"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746\u20131751, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lipton_2016_a",
            "entry": "Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03490"
        },
        {
            "id": "Lundberg_2017_a",
            "entry": "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 4765\u20134774. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf.",
            "url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "Maas_et+al_2011_a",
            "entry": "Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 142\u2013150. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011"
        },
        {
            "id": "Murdoch_2017_a",
            "entry": "W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. arXiv preprint arXiv:1702.02540, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02540"
        },
        {
            "id": "Myerson_1977_a",
            "entry": "Roger B Myerson. Graphs and cooperation in games. Mathematics of Operations Research, 2(3): 225\u2013229, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Myerson%2C%20Roger%20B.%20Graphs%20and%20cooperation%20in%20games%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Myerson%2C%20Roger%20B.%20Graphs%20and%20cooperation%20in%20games%201977"
        },
        {
            "id": "Ribeiro_et+al_2016_a",
            "entry": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should I trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135\u20131144. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20I%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20I%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016"
        },
        {
            "id": "Shapley_1953_a",
            "entry": "Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307\u2013 317, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapley%2C%20Lloyd%20S.%20A%20value%20for%20n-person%20games.%20Contributions%20to%20the%20Theory%20of%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapley%2C%20Lloyd%20S.%20A%20value%20for%20n-person%20games.%20Contributions%20to%20the%20Theory%20of%201953"
        },
        {
            "id": "Shrikumar_et+al_2017_a",
            "entry": "Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 3145\u20133153. PMLR, 06\u201311 Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Kundaje%2C%20Anshul%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Kundaje%2C%20Anshul%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017-08"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Strobelt_et+al_2018_a",
            "entry": "Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M Rush. Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on visualization and computer graphics, 24(1):667\u2013676, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strobelt%2C%20Hendrik%20Gehrmann%2C%20Sebastian%20Pfister%2C%20Hanspeter%20Rush%2C%20Alexander%20M.%20Lstmvis%3A%20A%20tool%20for%20visual%20analysis%20of%20hidden%20state%20dynamics%20in%20recurrent%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strobelt%2C%20Hendrik%20Gehrmann%2C%20Sebastian%20Pfister%2C%20Hanspeter%20Rush%2C%20Alexander%20M.%20Lstmvis%3A%20A%20tool%20for%20visual%20analysis%20of%20hidden%20state%20dynamics%20in%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "Trumbelj_2010_a",
            "entry": "Erik \u0160trumbelj and Igor Kononenko. An efficient explanation of individual classifications using game theory. Journal of Machine Learning Research, 11:1\u201318, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%A0trumbelj%2C%20Erik%20Kononenko%2C%20Igor%20An%20efficient%20explanation%20of%20individual%20classifications%20using%20game%20theory%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%C5%A0trumbelj%2C%20Erik%20Kononenko%2C%20Igor%20An%20efficient%20explanation%20of%20individual%20classifications%20using%20game%20theory%202010"
        },
        {
            "id": "Sundararajan_et+al_2017_a",
            "entry": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning, pp. 3319\u20133328, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017"
        },
        {
            "id": "Young_1985_a",
            "entry": "H Peyton Young. Monotonic solutions of cooperative games. International Journal of Game Theory, 14(2):65\u201372, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Young%2C%20H.Peyton%20Monotonic%20solutions%20of%20cooperative%20games%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Young%2C%20H.Peyton%20Monotonic%20solutions%20of%20cooperative%20games%201985"
        },
        {
            "id": "Zhang_et+al_2015_a",
            "entry": "Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, pp. 649\u2013657, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015"
        },
        {
            "id": "IMDB_2011_a",
            "entry": "IMDB Review with Word-CNN The Internet Movie Review Dataset (IMDB) is a dataset of movie reviews for sentiment classification (Maas et al., 2011), which contains 50, 000 binary labeled movie reviews, with a split of 25, 000 for training and 25, 000 for testing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=IMDB%20Review%20with%20WordCNN%20The%20Internet%20Movie%20Review%20Dataset%20IMDB%20is%20a%20dataset%20of%20movie%20reviews%20for%20sentiment%20classification%20Maas%20et%20al%202011%20which%20contains%2050%20000%20binary%20labeled%20movie%20reviews%20with%20a%20split%20of%2025%20000%20for%20training%20and%2025%20000%20for%20testing"
        },
        {
            "id": "AG_2015_a",
            "entry": "AG news with Char-CNN The AG news corpus is composed of titles and descriptions of 196, 000 news articles from 2, 000 news sources (Zhang et al., 2015). It is segmented into four classes, each containing 30, 000 training samples and 1, 900 testing samples.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AG%20news%20with%20CharCNN%20The%20AG%20news%20corpus%20is%20composed%20of%20titles%20and%20descriptions%20of%20196%20000%20news%20articles%20from%202%20000%20news%20sources%20Zhang%20et%20al%202015%20It%20is%20segmented%20into%20four%20classes%20each%20containing%2030%20000%20training%20samples%20and%201%20900%20testing%20samples",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AG%20news%20with%20CharCNN%20The%20AG%20news%20corpus%20is%20composed%20of%20titles%20and%20descriptions%20of%20196%20000%20news%20articles%20from%202%20000%20news%20sources%20Zhang%20et%20al%202015%20It%20is%20segmented%20into%20four%20classes%20each%20containing%2030%20000%20training%20samples%20and%201%20900%20testing%20samples"
        },
        {
            "id": "MNIST_1998_a",
            "entry": "MNIST The MNIST data set contains 28 \u00d7 28 images of handwritten digits with ten categories 0 \u2212 9 (LeCun et al., 1998). A subset of MNIST data set composed of digits 3 and 8 is used for better visualization, with 12, 000 images for training and 1, 000 images for testing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MNIST%20The%20MNIST%20data%20set%20contains%2028%20%2028%20images%20of%20handwritten%20digits%20with%20ten%20categories%200%20%209%20LeCun%20et%20al%201998%20A%20subset%20of%20MNIST%20data%20set%20composed%20of%20digits%203%20and%208%20is%20used%20for%20better%20visualization%20with%2012%20000%20images%20for%20training%20and%201%20000%20images%20for%20testing",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MNIST%20The%20MNIST%20data%20set%20contains%2028%20%2028%20images%20of%20handwritten%20digits%20with%20ten%20categories%200%20%209%20LeCun%20et%20al%201998%20A%20subset%20of%20MNIST%20data%20set%20composed%20of%20digits%203%20and%208%20is%20used%20for%20better%20visualization%20with%2012%20000%20images%20for%20training%20and%201%20000%20images%20for%20testing"
        },
        {
            "id": "CIFAR10_2009_a",
            "entry": "CIFAR10 The CIFAR10 data set (Krizhevsky, 2009) contains 32 \u00d7 32 images in ten classes. A subset of CIFAR10 data set composed of deers and horses is used for better visualization, with 10, 000 images for training and 2, 000 images for testing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CIFAR10%20The%20CIFAR10%20data%20set%20Krizhevsky%202009%20contains%2032%20%2032%20images%20in%20ten%20classes%20A%20subset%20of%20CIFAR10%20data%20set%20composed%20of%20deers%20and%20horses%20is%20used%20for%20better%20visualization%20with%2010%20000%20images%20for%20training%20and%202%20000%20images%20for%20testing",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CIFAR10%20The%20CIFAR10%20data%20set%20Krizhevsky%202009%20contains%2032%20%2032%20images%20in%20ten%20classes%20A%20subset%20of%20CIFAR10%20data%20set%20composed%20of%20deers%20and%20horses%20is%20used%20for%20better%20visualization%20with%2010%20000%20images%20for%20training%20and%202%20000%20images%20for%20testing"
        },
        {
            "id": "IMDB_2014_a",
            "entry": "IMDB Review with Word-CNN The word-based CNN model is composed of a 50-dimensional word embedding, a 1-D convolutional layer of 250 filters and kernel size three, a max-pooling and a 250-dimensional dense layer as hidden layers. Both the convolutional and the dense layers are followed by ReLU as nonlinearity, and Dropout Srivastava et al. (2014) as regularization. The model is trained with rmsprop Hinton et al.. The model achieves an accuracy of 90.1% on the test data set.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=IMDB%20Review%20with%20WordCNN%20The%20wordbased%20CNN%20model%20is%20composed%20of%20a%2050dimensional%20word%20embedding%20a%201D%20convolutional%20layer%20of%20250%20filters%20and%20kernel%20size%20three%20a%20maxpooling%20and%20a%20250dimensional%20dense%20layer%20as%20hidden%20layers%20Both%20the%20convolutional%20and%20the%20dense%20layers%20are%20followed%20by%20ReLU%20as%20nonlinearity%20and%20Dropout%20Srivastava%20et%20al%202014%20as%20regularization%20The%20model%20is%20trained%20with%20rmsprop%20Hinton%20et%20al%20The%20model%20achieves%20an%20accuracy%20of%20901%20on%20the%20test%20data%20set",
            "oa_query": "https://api.scholarcy.com/oa_version?query=IMDB%20Review%20with%20WordCNN%20The%20wordbased%20CNN%20model%20is%20composed%20of%20a%2050dimensional%20word%20embedding%20a%201D%20convolutional%20layer%20of%20250%20filters%20and%20kernel%20size%20three%20a%20maxpooling%20and%20a%20250dimensional%20dense%20layer%20as%20hidden%20layers%20Both%20the%20convolutional%20and%20the%20dense%20layers%20are%20followed%20by%20ReLU%20as%20nonlinearity%20and%20Dropout%20Srivastava%20et%20al%202014%20as%20regularization%20The%20model%20is%20trained%20with%20rmsprop%20Hinton%20et%20al%20The%20model%20achieves%20an%20accuracy%20of%20901%20on%20the%20test%20data%20set"
        },
        {
            "id": "AG\u2019s_2015_b",
            "entry": "AG\u2019s news with Char-CNN The character-based CNN has the same structure as the one proposed in Zhang et al. (2015), composed of six convolutional layers, three max-pooling layers, and two dense layers. The model is trained with SGD with momentum 0.9 and decreasing step size initialized at 0.01. (Details can be found in Zhang et al. (2015).) The model reaches accuracy of 90.09% on the test data set.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AGs%20news%20with%20CharCNN%20The%20characterbased%20CNN%20has%20the%20same%20structure%20as%20the%20one%20proposed%20in%20Zhang%20et%20al%202015%20composed%20of%20six%20convolutional%20layers%20three%20maxpooling%20layers%20and%20two%20dense%20layers%20The%20model%20is%20trained%20with%20SGD%20with%20momentum%2009%20and%20decreasing%20step%20size%20initialized%20at%20001%20Details%20can%20be%20found%20in%20Zhang%20et%20al%202015%20The%20model%20reaches%20accuracy%20of%209009%20on%20the%20test%20data%20set",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AGs%20news%20with%20CharCNN%20The%20characterbased%20CNN%20has%20the%20same%20structure%20as%20the%20one%20proposed%20in%20Zhang%20et%20al%202015%20composed%20of%20six%20convolutional%20layers%20three%20maxpooling%20layers%20and%20two%20dense%20layers%20The%20model%20is%20trained%20with%20SGD%20with%20momentum%2009%20and%20decreasing%20step%20size%20initialized%20at%20001%20Details%20can%20be%20found%20in%20Zhang%20et%20al%202015%20The%20model%20reaches%20accuracy%20of%209009%20on%20the%20test%20data%20set"
        },
        {
            "id": "Yahoo!_2015_c",
            "entry": "Yahoo! Answers with LSTM The network consists of a 300-dimensional randomly-initialized word embedding, a bidirectional LSTM, each LSTM unit of dimension 256, and a dropout layer as hidden layers. The model is trained with rmsprop Hinton et al.. The model reaches accuracy of 70.84% on the test data set, close to the state-of-the-art accuracy of 71.2% obtained by characterbased CNN Zhang et al. (2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yahoo%20Answers%20with%20LSTM%20The%20network%20consists%20of%20a%20300dimensional%20randomlyinitialized%20word%20embedding%20a%20bidirectional%20LSTM%20each%20LSTM%20unit%20of%20dimension%20256%20and%20a%20dropout%20layer%20as%20hidden%20layers%20The%20model%20is%20trained%20with%20rmsprop%20Hinton%20et%20al%20The%20model%20reaches%20accuracy%20of%207084%20on%20the%20test%20data%20set%20close%20to%20the%20stateoftheart%20accuracy%20of%20712%20obtained%20by%20characterbased%20CNN%20Zhang%20et%20al%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yahoo%20Answers%20with%20LSTM%20The%20network%20consists%20of%20a%20300dimensional%20randomlyinitialized%20word%20embedding%20a%20bidirectional%20LSTM%20each%20LSTM%20unit%20of%20dimension%20256%20and%20a%20dropout%20layer%20as%20hidden%20layers%20The%20model%20is%20trained%20with%20rmsprop%20Hinton%20et%20al%20The%20model%20reaches%20accuracy%20of%207084%20on%20the%20test%20data%20set%20close%20to%20the%20stateoftheart%20accuracy%20of%20712%20obtained%20by%20characterbased%20CNN%20Zhang%20et%20al%202015"
        },
        {
            "id": "CIFAR10_2012_a",
            "entry": "CIFAR10 A convolutional neural network modified from AlexNet Krizhevsky et al. (2012) is trained on the subset. It is composed of six convolutional layers of kernel size 3 \u00d7 3 and two dense linear layers of dimension 512 and 256 at last. The six convolutional layers contain 48,48,96,96,192,192 filters respectively, and every two convolutional layers are followed by a maxpooling layer of pool size two and a dropout layer. The CNN model is trained with the Adam optimizer Kingma & Ba (2015) and achieves 96.1% accuracy on the test data set.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CIFAR10%20A%20convolutional%20neural%20network%20modified%20from%20AlexNet%20Krizhevsky%20et%20al%202012%20is%20trained%20on%20the%20subset%20It%20is%20composed%20of%20six%20convolutional%20layers%20of%20kernel%20size%203%20%203%20and%20two%20dense%20linear%20layers%20of%20dimension%20512%20and%20256%20at%20last%20The%20six%20convolutional%20layers%20contain%2048489696192192%20filters%20respectively%20and%20every%20two%20convolutional%20layers%20are%20followed%20by%20a%20maxpooling%20layer%20of%20pool%20size%20two%20and%20a%20dropout%20layer%20The%20CNN%20model%20is%20trained%20with%20the%20Adam%20optimizer%20Kingma%20%20Ba%202015%20and%20achieves%20961%20accuracy%20on%20the%20test%20data%20set",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CIFAR10%20A%20convolutional%20neural%20network%20modified%20from%20AlexNet%20Krizhevsky%20et%20al%202012%20is%20trained%20on%20the%20subset%20It%20is%20composed%20of%20six%20convolutional%20layers%20of%20kernel%20size%203%20%203%20and%20two%20dense%20linear%20layers%20of%20dimension%20512%20and%20256%20at%20last%20The%20six%20convolutional%20layers%20contain%2048489696192192%20filters%20respectively%20and%20every%20two%20convolutional%20layers%20are%20followed%20by%20a%20maxpooling%20layer%20of%20pool%20size%20two%20and%20a%20dropout%20layer%20The%20CNN%20model%20is%20trained%20with%20the%20Adam%20optimizer%20Kingma%20%20Ba%202015%20and%20achieves%20961%20accuracy%20on%20the%20test%20data%20set"
        },
        {
            "id": "We_1975_b",
            "entry": "We address the problem of how the rank of features produced by various approximation algorithms correlates with the rank produced by the true Shapley value. We sample a subset of test data from Yahoo! Answers with 9-12 words, so that the underlying Shapley scores can be accurately computed. We employ two common metrics, Kendall\u2019s \u03c4 and Spearman\u2019s \u03c1 (Kendall, 1975), to measure the similarity (correlation) between two ranks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20address%20the%20problem%20of%20how%20the%20rank%20of%20features%20produced%20by%20various%20approximation%20algorithms%20correlates%20with%20the%20rank%20produced%20by%20the%20true%20Shapley%20value%20We%20sample%20a%20subset%20of%20test%20data%20from%20Yahoo%20Answers%20with%20912%20words%20so%20that%20the%20underlying%20Shapley%20scores%20can%20be%20accurately%20computed%20We%20employ%20two%20common%20metrics%20Kendalls%20%CF%84%20and%20Spearmans%20%CF%81%20Kendall%201975%20to%20measure%20the%20similarity%20correlation%20between%20two%20ranks"
        }
    ]
}
