{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HIERARCHICAL VISUOMOTOR CONTROL",
        "author": "OF HUMANOIDS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJfYvo09Y7"
        },
        "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link1"
    },
    "keywords": [
        {
            "term": "high level",
            "url": "https://en.wikipedia.org/wiki/high_level"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "motor control",
            "url": "https://en.wikipedia.org/wiki/motor_control"
        },
        {
            "term": "motion capture",
            "url": "https://en.wikipedia.org/wiki/motion_capture"
        },
        {
            "term": "motor skill",
            "url": "https://en.wikipedia.org/wiki/motor_skill"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "visual perception",
            "url": "https://en.wikipedia.org/wiki/visual_perception"
        },
        {
            "term": "robotics",
            "url": "https://en.wikipedia.org/wiki/robotics"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "Gaps": "gapped platforms"
    },
    "highlights": [
        "In reinforcement learning (RL), a major challenge is to simultaneously cope with high-dimensional input and high-dimensional action spaces",
        "In this work we explored the problem of learning to reuse motor skills to solve whole body humanoid tasks from egocentric camera observations",
        "We compared a range of approaches for reusing lowlevel motor skills that were obtained from motion capture data, including variations related to those presented in <a class=\"ref-link\" id=\"cLiu_2017_a\" href=\"#rLiu_2017_a\">Liu & Hodgins (2017</a>); <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a>)",
        "There is limited learning-based work on humanoids in simulation reusing motor skills to solve new tasks, and much of what does exist is in the animation literature",
        "A technical contribution of the present work was to move past hand-designed observation features; <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a>)) towards a more ecological observation setting: using a front-facing camera is more similar to the kinds of observations a real-world, embodied agent would have",
        "We show that hierarchical motor skill reuse allowed us to solve tasks that we could not with a flat policy"
    ],
    "key_statements": [
        "In reinforcement learning (RL), a major challenge is to simultaneously cope with high-dimensional input and high-dimensional action spaces",
        "While the general details of the reinforcement learning algorithm are not pertinent to the success of this approach (e.g. <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\"><a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a></a>) used on-policy reinforcement learning), we found two details to be critical, and both were consistent with the results reported in <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\"><a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a></a>)",
        "In this work we explored the problem of learning to reuse motor skills to solve whole body humanoid tasks from egocentric camera observations",
        "We compared a range of approaches for reusing lowlevel motor skills that were obtained from motion capture data, including variations related to those presented in <a class=\"ref-link\" id=\"cLiu_2017_a\" href=\"#rLiu_2017_a\">Liu & Hodgins (2017</a>); <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a>)",
        "There is limited learning-based work on humanoids in simulation reusing motor skills to solve new tasks, and much of what does exist is in the animation literature",
        "A technical contribution of the present work was to move past hand-designed observation features; <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al (2018</a>)) towards a more ecological observation setting: using a front-facing camera is more similar to the kinds of observations a real-world, embodied agent would have",
        "We show that hierarchical motor skill reuse allowed us to solve tasks that we could not with a flat policy"
    ],
    "summary": [
        "In reinforcement learning (RL), a major challenge is to simultaneously cope with high-dimensional input and high-dimensional action spaces.",
        "Our work aims to contribute to this multi-disciplinary literature by demonstrating concretely how control-fragment-like low-level movements can be coupled to and controlled by a vision and memory-based high-level controller to solve tasks.",
        "We present a system capable of solving tasks from vision by switching among low-level motor controllers for the humanoid body.",
        "Cold-switching of behaviors and control fragments We can leave the task of sequencing behaviors to the high-level controller, instead of building structured low-level policies with explicit, designed transitions.",
        "2.3 TRAINING HL-POLICIES TO SOLVE TASKS USING LL-CONTROLLERS We integrated the low-level controllers into an agent architecture with vision and and an LSTM memory in order to apply it to tasks including directed movements to target locations, a running course with wall or gap obstacles, a foraging task for \u201cballs\u201d, and a simple memory task involving detecting and memorizing the reward value of the balls.",
        "The high-level controller senses inputs from proprioceptive data and, for visual tasks, an egocentric camera mounted at the root of the body (Fig. 4).",
        "In the Gaps task, we were able to use the control fragments approach with 12 single-clip policies, where it would be laborious to pretrain transitions for each of these.",
        "The high-level controller selected between the 4 original stand, run and turn policies as well as 8 additional jumps, resulting in 359 fragments, and was able to synthesize them to move forward along the separated platforms.",
        "For learning low-level tracking policies from motion capture data, we employed a manually specified similarity measure against motion capture reference trajectories, consistent with previous work (<a class=\"ref-link\" id=\"cLiu_et+al_2010_a\" href=\"#rLiu_et+al_2010_a\">Liu et al, 2010</a>; 2015; <a class=\"ref-link\" id=\"cPeng_et+al_2018_a\" href=\"#rPeng_et+al_2018_a\">Peng et al, 2018</a>).",
        "Compared to previous work using control fragments (<a class=\"ref-link\" id=\"cLiu_2017_a\" href=\"#rLiu_2017_a\">Liu & Hodgins, 2017</a>), our low-level controllers were built without a sampling-based planner and were parameterized as neural networks rather than linear-feedback policies.",
        "We want to make clear that the graph-transition and steerable structured low-level control approaches require significant manual curation and design: motion capture clips must be segmented by hand, possibly manipulated by blending/smoothing clips from the end of one clip to the beginning of another.",
        "Each approach involving motion capture data can suffer from distinct artifacts, especially without detailed manual editing \u2013 the hand-designed controllers have artifacts at transitions due to imprecise kinematic blending but are smooth within a behavior, whereas the control fragments have a lesser but consistent level of jitter throughout due to frequent switching."
    ],
    "headline": "We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies",
    "reference_links": [
        {
            "id": "Beattie_et+al_2016_a",
            "entry": "Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler, Andrew Lefrancq, Simon Green, V\u0131ctor Valdes, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03801"
        },
        {
            "id": "Bizzi_2008_a",
            "entry": "E Bizzi, VCK Cheung, A d\u2019Avella, P Saltiel, and Me Tresch. Combining modules for movement. Brain research reviews, 57(1):125\u2013133, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bizzi%2C%20E.%20Cheung%2C%20V.C.K.%20A%20d%E2%80%99Avella%2C%20P%20Saltiel%2C%20and%20Me%20Tresch.%20Combining%20modules%20for%20movement%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bizzi%2C%20E.%20Cheung%2C%20V.C.K.%20A%20d%E2%80%99Avella%2C%20P%20Saltiel%2C%20and%20Me%20Tresch.%20Combining%20modules%20for%20movement%202008"
        },
        {
            "id": "Ding_et+al_2015_a",
            "entry": "Kai Ding, Libin Liu, Michiel Van de Panne, and KangKang Yin. Learning reduced-order feedback policies for motion skills. In Proceedings of the 14th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pp. 83\u201392. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Kai%20Liu%2C%20Libin%20de%20Panne%2C%20Michiel%20Van%20Yin%2C%20KangKang%20Learning%20reduced-order%20feedback%20policies%20for%20motion%20skills%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Kai%20Liu%2C%20Libin%20de%20Panne%2C%20Michiel%20Van%20Yin%2C%20KangKang%20Learning%20reduced-order%20feedback%20policies%20for%20motion%20skills%202015"
        },
        {
            "id": "Dulac-Arnold_et+al_2015_a",
            "entry": "Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.07679"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 1407\u20131416, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Espeholt%2C%20Lasse%20Soyer%2C%20Hubert%20Munos%2C%20Remi%20Simonyan%2C%20Karen%20IMPALA%3A%20Scalable%20distributed%20deep-RL%20with%20importance%20weighted%20actor-learner%20architectures%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Espeholt%2C%20Lasse%20Soyer%2C%20Hubert%20Munos%2C%20Remi%20Simonyan%2C%20Karen%20IMPALA%3A%20Scalable%20distributed%20deep-RL%20with%20importance%20weighted%20actor-learner%20architectures%202018"
        },
        {
            "id": "Faloutsos_et+al_2001_a",
            "entry": "Petros Faloutsos, Michiel Van de Panne, and Demetri Terzopoulos. Composable controllers for physics-based character animation. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pp. 251\u2013260. ACM, 2001a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faloutsos%2C%20Petros%20de%20Panne%2C%20Michiel%20Van%20Terzopoulos%2C%20Demetri%20Composable%20controllers%20for%20physics-based%20character%20animation%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faloutsos%2C%20Petros%20de%20Panne%2C%20Michiel%20Van%20Terzopoulos%2C%20Demetri%20Composable%20controllers%20for%20physics-based%20character%20animation%202001"
        },
        {
            "id": "Faloutsos_et+al_2001_b",
            "entry": "Petros Faloutsos, Michiel van de Panne, and Demetri Terzopoulos. The virtual stuntman: dynamic characters with a repertoire of autonomous motor skills. Computers & Graphics, 25(6):933\u2013953, 2001b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faloutsos%2C%20Petros%20van%20de%20Panne%2C%20Michiel%20Terzopoulos%2C%20Demetri%20The%20virtual%20stuntman%3A%20dynamic%20characters%20with%20a%20repertoire%20of%20autonomous%20motor%20skills%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faloutsos%2C%20Petros%20van%20de%20Panne%2C%20Michiel%20Terzopoulos%2C%20Demetri%20The%20virtual%20stuntman%3A%20dynamic%20characters%20with%20a%20repertoire%20of%20autonomous%20motor%20skills%202001"
        },
        {
            "id": "Flash_2005_a",
            "entry": "Tamar Flash and Binyamin Hochner. Motor primitives in vertebrates and invertebrates. Current opinion in neurobiology, 15(6):660\u2013666, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flash%2C%20Tamar%20Hochner%2C%20Binyamin%20Motor%20primitives%20in%20vertebrates%20and%20invertebrates%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flash%2C%20Tamar%20Hochner%2C%20Binyamin%20Motor%20primitives%20in%20vertebrates%20and%20invertebrates%202005"
        },
        {
            "id": "Gibson_2014_a",
            "entry": "James J Gibson. The ecological approach to visual perception: classic edition. Psychology Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gibson%2C%20James%20J.%20The%20ecological%20approach%20to%20visual%20perception%3A%20classic%20edition%202014"
        },
        {
            "id": "Grillner_et+al_2005_a",
            "entry": "Sten Grillner, Jeanette Hellgren, Ariane Menard, Kazuya Saitoh, and Martin A Wikstrom. Mechanisms for selection of basic motor programs\u2013roles for the striatum and pallidum. Trends in neurosciences, 28(7):364\u2013370, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grillner%2C%20Sten%20Hellgren%2C%20Jeanette%20Menard%2C%20Ariane%20Saitoh%2C%20Kazuya%20Mechanisms%20for%20selection%20of%20basic%20motor%20programs%E2%80%93roles%20for%20the%20striatum%20and%20pallidum%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grillner%2C%20Sten%20Hellgren%2C%20Jeanette%20Menard%2C%20Ariane%20Saitoh%2C%20Kazuya%20Mechanisms%20for%20selection%20of%20basic%20motor%20programs%E2%80%93roles%20for%20the%20striatum%20and%20pallidum%202005"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rk07ZXZRb.accepted as poster.",
            "url": "https://openreview.net/forum?id=rk07ZXZRb.accepted",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Heess_et+al_2016_a",
            "entry": "Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05182"
        },
        {
            "id": "Nicolas_et+al_2017_a",
            "entry": "Nicolas Heess, TB Dhruva, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, SM Ali Eslami, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Holden_et+al_2017_a",
            "entry": "Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4):42, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017"
        },
        {
            "id": "Ijspeert_et+al_2003_a",
            "entry": "Auke J Ijspeert, Jun Nakanishi, and Stefan Schaal. Learning attractor landscapes for learning motor primitives. In Advances in neural information processing systems, pp. 1547\u20131554, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ijspeert%2C%20Auke%20J.%20Nakanishi%2C%20Jun%20Schaal%2C%20Stefan%20Learning%20attractor%20landscapes%20for%20learning%20motor%20primitives%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ijspeert%2C%20Auke%20J.%20Nakanishi%2C%20Jun%20Schaal%2C%20Stefan%20Learning%20attractor%20landscapes%20for%20learning%20motor%20primitives%202003"
        },
        {
            "id": "Ijspeert_et+al_2002_a",
            "entry": "Auke Jan Ijspeert, Jun Nakanishi, and Stefan Schaal. Movement imitation with nonlinear dynamical systems in humanoid robots. In Robotics and Automation, 2002. Proceedings. ICRA\u201902. IEEE International Conference on, volume 2, pp. 1398\u20131403. IEEE, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ijspeert%2C%20Auke%20Jan%20Nakanishi%2C%20Jun%20Schaal%2C%20Stefan%20Movement%20imitation%20with%20nonlinear%20dynamical%20systems%20in%20humanoid%20robots%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ijspeert%2C%20Auke%20Jan%20Nakanishi%2C%20Jun%20Schaal%2C%20Stefan%20Movement%20imitation%20with%20nonlinear%20dynamical%20systems%20in%20humanoid%20robots%202002"
        },
        {
            "id": "Jaderberg_et+al_2018_a",
            "entry": "Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in first-person multiplayer games with population-based deep reinforcement learning. arXiv preprint arXiv:1807.01281, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01281"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kober_2009_a",
            "entry": "Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural information processing systems, pp. 849\u2013856, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009"
        },
        {
            "id": "Kober_et+al_2008_a",
            "entry": "Jens Kober, Betty Mohler, and Jan Peters. Learning perceptual coupling for motor primitives. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pp. 834\u2013839. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Mohler%2C%20Betty%20Peters%2C%20Jan%20Learning%20perceptual%20coupling%20for%20motor%20primitives.%20In%20Intelligent%20Robots%20and%20Systems%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Mohler%2C%20Betty%20Peters%2C%20Jan%20Learning%20perceptual%20coupling%20for%20motor%20primitives.%20In%20Intelligent%20Robots%20and%20Systems%202008"
        },
        {
            "id": "Konidaris_et+al_2012_a",
            "entry": "George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Robot learning from demonstration by constructing skill trees. The International Journal of Robotics Research, 31(3): 360\u2013375, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Kuindersma%2C%20Scott%20Grupen%2C%20Roderic%20Barto%2C%20Andrew%20Robot%20learning%20from%20demonstration%20by%20constructing%20skill%20trees%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20George%20Kuindersma%2C%20Scott%20Grupen%2C%20Roderic%20Barto%2C%20Andrew%20Robot%20learning%20from%20demonstration%20by%20constructing%20skill%20trees%202012"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Liu_2017_a",
            "entry": "Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based characters using deep q-learning. ACM Transactions on Graphics (TOG), 36(3):29, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20to%20schedule%20control%20fragments%20for%20physics-based%20characters%20using%20deep%20q-learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20to%20schedule%20control%20fragments%20for%20physics-based%20characters%20using%20deep%20q-learning%202017"
        },
        {
            "id": "Liu_2018_a",
            "entry": "Libin Liu and Jessica Hodgins. Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning. ACM Transactions on Graphics (TOG), 37(4):142, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20basketball%20dribbling%20skills%20using%20trajectory%20optimization%20and%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Hodgins%2C%20Jessica%20Learning%20basketball%20dribbling%20skills%20using%20trajectory%20optimization%20and%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Liu_et+al_2010_a",
            "entry": "Libin Liu, KangKang Yin, Michiel van de Panne, Tianjia Shao, and Weiwei Xu. Sampling-based contact-rich motion control. In ACM Transactions on Graphics (TOG), volume 29, pp. 128. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Shao%2C%20Tianjia%20Sampling-based%20contact-rich%20motion%20control%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Shao%2C%20Tianjia%20Sampling-based%20contact-rich%20motion%20control%202010"
        },
        {
            "id": "Liu_et+al_2012_a",
            "entry": "Libin Liu, KangKang Yin, Michiel van de Panne, and Baining Guo. Terrain runner: control, parameterization, composition, and planning for highly dynamic motions. ACM Transactions on Graphics (TOG), 31(6):154, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Guo%2C%20Baining%20Terrain%20runner%3A%20control%2C%20parameterization%2C%20composition%2C%20and%20planning%20for%20highly%20dynamic%20motions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Yin%2C%20KangKang%20van%20de%20Panne%2C%20Michiel%20Guo%2C%20Baining%20Terrain%20runner%3A%20control%2C%20parameterization%2C%20composition%2C%20and%20planning%20for%20highly%20dynamic%20motions%202012"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Libin Liu, KangKang Yin, and Baining Guo. Improving sampling-based motion control. In Computer Graphics Forum, volume 34, pp. 415\u2013423. Wiley Online Library, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Libin%20Yin%2C%20KangKang%20Guo%2C%20Baining%20Improving%20sampling-based%20motion%20control%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Libin%20Yin%2C%20KangKang%20Guo%2C%20Baining%20Improving%20sampling-based%20motion%20control%202015"
        },
        {
            "id": "Merel_et+al_2017_a",
            "entry": "Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02201"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Neumann_et+al_2014_a",
            "entry": "Gerhard Neumann, Christian Daniel, Alexandros Paraschos, Andras Kupcsik, and Jan Peters. Learning modular policies for robotics. Frontiers in computational neuroscience, 8:62, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20Gerhard%20Daniel%2C%20Christian%20Paraschos%2C%20Alexandros%20Kupcsik%2C%20Andras%20Learning%20modular%20policies%20for%20robotics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20Gerhard%20Daniel%2C%20Christian%20Paraschos%2C%20Alexandros%20Kupcsik%2C%20Andras%20Learning%20modular%20policies%20for%20robotics%202014"
        },
        {
            "id": "Openai_2018_a",
            "entry": "OpenAI. Openai five. journal=https://blog.openai.com/openai-five/, 2018.",
            "url": "https://blog.openai.com/openai-five/"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (TOG), 36(4):41, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Peng_et+al_2018_a",
            "entry": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sentis_2005_a",
            "entry": "Luis Sentis and Oussama Khatib. Synthesis of whole-body behaviors through hierarchical control of behavioral primitives. International Journal of Humanoid Robotics, 2(04):505\u2013518, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sentis%2C%20Luis%20Khatib%2C%20Oussama%20Synthesis%20of%20whole-body%20behaviors%20through%20hierarchical%20control%20of%20behavioral%20primitives%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sentis%2C%20Luis%20Khatib%2C%20Oussama%20Synthesis%20of%20whole-body%20behaviors%20through%20hierarchical%20control%20of%20behavioral%20primitives%202005"
        },
        {
            "id": "Simonyan_et+al_2013_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013 211, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "Tassa_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012. Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka GrabskaBarwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        }
    ]
}
