{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GANSYNTH: ADVERSARIAL NEURAL AUDIO SYNTHESIS",
        "author": "Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, & Adam Roberts Google AI Mountain View, CA 94043, USA",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1xQVn09FX"
        },
        "abstract": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure but have slow iterative sampling and lack global latent structure. In contrast, Generative Adversarial Networks (GANs) have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.1"
    },
    "keywords": [
        {
            "term": "high fidelity",
            "url": "https://en.wikipedia.org/wiki/high_fidelity"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "instantaneous frequency",
            "url": "https://en.wikipedia.org/wiki/instantaneous_frequency"
        },
        {
            "term": "autoregressive model",
            "url": "https://en.wikipedia.org/wiki/autoregressive_model"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "audio synthesis",
            "url": "https://en.wikipedia.org/wiki/audio_synthesis"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "IF": "instantaneous frequency",
        "NDB": "Number of Statistically-Different Bins",
        "IS": "Inception Score",
        "PA": "Pitch Accuracy",
        "PE": "Pitch Entropy",
        "FID": "Frechet Inception Distance"
    },
    "highlights": [
        "Neural audio synthesis, training generative models to efficiently produce audio with both highfidelity and global structure, is a challenging open problem as it requires modeling temporal scales over at least five orders of magnitude (\u223c0.1ms to \u223c100s)",
        "Due to their high quality, a lot of research has gone into speeding up generation, but the methods introduce significant overhead such as training a secondary student network or writing highly customized low-level kernels. Since these large models operate at a fine timescale, their autoencoder variants are restricted to only modeling local latent structure due to memory constraints (<a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\">Engel et al, 2017</a>)",
        "The WaveNet baseline produces high-fidelity sounds, but occasionally breaks down into feedback and self oscillation, resulting in a score that is comparable to the instantaneous frequency Generative Adversarial Networks",
        "By carefully controlling the audio representation used for generative modeling, we have demonstrated high-quality audio generation with Generative Adversarial Networks on the NSynth dataset, exceeding the fidelity of a strong WaveNet baseline while generating samples tens of thousands of times faster",
        "While this is a major advance for audio generation with Generative Adversarial Networks, this study focused on a specific controlled dataset, and further work is needed to validate and expand it to a broader class of signals including speech and other types of natural sound",
        "Issues of mode collapse and diversity common to Generative Adversarial Networks exist for audio as well, and we leave it to further work to consider combining adversarial losses with encoders or more straightforward regression losses to better capture the full data distribution.\n8http://g.co/nsynthsuper"
    ],
    "key_statements": [
        "Neural audio synthesis, training generative models to efficiently produce audio with both highfidelity and global structure, is a challenging open problem as it requires modeling temporal scales over at least five orders of magnitude (\u223c0.1ms to \u223c100s)",
        "Due to their high quality, a lot of research has gone into speeding up generation, but the methods introduce significant overhead such as training a secondary student network or writing highly customized low-level kernels. Since these large models operate at a fine timescale, their autoencoder variants are restricted to only modeling local latent structure due to memory constraints (<a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\">Engel et al, 2017</a>)",
        "We investigate the interplay of architecture and representation in synthesizing coherent audio with Generative Adversarial Networks",
        "Prior work on the NSynth dataset used an WaveNet autoencoder to interpolate between sounds (<a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\">Engel et al, 2017</a>), but is not a generative model as it requires conditioning on the original audio",
        "Quality decreases as output representations move from instantaneous frequency-Mel, instantaneous frequency, Phase, to Waveform",
        "The WaveNet baseline produces high-fidelity sounds, but occasionally breaks down into feedback and self oscillation, resulting in a score that is comparable to the instantaneous frequency Generative Adversarial Networks",
        "By carefully controlling the audio representation used for generative modeling, we have demonstrated high-quality audio generation with Generative Adversarial Networks on the NSynth dataset, exceeding the fidelity of a strong WaveNet baseline while generating samples tens of thousands of times faster",
        "While this is a major advance for audio generation with Generative Adversarial Networks, this study focused on a specific controlled dataset, and further work is needed to validate and expand it to a broader class of signals including speech and other types of natural sound",
        "Issues of mode collapse and diversity common to Generative Adversarial Networks exist for audio as well, and we leave it to further work to consider combining adversarial losses with encoders or more straightforward regression losses to better capture the full data distribution.\n8http://g.co/nsynthsuper"
    ],
    "summary": [
        "Neural audio synthesis, training generative models to efficiently produce audio with both highfidelity and global structure, is a challenging open problem as it requires modeling temporal scales over at least five orders of magnitude (\u223c0.1ms to \u223c100s).",
        "On the NSynth dataset, GANs can outperform a strong WaveNet baseline in automatic and human evaluations, and generate examples \u223c54,000 times faster.",
        "As we wanted to included human evaluations on audio quality, we restricted ourselves to training on the subset of acoustic instruments and fundamental pitches ranging from MIDI 24-84 (\u223c32-1000Hz), as those timbres are most likely to sound natural to an average listener.",
        "It is important for us to compare against strong baselines, so we adapt WaveGAN (<a class=\"ref-link\" id=\"cDonahue_et+al_2019_a\" href=\"#rDonahue_et+al_2019_a\">Donahue et al, 2019</a>), the current state of the art in waveform generation with GANs, to accept pitch conditioning and retrain it on our subset of the NSynth dataset.",
        "Prior work on the NSynth dataset used an WaveNet autoencoder to interpolate between sounds (<a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\"><a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\">Engel et al, 2017</a></a>), but is not a generative model as it requires conditioning on the original audio.",
        "The WaveNet baseline produces high-fidelity sounds, but occasionally breaks down into feedback and self oscillation, resulting in a score that is comparable to the IF GANs. While there is no a priori reason that sample diversity should correlate with audio quality, we find that NDB follows the same trend as the human evaluation.",
        "5.2 INTERPOLATION As discussed in the introduction, GANs allow conditioning on the same latent vector the entire sequence, as opposed to only short subsequences for memory intensive autoregressive models like WaveNet. WaveNet autoencoders, such as ones in (<a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\"><a class=\"ref-link\" id=\"cEngel_et+al_2017_a\" href=\"#rEngel_et+al_2017_a\">Engel et al, 2017</a></a>), learn local latent codes that control generation on the scale of milliseconds but have limited scope, and have a structure of their own that must be modelled and does not fit a compact prior.",
        "<a class=\"ref-link\" id=\"cDefossez_et+al_2018_a\" href=\"#rDefossez_et+al_2018_a\">Defossez et al (2018</a>) achieve significant sampling speedups (\u223c2,500x) over wavenet autoencoders by training a frame-based regression model to map from pitch and instrument labels to raw waveforms.",
        "By carefully controlling the audio representation used for generative modeling, we have demonstrated high-quality audio generation with GANs on the NSynth dataset, exceeding the fidelity of a strong WaveNet baseline while generating samples tens of thousands of times faster.",
        "While this is a major advance for audio generation with GANs, this study focused on a specific controlled dataset, and further work is needed to validate and expand it to a broader class of signals including speech and other types of natural sound.",
        "Issues of mode collapse and diversity common to GANs exist for audio as well, and we leave it to further work to consider combining adversarial losses with encoders or more straightforward regression losses to better capture the full data distribution"
    ],
    "headline": "We demonstrate that Generative Adversarial Networks can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain",
    "reference_links": [
        {
            "id": "Andreux_2018_a",
            "entry": "Mathieu Andreux and Stephane Mallat. Music generation and transformation with moment matching-scattering inverse networks. In International Society for Music Information Retrevial Conference, 2018. URL http://ismir2018.ircam.fr/doc/pdfs/131_Paper.pdf.",
            "url": "http://ismir2018.ircam.fr/doc/pdfs/131_Paper.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreux%2C%20Mathieu%20Mallat%2C%20Stephane%20Music%20generation%20and%20transformation%20with%20moment%20matching-scattering%20inverse%20networks%202018"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214\u2013223, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/arjovsky17a.html.",
            "url": "http://proceedings.mlr.press/v70/arjovsky17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017-08"
        },
        {
            "id": "Berthelot_et+al_2017_a",
            "entry": "David Berthelot, Thomas Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "Boashash_1992_a",
            "entry": "Boualem Boashash. Estimating and interpreting the instantaneous frequency of a signal. Proceedings of the IEEE, 80(4):540\u2013568, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boashash%2C%20Boualem%20Estimating%20and%20interpreting%20the%20instantaneous%20frequency%20of%20a%20signal%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boashash%2C%20Boualem%20Estimating%20and%20interpreting%20the%20instantaneous%20frequency%20of%20a%20signal%201992"
        },
        {
            "id": "Brock_et+al_2019_a",
            "entry": "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm.",
            "url": "https://openreview.net/forum?id=B1xsqj09Fm",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Donahue%2C%20Jeff%20Simonyan%2C%20Karen%20Large%20scale%20GAN%20training%20for%20high%20fidelity%20natural%20image%20synthesis%202019"
        },
        {
            "id": "Defossez_et+al_2018_a",
            "entry": "Alexandre Defossez, Neil Zeghidour, Nicolas Usunier, Leon Bottou, and Francis Bach. Sing: Symbol-to-instrument neural generator. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9041\u20139051. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8118-sing-symbol-to-instrument-neural-generator.pdf.",
            "url": "http://papers.nips.cc/paper/8118-sing-symbol-to-instrument-neural-generator.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defossez%2C%20Alexandre%20Zeghidour%2C%20Neil%20Usunier%2C%20Nicolas%20Bottou%2C%20Leon%20Sing%3A%20Symbol-to-instrument%20neural%20generator%202018"
        },
        {
            "id": "Dieleman_2014_a",
            "entry": "Sander Dieleman and Benjamin Schrauwen. End-to-end learning for music audio. In ICASSP, pp. 6964\u20136968, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieleman%2C%20Sander%20Schrauwen%2C%20Benjamin%20End-to-end%20learning%20for%20music%20audio%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieleman%2C%20Sander%20Schrauwen%2C%20Benjamin%20End-to-end%20learning%20for%20music%20audio%202014"
        },
        {
            "id": "Dolson_1986_a",
            "entry": "Mark Dolson. The phase vocoder: A tutorial. Computer Music Journal, 10(4):14\u201327, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dolson%2C%20Mark%20The%20phase%20vocoder%3A%20A%20tutorial%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dolson%2C%20Mark%20The%20phase%20vocoder%3A%20A%20tutorial%201986"
        },
        {
            "id": "Donahue_et+al_2019_a",
            "entry": "Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Chris%20McAuley%2C%20Julian%20Puckette%2C%20Miller%20Adversarial%20audio%20synthesis%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Chris%20McAuley%2C%20Julian%20Puckette%2C%20Miller%20Adversarial%20audio%20synthesis%202019"
        },
        {
            "id": "Engel_et+al_2017_a",
            "entry": "Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with WaveNet autoencoders. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017"
        },
        {
            "id": "Esling_et+al_2018_a",
            "entry": "Philippe Esling, Axel Chemla-Romeu-Santos, and Adrien Bitton. Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces. In Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018, pp. 175\u2013181, 2018. URL http://ismir2018.ircam.fr/doc/pdfs/219_Paper.pdf.",
            "url": "http://ismir2018.ircam.fr/doc/pdfs/219_Paper.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esling%2C%20Philippe%20Chemla-Romeu-Santos%2C%20Axel%20Bitton%2C%20Adrien%20Bridging%20audio%20analysis%2C%20perception%20and%20synthesis%20with%20perceptually-regularized%20variational%20timbre%20spaces%202018-09-23"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In NIPS, pp. 5767\u20135777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NIPS, pp. 6626\u20136637. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "Yanghua Jin, Jiakai Zhang, Minjun Li, Yingtao Tian, Huachun Zhu, and Zhihao Fang. Towards the automatic anime characters creation with generative adversarial networks. arXiv preprint arXiv:1708.05509, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05509"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Karras_et+al_2018_b",
            "entry": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. CoRR, abs/1812.04948, 2018b. URL http://arxiv.org/abs/1812.04948.",
            "url": "http://arxiv.org/abs/1812.04948",
            "arxiv_url": "https://arxiv.org/pdf/1812.04948"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kodali_et+al_2017_a",
            "entry": "Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07215"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Mehri_et+al_2017_a",
            "entry": "Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. SampleRNN: An unconditional end-to-end neural audio generation model. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Gulrajani%2C%20Ishaan%20Kumar%2C%20Rithesh%20SampleRNN%3A%20An%20unconditional%20end-to-end%20neural%20audio%20generation%20model%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Gulrajani%2C%20Ishaan%20Kumar%2C%20Rithesh%20SampleRNN%3A%20An%20unconditional%20end-to-end%20neural%20audio%20generation%20model%202017"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Mor_et+al_2019_a",
            "entry": "Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taigman. Autoencoder-based music translation. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJGkisCcKm.",
            "url": "https://openreview.net/forum?id=HJGkisCcKm",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mor%2C%20Noam%20Wolf%2C%20Lior%20Polyak%2C%20Adam%20Taigman%2C%20Yaniv%20Autoencoder-based%20music%20translation%202019"
        },
        {
            "id": "Odena_et+al_2017_a",
            "entry": "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202017"
        },
        {
            "id": "Paine_et+al_2016_a",
            "entry": "Tom Le Paine, Pooya Khorrami, Shiyu Chang, Yang Zhang, Prajit Ramachandran, Mark A Hasegawa-Johnson, and Thomas S Huang. Fast WaveNet generation algorithm. arXiv preprint arXiv:1611.09482, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09482"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Richardson_2018_a",
            "entry": "Eitan Richardson and Yair Weiss. On GANs and GMMs. CoRR, abs/1805.12462, 2018. URL http://arxiv.org/abs/1805.12462.",
            "url": "http://arxiv.org/abs/1805.12462",
            "arxiv_url": "https://arxiv.org/pdf/1805.12462"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017. URL http://arxiv.org/abs/1701.05517.",
            "url": "http://arxiv.org/abs/1701.05517",
            "arxiv_url": "https://arxiv.org/pdf/1701.05517"
        },
        {
            "id": "Sotelo_et+al_2017_a",
            "entry": "Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio. Char2wav: End-to-end speech synthesis. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sotelo%2C%20Jose%20Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Santos%2C%20Joao%20Felipe%20Char2wav%3A%20End-to-end%20speech%20synthesis%202017"
        },
        {
            "id": "Theis_et+al_2016_a",
            "entry": "Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20Lucas%20van%20den%20Oord%2C%20Aaron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20Lucas%20van%20den%20Oord%2C%20Aaron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In SSW, pp. 125, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, and Demis Hassabis. Parallel WaveNet: Fast high-fidelity speech synthesis. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3918\u20133926, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/oord18a.html.",
            "url": "http://proceedings.mlr.press/v80/oord18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Li%2C%20Yazhe%20Babuschkin%2C%20Igor%20Simonyan%2C%20Karen%20Parallel%20WaveNet%3A%20Fast%20high-fidelity%20speech%20synthesis%202018-07"
        },
        {
            "id": "Yuxuan_et+al_2017_a",
            "entry": "Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. In INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuxuan%20Wang%2C%20R.J.Skerry-Ryan%20Stanton%2C%20Daisy%20Wu%2C%20Yonghui%20Weiss%2C%20Ron%20J.%20Tacotron%3A%20Towards%20end-to-end%20speech%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuxuan%20Wang%2C%20R.J.Skerry-Ryan%20Stanton%2C%20Daisy%20Wu%2C%20Yonghui%20Weiss%2C%20Ron%20J.%20Tacotron%3A%20Towards%20end-to-end%20speech%20synthesis%202017"
        },
        {
            "id": "Wolf_et+al_2017_a",
            "entry": "Lior Wolf, Yaniv Taigman, and Adam Polyak. Unsupervised creation of parameterized avatars. CoRR, abs/1704.05693, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05693"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Zhenyao Zhu, Jesse H Engel, and Awni Y Hannun. Learning multiscale features directly fromwaveforms. CoRR, vol. abs/1603.09509, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.09509"
        }
    ]
}
