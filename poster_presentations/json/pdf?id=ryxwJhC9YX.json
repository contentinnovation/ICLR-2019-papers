{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "INSTAGAN: INSTANCE-AWARE IMAGE-TO-IMAGE TRANSLATION",
        "author": "Sangwoo Mo, Minsu Cho, Jinwoo Shin,\u2021 \u2217Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea \u2020Pohang University of Science and Technology (POSTECH), Pohang, Korea \u2021AItrics, Seoul, Korea \u2217{swmo, jinwoos}@kaist.ac.kr, \u2020mscho@postech.ac.kr",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryxwJhC9YX"
        },
        "journal": "Recent GAN",
        "volume": "1",
        "abstract": "Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs). However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images. To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration. The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances. To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances. We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances. Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases. Code and results are available in https://github.com/sangwoomo/instagan."
    },
    "keywords": [
        {
            "term": "image synthesis",
            "url": "https://en.wikipedia.org/wiki/image_synthesis"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        }
    ],
    "abbreviations": {
        "GANs": "generative adversarial networks",
        "GAN": "generative adversarial networks",
        "NRF": "National Research Foundation of Korea"
    },
    "highlights": [
        "Cross-domain generation arises in many machine learning tasks, including neural machine translation (<a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\"><a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\">Artetxe et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\"><a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\">Lample et al, 2017</a></a>), image synthesis (<a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\"><a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\">Reed et al, 2016</a></a>; Zhu et al, 2016), text style transfer (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\"><a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a></a>), and video generation (<a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\"><a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\">Bansal et al, 2018</a></a>; <a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\"><a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\"><a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\">Chan et al, 2018</a></a>)",
        "The unpaired image-to-image translation has achieved an impressive progress based on variants of generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cChoi_et+al_2017_a\" href=\"#rChoi_et+al_2017_a\">Choi et al, 2017</a>; <a class=\"ref-link\" id=\"cAlmahairi_et+al_2018_a\" href=\"#rAlmahairi_et+al_2018_a\">Almahairi et al, 2018</a>; <a class=\"ref-link\" id=\"cHuang_et+al_2018_a\" href=\"#rHuang_et+al_2018_a\">Huang et al, 2018</a>; <a class=\"ref-link\" id=\"cLee_et+al_2018_a\" href=\"#rLee_et+al_2018_a\">Lee et al, 2018</a>), and has drawn considerable attention due to its practical applications including colorization (Zhang et al, 2016), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a>), semantic manipulation (Wang et al, 2018b), and domain adaptation (<a class=\"ref-link\" id=\"cBousmalis_et+al_2017_a\" href=\"#rBousmalis_et+al_2017_a\">Bousmalis et al, 2017</a>; <a class=\"ref-link\" id=\"cShrivastava_et+al_2017_a\" href=\"#rShrivastava_et+al_2017_a\">Shrivastava et al, 2017</a>; <a class=\"ref-link\" id=\"cHoffman_et+al_2017_a\" href=\"#rHoffman_et+al_2017_a\">Hoffman et al, 2017</a>)",
        "We propose a novel method that incorporates the instance information of multiple target objectsin the framework of generative adversarial networks (GAN); we called it instance-aware generative adversarial networks (InstaGAN)",
        "Our method can control the instances to translate by conditioning the input, as shown in Figure 7. Such a control is impossible under CycleGAN",
        "We have proposed a novel method incorporating the set of instance attributes for image-to-image translation",
        "The experiments on different datasets have shown successful image-to-image translation on the challenging tasks of multi-instance transfiguration, including new tasks, e.g., translating jeans to skirt in fashion images"
    ],
    "key_statements": [
        "Cross-domain generation arises in many machine learning tasks, including neural machine translation (<a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\"><a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\">Artetxe et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\"><a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\">Lample et al, 2017</a></a>), image synthesis (<a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\"><a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\">Reed et al, 2016</a></a>; Zhu et al, 2016), text style transfer (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\"><a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a></a>), and video generation (<a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\"><a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\">Bansal et al, 2018</a></a>; <a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\"><a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\"><a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\">Chan et al, 2018</a></a>)",
        "The unpaired image-to-image translation has achieved an impressive progress based on variants of generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cChoi_et+al_2017_a\" href=\"#rChoi_et+al_2017_a\">Choi et al, 2017</a>; <a class=\"ref-link\" id=\"cAlmahairi_et+al_2018_a\" href=\"#rAlmahairi_et+al_2018_a\">Almahairi et al, 2018</a>; <a class=\"ref-link\" id=\"cHuang_et+al_2018_a\" href=\"#rHuang_et+al_2018_a\">Huang et al, 2018</a>; <a class=\"ref-link\" id=\"cLee_et+al_2018_a\" href=\"#rLee_et+al_2018_a\">Lee et al, 2018</a>), and has drawn considerable attention due to its practical applications including colorization (Zhang et al, 2016), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a>), semantic manipulation (Wang et al, 2018b), and domain adaptation (<a class=\"ref-link\" id=\"cBousmalis_et+al_2017_a\" href=\"#rBousmalis_et+al_2017_a\">Bousmalis et al, 2017</a>; <a class=\"ref-link\" id=\"cShrivastava_et+al_2017_a\" href=\"#rShrivastava_et+al_2017_a\">Shrivastava et al, 2017</a>; <a class=\"ref-link\" id=\"cHoffman_et+al_2017_a\" href=\"#rHoffman_et+al_2017_a\">Hoffman et al, 2017</a>)",
        "We propose a novel method that incorporates the instance information of multiple target objectsin the framework of generative adversarial networks (GAN); we called it instance-aware generative adversarial networks (InstaGAN)",
        "Our method can control the instances to translate by conditioning the input, as shown in Figure 7. Such a control is impossible under CycleGAN",
        "Unlike our architecture which translates the set of instance masks, CycleGAN+Seg translates the union of all masks at once",
        "We have proposed a novel method incorporating the set of instance attributes for image-to-image translation",
        "The experiments on different datasets have shown successful image-to-image translation on the challenging tasks of multi-instance transfiguration, including new tasks, e.g., translating jeans to skirt in fashion images",
        "We remark that our ideas utilizing the set-structured side information have potential to be applied to other cross-domain generations tasks, e.g., neural machine translation or video generation"
    ],
    "summary": [
        "Cross-domain generation arises in many machine learning tasks, including neural machine translation (<a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\"><a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\">Artetxe et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\"><a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\">Lample et al, 2017</a></a>), image synthesis (<a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\"><a class=\"ref-link\" id=\"cReed_et+al_2016_a\" href=\"#rReed_et+al_2016_a\">Reed et al, 2016</a></a>; Zhu et al, 2016), text style transfer (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\"><a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a></a>), and video generation (<a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\"><a class=\"ref-link\" id=\"cBansal_et+al_2018_a\" href=\"#rBansal_et+al_2018_a\">Bansal et al, 2018</a></a>; <a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\"><a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\"><a class=\"ref-link\" id=\"cChan_et+al_2018_a\" href=\"#rChan_et+al_2018_a\">Chan et al, 2018</a></a>).",
        "Our main contribution is three-fold: an instance-augmented neural architecture, a context preserving loss, and a sequential mini-batch inference/training technique.",
        "We propose a neural network architecture that translates both an image and the corresponding set of instance attributes.",
        "We propose a context preserving loss that encourages the network to focus on target instances in translation and learn an identity function outside of them.",
        "We propose a sequential mini-batch inference/training technique, i.e., translating the mini-batches of instance attributes sequentially, instead of doing the entire set at once.",
        "CycleGAN (<a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a>), we use the GAN loss for the domain loss, and consider both the cycle-consistency loss (<a class=\"ref-link\" id=\"cKim_et+al_2017_a\" href=\"#rKim_et+al_2017_a\">Kim et al, 2017</a>; <a class=\"ref-link\" id=\"cYi_et+al_2017_a\" href=\"#rYi_et+al_2017_a\">Yi et al, 2017</a>) and the identity mapping loss (<a class=\"ref-link\" id=\"cTaigman_et+al_2016_a\" href=\"#rTaigman_et+al_2016_a\">Taigman et al, 2016</a>) for the content losses.1 In addition, we propose a new content loss, coined context preserving loss, using the original and predicted segmentation information.",
        "Our newly proposed context preserving loss Lctx enforces to translate instances only, while keeping outside of them, i.e., background.",
        "We found that the context preserving loss not only keeps the background better, but improves the quality of generated instance segmentations.",
        "We apply content loss (4-6) to the intermediate samples of current mini-batch am, as it is just a function of inputs and outputs of the generator G.2 In contrast, we apply GAN loss (3) to the samples of aggregated mini-batches, because the network fails to align images and masks when using only a partial subset of instance masks.",
        "While CycleGAN mostly fails, our method generates reasonable shapes of the target instances and keeps the original contexts by focusing on the instances via the context preserving loss.",
        "Our method is composed of the InstaGAN architecture, the context preserving loss Lctx, and the sequential minibatch inference/training technique.",
        "We train a CycleGAN model with an additional input channel, which translates the mask-augmented image, we call it CycleGAN+Seg. Unlike our architecture which translates the set of instance masks, CycleGAN+Seg translates the union of all masks at once.",
        "4 For the results in Figure 8, we trained a pix2pix (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al, 2017</a>) model to predict a single mask from an image, but one can utilize recent methods to predict instance masks in supervised (<a class=\"ref-link\" id=\"cHe_et+al_2017_a\" href=\"#rHe_et+al_2017_a\">He et al, 2017</a>) or weakly-supervised (<a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\">Zhou et al, 2018</a>) way.",
        "Investigating new tasks and new information could be an interesting research direction in the future"
    ],
    "headline": "We propose a novel method, coined instance-aware generative adversarial networks, that incorporates the instance information and improves multi-instance transfiguration",
    "reference_links": [
        {
            "id": "Almahairi_et+al_2018_a",
            "entry": "Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint arXiv:1802.10151, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10151"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Artetxe_et+al_2017_a",
            "entry": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11041"
        },
        {
            "id": "Bansal_et+al_2018_a",
            "entry": "Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. Recycle-gan: Unsupervised video retargeting. arXiv preprint arXiv:1808.05174, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05174"
        },
        {
            "id": "Benaim_2017_a",
            "entry": "Sagie Benaim and Lior Wolf. One-sided unsupervised domain mapping. In Advances in Neural Information Processing Systems, pp. 752\u2013762, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benaim%2C%20Sagie%20Wolf%2C%20Lior%20One-sided%20unsupervised%20domain%20mapping%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benaim%2C%20Sagie%20Wolf%2C%20Lior%20One-sided%20unsupervised%20domain%20mapping%202017"
        },
        {
            "id": "Bousmalis_et+al_2017_a",
            "entry": "Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 7, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20Konstantinos%20Silberman%2C%20Nathan%20Dohan%2C%20David%20Erhan%2C%20Dumitru%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20Konstantinos%20Silberman%2C%20Nathan%20Dohan%2C%20David%20Erhan%2C%20Dumitru%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Chan_et+al_2018_a",
            "entry": "Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. arXiv preprint arXiv:1808.07371, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07371"
        },
        {
            "id": "Choi_et+al_2017_a",
            "entry": "Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. arXiv preprint arXiv:1711.09020, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09020"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Galanti_et+al_2018_a",
            "entry": "Tomer Galanti, Lior Wolf, and Sagie Benaim. The role of minimal complexity functions in unsupervised learning of semantic mappings. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galanti%2C%20Tomer%20Wolf%2C%20Lior%20Benaim%2C%20Sagie%20The%20role%20of%20minimal%20complexity%20functions%20in%20unsupervised%20learning%20of%20semantic%20mappings%202018"
        },
        {
            "id": "Gokaslan_et+al_2018_a",
            "entry": "Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie, Kwang In Kim, and James Tompkin. Improving shape deformation in unsupervised image-to-image translation. arXiv preprint arXiv:1808.04325, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04325"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767\u20135777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "He_et+al_2017_a",
            "entry": "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2980\u20132988. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Dollar%20and%20Ross%20Girshick%20Mask%20rcnn%20In%20Computer%20Vision%20ICCV%202017%20IEEE%20International%20Conference%20on%20pp%2029802988%20IEEE%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Dollar%20and%20Ross%20Girshick%20Mask%20rcnn%20In%20Computer%20Vision%20ICCV%202017%20IEEE%20International%20Conference%20on%20pp%2029802988%20IEEE%202017"
        },
        {
            "id": "Hoffman_et+al_2017_a",
            "entry": "Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03213"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-toimage translation. arXiv preprint arXiv:1804.04732, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04732"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.05192"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lample_et+al_2017_a",
            "entry": "Guillaume Lample, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00043"
        },
        {
            "id": "Ledig_et+al_2017_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, volume 2, pp. 4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. arXiv preprint arXiv:1808.00948, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.00948"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pp. 2203\u20132213, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "Liang_2017_a",
            "entry": "Xiaodan Liang, Hao Zhang, and Eric P Xing. Generative semantic manipulation with contrasting gan. arXiv preprint arXiv:1708.00315, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00315"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pp. 700\u2013708, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Mejjati_et+al_2018_a",
            "entry": "Youssef A Mejjati, Christian Richardt, James Tompkin, Darren Cosker, and Kwang In Kim. Unsupervised attention-guided image to image translation. arXiv preprint arXiv:1806.02311, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02311"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Mroueh_et+al_2017_a",
            "entry": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev gan. arXiv preprint arXiv:1711.04894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04894"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Reed_et+al_2016_a",
            "entry": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.05396"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Information Processing Systems, pp. 6830\u20136841, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017"
        },
        {
            "id": "Shrivastava_et+al_2017_a",
            "entry": "Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learning from simulated and unsupervised images through adversarial training. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 3, pp. 6, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrivastava%2C%20Ashish%20Pfister%2C%20Tomas%20Tuzel%2C%20Oncel%20Susskind%2C%20Josh%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrivastava%2C%20Ashish%20Pfister%2C%20Tomas%20Tuzel%2C%20Oncel%20Susskind%2C%20Josh%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Taigman_et+al_2016_a",
            "entry": "Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02200"
        },
        {
            "id": "Ulyanov_2016_a",
            "entry": "Vedaldi Ulyanov and Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Wang_et+al_0000_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. arXiv preprint arXiv:1808.06601, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1808.06601"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 5, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20Highresolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20Highresolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018"
        },
        {
            "id": "Yang_et+al_2014_a",
            "entry": "Wei Yang, Ping Luo, and Liang Lin. Clothing co-parsing by joint image segmentation and labeling. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3182\u2013 3189, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Wei%20Luo%2C%20Ping%20Lin%2C%20Liang%20Clothing%20co-parsing%20by%20joint%20image%20segmentation%20and%20labeling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Wei%20Luo%2C%20Ping%20Lin%2C%20Liang%20Clothing%20co-parsing%20by%20joint%20image%20segmentation%20and%20labeling%202014"
        },
        {
            "id": "Yi_et+al_2017_a",
            "entry": "Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for imageto-image translation. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20Zili%20Zhang%2C%20Hao%20Tan%2C%20Ping%20Gong%2C%20Minglun%20Dualgan%3A%20Unsupervised%20dual%20learning%20for%20imageto-image%20translation.%20arXiv%20p%202017"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp. 3391\u20133401, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033913401%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033913401%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhao_et+al_2018_a",
            "entry": "Jian Zhao, Jianshu Li, Yu Cheng, Li Zhou, Terence Sim, Shuicheng Yan, and Jiashi Feng. Understanding humans in crowded scenes: Deep nested adversarial learning and a new benchmark for multi-human parsing. arXiv preprint arXiv:1804.03287, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03287"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Weakly supervised instance segmentation using class peak response. arXiv preprint arXiv:1804.00880, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00880"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "And_2018_a",
            "entry": "For all the experiments, we simply set \u03bbcyc = 10, \u03bbidt = 10, and \u03bbctx = 10 for our loss (7). We used Adam (Kingma & Ba, 2014) optimizer with batch size 4, training with 4 GPUs in parallel. All networks were trained from scratch, with learning rate of 0.0002 for G and 0.0001 for D, and \u03b21 = 0.5, \u03b22 = 0.999 for the optimizer. Similar to CycleGAN (Zhu et al., 2017), we kept learning rate for first 100 epochs and linearly decayed to zero for next 100 epochs for multi-human parsing (MHP) (Zhao et al., 2018) and COCO (Lin et al., 2014) dataset, and kept learning rate for first 400 epochs and linearly decayed for next 200 epochs for clothing co-parsing (CCP) (Yang et al., 2014) dataset, as it contains smaller number of samples. We sampled two classes from the datasets above, and used it as two domains for translation. We resized images with size 300\u00d7200 (height\u00d7width) for CCP dataset, 240\u00d7160 for MHP dataset, and 200\u00d7200 for COCO dataset, respectively.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20all%20the%20experiments%20we%20simply%20set%20%CE%BBcyc%20%2010%20%CE%BBidt%20%2010%20and%20%CE%BBctx%20%2010%20for%20our%20loss%207%20We%20used%20Adam%20Kingma%20%20Ba%202014%20optimizer%20with%20batch%20size%204%20training%20with%204%20GPUs%20in%20parallel%20All%20networks%20were%20trained%20from%20scratch%20with%20learning%20rate%20of%2000002%20for%20G%20and%2000001%20for%20D%20and%20%CE%B21%20%2005%20%CE%B22%20%200999%20for%20the%20optimizer%20Similar%20to%20CycleGAN%20Zhu%20et%20al%202017%20we%20kept%20learning%20rate%20for%20first%20100%20epochs%20and%20linearly%20decayed%20to%20zero%20for%20next%20100%20epochs%20for%20multihuman%20parsing%20MHP%20Zhao%20et%20al%202018%20and%20COCO%20Lin%20et%20al%202014%20dataset%20and%20kept%20learning%20rate%20for%20first%20400%20epochs%20and%20linearly%20decayed%20for%20next%20200%20epochs%20for%20clothing%20coparsing%20CCP%20Yang%20et%20al%202014%20dataset%20as%20it%20contains%20smaller%20number%20of%20samples%20We%20sampled%20two%20classes%20from%20the%20datasets%20above%20and%20used%20it%20as%20two%20domains%20for%20translation%20We%20resized%20images%20with%20size%20300200%20heightwidth%20for%20CCP%20dataset%20240160%20for%20MHP%20dataset%20and%20200200%20for%20COCO%20dataset%20respectively",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20all%20the%20experiments%20we%20simply%20set%20%CE%BBcyc%20%2010%20%CE%BBidt%20%2010%20and%20%CE%BBctx%20%2010%20for%20our%20loss%207%20We%20used%20Adam%20Kingma%20%20Ba%202014%20optimizer%20with%20batch%20size%204%20training%20with%204%20GPUs%20in%20parallel%20All%20networks%20were%20trained%20from%20scratch%20with%20learning%20rate%20of%2000002%20for%20G%20and%2000001%20for%20D%20and%20%CE%B21%20%2005%20%CE%B22%20%200999%20for%20the%20optimizer%20Similar%20to%20CycleGAN%20Zhu%20et%20al%202017%20we%20kept%20learning%20rate%20for%20first%20100%20epochs%20and%20linearly%20decayed%20to%20zero%20for%20next%20100%20epochs%20for%20multihuman%20parsing%20MHP%20Zhao%20et%20al%202018%20and%20COCO%20Lin%20et%20al%202014%20dataset%20and%20kept%20learning%20rate%20for%20first%20400%20epochs%20and%20linearly%20decayed%20for%20next%20200%20epochs%20for%20clothing%20coparsing%20CCP%20Yang%20et%20al%202014%20dataset%20as%20it%20contains%20smaller%20number%20of%20samples%20We%20sampled%20two%20classes%20from%20the%20datasets%20above%20and%20used%20it%20as%20two%20domains%20for%20translation%20We%20resized%20images%20with%20size%20300200%20heightwidth%20for%20CCP%20dataset%20240160%20for%20MHP%20dataset%20and%20200200%20for%20COCO%20dataset%20respectively"
        }
    ]
}
