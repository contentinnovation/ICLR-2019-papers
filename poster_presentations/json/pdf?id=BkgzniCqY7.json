{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "STRUCTURED ADVERSARIAL ATTACK: TOWARDS GENERAL IMPLEMENTATION AND BETTER INTERPRETABILITY",
        "author": "Kaidi Xu,\u2217 Sijia Liu,\u2217 Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi Wang, Xue Lin, 1Northeastern University, USA 2MIT-IBM Watson AI Lab, IBM Research, USA 3University of California, Los Angeles, USA",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkgzniCqY7"
        },
        "journal": "Generality",
        "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), p norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of p-norm distortion (p \u2208 {1, 2, \u221e}) as the stateof-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Papernot et al., 2016b) and class activation map (Zhou et al., 2016). Our code is available at https://github.com/KaidiXu/StrAttack."
    },
    "keywords": [
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "art attack",
            "url": "https://en.wikipedia.org/wiki/art_attack"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "alternating direction method of multipliers",
            "url": "https://en.wikipedia.org/wiki/alternating_direction_method_of_multipliers"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "ADMM": "alternating direction method of multipliers",
        "ASM": "adversarial saliency map",
        "CAM": "class activation map",
        "ASR": "attack success rate",
        "IS": "interpretability score"
    },
    "highlights": [
        "Deep learning achieves exceptional successes in domains such as image recognition (He et al, 2016; Geifman & ElYaniv, 2017), natural language processing (Hinton et al, 2012; Harwath et al, 2016), medical diagnostics (Chen et al, 2016; Shi et al, 2018) and advanced control (Silver et al, 2016; Fu et al, 2017)",
        "(Efficient implementation) We develop an efficient algorithm to generate structured adversarial perturbations by leveraging the alternating direction method of multipliers (ADMM)",
        "We introduce the concept of StrAttack, motivated by the question: \u2018what possible structures could adversarial perturbations have to fool deep neural networks?\u2019 Our idea is to divide an image into sub-groups of pixels and penalize the corresponding group-wise sparsity",
        "Besides adversarial saliency map, we show that the effect of adversarial perturbations can be visually explained through the class-specific discriminative image regions localized by class activation map (Zhou et al, 2016)",
        "Different from previous works that use p norm to measure the similarity between an original image and an adversarial example, this work incorporates group-sparsity regularization into the problem formulation of generating adversarial examples and achieves strong group sparsity in the obtained adversarial perturbations",
        "Leveraging alternating direction method of multipliers, we develop an efficient implementation to generate structured adversarial perturbations, which can be further used to refine an arbitrary adversarial attack under fixed group sparse structures"
    ],
    "key_statements": [
        "Deep learning achieves exceptional successes in domains such as image recognition (He et al, 2016; Geifman & ElYaniv, 2017), natural language processing (Hinton et al, 2012; Harwath et al, 2016), medical diagnostics (Chen et al, 2016; Shi et al, 2018) and advanced control (Silver et al, 2016; Fu et al, 2017)",
        "Crafted adversarial examples can mislead a deep neural networks to recognize them as any target image label, while the perturbations appears unnoticeable to human eyes",
        "(Efficient implementation) We develop an efficient algorithm to generate structured adversarial perturbations by leveraging the alternating direction method of multipliers (ADMM)",
        "With the aid of adversarial saliency map (Papernot et al, 2016b) and class activation map (Zhou et al, 2016), we show that the obtained group-sparse adversarial patterns better shed light on the mechanisms of adversarial perturbations to fool deep neural networks",
        "We argue that imperceptibility could be important since it helps us to understand how/why deep neural networks are vulnerable to adversarial attacks while perturbing natural examples just by indistinguished adversarial noise",
        "We introduce the concept of StrAttack, motivated by the question: \u2018what possible structures could adversarial perturbations have to fool deep neural networks?\u2019 Our idea is to divide an image into sub-groups of pixels and penalize the corresponding group-wise sparsity",
        "We show that the process of generating structured adversarial examples leads to an optimization problem that is difficult to solve using the existing optimizers Adam and FISTA (Carlini & Wagner, 2017; Chen et al, 2017a)",
        "We develop an efficient optimization method via alternating direction method of multipliers (ADMM)",
        "We evaluate the performance of the proposed StrAttack on three image classification datasets, MNIST (Lecun et al, 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al, 2009)",
        "We evaluate attack success rate (ASR)1 as well as p distortion metrics for p \u2208 {0, 1, 2, \u221e}",
        "As applying a threshold to have the same 0 norm as our attack, we find that only 6.7% of adversarial examples generated from C&W attack remain valid",
        "In Table 2, we demonstrate the attack success rate and the number of perturbed pixels for various attacks over 5000 adversarial examples",
        "We recall that adversarial saliency map measures the impact of pixel-level perturbations on label classification, and class activation map localizes class-specific image discriminative regions that we use to visually explain adversarial perturbations (Xiao et al, 2018)",
        "We will show that compared to C&W attack, StrAttack meets better interpretability in terms of (a) a higher adversarial saliency map score and (b) a tighter connection with class activation map, where the metric (a) implies interpretability at a micro-level, namely, perturbing pixels with largest impact on image classification, and the metric (b) demonstrates interpretability at a macro-level, namely, perturbations can be mapped to the most discriminative image regions localized by class activation map",
        "Besides adversarial saliency map, we show that the effect of adversarial perturbations can be visually explained through the class-specific discriminative image regions localized by class activation map (Zhou et al, 2016)",
        "To better interpret the mechanism of adversarial examples, we study adversarial attacks on some complex images, where the objects of the original and target labels exist simultaneously as shown in Fig. 4",
        "Different from previous works that use p norm to measure the similarity between an original image and an adversarial example, this work incorporates group-sparsity regularization into the problem formulation of generating adversarial examples and achieves strong group sparsity in the obtained adversarial perturbations",
        "Leveraging alternating direction method of multipliers, we develop an efficient implementation to generate structured adversarial perturbations, which can be further used to refine an arbitrary adversarial attack under fixed group sparse structures"
    ],
    "summary": [
        "Deep learning achieves exceptional successes in domains such as image recognition (He et al, 2016; Geifman & ElYaniv, 2017), natural language processing (Hinton et al, 2012; Harwath et al, 2016), medical diagnostics (Chen et al, 2016; Shi et al, 2018) and advanced control (Silver et al, 2016; Fu et al, 2017).",
        "We generalize our proposed ADMM solution framework to the case of generating adversarial perturbations with overlapping group structures.",
        "StrAttack is able to highlight the most important group structures of adversarial perturbations without attacking other pixels.",
        "StrAttack can achieve the similar ASR compared to other attack methods, it perturbs a much less number of pixels.",
        "We recall that ASM measures the impact of pixel-level perturbations on label classification, and CAM localizes class-specific image discriminative regions that we use to visually explain adversarial perturbations (Xiao et al, 2018).",
        "We will show that compared to C&W attack, StrAttack meets better interpretability in terms of (a) a higher ASM score and (b) a tighter connection with CAM, where the metric (a) implies interpretability at a micro-level, namely, perturbing pixels with largest impact on image classification, and the metric (b) demonstrates interpretability at a macro-level, namely, perturbations can be mapped to the most discriminative image regions localized by CAM.",
        "We obsreve that our attack outperforms C&W attack in terms of IS, since the former is able to extract important local structures of images by penalizing the group sparsity of adversarial perturbations.",
        "To better interpret the mechanism of adversarial examples, we study adversarial attacks on some complex images, where the objects of the original and target labels exist simultaneously as shown in Fig. 4.",
        "It can be visualized from CAM that both C&W attack and StrAttack yields similar adversarial effects on natural images: Adversarial perturbations are used to suppress the most discriminative region with respect to the true label, and simultaneously promotes the discriminative region of the target label.",
        "Compared to C&W attack, StrAttack perturbs much less but \u2018right\u2019 pixels which have better correspondence with class-specific discriminative image regions localized by CAM.",
        "Different from previous works that use p norm to measure the similarity between an original image and an adversarial example, this work incorporates group-sparsity regularization into the problem formulation of generating adversarial examples and achieves strong group sparsity in the obtained adversarial perturbations.",
        "Leveraging ADMM, we develop an efficient implementation to generate structured adversarial perturbations, which can be further used to refine an arbitrary adversarial attack under fixed group sparse structures.",
        "We perform extensive experiments using MNIST, CIFAR-10 and ImageNet datasets, showing that our structured adversarial attack (StrAttack) is much stronger than the existing attacks and its better interpretability from group sparse structures aids in uncovering the origins of adversarial examples."
    ],
    "headline": "We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet",
    "reference_links": [
        {
            "id": "Anonymous_2019_a",
            "entry": "Anonymous. Random mask: Towards robust convolutional neural networks. In Submitted to International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SkgkJn05YX.under review.",
            "url": "https://openreview.net/forum?id=SkgkJn05YX.under",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anonymous%20Random%20mask%3A%20Towards%20robust%20convolutional%20neural%20networks%202019"
        },
        {
            "id": "Bach_et+al_2012_a",
            "entry": "F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends R in Machine Learning, 4(1):1\u2013106, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Jenatton%2C%20R.%20Mairal%2C%20J.%20Obozinski%2C%20G.%20Optimization%20with%20sparsity-inducing%20penalties%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Jenatton%2C%20R.%20Mairal%2C%20J.%20Obozinski%2C%20G.%20Optimization%20with%20sparsity-inducing%20penalties%202012"
        },
        {
            "id": "Beck_2009_a",
            "entry": "A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Teboulle%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Teboulle%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "Boyd_et+al_2011_a",
            "entry": "S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine Learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011"
        }
    ]
}
