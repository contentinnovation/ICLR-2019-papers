{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NON-VACUOUS GENERALIZATION BOUNDS AT THE IMAGENET SCALE: A PAC-BAYESIAN COMPRESSION APPROACH",
        "author": "Wenda Zhou Columbia University New York, NY wz,@columbia.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJgqqsAct7"
        },
        "abstract": "Modern neural networks are highly overparameterized, with capacity to substantially overfit to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be \u201ccompressed\u201d to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size that, combined with off-theshelf compression algorithms, leads to state-of-the-art generalization guarantees. In particular, we provide the first non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classification problem. Additionally, we show that compressibility of models that tend to overfit is limited. Empirical results show that an increase in overfitting increases the number of bits required to describe a trained network."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "vc dimension",
            "url": "https://en.wikipedia.org/wiki/vc_dimension"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Minimum description length",
            "url": "https://en.wikipedia.org/wiki/Minimum_description_length"
        }
    ],
    "abbreviations": {
        "MDL": "Minimum description length"
    },
    "highlights": [
        "A pivotal question in machine learning is why deep networks perform well despite overparameterization",
        "Explaining the generalization performance of neural networks is an active area of current research",
        "Inspired by classical results relating small model size and generalization performance, we establish a new generalization bound based on the effective compressed size of a trained neural network",
        "We have shown that compression results directly imply generalization bounds, and that these may be applied effectively to obtain non-vacuous bounds on neural networks",
        "It has been a long standing observation by practitioners that despite the large capacity of models used in deep learning practice, empirical results demonstrate good generalization performance",
        "We show that with no modifications, a standard engineering pipeline of training and compressing a network leads to demonstrable and non-vacuous generalization guarantees"
    ],
    "key_statements": [
        "A pivotal question in machine learning is why deep networks perform well despite overparameterization",
        "Explaining the generalization performance of neural networks is an active area of current research",
        "Inspired by classical results relating small model size and generalization performance, we establish a new generalization bound based on the effective compressed size of a trained neural network",
        "We have shown that compression results directly imply generalization bounds, and that these may be applied effectively to obtain non-vacuous bounds on neural networks",
        "It has been a long standing observation by practitioners that despite the large capacity of models used in deep learning practice, empirical results demonstrate good generalization performance",
        "We show that with no modifications, a standard engineering pipeline of training and compressing a network leads to demonstrable and non-vacuous generalization guarantees"
    ],
    "summary": [
        "A pivotal question in machine learning is why deep networks perform well despite overparameterization.",
        "Inspired by classical results relating small model size and generalization performance, we establish a new generalization bound based on the effective compressed size of a trained neural network.",
        "A variant of a known compression bound for a PAC-Bayesian formulation) is applied to bound the generalization error of this simpler network in terms of its code length.",
        "The present paper develops a tool to leverage existing neural network compression algorithms to obtain strong generalization bounds.",
        "Like the present paper, <a class=\"ref-link\" id=\"cLangford_2002_b\" href=\"#rLangford_2002_b\">Langford & Caruana (2002</a>); <a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\"><a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite & Roy (2017</a></a>) work with a random classifier given by considering a normal distribution over the weights centered at the output of the training procedure.",
        "Following <a class=\"ref-link\" id=\"cLangford_2002_b\" href=\"#rLangford_2002_b\">Langford & Caruana (2002</a>); <a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\"><a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite & Roy (2017</a></a>), we bound the generalization error of a stochastic estimator given by applying independent random normal noise to the nonzero weights of the network.",
        "We present examples combining our theoretical arguments with state-of-the-art neural network compression schemes.1 Recall that almost all other approaches to bounding generalization error of deep neural networks yield vacuous bounds for realistic problems.",
        "We give two examples applying our generalization bounds to the models output by modern neural net compression schemes.",
        "By combining neural net compression schemes with parsimonious models of this kind, we demonstrate a non-vacuous bounds on models with better performance than AlexNet. Our simple Occam bound requires only minimal assumptions, and can be directly applied to existing compressed networks.",
        "We consider the stochastic classifier given by adding Gaussian noise to the non-zero weights, with the variance set in each layer so as not to degrade our prediction performance.",
        "The small effective compressed size and high training data accuracy yield non-vacuous bounds for top-1 and top-5 test error.",
        "We have shown that compression results directly imply generalization bounds, and that these may be applied effectively to obtain non-vacuous bounds on neural networks.",
        "It has been a long standing observation by practitioners that despite the large capacity of models used in deep learning practice, empirical results demonstrate good generalization performance.",
        "We show that with no modifications, a standard engineering pipeline of training and compressing a network leads to demonstrable and non-vacuous generalization guarantees.",
        "If a network trained on ImageNet to 90% training and 70% testing accuracy could be compressed to an effective size of 30 KiB\u2014about one order of magnitude smaller than our current compression\u2014that would yield a sharp bound on the generalization error."
    ],
    "headline": "We provide the first non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classification problem",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets via a compression approach, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20S.%20Ge%2C%20R.%20Neyshabur%2C%20B.%20Zhang%2C%20Y.%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018-02"
        },
        {
            "id": "Arora_2017_a",
            "entry": "Sanjeev Arora. Generalization Theory and Deep Nets, An introduction. http://www.offconvex.org/2017/12/08/generalization1/, 2017.",
            "url": "http://www.offconvex.org/2017/12/08/generalization1/"
        },
        {
            "id": "Baldassi_et+al_2015_a",
            "entry": "Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses. Phys. Rev. Lett., 115:128101, Sep 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldassi%2C%20Carlo%20Ingrosso%2C%20Alessandro%20Lucibello%2C%20Carlo%20Saglietti%2C%20Luca%20Subdominant%20dense%20clusters%20allow%20for%20simple%20learning%20and%20high%20computational%20performance%20in%20neural%20networks%20with%20discrete%20synapses%202015-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldassi%2C%20Carlo%20Ingrosso%2C%20Alessandro%20Lucibello%2C%20Carlo%20Saglietti%2C%20Luca%20Subdominant%20dense%20clusters%20allow%20for%20simple%20learning%20and%20high%20computational%20performance%20in%20neural%20networks%20with%20discrete%20synapses%202015-09"
        },
        {
            "id": "Baldassi_et+al_2016_a",
            "entry": "Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes. Proceedings of the National Academy of Sciences, 113(48):E7655\u2013E7662, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldassi%2C%20Carlo%20Borgs%2C%20Christian%20Chayes%2C%20Jennifer%20T.%20Ingrosso%2C%20Alessandro%20Unreasonable%20effectiveness%20of%20learning%20neural%20networks%3A%20From%20accessible%20states%20and%20robust%20ensembles%20to%20basic%20algorithmic%20schemes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldassi%2C%20Carlo%20Borgs%2C%20Christian%20Chayes%2C%20Jennifer%20T.%20Ingrosso%2C%20Alessandro%20Unreasonable%20effectiveness%20of%20learning%20neural%20networks%3A%20From%20accessible%20states%20and%20robust%20ensembles%20to%20basic%20algorithmic%20schemes%202016"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Blumer_et+al_1987_a",
            "entry": "Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occam\u2019s razor. Information Processing Letters, 24(6):377 \u2013 380, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumer%2C%20Anselm%20Ehrenfeucht%2C%20Andrzej%20Haussler%2C%20David%20Warmuth%2C%20Manfred%20K.%20Occam%E2%80%99s%20razor%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumer%2C%20Anselm%20Ehrenfeucht%2C%20Andrzej%20Haussler%2C%20David%20Warmuth%2C%20Manfred%20K.%20Occam%E2%80%99s%20razor%201987"
        },
        {
            "id": "B_et+al_2014_a",
            "entry": "Luc B\u00e9gin, Pascal Germain, Fran\u00e7ois Laviolette, and Jean-Francis Roy. PAC-Bayesian Theory for Transductive Learning. In Samuel Kaski and Jukka Corander (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, volume 33 of Proceedings of Machine Learning Research, pp. 105\u2013113, Reykjavik, Iceland, 22\u201325 Apr 2014. PMLR. URL http://proceedings.mlr.press/v33/begin14.html.",
            "url": "http://proceedings.mlr.press/v33/begin14.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%C3%A9gin%2C%20Luc%20Germain%2C%20Pascal%20Laviolette%2C%20Fran%C3%A7ois%20Roy%2C%20Jean-Francis%20PAC-Bayesian%20Theory%20for%20Transductive%20Learning%202014-04"
        },
        {
            "id": "Catoni_2007_a",
            "entry": "Olivier Catoni. Pac-Bayesian supervised classification: the thermodynamics of statistical learning, volume 56 of Institute of Mathematical Statistics Lecture Notes\u2014Monograph Series. Institute of Mathematical Statistics, Beachwood, OH, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catoni%2C%20Olivier%20Pac-Bayesian%20supervised%20classification%3A%20the%20thermodynamics%20of%20statistical%20learning%2C%20volume%2056%202007"
        },
        {
            "id": "Chaudhari_et+al_2017_a",
            "entry": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-SGD%3A%20Biasing%20gradient%20descent%20into%20wide%20valleys%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-SGD%3A%20Biasing%20gradient%20descent%20into%20wide%20valleys%202017"
        },
        {
            "id": "Cheng_et+al_2018_a",
            "entry": "Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration for deep neural networks: The principles, progress, and challenges. IEEE Signal Processing Magazine, 35(1): 126\u2013136, Jan 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Y.%20Wang%2C%20D.%20Zhou%2C%20P.%20Zhang%2C%20T.%20Model%20compression%20and%20acceleration%20for%20deep%20neural%20networks%3A%20The%20principles%2C%20progress%2C%20and%20challenges%202018-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Y.%20Wang%2C%20D.%20Zhou%2C%20P.%20Zhang%2C%20T.%20Model%20compression%20and%20acceleration%20for%20deep%20neural%20networks%3A%20The%20principles%2C%20progress%2C%20and%20challenges%202018-01"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Conference on Uncertainty in Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017"
        },
        {
            "id": "Dziugaite_2018_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Entropy-sgd optimizes the prior of a pac-bayes bound: Generalization properties of entropy-sgd and data-dependent priors. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Entropy-sgd%20optimizes%20the%20prior%20of%20a%20pac-bayes%20bound%3A%20Generalization%20properties%20of%20entropy-sgd%20and%20data-dependent%20priors%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Entropy-sgd%20optimizes%20the%20prior%20of%20a%20pac-bayes%20bound%3A%20Generalization%20properties%20of%20entropy-sgd%20and%20data-dependent%20priors%202018"
        },
        {
            "id": "Guo_et+al_2016_a",
            "entry": "Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20DNNs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20DNNs%202016"
        },
        {
            "id": "Han_et+al_2016_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "Harvey_et+al_2017_a",
            "entry": "Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1064\u20131068, Amsterdam, Netherlands, 07\u201310 Jul 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017-07"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqin Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqin%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqin%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06"
        },
        {
            "id": "Hinton_1993_a",
            "entry": "Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, pp. 5\u201313, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, January 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997-01"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Howard%2C%20Andrew%20G.%20Zhu%2C%20Menglong%20Chen%2C%20Bo%20Kalenichenko%2C%20Dmitry%20Mobilenets%3A%20Efficient%20convolutional%20neural%20networks%20for%20mobile%20vision%20applications%202017"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5mb model size. arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Keskar_et+al_2017_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, pp. 1097\u20131105. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Langford_2002_a",
            "entry": "John Langford. Quantitatively tight sample complexity bounds. PhD thesis, Carnegie Mellon University, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Quantitatively%20tight%20sample%20complexity%20bounds%202002"
        },
        {
            "id": "Langford_2002_b",
            "entry": "John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information Processing Systems, pp. 809\u2013816, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Caruana%2C%20Rich%20bounding%20the%20true%20error%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Caruana%2C%20Rich%20bounding%20the%20true%20error%202002"
        },
        {
            "id": "Laviolette_2017_a",
            "entry": "Fran\u00e7ois Laviolette. A tutorial on pac-bayesian theory. Talk at the NIPS 2017 Workshop: (Almost) 50 Shades of PAC-Bayesian Learning: PAC-Bayesian trends and insights, 2017. URL https://bguedj.github.io/nips2017/pdf/laviolette_nips2017.pdf.",
            "url": "https://bguedj.github.io/nips2017/pdf/laviolette_nips2017.pdf"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 11 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Mackay_1992_a",
            "entry": "David MacKay. Bayesian model comparison and backprop nets. In Advances in Neural Information Processing Systems, pp. 839\u2013846. 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20Bayesian%20model%20comparison%20and%20backprop%20nets%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20David%20Bayesian%20model%20comparison%20and%20backprop%20nets%201992"
        },
        {
            "id": "Mcallester_2003_a",
            "entry": "David McAllester. Simplified pac-bayesian margin bounds. In Bernhard Sch\u00f6lkopf and Manfred K. Warmuth (eds.), Learning Theory and Kernel Machines, pp. 203\u2013215, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg. ISBN 978-3-540-45167-9.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20Simplified%20pac-bayesian%20margin%20bounds%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20Simplified%20pac-bayesian%20margin%20bounds%202003"
        },
        {
            "id": "We_2016_a",
            "entry": "We carry out the pruning using Dynamic Network Surgery (Guo et al., 2016). The threshold is selected per layer as the mean of the layer coefficients offset by a constant multiple of the standard deviation of the coefficients, where the multiple is piecewise constant starting at 0.0 and ending at 4.0. We choose the pruning probability as a piecewise constant starting at 1.0 and decaying to 10\u22123. We train for 30000 steps using the ADAM optimizer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20carry%20out%20the%20pruning%20using%20Dynamic%20Network%20Surgery%20Guo%20et%20al%202016%20The%20threshold%20is%20selected%20per%20layer%20as%20the%20mean%20of%20the%20layer%20coefficients%20offset%20by%20a%20constant%20multiple%20of%20the%20standard%20deviation%20of%20the%20coefficients%20where%20the%20multiple%20is%20piecewise%20constant%20starting%20at%2000%20and%20ending%20at%2040%20We%20choose%20the%20pruning%20probability%20as%20a%20piecewise%20constant%20starting%20at%2010%20and%20decaying%20to%20103%20We%20train%20for%2030000%20steps%20using%20the%20ADAM%20optimizer",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20carry%20out%20the%20pruning%20using%20Dynamic%20Network%20Surgery%20Guo%20et%20al%202016%20The%20threshold%20is%20selected%20per%20layer%20as%20the%20mean%20of%20the%20layer%20coefficients%20offset%20by%20a%20constant%20multiple%20of%20the%20standard%20deviation%20of%20the%20coefficients%20where%20the%20multiple%20is%20piecewise%20constant%20starting%20at%2000%20and%20ending%20at%2040%20We%20choose%20the%20pruning%20probability%20as%20a%20piecewise%20constant%20starting%20at%2010%20and%20decaying%20to%20103%20We%20train%20for%2030000%20steps%20using%20the%20ADAM%20optimizer"
        },
        {
            "id": "We_2016_a",
            "entry": "We quantize all the weights using a 4 bit codebook (Han et al., 2016) per layer initialized using k-means. A single cluster in each weight is given to be exactly zero and contains the pruned weights. The remaining clusters centers are learned using the ADAM optimizer over 1000 steps.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20quantize%20all%20the%20weights%20using%20a%204%20bit%20codebook%20Han%20et%20al%202016%20per%20layer%20initialized%20using%20kmeans%20A%20single%20cluster%20in%20each%20weight%20is%20given%20to%20be%20exactly%20zero%20and%20contains%20the%20pruned%20weights%20The%20remaining%20clusters%20centers%20are%20learned%20using%20the%20ADAM%20optimizer%20over%201000%20steps"
        }
    ]
}
