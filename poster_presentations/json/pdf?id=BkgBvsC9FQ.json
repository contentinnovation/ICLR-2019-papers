{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "DIALOGWAE: MULTIMODAL RESPONSE GENERATION WITH CONDITIONAL WASSERSTEIN AUTO-ENCODER",
        "author": "Xiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, Sunghun Kim, 1Hong Kong University of Science and Technology, 2New York Universidy, 3Clova AI Research, NAVER, 4CIFAR Azrieli Global Scholar 1guxiaodong,@126.com, hunkim@cse.ust.hk 2kyunghyun.cho@nyu.edu, 3jungwoo.ha@navercorp.com",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkgBvsC9FQ"
        },
        "abstract": "Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., unimodal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two popular datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses."
    },
    "keywords": [
        {
            "term": "auto encoder",
            "url": "https://en.wikipedia.org/wiki/auto_encoder"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "BLEU",
            "url": "https://en.wikipedia.org/wiki/BLEU"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        }
    ],
    "abbreviations": {
        "VAEs": "Variational autoencoders",
        "WAE": "Wasserstein autoencoder",
        "CVAE": "Conditional variational autoencoders",
        "RL": "Reinforcement Learning",
        "VAE": "variational autoencoder",
        "VHCR": "variational hierarchical conversation RNN",
        "ELBO": "evidence lower bound",
        "AAE": "adversarial auto-encoder",
        "PriNet": "prior network",
        "RecNet": "recognition network",
        "GRU": "gated recurrent units",
        "NSML": "NAVER Smart Machine Learning"
    },
    "highlights": [
        "Neural response generation has been a long interest of natural language research",
        "Conditional variational autoencoders generates the response conditioned on a latent variable - representing topics, tones and situations of the response - and approximate the posterior distribution over latent variables using a neural network",
        "Previous studies have shown that Variational autoencoders models tend to suffer from the posterior collapse problem, where the decoder learns to ignore the latent variable and degrades to a vanilla RNN (<a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a>; <a class=\"ref-link\" id=\"cPark_et+al_2018_a\" href=\"#rPark_et+al_2018_a\">Park et al, 2018</a>; <a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>)",
        "Due to the space limitation, we report the results of Conditional variational autoencoders-CO and DialogWAE-GMP, which are the representative models among the baselines and the proposed models",
        "We introduced a new approach, named DialogWAE, for dialogue modeling",
        "Experiments on two widely used datasets show that our model outperforms state-of-the-art Variational autoencoders models and generates more coherent, informative and diverse responses"
    ],
    "key_statements": [
        "Neural response generation has been a long interest of natural language research",
        "Conditional variational autoencoders generates the response conditioned on a latent variable - representing topics, tones and situations of the response - and approximate the posterior distribution over latent variables using a neural network",
        "Previous studies have shown that Variational autoencoders models tend to suffer from the posterior collapse problem, where the decoder learns to ignore the latent variable and degrades to a vanilla RNN (<a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a>; <a class=\"ref-link\" id=\"cPark_et+al_2018_a\" href=\"#rPark_et+al_2018_a\">Park et al, 2018</a>; <a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>)",
        "We propose DialogWAE, a novel variant of GAN for neural conversation modeling",
        "The results demonstrate that our model substantially outperforms the state-of-the-art methods in terms of BLEU, word embedding similarity, and distinct",
        "We introduce a continuous latent variable z that represents the high-level representation of the response",
        "Inspired by GAN and the adversarial auto-encoder (AAE) (<a class=\"ref-link\" id=\"cMakhzani_et+al_2015_a\" href=\"#rMakhzani_et+al_2015_a\">Makhzani et al, 2015</a>; <a class=\"ref-link\" id=\"cTolstikhin_et+al_2017_a\" href=\"#rTolstikhin_et+al_2017_a\">Tolstikhin et al, 2017</a>; <a class=\"ref-link\" id=\"cZhao_et+al_2018_a\" href=\"#rZhao_et+al_2018_a\">Zhao et al, 2018</a>), we model the distribution of z by training a GAN within the latent space",
        "To capture multiple modes in the probability distribution over the latent variable, we further propose to use a distribution that explicitly defines more than one mode",
        "Due to the space limitation, we report the results of Conditional variational autoencoders-CO and DialogWAE-GMP, which are the representative models among the baselines and the proposed models",
        "We further investigate the interpretability of Gaussian components in the prior network, that is, what each Gaussian model has captured before generation",
        "We introduced a new approach, named DialogWAE, for dialogue modeling",
        "Experiments on two widely used datasets show that our model outperforms state-of-the-art Variational autoencoders models and generates more coherent, informative and diverse responses"
    ],
    "summary": [
        "Neural response generation has been a long interest of natural language research.",
        "Adversarial training with the Gaussian mixture prior network enables DialogWAE to capture a richer latent space, yielding more coherent, informative and diverse responses.",
        "We highlight how the GAN architecture with a Gaussian mixture prior network facilitates the generation of more diverse and informative responses.",
        "<a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al (2018</a>) propose a collaborative CVAE model which samples the latent variable by transforming a Gaussian noise using neural networks and matches the prior and posterior distributions of the Gaussian noise with KL divergence.",
        "DialogWAE differs from exiting GAN conversation models in that it shapes the distribution of responses in a high level latent space rather than direct tokens and does not rely on RL where the gradient variances are large.",
        "The model infers the posterior distribution of the latent variable conditioned on the context c and the response x.",
        "The generator Q transforms the Gaussian noise into a sample of latent variable z through a feed-forward network.",
        "We train the model iteratively by alternating two phases \u2212 an AE phase during which the reconstruction loss of decoded responses is minimized, and a GAN phase which minimizes the Wasserstein distance between the prior and approximate posterior distributions over the latent variables.",
        "Baselines We compare the performance of DialogWAE with seven recently-proposed baselines for dialogue modeling: (i) HRED: a generalized sequence-to-sequence model with hierarchical RNN encoder (<a class=\"ref-link\" id=\"cSerban_et+al_2016_a\" href=\"#rSerban_et+al_2016_a\">Serban et al, 2016</a>), SeqGAN: a GAN based model for sequence generation (<a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al, 2017a</a>), CVAE: a conditional VAE model with KL-annealing (<a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\">Zhao et al, 2017</a></a>), CVAEBOW: a conditional VAE model with a BOW loss (<a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\">Zhao et al, 2017</a></a>), (v) CVAE-CO: a collaborative conditional VAE model (<a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\"><a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a></a>), VHRED: a hierarchical VAE model (<a class=\"ref-link\" id=\"cSerban_et+al_2017_a\" href=\"#rSerban_et+al_2017_a\">Serban et al, 2017</a>), and VHCR: a hierarchical VAE model with conversation modeling (<a class=\"ref-link\" id=\"cPark_et+al_2018_a\" href=\"#rPark_et+al_2018_a\">Park et al, 2018</a>).",
        "For each context in the test set, we show three samples of generated responses from each model.",
        "We pick a dialogue context \u201cI\u2019d like to invite you to dinner tonight, do you have time?\u201d which is used in (<a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\"><a class=\"ref-link\" id=\"cShen_et+al_2018_a\" href=\"#rShen_et+al_2018_a\">Shen et al, 2018</a></a>) for analysis and generate five responses for each Gaussian component.",
        "Eg.1: Yes, I\u2019d like to go with Eg.1: I\u2019m not sure.",
        "Different from existing VAE models which impose a simple prior distribution over the latent variables, DialogWAE samples the prior and posterior samples of latent variables by transforming contextdependent Gaussian noise using neural networks, and minimizes the Wasserstein distance between the prior and posterior distributions.",
        "Experiments on two widely used datasets show that our model outperforms state-of-the-art VAE models and generates more coherent, informative and diverse responses."
    ],
    "headline": "We propose DialogWAE, a conditional Wasserstein autoencoder specially designed for dialogue modeling",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowman%2C%20Samuel%20R.%20Angeli%2C%20Gabor%20Potts%2C%20Christopher%20Manning%2C%20Christopher%20D.%20A%20large%20annotated%20corpus%20for%20learning%20natural%20language%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20Samuel%20R.%20Angeli%2C%20Gabor%20Potts%2C%20Christopher%20Manning%2C%20Christopher%20D.%20A%20large%20annotated%20corpus%20for%20learning%20natural%20language%20inference%202015"
        },
        {
            "id": "Boxing_2014_a",
            "entry": "Boxing Chen and Colin Cherry. A systematic comparison of smoothing techniques for sentencelevel BLEU. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362\u2013367, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boxing%20Chen%20and%20Colin%20Cherry%20A%20systematic%20comparison%20of%20smoothing%20techniques%20for%20sentencelevel%20BLEU%20In%20Proceedings%20of%20the%20Ninth%20Workshop%20on%20Statistical%20Machine%20Translation%20pages%20362367%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boxing%20Chen%20and%20Colin%20Cherry%20A%20systematic%20comparison%20of%20smoothing%20techniques%20for%20sentencelevel%20BLEU%20In%20Proceedings%20of%20the%20Ninth%20Workshop%20on%20Statistical%20Machine%20Translation%20pages%20362367%202014"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Calar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN Encoder\u2013Decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, October 2014. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Calar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20Encoder%E2%80%93Decoder%20for%20statistical%20machine%20translation%202014-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Calar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20Encoder%E2%80%93Decoder%20for%20statistical%20machine%20translation%202014-10"
        },
        {
            "id": "Forgues_et+al_2014_a",
            "entry": "Gabriel Forgues, Joelle Pineau, Jean-Marie Larcheveque, and Real Tremblay. Bootstrapping dialog systems with word embeddings. In NIPS, modern machine learning and natural language processing workshop, volume 2, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forgues%2C%20Gabriel%20Pineau%2C%20Joelle%20Larcheveque%2C%20Jean-Marie%20Tremblay%2C%20Real%20Bootstrapping%20dialog%20systems%20with%20word%20embeddings%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Forgues%2C%20Gabriel%20Pineau%2C%20Joelle%20Larcheveque%2C%20Jean-Marie%20Tremblay%2C%20Real%20Bootstrapping%20dialog%20systems%20with%20word%20embeddings%202014"
        },
        {
            "id": "Godfrey_1997_a",
            "entry": "John J Godfrey and Edward Holliman. Switchboard-1 release 2. Linguistic Data Consortium, Philadelphia, 926:927, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Godfrey%2C%20John%20J.%20Holliman%2C%20Edward%20Switchboard-1%20release%202.%20Linguistic%20Data%20Consortium%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Godfrey%2C%20John%20J.%20Holliman%2C%20Edward%20Switchboard-1%20release%202.%20Linguistic%20Data%20Consortium%201997"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, and Eric P Xing. Nonparametric variational auto-encoders for hierarchical representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5094\u20135102, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Prasoon%20Hu%2C%20Zhiting%20Liang%2C%20Xiaodan%20Chenyu%20Wang%2C%20and%20Eric%20P%20Xing.%20Nonparametric%20variational%20auto-encoders%20for%20hierarchical%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Prasoon%20Hu%2C%20Zhiting%20Liang%2C%20Xiaodan%20Chenyu%20Wang%2C%20and%20Eric%20P%20Xing.%20Nonparametric%20variational%20auto-encoders%20for%20hierarchical%20representation%20learning%202017"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with a real-world case study. arXiv preprint arXiv:1810.09957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.09957"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "Kusner_2016_a",
            "entry": "Matt J Kusner and Jose Miguel Hernandez-Lobato. GANs for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04051"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.03055"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06547"
        },
        {
            "id": "Li_et+al_2017_b",
            "entry": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 986\u2013995, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yanran%20Su%2C%20Hui%20Shen%2C%20Xiaoyu%20Li%2C%20Wenjie%20DailyDialog%3A%20A%20manually%20labelled%20multi-turn%20dialogue%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yanran%20Su%2C%20Hui%20Shen%2C%20Xiaoyu%20Li%2C%20Wenjie%20DailyDialog%3A%20A%20manually%20labelled%20multi-turn%20dialogue%20dataset%202017"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08023"
        },
        {
            "id": "Makhzani_et+al_2015_a",
            "entry": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "Mitchell_2008_a",
            "entry": "Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings of ACL-08: HLT, pages 236\u2013244, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitchell%2C%20Jeff%20Lapata%2C%20Mirella%20Vector-based%20models%20of%20semantic%20composition.%20proceedings%20of%20ACL-%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mitchell%2C%20Jeff%20Lapata%2C%20Mirella%20Vector-based%20models%20of%20semantic%20composition.%20proceedings%20of%20ACL-%202008"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013 814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Papineni_et+al_2002_a",
            "entry": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20BLEU%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20BLEU%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "Park_et+al_2018_a",
            "entry": "Yookoon Park, Jaemin Cho, and Gunhee Kim. A hierarchical latent structure for variational conversation modeling. arXiv preprint arXiv:1804.03424, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03424"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pages 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Rus_2012_a",
            "entry": "Vasile Rus and Mihai Lintean. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157\u2013162. Association for Computational Linguistics, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rus%2C%20Vasile%20Lintean%2C%20Mihai%20A%20comparison%20of%20greedy%20and%20optimal%20assessment%20of%20natural%20language%20student%20input%20using%20word-to-word%20similarity%20metrics%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rus%2C%20Vasile%20Lintean%2C%20Mihai%20A%20comparison%20of%20greedy%20and%20optimal%20assessment%20of%20natural%20language%20student%20input%20using%20word-to-word%20similarity%20metrics%202012"
        },
        {
            "id": "Sato_et+al_2017_a",
            "entry": "Shoetsu Sato, Naoki Yoshinaga, Masashi Toyoda, and Masaru Kitsuregawa. Modeling situations in neural chat bots. In Proceedings of ACL 2017, Student Research Workshop, pages 120\u2013127, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sato%2C%20Shoetsu%20Yoshinaga%2C%20Naoki%20Toyoda%2C%20Masashi%20Kitsuregawa%2C%20Masaru%20Modeling%20situations%20in%20neural%20chat%20bots%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sato%2C%20Shoetsu%20Yoshinaga%2C%20Naoki%20Toyoda%2C%20Masashi%20Kitsuregawa%2C%20Masaru%20Modeling%20situations%20in%20neural%20chat%20bots%202017"
        },
        {
            "id": "Serban_et+al_2016_a",
            "entry": "Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In AAAI, volume 16, pages 3776\u20133784, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20C.%20Building%20end-to-end%20dialogue%20systems%20using%20generative%20hierarchical%20neural%20network%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20C.%20Building%20end-to-end%20dialogue%20systems%20using%20generative%20hierarchical%20neural%20network%20models%202016"
        },
        {
            "id": "Serban_et+al_2017_a",
            "entry": "Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, pages 3295\u20133301, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Lowe%2C%20Ryan%20Charlin%2C%20Laurent%20and%20Yoshua%20Bengio.%20A%20hierarchical%20latent%20variable%20encoder-decoder%20model%20for%20generating%20dialogues%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Lowe%2C%20Ryan%20Charlin%2C%20Laurent%20and%20Yoshua%20Bengio.%20A%20hierarchical%20latent%20variable%20encoder-decoder%20model%20for%20generating%20dialogues%202017"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Information Processing Systems, pages 6833\u20136844, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Xiaoyu Shen, Hui Su, Shuzi Niu, and Vera Demberg. Improving variational encoder-decoders in dialogue generation. arXiv preprint arXiv:1802.02032, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02032"
        },
        {
            "id": "Sung_et+al_2017_a",
            "entry": "Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim, Leonard Lausen, Youngkwan Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al. Nsml: A machine learning platform that enables you to focus on your models. arXiv preprint arXiv:1712.05902, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05902"
        },
        {
            "id": "Ilya_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilya%20Sutskever%2C%20Oriol%20Vinyals%20Le%2C%20Quoc%20VV%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilya%20Sutskever%2C%20Oriol%20Vinyals%20Le%2C%20Quoc%20VV%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tieleman_0000_a",
            "entry": "T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning. Technical report, Technical Report. Available online: https://zh.coursera.org/learn/neuralnetworks/lecture/YQHki/rmsprop-divide-thegradient-by-a-running-average-of-its-recent-magnitude (accessed on 21 April 2017).",
            "url": "https://zh.coursera.org/learn/neuralnetworks/lecture/YQHki/rmsprop-divide-thegradient-by-a-running-average-of-its-recent-magnitude"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. arXiv preprint arXiv:1711.01558, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01558"
        },
        {
            "id": "Xing_et+al_2017_a",
            "entry": "Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. Topic aware neural response generation. In AAAI, volume 17, pages 3351\u20133357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%2C%20Chen%20Wu%2C%20Wei%20Wu%2C%20Yu%20Liu%2C%20Jie%20and%20Wei-Ying%20Ma.%20Topic%20aware%20neural%20response%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%2C%20Chen%20Wu%2C%20Wei%20Wu%2C%20Yu%20Liu%2C%20Jie%20and%20Wei-Ying%20Ma.%20Topic%20aware%20neural%20response%20generation%202017"
        },
        {
            "id": "Zhen_2017_a",
            "entry": "Zhen Xu, Bingquan Liu, Baoxun Wang, SUN Chengjie, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural response generation via GAN with an approximate embedding layer. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 617\u2013626, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhen%20Xu%20Bingquan%20Liu%20Baoxun%20Wang%20SUN%20Chengjie%20Xiaolong%20Wang%20Zhuoran%20Wang%20and%20Chao%20Qi%20Neural%20response%20generation%20via%20GAN%20with%20an%20approximate%20embedding%20layer%20In%20Proceedings%20of%20the%202017%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pages%20617626%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhen%20Xu%20Bingquan%20Liu%20Baoxun%20Wang%20SUN%20Chengjie%20Xiaolong%20Wang%20Zhuoran%20Wang%20and%20Chao%20Qi%20Neural%20response%20generation%20via%20GAN%20with%20an%20approximate%20embedding%20layer%20In%20Proceedings%20of%20the%202017%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pages%20617626%202017"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10960"
        },
        {
            "id": "Zhao_et+al_2018_a",
            "entry": "Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. Adversarially regularized autoencoders. In Proceedings of the Thirty-fifth International Conference on Machine Learning, ICML 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Junbo%20Kim%2C%20Yoon%20Zhang%2C%20Kelly%20Rush%2C%20Alexander%20M.%20Adversarially%20regularized%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Junbo%20Kim%2C%20Yoon%20Zhang%2C%20Kelly%20Rush%2C%20Alexander%20M.%20Adversarially%20regularized%20autoencoders%202018"
        }
    ]
}
