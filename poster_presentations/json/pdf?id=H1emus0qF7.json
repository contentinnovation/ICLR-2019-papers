{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NEAR-OPTIMAL REPRESENTATION LEARNING FOR HIERARCHICAL REINFORCEMENT LEARNING",
        "author": "Ofir Nachum, Shixiang Gu, Honglak Lee & Sergey Levine, Google Brain {ofirnachum,shanegu,honglak,slevine}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1emus0qF7"
        },
        "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation \u2013 the mapping of observation space to goal space \u2013 is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods.12"
    },
    "keywords": [
        {
            "term": "high level",
            "url": "https://en.wikipedia.org/wiki/high_level"
        },
        {
            "term": "abstract space",
            "url": "https://en.wikipedia.org/wiki/abstract_space"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning"
    },
    "highlights": [
        "Hierarchical reinforcement learning has long held the promise of extending the successes of existing reinforcement learning (RL) methods (<a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\"><a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\">Gu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\">Schulman et al, 2015</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>) to more complex, difficult, and temporally extended tasks (<a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\"><a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\">Parr & Russell, 1998</a></a>; <a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\"><a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999</a></a>; <a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\"><a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\">Barto & Mahadevan, 2003</a></a>)",
        "Goal-conditioned hierarchical designs, in which higher-level policies communicate goals to lower-levels and lower-level policies are rewarded for reaching states which are close to these desired goals, have emerged as an effective paradigm for hierarchical reinforcement learning (<a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al (2018</a>); <a class=\"ref-link\" id=\"cLevy_et+al_2017_a\" href=\"#rLevy_et+al_2017_a\">Levy et al (2017</a>); <a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al (2017</a>), inspired by earlier work <a class=\"ref-link\" id=\"cDayan_1993_a\" href=\"#rDayan_1993_a\">Dayan & Hinton (1993</a>); <a class=\"ref-link\" id=\"cSchmidhuber_1993_a\" href=\"#rSchmidhuber_1993_a\">Schmidhuber & Wahnsiedler (1993</a>))",
        "We introduce a notion of sub-optimality with respect to the form of \u03a8: lLoewt -\u03c0leh\u2217vivbioersth. eLoept t\u03c0imh\u2217iearl higher-level policy acting be the corresponding full on G and using \u03a8 as the mapping from G to hierarchical policy on M",
        "(3) Our approach is provably good for use in hierarchical reinforcement learning, and our theoretical results may justify some of the good performance observed by others using mutual information estimators for representation learning",
        "While previous successful applications of goal-conditioned hierarchical designs have either learned representations naively end-to-end (<a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al, 2017</a>), or not learned them at all (<a class=\"ref-link\" id=\"cLevy_et+al_2017_a\" href=\"#rLevy_et+al_2017_a\">Levy et al, 2017</a>; <a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a>), we take a principled approach to representation learning in hierarchical reinforcement learning, translating a bound on sub-optimality to a practical learning objective",
        "We have presented a principled approach to representation learning in hierarchical reinforcement learning"
    ],
    "key_statements": [
        "Hierarchical reinforcement learning has long held the promise of extending the successes of existing reinforcement learning (RL) methods (<a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\"><a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\">Gu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\">Schulman et al, 2015</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>) to more complex, difficult, and temporally extended tasks (<a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\"><a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\">Parr & Russell, 1998</a></a>; <a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\"><a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999</a></a>; <a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\"><a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\">Barto & Mahadevan, 2003</a></a>)",
        "Goal-conditioned hierarchical designs, in which higher-level policies communicate goals to lower-levels and lower-level policies are rewarded for reaching states which are close to these desired goals, have emerged as an effective paradigm for hierarchical reinforcement learning (<a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al (2018</a>); <a class=\"ref-link\" id=\"cLevy_et+al_2017_a\" href=\"#rLevy_et+al_2017_a\">Levy et al (2017</a>); <a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al (2017</a>), inspired by earlier work <a class=\"ref-link\" id=\"cDayan_1993_a\" href=\"#rDayan_1993_a\">Dayan & Hinton (1993</a>); <a class=\"ref-link\" id=\"cSchmidhuber_1993_a\" href=\"#rSchmidhuber_1993_a\">Schmidhuber & Wahnsiedler (1993</a>))",
        "We introduce a notion of sub-optimality with respect to the form of \u03a8: lLoewt -\u03c0leh\u2217vivbioersth. eLoept t\u03c0imh\u2217iearl higher-level policy acting be the corresponding full on G and using \u03a8 as the mapping from G to hierarchical policy on M",
        "(3) Our approach is provably good for use in hierarchical reinforcement learning, and our theoretical results may justify some of the good performance observed by others using mutual information estimators for representation learning",
        "While previous successful applications of goal-conditioned hierarchical designs have either learned representations naively end-to-end (<a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al, 2017</a>), or not learned them at all (<a class=\"ref-link\" id=\"cLevy_et+al_2017_a\" href=\"#rLevy_et+al_2017_a\">Levy et al, 2017</a>; <a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a>), we take a principled approach to representation learning in hierarchical reinforcement learning, translating a bound on sub-optimality to a practical learning objective",
        "We have presented a principled approach to representation learning in hierarchical reinforcement learning"
    ],
    "summary": [
        "Hierarchical reinforcement learning has long held the promise of extending the successes of existing reinforcement learning (RL) methods (<a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\"><a class=\"ref-link\" id=\"cGu_et+al_2017_a\" href=\"#rGu_et+al_2017_a\">Gu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\">Schulman et al, 2015</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>) to more complex, difficult, and temporally extended tasks (<a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\"><a class=\"ref-link\" id=\"cParr_1998_a\" href=\"#rParr_1998_a\">Parr & Russell, 1998</a></a>; <a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\"><a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999</a></a>; <a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\"><a class=\"ref-link\" id=\"cBarto_2003_a\" href=\"#rBarto_2003_a\">Barto & Mahadevan, 2003</a></a>).",
        "This is the first result showing that hierarchical goal-setting policies with learned representations and temporal abstraction can achieve bounded sub-optimality against the optimal policy.",
        "Our main result will show that if one defines \u03a8 as a slight modification of the traditional objective given in Equation 1, one may translate sub-optimality of \u03a8 to a practical representation learning objective for f .",
        "Our main result is Claim 4, which connects the sub-optimality of \u03a8 to both goalconditioned policy objectives and representation learning.",
        "We make use of an auxiliary inverse goal model \u03c6(s, a), which aims to predict which goal g will cause \u03a8 to yield an action a = \u03a8(s, g) that induces a state distribution P (s |s, a) similar to P (s |s, a).3 We have the following theorem, which bounds the sub-optimality in terms of total variation divergences between P (s |s, a) and P (s |s, a): Theorem 1.",
        "We have the following claim, which connects the sub-optimality of \u03a8 to both representation learning and the form of the low-level objective.",
        "As in the previous claim, we have a strong statement, saying that if the low-level objective is defined as in Equation 9, minimizing the sub-optimality may be done by optimizing a representation learning objective based on Equation 8.",
        "(3) Our approach is provably good for use in hierarchical RL, and our theoretical results may justify some of the good performance observed by others using mutual information estimators for representation learning.",
        "In goal-conditioned hierarchical designs, the representation is learned on states, it is a form of action abstraction.",
        "While previous successful applications of goal-conditioned hierarchical designs have either learned representations naively end-to-end (<a class=\"ref-link\" id=\"cVezhnevets_et+al_2017_a\" href=\"#rVezhnevets_et+al_2017_a\">Vezhnevets et al, 2017</a>), or not learned them at all (<a class=\"ref-link\" id=\"cLevy_et+al_2017_a\" href=\"#rLevy_et+al_2017_a\">Levy et al, 2017</a>; <a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a>), we take a principled approach to representation learning in hierarchical RL, translating a bound on sub-optimality to a practical learning objective.",
        "Similar information theoretic measures have been used previously for exploration in RL (<a class=\"ref-link\" id=\"cStill_2012_a\" href=\"#rStill_2012_a\">Still & Precup, 2012</a>), to our knowledge, no prior work has connected these mutual information estimators to representation learning in hierarchical RL, and ours is the first to formulate theoretical guarantees on sub-optimality of the resulting representations in such a framework.",
        "Our resulting representation learning objective is practical and achieves impressive results on a suite of high-dimensional, continuous-control tasks"
    ],
    "headline": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning",
    "reference_links": [
        {
            "id": "David_2017_a",
            "entry": "David Abel, D Ellis Hershkowitz, and Michael L Littman. Near optimal behavior via approximate state abstraction. arXiv preprint arXiv:1701.04113, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04113"
        },
        {
            "id": "Achiam_et+al_2017_a",
            "entry": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv preprint arXiv:1705.10528, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10528"
        },
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pp. 1726\u2013 1734, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Barto_2003_a",
            "entry": "Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(4):341\u2013379, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barto%2C%20Andrew%20G.%20Mahadevan%2C%20Sridhar%20Recent%20advances%20in%20hierarchical%20reinforcement%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barto%2C%20Andrew%20G.%20Mahadevan%2C%20Sridhar%20Recent%20advances%20in%20hierarchical%20reinforcement%20learning%202003"
        },
        {
            "id": "Bertsekas_1989_a",
            "entry": "Dimitri P Bertsekas and David Alfred Castanon. Adaptive aggregation methods for infinite horizon dynamic programming. IEEE transactions on Automatic Control, 34(6):589\u2013598, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Castanon%2C%20David%20Alfred%20Adaptive%20aggregation%20methods%20for%20infinite%20horizon%20dynamic%20programming%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Castanon%2C%20David%20Alfred%20Adaptive%20aggregation%20methods%20for%20infinite%20horizon%20dynamic%20programming%201989"
        },
        {
            "id": "Brunskill_2014_a",
            "entry": "Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In International Conference on Machine Learning, pp. 316\u2013324, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brunskill%2C%20Emma%20Li%2C%20Lihong%20Pac-inspired%20option%20discovery%20in%20lifelong%20reinforcement%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brunskill%2C%20Emma%20Li%2C%20Lihong%20Pac-inspired%20option%20discovery%20in%20lifelong%20reinforcement%20learning%202014"
        },
        {
            "id": "Dayan_1993_a",
            "entry": "Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pp. 271\u2013278, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201993"
        },
        {
            "id": "Dean_1997_a",
            "entry": "Thomas Dean and Robert Givan. Model minimization in markov decision processes. In AAAI/IAAI, pp. 106\u2013111, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Thomas%20Givan%2C%20Robert%20Model%20minimization%20in%20markov%20decision%20processes%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Thomas%20Givan%2C%20Robert%20Model%20minimization%20in%20markov%20decision%20processes%201997"
        },
        {
            "id": "Dietterich_2000_a",
            "entry": "Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of Artificial Intelligence Research, 13:227\u2013303, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dietterich%2C%20Thomas%20G.%20Hierarchical%20reinforcement%20learning%20with%20the%20maxq%20value%20function%20decomposition%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dietterich%2C%20Thomas%20G.%20Hierarchical%20reinforcement%20learning%20with%20the%20maxq%20value%20function%20decomposition%202000"
        },
        {
            "id": "Gu_et+al_2017_a",
            "entry": "Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 3389\u20133396. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017"
        },
        {
            "id": "Higgins_et+al_2016_a",
            "entry": "Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed, and Alexander Lerchner. Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05579"
        },
        {
            "id": "Hjelm_et+al_2018_a",
            "entry": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.06670"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: Mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04062"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Levy_et+al_2017_a",
            "entry": "Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00948"
        },
        {
            "id": "Li_et+al_2006_a",
            "entry": "Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for mdps. In ISAIM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Walsh%2C%20Thomas%20J.%20Littman%2C%20Michael%20L.%20Towards%20a%20unified%20theory%20of%20state%20abstraction%20for%20mdps%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Walsh%2C%20Thomas%20J.%20Littman%2C%20Michael%20L.%20Towards%20a%20unified%20theory%20of%20state%20abstraction%20for%20mdps%202006"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nachum%2C%20Ofir%20Gu%2C%20Shane%20Lee%2C%20Honglak%20Levine%2C%20Sergey%20Data-efficient%20hierarchical%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nachum%2C%20Ofir%20Gu%2C%20Shane%20Lee%2C%20Honglak%20Levine%2C%20Sergey%20Data-efficient%20hierarchical%20reinforcement%20learning%202018"
        },
        {
            "id": "Parr_1998_a",
            "entry": "Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances in neural information processing systems, pp. 1043\u20131049, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parr%2C%20Ronald%20Russell%2C%20Stuart%20J.%20Reinforcement%20learning%20with%20hierarchies%20of%20machines%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parr%2C%20Ronald%20Russell%2C%20Stuart%20J.%20Reinforcement%20learning%20with%20hierarchies%20of%20machines%201998"
        },
        {
            "id": "Rogers_et+al_1991_a",
            "entry": "David F Rogers, Robert D Plante, Richard T Wong, and James R Evans. Aggregation and disaggregation techniques and methodology in optimization. Operations Research, 39(4):553\u2013582, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rogers%2C%20David%20F.%20Plante%2C%20Robert%20D.%20Wong%2C%20Richard%20T.%20Evans%2C%20James%20R.%20Aggregation%20and%20disaggregation%20techniques%20and%20methodology%20in%20optimization%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rogers%2C%20David%20F.%20Plante%2C%20Robert%20D.%20Wong%2C%20Richard%20T.%20Evans%2C%20James%20R.%20Aggregation%20and%20disaggregation%20techniques%20and%20methodology%20in%20optimization%201991"
        },
        {
            "id": "Schmidhuber_1993_a",
            "entry": "Jurgen Schmidhuber and Reiner Wahnsiedler. Planning simple trajectories using neural subgoal generators. In From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, volume 2, pp. 196. MIT Press, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Wahnsiedler%2C%20Reiner%20Planning%20simple%20trajectories%20using%20neural%20subgoal%20generators.%20In%20From%20Animals%20to%20Animats%202%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Wahnsiedler%2C%20Reiner%20Planning%20simple%20trajectories%20using%20neural%20subgoal%20generators.%20In%20From%20Animals%20to%20Animats%202%201993"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Sorg_2009_a",
            "entry": "Jonathan Sorg and Satinder Singh. Transfer via soft homomorphisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 741\u2013748. International Foundation for Autonomous Agents and Multiagent Systems, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20Transfer%20via%20soft%20homomorphisms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20Transfer%20via%20soft%20homomorphisms%202009"
        },
        {
            "id": "Still_2012_a",
            "entry": "Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139\u2013148, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Still%2C%20Susanne%20Precup%2C%20Doina%20An%20information-theoretic%20approach%20to%20curiosity-driven%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Still%2C%20Susanne%20Precup%2C%20Doina%20An%20information-theoretic%20approach%20to%20curiosity-driven%20reinforcement%20learning%202012"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013 211, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "Thomas_2012_a",
            "entry": "Philip S Thomas and Andrew G Barto. Motor primitive discovery. In Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, pp. 1\u20138. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%2C%20Philip%20S.%20Barto%2C%20Andrew%20G.%20Motor%20primitive%20discovery.%20In%20Development%20and%20Learning%20and%20Epigenetic%20Robotics%202012"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Roy_2006_a",
            "entry": "Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation. Mathematics of Operations Research, 31(2):234\u2013244, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roy%2C%20Benjamin%20Van%20Performance%20loss%20bounds%20for%20approximate%20value%20iteration%20with%20state%20aggregation%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roy%2C%20Benjamin%20Van%20Performance%20loss%20bounds%20for%20approximate%20value%20iteration%20with%20state%20aggregation%202006"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01161"
        },
        {
            "id": "Watter_et+al_2015_a",
            "entry": "Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pp. 2746\u20132754, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015"
        },
        {
            "id": "Of_1978_a",
            "entry": "Ward Whitt. Approximations of dynamic programs, i. Mathematics of Operations Research, 3(3): 231\u2013243, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ward%20Whitt%20Approximations%20of%20dynamic%20programs%20i%20Mathematics%20of%20Operations%20Research%2033%20231243%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ward%20Whitt%20Approximations%20of%20dynamic%20programs%20i%20Mathematics%20of%20Operations%20Research%2033%20231243%201978"
        },
        {
            "id": "Hier_2017_a",
            "entry": "Let \u03c0hier be the corresponding hierarchical policy. We will bound the quantity V \u03c0\u2217 (s0)\u2212V \u03c0hier (s0), which will bound V \u03c0\u2217 (s0) \u2212 V \u03c0h\u2217ier (s0). We follow logic similar to Achiam et al. (2017) and begin by bounding the total variation divergence between the \u03b3-discounted state visitation frequencies of the two policies.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%CF%80hier%20be%2C%20Let%20the%20corresponding%20hierarchical%20policy.%20We%20will%20bound%20the%20quantity%20V%20%CF%80%E2%88%97%20%28s0%29%E2%88%92V%20%CF%80hier%20%28s0%29%2C%20which%20will%20bound%20V%20%CF%80%E2%88%97%20%28s0%29%20%E2%88%92%20V%20%CF%80h%E2%88%97ier%20%28s0%29.%20We%20follow%20logic%20similar%20to%20Achiam%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%CF%80hier%20be%2C%20Let%20the%20corresponding%20hierarchical%20policy.%20We%20will%20bound%20the%20quantity%20V%20%CF%80%E2%88%97%20%28s0%29%E2%88%92V%20%CF%80hier%20%28s0%29%2C%20which%20will%20bound%20V%20%CF%80%E2%88%97%20%28s0%29%20%E2%88%92%20V%20%CF%80h%E2%88%97ier%20%28s0%29.%20We%20follow%20logic%20similar%20to%20Achiam%202017"
        },
        {
            "id": "The_2018_a",
            "entry": "The environments for Ant Maze, Ant Push, and Ant Fall are as described in Nachum et al. (2018). During training, target (x, y) locations are selected randomly from all possible points in the environment (in Ant Fall, the target includes a z coordinate as well). Final results are evaluated on a single difficult target point, equal to that used in Nachum et al. (2018).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20environments%20for%20Ant%20Maze%20Ant%20Push%20and%20Ant%20Fall%20are%20as%20described%20in%20Nachum%20et%20al%202018%20During%20training%20target%20x%20y%20locations%20are%20selected%20randomly%20from%20all%20possible%20points%20in%20the%20environment%20in%20Ant%20Fall%20the%20target%20includes%20a%20z%20coordinate%20as%20well%20Final%20results%20are%20evaluated%20on%20a%20single%20difficult%20target%20point%20equal%20to%20that%20used%20in%20Nachum%20et%20al%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20environments%20for%20Ant%20Maze%20Ant%20Push%20and%20Ant%20Fall%20are%20as%20described%20in%20Nachum%20et%20al%202018%20During%20training%20target%20x%20y%20locations%20are%20selected%20randomly%20from%20all%20possible%20points%20in%20the%20environment%20in%20Ant%20Fall%20the%20target%20includes%20a%20z%20coordinate%20as%20well%20Final%20results%20are%20evaluated%20on%20a%20single%20difficult%20target%20point%20equal%20to%20that%20used%20in%20Nachum%20et%20al%202018"
        }
    ]
}
