{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ARM: AUGMENT-REINFORCE-MERGE GRADIENT FOR STOCHASTIC BINARY NETWORKS",
        "author": "Mingzhang Yin Department of Statistics and Data Sciences The University of Texas at Austin Austin, TX 78712 mzyin@utexas.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lg0jAcYm"
        },
        "abstract": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric \u201cself-control\u201d baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available."
    },
    "keywords": [
        {
            "term": "random variable",
            "url": "https://en.wikipedia.org/wiki/random_variable"
        },
        {
            "term": "straight through",
            "url": "https://en.wikipedia.org/wiki/Straight_Through"
        },
        {
            "term": "feedforward neural network",
            "url": "https://en.wikipedia.org/wiki/feedforward_neural_network"
        },
        {
            "term": "belief network",
            "url": "https://en.wikipedia.org/wiki/belief_network"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        }
    ],
    "abbreviations": {
        "ELBO": "evidence lower bound",
        "SGD": "stochastic gradient descent",
        "VAE": "variational auto-encoder",
        "ST": "straight through"
    },
    "highlights": [
        "To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>; <a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\"><a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a></a>), unbiased but high-variance ones (<a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), or unbiased REINFORCE combined with computationally expensive baselines, whose parameters are estimated by minimizing the sample variance of the estimator with stochastic gradient descent (Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2018_a\" href=\"#rGrathwohl_et+al_2018_a\">Grathwohl et al, 2018</a>)",
        "We find that the values of the ratio shown above are almost exactly matched by the values of SNRg = |g|/sg under the ARM estimator, shown in the bottom right subplot of Figure 5",
        "To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a>; <a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a>), unbiased but high-variance ones (<a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), or unbiased REINFORCE combined with computationally expensive baselines, whose parameters are estimated by minimizing the sample variance of the estimator with stochastic gradient descent (Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2018_a\" href=\"#rGrathwohl_et+al_2018_a\">Grathwohl et al, 2018</a>)",
        "The performance of ARM on MNIST-threshold is significantly better, suggesting ARM is more robust, better resists overfitting, and has better generalization ability. On both MNIST-static and OMNIGLOT, with the \u201cNonlinear\u201d network architecture, both REBAR and RELAX exhibit severe overfitting, which could be caused by their training procedure, which updates the parameters of the baseline function by minimizing the sample variance of the gradient estimator using stochastic gradient descent",
        "On the MNIST-static dataset with the \u201cNonlinear\u201d network, the left subplot of Figure 3 shows that both REBAR and RELAX exhibit lower variance than ARM does for their single-Monte-Carlo-sample based gradient estimates; the corresponding trace plots of the validation negative evidence lower bound, shown in Figure 2(a), suggest they both severely overfit the training data as the learning progresses; our hypothesis for this phenomenon is that REBAR and RELAX may favor suboptimal solutions that are associated with lower gradient variance; in other words, they may have difficulty in converging to local optimal solutions that are associated with high gradient variance",
        "To train a discrete latent variable model with one or multiple stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator to provide unbiased and low-variance gradient estimates of the parameters of Bernoulli distributions"
    ],
    "key_statements": [
        "To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>; <a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\"><a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a></a>), unbiased but high-variance ones (<a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), or unbiased REINFORCE combined with computationally expensive baselines, whose parameters are estimated by minimizing the sample variance of the estimator with stochastic gradient descent (Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2018_a\" href=\"#rGrathwohl_et+al_2018_a\">Grathwohl et al, 2018</a>)",
        "We show by rewriting the expectation with respect to Bernoulli random variables as one with respect to augmented exponential random variables, and expressing the gradient as an expectation via REINFORCE, one can derive the ARM estimator in the augmented space with the assistance of appropriate reparameterization",
        "We show below that the ARM estimator has smaller worst-case variance than REINFORCE does",
        "We find that the values of the ratio shown above are almost exactly matched by the values of SNRg = |g|/sg under the ARM estimator, shown in the bottom right subplot of Figure 5",
        "To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a>; <a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a>), unbiased but high-variance ones (<a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), or unbiased REINFORCE combined with computationally expensive baselines, whose parameters are estimated by minimizing the sample variance of the estimator with stochastic gradient descent (Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2018_a\" href=\"#rGrathwohl_et+al_2018_a\">Grathwohl et al, 2018</a>)",
        "We summarize the test negative evidence lower bound in Table 4 of the Appendix, and provide related trace plots of the training and validation negative evidence lower bound on MNIST-static in Figure 2, and these on MNIST-threshold and OMNIGLOT in Figures 6 and 7 of the Appendix, respectively",
        "The performance of ARM on MNIST-threshold is significantly better, suggesting ARM is more robust, better resists overfitting, and has better generalization ability. On both MNIST-static and OMNIGLOT, with the \u201cNonlinear\u201d network architecture, both REBAR and RELAX exhibit severe overfitting, which could be caused by their training procedure, which updates the parameters of the baseline function by minimizing the sample variance of the gradient estimator using stochastic gradient descent",
        "On the MNIST-static dataset with the \u201cNonlinear\u201d network, the left subplot of Figure 3 shows that both REBAR and RELAX exhibit lower variance than ARM does for their single-Monte-Carlo-sample based gradient estimates; the corresponding trace plots of the validation negative evidence lower bound, shown in Figure 2(a), suggest they both severely overfit the training data as the learning progresses; our hypothesis for this phenomenon is that REBAR and RELAX may favor suboptimal solutions that are associated with lower gradient variance; in other words, they may have difficulty in converging to local optimal solutions that are associated with high gradient variance",
        "To train a discrete latent variable model with one or multiple stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator to provide unbiased and low-variance gradient estimates of the parameters of Bernoulli distributions"
    ],
    "summary": [
        "To optimize a variational auto-encoder (VAE) for a discrete latent variable model, existing solutions often rely on biased but low-variance stochastic gradient estimators (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>; <a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\"><a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a></a>), unbiased but high-variance ones (<a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), or unbiased REINFORCE combined with computationally expensive baselines, whose parameters are estimated by minimizing the sample variance of the estimator with SGD (Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2018_a\" href=\"#rGrathwohl_et+al_2018_a\">Grathwohl et al, 2018</a>).",
        "On both MNIST-static and OMNIGLOT, with the \u201cNonlinear\u201d network architecture, both REBAR and RELAX exhibit severe overfitting, which could be caused by their training procedure, which updates the parameters of the baseline function by minimizing the sample variance of the gradient estimator using SGD.",
        "On the MNIST-static dataset with the \u201cNonlinear\u201d network, the left subplot of Figure 3 shows that both REBAR and RELAX exhibit lower variance than ARM does for their single-Monte-Carlo-sample based gradient estimates; the corresponding trace plots of the validation negative ELBOs, shown in Figure 2(a), suggest they both severely overfit the training data as the learning progresses; our hypothesis for this phenomenon is that REBAR and RELAX may favor suboptimal solutions that are associated with lower gradient variance; in other words, they may have difficulty in converging to local optimal solutions that are associated with high gradient variance.",
        "As shown in Table 3, optimizing a stochastic binary network with the ARM estimator, which is unbiased and computationally efficient, achieves the lowest test negative log-likelihood, outperforming previously proposed biased stochastic gradient estimators on structured stochastic networks, including DARN (<a class=\"ref-link\" id=\"cGregor_et+al_2013_a\" href=\"#rGregor_et+al_2013_a\">Gregor et al, 2013</a>), straight through (ST) (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>), slope-annealed ST (<a class=\"ref-link\" id=\"cChung_et+al_2016_a\" href=\"#rChung_et+al_2016_a\">Chung et al, 2016</a>), and ST Gumbel-softmax (<a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\"><a class=\"ref-link\" id=\"cJang_et+al_2017_a\" href=\"#rJang_et+al_2017_a\">Jang et al, 2017</a></a>), and unbiased ones, including score-function (SF) and MuProp (<a class=\"ref-link\" id=\"cGu_et+al_2016_a\" href=\"#rGu_et+al_2016_a\">Gu et al, 2016</a>).",
        "To train a discrete latent variable model with one or multiple stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator to provide unbiased and low-variance gradient estimates of the parameters of Bernoulli distributions.",
        "Applying the ARM gradient leads to not only fast convergence, but low test negative log-likelihoods, on both auto-encoding variational inference and maximum likelihood estimation for stochastic binary feedforward neural networks.",
        "Some natural extensions of the proposed ARM estimator include generalizing it to multivariate categorical latent variables, combining it with a baseline or local-expectation based variance reduction method, and applying it to reinforcement learning whose action space is discrete."
    ],
    "headline": "To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge estimator that is unbiased, exhibits low variance, and has low computational complexity",
    "reference_links": [
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Bishop_1995_a",
            "entry": "Christopher M Bishop. Neural Networks for Pattern Recognition. Oxford university press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20Christopher%20M.%20Neural%20Networks%20for%20Pattern%20Recognition%201995"
        },
        {
            "id": "Blei_et+al_2017_a",
            "entry": "David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "Casella_1996_a",
            "entry": "George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83 (1):81\u201394, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Casella%2C%20George%20Robert%2C%20Christian%20P.%20Rao-blackwellisation%20of%20sampling%20schemes%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Casella%2C%20George%20Robert%2C%20Christian%20P.%20Rao-blackwellisation%20of%20sampling%20schemes%201996"
        },
        {
            "id": "Chung_et+al_2016_a",
            "entry": "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.01704"
        },
        {
            "id": "Fu_2006_a",
            "entry": "Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13: 575\u2013616, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Michael%20C.%20Gradient%20estimation.%20Handbooks%20in%20operations%20research%20and%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Michael%20C.%20Gradient%20estimation.%20Handbooks%20in%20operations%20research%20and%202006"
        },
        {
            "id": "Glynn_1990_a",
            "entry": "Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75\u201384, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glynn%2C%20Peter%20W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glynn%2C%20Peter%20W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990"
        },
        {
            "id": "Grathwohl_et+al_2018_a",
            "entry": "Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the Void: Optimizing control variates for black-box gradient estimation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grathwohl%2C%20Will%20Choi%2C%20Dami%20Wu%2C%20Yuhuai%20Roeder%2C%20Geoff%20Backpropagation%20through%20the%20Void%3A%20Optimizing%20control%20variates%20for%20black-box%20gradient%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grathwohl%2C%20Will%20Choi%2C%20Dami%20Wu%2C%20Yuhuai%20Roeder%2C%20Geoff%20Backpropagation%20through%20the%20Void%3A%20Optimizing%20control%20variates%20for%20black-box%20gradient%20estimation%202018"
        },
        {
            "id": "Gregor_et+al_2013_a",
            "entry": "Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1310.8499"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. MuProp: Unbiased backpropagation for stochastic neural networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Levine%2C%20Sergey%20Sutskever%2C%20Ilya%20Mnih%2C%20Andriy%20MuProp%3A%20Unbiased%20backpropagation%20for%20stochastic%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Levine%2C%20Sergey%20Sutskever%2C%20Ilya%20Mnih%2C%20Andriy%20MuProp%3A%20Unbiased%20backpropagation%20for%20stochastic%20neural%20networks%202016"
        },
        {
            "id": "Hinton_et+al_1995_a",
            "entry": "Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The \u201cwake-sleep\u201d algorithm for unsupervised neural networks. Science, 268(5214):1158\u20131161, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Dayan%2C%20Peter%20Frey%2C%20Brendan%20J.%20Neal%2C%20Radford%20M.%20The%20%E2%80%9Cwake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Dayan%2C%20Peter%20Frey%2C%20Brendan%20J.%20Neal%2C%20Radford%20M.%20The%20%E2%80%9Cwake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995"
        },
        {
            "id": "Jang_et+al_2017_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017"
        },
        {
            "id": "Jordan_et+al_1999_a",
            "entry": "Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kucukelbir_et+al_2017_a",
            "entry": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14):1\u201345, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kucukelbir%2C%20Alp%20Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Gelman%2C%20Andrew%20Automatic%20differentiation%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kucukelbir%2C%20Alp%20Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Gelman%2C%20Andrew%20Automatic%20differentiation%20variational%20inference%202017"
        },
        {
            "id": "Larochelle_2011_a",
            "entry": "Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29\u201337, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011"
        },
        {
            "id": "Maas_2013_a",
            "entry": "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20and%20Andrew%20Y%20Ng.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models.%20In%20ICML%202013"
        },
        {
            "id": "Maddison_2017_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20and%20Yee%20Whye%20Teh.%20The%20Concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20and%20Yee%20Whye%20Teh.%20The%20Concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017"
        },
        {
            "id": "Mnih_2014_a",
            "entry": "Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In ICML, pp. 1791\u20131799, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Andriy%20Gregor%2C%20Karol%20Neural%20variational%20inference%20and%20learning%20in%20belief%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Andriy%20Gregor%2C%20Karol%20Neural%20variational%20inference%20and%20learning%20in%20belief%20networks%202014"
        },
        {
            "id": "Mnih_2016_a",
            "entry": "Andriy Mnih and Danilo J Rezende. Variational inference for Monte Carlo objectives. arXiv preprint arXiv:1602.06725, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.06725"
        },
        {
            "id": "Naesseth_et+al_2017_a",
            "entry": "Christian Naesseth, Francisco Ruiz, Scott Linderman, and David Blei. Reparameterization gradients through acceptance-rejection sampling algorithms. In AISTATS, pp. 489\u2013498, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naesseth%2C%20Christian%20Ruiz%2C%20Francisco%20Linderman%2C%20Scott%20Blei%2C%20David%20Reparameterization%20gradients%20through%20acceptance-rejection%20sampling%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naesseth%2C%20Christian%20Ruiz%2C%20Francisco%20Linderman%2C%20Scott%20Blei%2C%20David%20Reparameterization%20gradients%20through%20acceptance-rejection%20sampling%20algorithms%202017"
        },
        {
            "id": "Neal_1992_a",
            "entry": "Radford M Neal. Connectionist learning of belief networks. Artificial Intelligence, 56(1):71\u2013113, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Connectionist%20learning%20of%20belief%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20Radford%20M.%20Connectionist%20learning%20of%20belief%20networks%201992"
        },
        {
            "id": "Owen_2013_a",
            "entry": "Art B. Owen. Monte Carlo Theory, Methods and Examples, chapter 8 Variance Reduction. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owen%2C%20Art%20B.%20Monte%20Carlo%20Theory%2C%20Methods%20and%20Examples%202013"
        },
        {
            "id": "Paisley_et+al_2012_a",
            "entry": "John Paisley, David M Blei, and Michael I Jordan. Variational Bayesian inference with stochastic search. In ICML, pp. 1363\u20131370, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paisley%2C%20John%20Blei%2C%20David%20M.%20Jordan%2C%20Michael%20I.%20Variational%20Bayesian%20inference%20with%20stochastic%20search%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paisley%2C%20John%20Blei%2C%20David%20M.%20Jordan%2C%20Michael%20I.%20Variational%20Bayesian%20inference%20with%20stochastic%20search%202012"
        },
        {
            "id": "Raiko_et+al_2014_a",
            "entry": "Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2989"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In AISTATS, pp. 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, pp. 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Ross_2006_a",
            "entry": "Sheldon M. Ross. Introduction to Probability Models. Academic Press, 10th edition, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Sheldon%20M.%20Introduction%20to%20Probability%20Models%202006"
        },
        {
            "id": "Ruiz_et+al_2016_a",
            "entry": "Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei. The generalized reparameterization gradient. In NIPS, pp. 460\u2013468, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruiz%2C%20Francisco%20J.R.%20Titsias%2C%20Michalis%20K.%20Blei%2C%20David%20M.%20The%20generalized%20reparameterization%20gradient%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruiz%2C%20Francisco%20J.R.%20Titsias%2C%20Michalis%20K.%20Blei%2C%20David%20M.%20The%20generalized%20reparameterization%20gradient%202016"
        },
        {
            "id": "Salakhutdinov_2008_a",
            "entry": "Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In ICML, pp. 872\u2013879, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20Ruslan%20Murray%2C%20Iain%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20Ruslan%20Murray%2C%20Iain%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008"
        },
        {
            "id": "Saul_et+al_1996_a",
            "entry": "Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61\u201376, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saul%2C%20Lawrence%20K.%20Jaakkola%2C%20Tommi%20Jordan%2C%20Michael%20I.%20Mean%20field%20theory%20for%20sigmoid%20belief%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saul%2C%20Lawrence%20K.%20Jaakkola%2C%20Tommi%20Jordan%2C%20Michael%20I.%20Mean%20field%20theory%20for%20sigmoid%20belief%20networks%201996"
        },
        {
            "id": "Tang_2013_a",
            "entry": "Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS, pp. 530\u2013538, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Yichuan%20Salakhutdinov%2C%20Ruslan%20R.%20Learning%20stochastic%20feedforward%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Yichuan%20Salakhutdinov%2C%20Ruslan%20R.%20Learning%20stochastic%20feedforward%20neural%20networks%202013"
        },
        {
            "id": "Martin_1987_a",
            "entry": "Martin A Tanner and Wing Hung Wong. The calculation of posterior distributions by data augmentation. J. Amer. Statist. Assoc., 82(398):528\u2013540, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20A%20Tanner%20and%20Wing%20Hung%20Wong.%20The%20calculation%20of%20posterior%20distributions%20by%20data%20augmentation%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20A%20Tanner%20and%20Wing%20Hung%20Wong.%20The%20calculation%20of%20posterior%20distributions%20by%20data%20augmentation%201987"
        },
        {
            "id": "Titsias_2015_a",
            "entry": "Michalis K Titsias and Miguel Lazaro-Gredilla. Local expectation gradients for black box variational inference. In NIPS, pp. 2638\u20132646. MIT Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20Michalis%20K.%20Lazaro-Gredilla%2C%20Miguel%20Local%20expectation%20gradients%20for%20black%20box%20variational%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20Michalis%20K.%20Lazaro-Gredilla%2C%20Miguel%20Local%20expectation%20gradients%20for%20black%20box%20variational%20inference%202015"
        },
        {
            "id": "Tucker_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS, pp. 2624\u20132633, 2017. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pp. 6306\u20136315, 2017. David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and Graphical Statistics, 10(1):1\u201350, 2001. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tucker%2C%20George%20Mnih%2C%20Andriy%20Maddison%2C%20Chris%20J.%20Lawson%2C%20John%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tucker%2C%20George%20Mnih%2C%20Andriy%20Maddison%2C%20Chris%20J.%20Lawson%2C%20John%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Springer_2018_a",
            "entry": "Springer, 1992. Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In ICML, pp. 5660\u20135669, 2018. Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling. arXiv preprint arXiv:1209.3442v1, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1209.3442v1"
        }
    ]
}
