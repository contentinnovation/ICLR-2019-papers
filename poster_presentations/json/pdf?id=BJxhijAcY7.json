{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Syntax Error: ExtGState 'pgf@ca1.0' is unknown",
        "author": "SIGN SGD WITH M AJORITY VOTE IS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJxhijAcY7"
        },
        "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning\u2014SIGN SGD."
    },
    "keywords": [
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/signal-to-noise_ratio"
        },
        {
            "term": "syntax error",
            "url": "https://en.wikipedia.org/wiki/syntax_error"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "SNR": "signal-to-noise ratio"
    },
    "highlights": [
        "The most powerful supercomputer in the world is currently a cluster of over 27,000 GPUs at Oak",
        "The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults",
        "We explore a simple algorithm for robust, communication-efficient learning\u2014SIGN stochastic gradient descent",
        "Under natural conditions verified by experiment, we prove that SIGN stochastic gradient descent converges in the large and mini-batch settings, establishing convergence for a parameter regime of A DAM as a byproduct",
        "We show how theoretically the behaviour of SIGN stochastic gradient descent changes as gradients move from high to low signal-to-noise ratio",
        "We have shown that SIGN stochastic gradient descent with majority vote aggregation is robust and communication efficient, whilst its per-iteration convergence rate is competitive with stochastic gradient descent for training large-scale convolutional neural nets on image datasets"
    ],
    "key_statements": [
        "The most powerful supercomputer in the world is currently a cluster of over 27,000 GPUs at Oak",
        "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines",
        "The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults",
        "We explore a simple algorithm for robust, communication-efficient learning\u2014SIGN stochastic gradient descent",
        "Under natural conditions verified by experiment, we prove that SIGN stochastic gradient descent converges in the large and mini-batch settings, establishing convergence for a parameter regime of A DAM as a byproduct",
        "Distributed algorithms designed for such large-scale systems typically involve both computation and communication: worker nodes compute intermediate results locally, before sharing them with their peers",
        "Whilst D3 is immmediate, we provide the first convergence guarantees for SIGN stochastic gradient descent in the mini-batch setting, providing theoretical grounds for",
        "We show how theoretically the behaviour of SIGN stochastic gradient descent changes as gradients move from high to low signal-to-noise ratio",
        "Combining Assumption 4 with an old tail bound of Gauss (1823) yields Lemma 1, which will be crucial for guaranteeing mini-batch convergence of SIGN stochastic gradient descent",
        "Per epoch, distributing by majority vote is able to attain a similar speedup to distributed stochastic gradient descent",
        "We have shown that SIGN stochastic gradient descent with majority vote aggregation is robust and communication efficient, whilst its per-iteration convergence rate is competitive with stochastic gradient descent for training large-scale convolutional neural nets on image datasets",
        "An important takeaway from our theory is that mini-batch SIGN stochastic gradient descent should converge if the gradient noise is Gaussian",
        "In the experiments in Figure 7, 1-bit max QSGD led to compressed vectors that were 5\u00d7 more compressed than SIGN stochastic gradient descent\u2014in our experimental setting this additional improvement turned out to be small relative to the time cost of backpropagation"
    ],
    "summary": [
        "The most powerful supercomputer in the world is currently a cluster of over 27,000 GPUs at Oak.",
        "Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote.",
        "Aggregating sign gradients by majority vote means that no individual worker has too much power.",
        "We refer to this algorithm as SIGN SGD with majority vote.",
        "Combining Assumption 4 with an old tail bound of Gauss (1823) yields Lemma 1, which will be crucial for guaranteeing mini-batch convergence of SIGN SGD.",
        "The bound characterises how the failure probability of a sign bit depends on the signal-to-noise ratio (SNR) of that gradient component.",
        "Theorem 2 (Non-convex convergence rate of majority vote with adversarial workers).",
        "Top: increasing the number of workers participating in the majority vote shows a similar convergence speedup to distributed SGD.",
        "Our adversarial workers take the sign of their stochastic gradient calculation, but send the negation to the Syntax Error: ExtGState 'pgf@ca1.0' is unknown parameter server.",
        "We have shown that SIGN SGD with majority vote aggregation is robust and communication efficient, whilst its per-iteration convergence rate is competitive with SGD for training large-scale convolutional neural nets on image datasets.",
        "An important takeaway from our theory is that mini-batch SIGN SGD should converge if the gradient noise is Gaussian.",
        "This means that the performance of SIGN SGD may be improved by increasing the per-worker mini-batch size, since this should make the noise \u2018more Gaussian\u2019 according to the",
        "1-bit L2 QSGD takes a gradient vector g and snaps ith coordinate gi to sign with probability",
        "In the experiments in Figure 7, 1-bit max QSGD led to compressed vectors that were 5\u00d7 more compressed than SIGN SGD\u2014in our experimental setting this additional improvement turned out to be small relative to the time cost of backpropagation.",
        "Theorem 1 (Non-convex convergence rate of mini-batch SIGN SGD).",
        "For a gradient component with true value g, let random variable Z \u2208 [0, M ] denote the number of correct sign bits received by the parameter server.",
        "For a given gradient component, again let random variable Z \u2208 [0, M ] denote the number of correct sign bits received by the parameter server.",
        "We have completed the first part of the proof by showing the key statement that for the ith gradient component with signal to noise ratio Si := |g\u03c3ii| , the failure probability of the majority vote is bounded by"
    ],
    "headline": "We explore a simple algorithm for robust, communication-efficient learning\u2014SIGN stochastic gradient descent",
    "reference_links": []
}
