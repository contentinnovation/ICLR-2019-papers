{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SELF-TUNING NETWORKS: BILEVEL OPTIMIZATION OF HYPERPARAMETERS USING STRUCTURED BEST-RESPONSE FUNCTIONS",
        "author": "Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, Roger Grosse {mmackay,pvicol,lorraine,duvenaud,rgrosse}@cs.toronto.edu University of Toronto Vector Institute",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1eEG20qKQ"
        },
        "abstract": "Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer. We justify this approximation by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. We fit this model using a gradient-based hyperparameter optimization algorithm which alternates between approximating the best-response around the current hyperparameters and optimizing the hyperparameters using the approximate best-response function. Unlike other gradient-based approaches, we do not require differentiating the training loss with respect to the hyperparameters, allowing us to tune discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Because the hyperparameters are adapted online, our approach discovers hyperparameter schedules that can outperform fixed hyperparameter values. Empirically, our approach outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, SelfTuning Networks (STNs)."
    },
    "keywords": [
        {
            "term": "hyperparameter optimization",
            "url": "https://en.wikipedia.org/wiki/hyperparameter_optimization"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "arg min",
            "url": "https://en.wikipedia.org/wiki/arg_min"
        },
        {
            "term": "bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/bayesian_optimization"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "response function",
            "url": "https://en.wikipedia.org/wiki/response_function"
        }
    ],
    "abbreviations": {
        "STNs": "Self-Tuning Networks",
        "ST-CNNs": "self-tuning CNNs",
        "PBT": "Population Based Training"
    },
    "highlights": [
        "Regularization hyperparameters such as weight decay, data augmentation, and dropout (<a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>) are crucial to the generalization of neural networks, but are difficult to tune",
        "Popular approaches to hyperparameter optimization include grid search, random search (<a class=\"ref-link\" id=\"cBergstra_2012_a\" href=\"#rBergstra_2012_a\">Bergstra & Bengio, 2012</a>), and Bayesian optimization (<a class=\"ref-link\" id=\"cSnoek_et+al_2012_a\" href=\"#rSnoek_et+al_2012_a\">Snoek et al, 2012</a>). These approaches work well with low-dimensional hyperparameter spaces and ample computational resources; they pose hyperparameter optimization as a black-box optimization problem, ignoring structure which can be exploited for faster convergence, and require many training runs",
        "We show the schedules the Self-Tuning Networks finds for each hyperparameter in Figures 4b and 4c; we observe that they are nontrivial, with some forms of dropout used to a greater extent at the start of training, some used throughout training, and some that are increased over the course of training.\n4.3",
        "We introduced Self-Tuning Networks (STNs), which efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting their hidden units",
        "We showed that Self-Tuning Networks discover hyperparameter schedules that can outperform fixed hyperparameters",
        "We believe Self-Tuning Networks offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks"
    ],
    "key_statements": [
        "Regularization hyperparameters such as weight decay, data augmentation, and dropout (<a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>) are crucial to the generalization of neural networks, but are difficult to tune",
        "Popular approaches to hyperparameter optimization include grid search, random search (<a class=\"ref-link\" id=\"cBergstra_2012_a\" href=\"#rBergstra_2012_a\">Bergstra & Bengio, 2012</a>), and Bayesian optimization (<a class=\"ref-link\" id=\"cSnoek_et+al_2012_a\" href=\"#rSnoek_et+al_2012_a\">Snoek et al, 2012</a>). These approaches work well with low-dimensional hyperparameter spaces and ample computational resources; they pose hyperparameter optimization as a black-box optimization problem, ignoring structure which can be exploited for faster convergence, and require many training runs",
        "To rule out the possibility that the improved performance is due to stochasticity introduced by sampling hyperparameters during Self-Tuning Networks training, we trained a standard LSTM while perturbing its dropout rate around the best value found by grid search",
        "We show the schedules the Self-Tuning Networks finds for each hyperparameter in Figures 4b and 4c; we observe that they are nontrivial, with some forms of dropout used to a greater extent at the start of training, some used throughout training, and some that are increased over the course of training.\n4.3",
        "We evaluated self-tuning CNNs on the CIFAR-10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton, 2009</a>) dataset, where it is easy to overfit with high-capacity networks",
        "We introduced Self-Tuning Networks (STNs), which efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting their hidden units",
        "We showed that Self-Tuning Networks discover hyperparameter schedules that can outperform fixed hyperparameters",
        "We believe Self-Tuning Networks offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks"
    ],
    "summary": [
        "Regularization hyperparameters such as weight decay, data augmentation, and dropout (<a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>) are crucial to the generalization of neural networks, but are difficult to tune.",
        "Let LT and LV be functions mapping parameters and hyperparameters to training and validation losses, respectively.",
        "The STN training algorithm does not require differentiating the training loss with respect to the hyperparameters, unlike other gradient-based approaches (<a class=\"ref-link\" id=\"cMaclaurin_et+al_2015_a\" href=\"#rMaclaurin_et+al_2015_a\">Maclaurin et al, 2015</a>; <a class=\"ref-link\" id=\"cLarsen_et+al_1996_a\" href=\"#rLarsen_et+al_1996_a\">Larsen et al, 1996</a>), allowing us to tune discrete hyperparameters, such as the number of holes to cut out of an image (<a class=\"ref-link\" id=\"cDevries_2017_a\" href=\"#rDevries_2017_a\">DeVries & Taylor, 2017</a>), data-augmentation hyperparameters, and discrete-noise dropout parameters.",
        "A more principled approach to solving Problem 4 is to use the best-response function (<a class=\"ref-link\" id=\"cGibbons_1992_a\" href=\"#rGibbons_1992_a\">Gibbons, 1992</a>).",
        "Gradient-based hyperparameter optimization methods can often be interpreted as approximating either the best-response w\u2217 or its Jacobian \u2202w\u2217/\u2202\u03bb, as detailed in Section 5.",
        "The sigmoidal gating architecture of the preceding section can be further simplified if one only needs to approximate the best-response function for a small range of hyperparameter values.",
        "Initialize: Best-response approximation parameters \u03c6, hyperparameters \u03bb, learning rates {\u03b1i}i3=1 while not converged do for t = 1, .",
        "Due to the joint optimization of the hypernetwork weights and hyperparameters, STNs do not use a single, fixed hyperparameter during training.",
        "The schedule discovered by an ST-LSTM for output dropout, shown in Figure 3, outperforms the best, fixed output dropout rate (0.68) found by a fine-grained grid search, achieving 82.58 vs 85.83 validation perplexity.",
        "To rule out the possibility that the improved performance is due to stochasticity introduced by sampling hyperparameters during STN training, we trained a standard LSTM while perturbing its dropout rate around the best value found by grid search.",
        "To further demonstrate the importance of the hyperparameter schedule, we trained a standard LSTM from scratch using the final dropout value found by the STN (0.78), and found that it did not perform as well as when following the schedule.",
        "We compared STNs to grid search, random search, and Bayesian optimization.6 Figure 4a shows the best validation perplexity achieved by each method over time.",
        "Because the architecture is not optimized during training, they require a large hypernetwork, unlike ours which locally approximates the best-response.",
        "Both approaches struggle with certain hyperparameters, since they differentiate gradient descent or the training loss with respect to the hyperparameters.",
        "STNs replace the population of networks by a single best-response approximation and use gradients to tune hyperparameters during a single training run.",
        "We introduced Self-Tuning Networks (STNs), which efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting their hidden units.",
        "We believe STNs offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks"
    ],
    "headline": "We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer",
    "reference_links": [
        {
            "id": "Allgower_2012_a",
            "entry": "Eugene L Allgower and Kurt Georg. Numerical Continuation Methods: An Introduction, volume 13. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allgower%2C%20Eugene%20L.%20Georg%2C%20Kurt%20Numerical%20Continuation%20Methods%3A%20An%20Introduction%2C%20volume%2013%202012"
        },
        {
            "id": "Bard_2013_a",
            "entry": "Jonathan F Bard. Practical Bilevel Optimization: Algorithms and Applications, volume 30. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bard%2C%20Jonathan%20F.%20Practical%20Bilevel%20Optimization%3A%20Algorithms%20and%20Applications%2C%20volume%2030%202013"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In International Conference on Machine Learning, pp. 41\u201348. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20Jerome%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20Jerome%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Bergstra_2012_a",
            "entry": "James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20Bengio%2C%20Yoshua%20Random%20search%20for%20hyper-parameter%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20Bengio%2C%20Yoshua%20Random%20search%20for%20hyper-parameter%20optimization%202012"
        },
        {
            "id": "Bergstra_et+al_2011_a",
            "entry": "James S. Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyperparameter optimization. In Advances in Neural Information Processing Systems, pp. 2546\u20132554. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20S.%20Bardenet%2C%20Remi%20Bengio%2C%20Yoshua%20Kegl%2C%20Balazs%20Algorithms%20for%20hyperparameter%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20S.%20Bardenet%2C%20Remi%20Bengio%2C%20Yoshua%20Kegl%2C%20Balazs%20Algorithms%20for%20hyperparameter%20optimization%202011"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05424"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. SMASH: One-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05344"
        },
        {
            "id": "Calatroni_et+al_2015_a",
            "entry": "Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane Schonlieb, and Tuomo Valkonen. Bilevel approaches for learning of variational imaging models. arXiv preprint arXiv:1505.02120, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.02120"
        },
        {
            "id": "Chapelle_et+al_2002_a",
            "entry": "Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing multiple parameters for Support Vector Machines. Machine Learning, 46(1-3):131\u2013159, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Vapnik%2C%20Vladimir%20Bousquet%2C%20Olivier%20Mukherjee%2C%20Sayan%20Choosing%20multiple%20parameters%20for%20Support%20Vector%20Machines%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Vapnik%2C%20Vladimir%20Bousquet%2C%20Olivier%20Mukherjee%2C%20Sayan%20Choosing%20multiple%20parameters%20for%20Support%20Vector%20Machines%202002"
        },
        {
            "id": "Colson_et+al_2005_a",
            "entry": "Beno\u0131t Colson, Patrice Marcotte, and Gilles Savard. A trust-region method for nonlinear bilevel programming: Algorithm and computational experience. Computational Optimization and Applications, 30(3):211\u2013227, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Colson%2C%20Beno%C4%B1t%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20A%20trust-region%20method%20for%20nonlinear%20bilevel%20programming%3A%20Algorithm%20and%20computational%20experience%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Colson%2C%20Beno%C4%B1t%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20A%20trust-region%20method%20for%20nonlinear%20bilevel%20programming%3A%20Algorithm%20and%20computational%20experience%202005"
        },
        {
            "id": "Colson_et+al_2007_a",
            "entry": "Beno\u0131t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of Operations Research, 153(1):235\u2013256, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Colson%2C%20Beno%C4%B1t%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Colson%2C%20Beno%C4%B1t%20Marcotte%2C%20Patrice%20Savard%2C%20Gilles%20An%20overview%20of%20bilevel%20optimization%202007"
        },
        {
            "id": "Denil_et+al_2013_a",
            "entry": "Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pp. 2148\u20132156, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Domke_2012_a",
            "entry": "Justin Domke. Generic methods for optimization-based modeling. In Proceedings of Machine Learning Research, pp. 318\u2013326, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Domke%2C%20Justin%20Generic%20methods%20for%20optimization-based%20modeling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Domke%2C%20Justin%20Generic%20methods%20for%20optimization-based%20modeling%202012"
        },
        {
            "id": "Duchi_2007_a",
            "entry": "John Duchi. Properties of the trace and matrix derivatives, 2007. URL https://web.stanford.edu/\u0303jduchi/projects/matrix_prop.pdf.",
            "url": "https://web.stanford.edu/\u0303jduchi/projects/matrix_prop.pdf"
        },
        {
            "id": "Fiacco_1990_a",
            "entry": "Anthony V Fiacco and Yo Ishizuka. Sensitivity and stability analysis for nonlinear programming. Annals of Operations Research, 27(1):215\u2013235, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiacco%2C%20Anthony%20V.%20Ishizuka%2C%20Yo%20Sensitivity%20and%20stability%20analysis%20for%20nonlinear%20programming%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fiacco%2C%20Anthony%20V.%20Ishizuka%2C%20Yo%20Sensitivity%20and%20stability%20analysis%20for%20nonlinear%20programming%201990"
        },
        {
            "id": "Foo_et+al_2008_a",
            "entry": "Chuan-sheng Foo, Chuong B Do, and Andrew Y Ng. Efficient multiple hyperparameter learning for log-linear models. In Advances in Neural Information Processing Systems, pp. 377\u2013384, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foo%2C%20Chuan-sheng%20Do%2C%20Chuong%20B.%20Ng%2C%20Andrew%20Y.%20Efficient%20multiple%20hyperparameter%20learning%20for%20log-linear%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foo%2C%20Chuan-sheng%20Do%2C%20Chuong%20B.%20Ng%2C%20Andrew%20Y.%20Efficient%20multiple%20hyperparameter%20learning%20for%20log-linear%20models%202008"
        },
        {
            "id": "Franceschi_et+al_2018_a",
            "entry": "Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04910"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1027\u20131035, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Gibbons_1992_a",
            "entry": "Robert Gibbons. A Primer in Game Theory. Harvester Wheatsheaf, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gibbons%2C%20Robert%20A%20Primer%20in%20Game%20Theory%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gibbons%2C%20Robert%20A%20Primer%20in%20Game%20Theory%201992"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "Hansen_et+al_1992_a",
            "entry": "Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194\u20131217, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20Pierre%20Jaumard%2C%20Brigitte%20Savard%2C%20Gilles%20New%20branch-and-bound%20rules%20for%20linear%20bilevel%20programming%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20Pierre%20Jaumard%2C%20Brigitte%20Savard%2C%20Gilles%20New%20branch-and-bound%20rules%20for%20linear%20bilevel%20programming%201992"
        },
        {
            "id": "Hastie_et+al_2001_a",
            "entry": "Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer, New York, NY, USA, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Friedman%2C%20Jerome%20The%20Elements%20of%20Statistical%20Learning.%20Springer%20Series%20in%20Statistics%202001"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20Beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20Beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population-based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "Jamieson_2016_a",
            "entry": "Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In International Conference on Artificial Intelligence and Statistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20Kevin%20Talwalkar%2C%20Ameet%20Non-stochastic%20best%20arm%20identification%20and%20hyperparameter%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20Kevin%20Talwalkar%2C%20Ameet%20Non-stochastic%20best%20arm%20identification%20and%20hyperparameter%20optimization%202016"
        },
        {
            "id": "Khan_et+al_2018_a",
            "entry": "Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. arXiv preprint arXiv:1806.04854, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04854"
        },
        {
            "id": "Klein_et+al_2017_a",
            "entry": "Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with Bayesian neural networks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klein%2C%20Aaron%20Falkner%2C%20Stefan%20Springenberg%2C%20Jost%20Tobias%20Hutter%2C%20Frank%20Learning%20curve%20prediction%20with%20Bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klein%2C%20Aaron%20Falkner%2C%20Stefan%20Springenberg%2C%20Jost%20Tobias%20Hutter%2C%20Frank%20Learning%20curve%20prediction%20with%20Bayesian%20neural%20networks%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. In Technical Report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images.%20In%20Technical%20Report%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kunisch_2013_a",
            "entry": "Karl Kunisch and Thomas Pock. A bilevel optimization approach for parameter learning in variational models. SIAM Journal on Imaging Sciences, 6(2):938\u2013983, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kunisch%2C%20Karl%20Pock%2C%20Thomas%20A%20bilevel%20optimization%20approach%20for%20parameter%20learning%20in%20variational%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kunisch%2C%20Karl%20Pock%2C%20Thomas%20A%20bilevel%20optimization%20approach%20for%20parameter%20learning%20in%20variational%20models%202013"
        },
        {
            "id": "Larsen_et+al_1996_a",
            "entry": "Jan Larsen, Lars Kai Hansen, Claus Svarer, and M Ohlsson. Design and regularization of neural networks: The optimal use of a validation set. In IEEE Workshop on Neural Networks for Signal Processing, pp. 62\u201371. IEEE, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsen%2C%20Jan%20Hansen%2C%20Lars%20Kai%20Svarer%2C%20Claus%20Ohlsson%2C%20M.%20Design%20and%20regularization%20of%20neural%20networks%3A%20The%20optimal%20use%20of%20a%20validation%20set%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsen%2C%20Jan%20Hansen%2C%20Lars%20Kai%20Svarer%2C%20Claus%20Ohlsson%2C%20M.%20Design%20and%20regularization%20of%20neural%20networks%3A%20The%20optimal%20use%20of%20a%20validation%20set%201996"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. Journal of Machine Learning Research, 18(1):6765\u20136816, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lisha%20Jamieson%2C%20Kevin%20DeSalvo%2C%20Giulia%20Rostamizadeh%2C%20Afshin%20Hyperband%3A%20Bandit-based%20configuration%20evaluation%20for%20hyperparameter%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lisha%20Jamieson%2C%20Kevin%20DeSalvo%2C%20Giulia%20Rostamizadeh%2C%20Afshin%20Hyperband%3A%20Bandit-based%20configuration%20evaluation%20for%20hyperparameter%20optimization%202017"
        },
        {
            "id": "Lorraine_2018_a",
            "entry": "Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernetworks. arXiv preprint arXiv:1802.09419, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09419"
        },
        {
            "id": "Luketina_et+al_2016_a",
            "entry": "Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In International Conference on Machine Learning, pp. 2952\u20132960, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luketina%2C%20Jelena%20Berglund%2C%20Mathias%20Greff%2C%20Klaus%20Raiko%2C%20Tapani%20Scalable%20gradient-based%20tuning%20of%20continuous%20regularization%20hyperparameters%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luketina%2C%20Jelena%20Berglund%2C%20Mathias%20Greff%2C%20Klaus%20Raiko%2C%20Tapani%20Scalable%20gradient-based%20tuning%20of%20continuous%20regularization%20hyperparameters%202016"
        },
        {
            "id": "Maclaurin_et+al_2015_a",
            "entry": "Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pp. 2113\u2013 2122, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015"
        },
        {
            "id": "Marcus_et+al_1993_a",
            "entry": "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313\u2013330, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20English%3A%20The%20Penn%20Treebank%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20English%3A%20The%20Penn%20Treebank%201993"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems, pp. 1825\u20131835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017"
        },
        {
            "id": "Metz_et+al_2016_a",
            "entry": "Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "Nesterov_2013_a",
            "entry": "Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Optimization%3A%20A%20Basic%20Course%2C%20volume%2087%202013"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In Advances in Neural Information Processing Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20Advances%20in%20Neural%20Information%20Processing%20Workshop%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20Advances%20in%20Neural%20Information%20Processing%20Workshop%202017"
        },
        {
            "id": "Pedregosa_2016_a",
            "entry": "Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Conference on Machine Learning, pp. 737\u2013746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20Fabian%20Hyperparameter%20optimization%20with%20approximate%20gradient%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20Fabian%20Hyperparameter%20optimization%20with%20approximate%20gradient%202016"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992"
        },
        {
            "id": "Seeger_2007_a",
            "entry": "Matthias Seeger. Cross-validation optimization for large scale hierarchical classification kernel methods. In Advances in Neural Information Processing Systems, pp. 1233\u20131240, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20Cross-validation%20optimization%20for%20large%20scale%20hierarchical%20classification%20kernel%20methods%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20Cross-validation%20optimization%20for%20large%20scale%20hierarchical%20classification%20kernel%20methods%202007"
        },
        {
            "id": "Sinha_et+al_2013_a",
            "entry": "Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. Efficient evolutionary algorithm for singleobjective bilevel optimization. arXiv preprint arXiv:1303.3901, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1303.3901"
        },
        {
            "id": "Snoek_et+al_2012_a",
            "entry": "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pp. 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "Snoek_et+al_2015_a",
            "entry": "Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, M Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning, pp. 2171\u20132180, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Rippel%2C%20Oren%20Swersky%2C%20Kevin%20Kiros%2C%20Ryan%20Scalable%20Bayesian%20optimization%20using%20deep%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Rippel%2C%20Oren%20Swersky%2C%20Kevin%20Kiros%2C%20Ryan%20Scalable%20Bayesian%20optimization%20using%20deep%20neural%20networks%202015"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Staines_2012_a",
            "entry": "Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.4507"
        },
        {
            "id": "Swersky_et+al_2014_a",
            "entry": "Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.3896"
        },
        {
            "id": "Vicente_et+al_1994_a",
            "entry": "Luis Vicente, Gilles Savard, and Joaquim Judice. Descent approaches for quadratic bilevel programming. Journal of Optimization Theory and Applications, 81(2):379\u2013399, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vicente%2C%20Luis%20Savard%2C%20Gilles%20Judice%2C%20Joaquim%20Descent%20approaches%20for%20quadratic%20bilevel%20programming%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vicente%2C%20Luis%20Savard%2C%20Gilles%20Judice%2C%20Joaquim%20Descent%20approaches%20for%20quadratic%20bilevel%20programming%201994"
        },
        {
            "id": "Stackelberg_2010_a",
            "entry": "Heinrich Von Stackelberg. Market Structure and Equilibrium. Springer Science & Business Media, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stackelberg%2C%20Heinrich%20Von%20Market%20Structure%20and%20Equilibrium%202010"
        },
        {
            "id": "Wan_et+al_2013_a",
            "entry": "Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using Dropconnect. In International Conference on Machine Learning, pp. 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20Dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20Dropconnect%202013"
        },
        {
            "id": "Wen_et+al_2018_a",
            "entry": "Published as a conference paper at ICLR 2019 Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudoindependent weight perturbations on mini-batches. International Conference on Learning Representations, 2018. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229\u2013256, 1992. Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In International Conference on Computer Vision, pp. 1476\u20131483, 2015. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        }
    ]
}
