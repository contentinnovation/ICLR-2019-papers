{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "REPRESENTATION DEGENERATION PROBLEM IN TRAINING NATURAL LANGUAGE GENERATION MOD-",
        "author": "Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, & Tie-Yan Liu, 1Department of Computer Science, University of Toronto jungao@cs.toronto.edu 2Vector Institute, Canada 3Key Laboratory of Machine Perception, MOE, School of EECS, Peking University di he@pku.edu.cn, wanglw@cis.pku.edu.cn 4Microsoft Research {xuta,taoqin,tyliu}@microsoft.com 5Center for Data Science, Peking University, Beijing Institute of Big Data Research",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkEYojRqtm"
        },
        "journal": "WikiText",
        "abstract": "We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the representation degeneration problem. We observe that when training a model for natural language generation tasks through likelihood maximization with the weight tying trick, especially with big training datasets, most of the learnt word embeddings tend to degenerate and be distributed into a narrow cone, which largely limits the representation power of word embeddings. We analyze the conditions and causes of this problem and propose a novel regularization method to address it. Experiments on language modeling and machine translation show that our method can largely mitigate the representation degeneration problem and achieve better performance than baseline algorithms."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "language modeling",
            "url": "https://en.wikipedia.org/wiki/language_modeling"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "singular value",
            "url": "https://en.wikipedia.org/wiki/singular_value"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "NN": "Neural Network",
        "NMT": "Neural Machine Translation"
    },
    "highlights": [
        "Neural Network (NN)-based algorithms have made significant progresses in natural language generation tasks, including language modeling (<a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\"><a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\">Kim et al, 2016</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>), machine translation (<a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\"><a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\">Britz et al, 2017</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a></a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>) and dialog systems (<a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\"><a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\">Shang et al, 2015</a></a>)",
        "We show in the previous section that in training natural language generation models, the learnt word embeddings are clustered into a narrow cone and the model faces the challenge of limited expressiveness",
        "We present experimental results for language modeling in Table 1 and machine translation in Table 2",
        "We described and analyzed the representation degeneration problem in training neural network models for natural language generation tasks both empirically and theoretically",
        "We proposed a novel regularization method to increase the representation power of word embeddings explicitly",
        "Experiments on language modeling and machine translation demonstrated the effectiveness of our method"
    ],
    "key_statements": [
        "Neural Network (NN)-based algorithms have made significant progresses in natural language generation tasks, including language modeling (<a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\"><a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\">Kim et al, 2016</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>), machine translation (<a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\"><a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\">Britz et al, 2017</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a></a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>) and dialog systems (<a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\"><a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\">Shang et al, 2015</a></a>)",
        "We prove that the representation degeneration problem is related to the structure of hidden states: the degeneration appears when the convex hull of the hidden states does not contain the origin and such condition is likely to happen when training with layer normalization (<a class=\"ref-link\" id=\"cBa_et+al_2016_a\" href=\"#rBa_et+al_2016_a\">Ba et al, 2016</a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a>; <a class=\"ref-link\" id=\"cMerity_et+al_2018_a\" href=\"#rMerity_et+al_2018_a\">Merity et al, 2018</a>)",
        "We empirically study the word embeddings learnt from sequence generation tasks, i.e., machine translation, and introduce the representation degeneration problem in neural sequence generation.\n3.1",
        "We show in the previous section that in training natural language generation models, the learnt word embeddings are clustered into a narrow cone and the model faces the challenge of limited expressiveness",
        "We present experimental results for language modeling in Table 1 and machine translation in Table 2",
        "We described and analyzed the representation degeneration problem in training neural network models for natural language generation tasks both empirically and theoretically",
        "We proposed a novel regularization method to increase the representation power of word embeddings explicitly",
        "Experiments on language modeling and machine translation demonstrated the effectiveness of our method",
        "It is interesting to combine with other approaches, e.g. (<a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\">Gong et al, 2018</a>), to enrich the representation of word embeddings"
    ],
    "summary": [
        "Neural Network (NN)-based algorithms have made significant progresses in natural language generation tasks, including language modeling (<a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\"><a class=\"ref-link\" id=\"cKim_et+al_2016_a\" href=\"#rKim_et+al_2016_a\">Kim et al, 2016</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>), machine translation (<a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>; <a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\"><a class=\"ref-link\" id=\"cBritz_et+al_2017_a\" href=\"#rBritz_et+al_2017_a\">Britz et al, 2017</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a></a></a>; <a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\"><a class=\"ref-link\" id=\"cGehring_et+al_2017_a\" href=\"#rGehring_et+al_2017_a\">Gehring et al, 2017</a></a>) and dialog systems (<a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\"><a class=\"ref-link\" id=\"cShang_et+al_2015_a\" href=\"#rShang_et+al_2015_a\">Shang et al, 2015</a></a>).",
        "Visualization of word embeddings trained from vanilla Transformer (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>) in English\u2192German translation task.",
        "Word embeddings trained from Word2Vec (<a class=\"ref-link\" id=\"cMikolov_et+al_2013_a\" href=\"#rMikolov_et+al_2013_a\">Mikolov et al, 2013</a>) and the parameters in the softmax layer of a classical classification task.",
        "We mainly study the expressiveness of word embeddings in language generation tasks.",
        "We empirically study the word embeddings learnt from sequence generation tasks, i.e., machine translation, and introduce the representation degeneration problem in neural sequence generation.",
        "In neural sequence generation tasks, the weights in word embeddings and softmax layer are tied.",
        "We treat the row of the parameter matrix in the last softmax layer as the embedding for the category, like how the word embedding is used in the softmax layer for neural language generation model with weight tying trick.",
        "We show in the previous section that in training natural language generation models, the learnt word embeddings are clustered into a narrow cone and the model faces the challenge of limited expressiveness.",
        "Discussion on whether the condition happens in real practice From the theorem, we can see that the existence of the uniformly negative direction is highly related to the structure of the hidden states, and affects the optimization of word embeddings.",
        "For both English \u2192 German and German \u2192 English tasks, we used the base version of Transformer (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>), which has a 6-layer encoder and 6-layer decoder, the size of hidden nodes and embedding are set to 512.",
        "We compare our method with vanilla AWD-LSTM (<a class=\"ref-link\" id=\"cMerity_et+al_2018_a\" href=\"#rMerity_et+al_2018_a\">Merity et al, 2018</a>) in three different settings, without finetune, with finetune and with further continuous cache pointer.",
        "For machine translation, comparing with original base Transformer (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a></a>), our method improves performance with 1.08/0.93 for the English \u2192 German and German \u2192 English tasks, respectively, and achieves 0.54 improvement on the big Transformer.",
        "This demonstrates that by regularizing the similarity between word embeddings, our proposed MLE-CosReg loss leads to better performance.",
        "For a fair comparison with the empirical study in Section 3, we analyze the word embeddings of our model on English \u2192 German translation task.",
        "We described and analyzed the representation degeneration problem in training neural network models for natural language generation tasks both empirically and theoretically.",
        "It is interesting to combine with other approaches, e.g. (<a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\"><a class=\"ref-link\" id=\"cGong_et+al_2018_a\" href=\"#rGong_et+al_2018_a\">Gong et al, 2018</a></a>), to enrich the representation of word embeddings"
    ],
    "headline": "We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the representation degeneration problem",
    "reference_links": [
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Britz_et+al_2017_a",
            "entry": "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration of neural machine translation architectures. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1442\u20131451. Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1151.",
            "crossref": "https://dx.doi.org/10.18653/v1/D17-1151",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/D17-1151"
        },
        {
            "id": "Dauphin_et+al_2017_a",
            "entry": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning, pp. 933\u2013941, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning, pp. 1243\u2013 1252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "Gong_et+al_2018_a",
            "entry": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic word representation. In Advances in Neural Information Processing Systems, pp. 1341\u20131352, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Chengyue%20He%2C%20Di%20Tan%2C%20Xu%20Qin%2C%20Tao%20and%20Tie-Yan%20Liu.%20Frage%3A%20frequency-agnostic%20word%20representation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Chengyue%20He%2C%20Di%20Tan%2C%20Xu%20Qin%2C%20Tao%20and%20Tie-Yan%20Liu.%20Frage%3A%20frequency-agnostic%20word%20representation%202018"
        },
        {
            "id": "Grave_et+al_2017_a",
            "entry": "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Usunier%2C%20Nicolas%20Improving%20neural%20language%20models%20with%20a%20continuous%20cache%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Usunier%2C%20Nicolas%20Improving%20neural%20language%20models%20with%20a%20continuous%20cache%202017"
        },
        {
            "id": "Inan_et+al_2017_a",
            "entry": "Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "Kim_et+al_2016_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In Thirtieth AAAI Conference on Artificial Intelligence, pp. 2741\u20132749, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016"
        },
        {
            "id": "Koehn_et+al_2006_a",
            "entry": "Philipp Koehn, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Brooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, et al. Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding. In Final Report of the 2006 JHU Summer Workshop, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koehn%2C%20Philipp%20Federico%2C%20Marcello%20Shen%2C%20Wade%20Bertoldi%2C%20Nicola%20Open%20source%20toolkit%20for%20statistical%20machine%20translation%3A%20Factored%20translation%20models%20and%20confusion%20network%20decoding%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koehn%2C%20Philipp%20Federico%2C%20Marcello%20Shen%2C%20Wade%20Bertoldi%2C%20Nicola%20Open%20source%20toolkit%20for%20statistical%20machine%20translation%3A%20Factored%20translation%20models%20and%20confusion%20network%20decoding%202006"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In International Conference on Machine Learning, pp. 507\u2013516, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Mandt_et+al_2017_a",
            "entry": "Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873\u20134907, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandt%2C%20Stephan%20Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandt%2C%20Stephan%20Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6297\u2013 6308, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017"
        },
        {
            "id": "Merikoski_1984_a",
            "entry": "Jorma Kaarlo Merikoski. On the trace and the sum of elements of a matrix. Linear algebra and its applications, 60:177\u2013185, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merikoski%2C%20Jorma%20Kaarlo%20On%20the%20trace%20and%20the%20sum%20of%20elements%20of%20a%20matrix%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merikoski%2C%20Jorma%20Kaarlo%20On%20the%20trace%20and%20the%20sum%20of%20elements%20of%20a%20matrix%201984"
        },
        {
            "id": "Merity_et+al_2017_a",
            "entry": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20lstm%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20lstm%20language%20models%202018"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mu_et+al_2018_a",
            "entry": "Jiaqi Mu, Suma Bhat, and Pramod Viswanath. All-but-the-top: simple and effective postprocessing for word representations. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mu%2C%20Jiaqi%20Bhat%2C%20Suma%20Viswanath%2C%20Pramod%20All-but-the-top%3A%20simple%20and%20effective%20postprocessing%20for%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mu%2C%20Jiaqi%20Bhat%2C%20Suma%20Viswanath%2C%20Pramod%20All-but-the-top%3A%20simple%20and%20effective%20postprocessing%20for%20word%20representations%202018"
        },
        {
            "id": "Papineni_et+al_2002_a",
            "entry": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311\u2013318. Association for Computational Linguistics, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Press_2017_a",
            "entry": "Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 157\u2013163. Association for Computational Linguistics, 2017. URL http://aclweb.org/anthology/E17-2025.",
            "url": "http://aclweb.org/anthology/E17-2025",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Press%2C%20Ofir%20Wolf%2C%20Lior%20Using%20the%20output%20embedding%20to%20improve%20language%20models%202017"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715\u20131725. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016"
        },
        {
            "id": "Shang_et+al_2015_a",
            "entry": "Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 1577\u20131586, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shang%2C%20Lifeng%20Lu%2C%20Zhengdong%20Li%2C%20Hang%20Neural%20responding%20machine%20for%20short-text%20conversation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shang%2C%20Lifeng%20Lu%2C%20Zhengdong%20Li%2C%20Hang%20Neural%20responding%20machine%20for%20short-text%20conversation%202015"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017"
        },
        {
            "id": "Vaswani_et+al_2018_a",
            "entry": "Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.",
            "url": "http://arxiv.org/abs/1803.07416",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Xia_et+al_2017_a",
            "entry": "Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Dual inference for machine learning. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp. 3112\u20133118, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%2C%20Yingce%20Bian%2C%20Jiang%20Qin%2C%20Tao%20Yu%2C%20Nenghai%20Dual%20inference%20for%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xia%2C%20Yingce%20Bian%2C%20Jiang%20Qin%2C%20Tao%20Yu%2C%20Nenghai%20Dual%20inference%20for%20machine%20learning%202017"
        },
        {
            "id": "Xia_et+al_2017_b",
            "entry": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning. In International Conference on Machine Learning, pp. 3789\u20133798, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%2C%20Yingce%20Qin%2C%20Tao%20Chen%2C%20Wei%20Bian%2C%20Jiang%20Dual%20supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xia%2C%20Yingce%20Qin%2C%20Tao%20Chen%2C%20Wei%20Bian%2C%20Jiang%20Dual%20supervised%20learning%202017"
        },
        {
            "id": "Yang_et+al_2018_a",
            "entry": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: a high-rank rnn language model. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20softmax%20bottleneck%3A%20a%20high-rank%20rnn%20language%20model%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20softmax%20bottleneck%3A%20a%20high-rank%20rnn%20language%20model%202018"
        },
        {
            "id": "Yue_et+al_2018_a",
            "entry": "Zhao Yue, Zhao Deli, Wan Shaohua, and Zhang Bo. Softmax supervision with isotropic normalization, 2018. URL https://openreview.net/forum?id=SyXNErg0W.",
            "url": "https://openreview.net/forum?id=SyXNErg0W"
        }
    ]
}
