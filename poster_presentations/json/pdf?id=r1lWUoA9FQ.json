{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ARE ADVERSARIAL EXAMPLES INEVITABLE?",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1lWUoA9FQ"
        },
        "journal": "Figure",
        "volume": "1",
        "abstract": "A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable?"
    },
    "keywords": [
        {
            "term": "elementary proof",
            "url": "https://en.wikipedia.org/wiki/elementary_proof"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "density function",
            "url": "https://en.wikipedia.org/wiki/density_function"
        },
        {
            "term": "isoperimetric inequality",
            "url": "https://en.wikipedia.org/wiki/isoperimetric_inequality"
        }
    ],
    "abbreviations": {
        "FGSM": "fast gradient sign method"
    },
    "highlights": [
        "A number of adversarial attacks on neural networks have been recently proposed",
        "There are a number of ways to escape the guarantees of adversarial examples made by Theorems 1-4",
        "One potential escape is for the class density functions to take on extremely large values; the dependence of Uc on n is addressed separately in Section 8",
        "Unbounded density functions and low-dimensional data manifolds In practice, image datasets might lie on low-dimensional manifolds within the cube, and the support of these distributions could have measure zero, making the density function infinite (i.e., Uc = \u221e)",
        "Adversarial examples of this expanded dataset can be crafted with perturbations of size 2",
        "This method of expanding the manifold before crafting adversarial examples is often used in practice"
    ],
    "key_statements": [
        "A number of adversarial attacks on neural networks have been recently proposed",
        "For certain classes of problems, adversarial examples are inescapable",
        "We identify a broad class of problems for which adversarial examples cannot be avoided",
        "We derive fundamental limits on the susceptibility of a classifier to adversarial attacks that depend on properties of the data distribution as well as the dimensionality of the dataset",
        "Adversarial training with fast gradient sign method examples was quickly shown to be vulnerable to more sophisticated multi-stage attacks (<a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>; Tramer et al, 2017a)",
        "More sophisticated defenses that rely on network distillation (Papernot et al, 2016b) and specialized activation functions (<a class=\"ref-link\" id=\"cZantedeschi_et+al_2017_a\" href=\"#rZantedeschi_et+al_2017_a\">Zantedeschi et al, 2017</a>) were toppled by strong attacks (<a class=\"ref-link\" id=\"cPapernot_et+al_2016_a\" href=\"#rPapernot_et+al_2016_a\">Papernot et al, 2016a</a>; Tramer et al, 2017b; <a class=\"ref-link\" id=\"cCarlini_2016_a\" href=\"#rCarlini_2016_a\">Carlini & Wagner, 2016</a>; 2017a)",
        "As CIFAR-10 and ImageNet are nowhere near that of MNIST, which leads some researchers to speculate that adversarial defense is fundamentally harder in higher dimensions \u2013 an issue we address in Section 8",
        "Most studies of adversarial examples measure the size of perturbation in either the 2 (Euclidean) norm or the \u221e norm, and so it is natural to wonder whether Theorem 1 depends strongly on the distance metric",
        "A number of papers have looked at sparse adversarial examples, in which a small number of image pixels, in some cases only one (<a class=\"ref-link\" id=\"cSu_et+al_2017_a\" href=\"#rSu_et+al_2017_a\">Su et al, 2017</a>), are changed to manipulate the class label",
        "Tighter bounds can be obtained if we only guarantee that adversarial examples exist for some data points in a class, without bounding the probability of this event",
        "There are a number of ways to escape the guarantees of adversarial examples made by Theorems 1-4",
        "One potential escape is for the class density functions to take on extremely large values; the dependence of Uc on n is addressed separately in Section 8",
        "Unbounded density functions and low-dimensional data manifolds In practice, image datasets might lie on low-dimensional manifolds within the cube, and the support of these distributions could have measure zero, making the density function infinite (i.e., Uc = \u221e)",
        "Adversarial examples of this expanded dataset can be crafted with perturbations of size 2",
        "This method of expanding the manifold before crafting adversarial examples is often used in practice",
        "Theory predicts that such highly concentrated datasets can be relatively safe from adversarial examples.\n1 Only the fully-connected layer is modified to handle the difference in dimensionality between datasets. 2Adversarial examples for MNIST/CIFAR-10 were produced as in <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al (2017</a>) using 100-step/20-step PGD",
        "It is impossible to know the exact properties of real-world image distributions or the resulting fundamental limits of adversarial training for specific datasets"
    ],
    "summary": [
        "A number of adversarial attacks on neural networks have been recently proposed.",
        "We derive fundamental limits on the susceptibility of a classifier to adversarial attacks that depend on properties of the data distribution as well as the dimensionality of the dataset.",
        "Adversarial examples occur when a small perturbation to an image changes its class label.",
        "Most studies of adversarial examples measure the size of perturbation in either the 2 (Euclidean) norm or the \u221e norm, and so it is natural to wonder whether Theorem 1 depends strongly on the distance metric.",
        "A number of papers have looked at sparse adversarial examples, in which a small number of image pixels, in some cases only one (<a class=\"ref-link\" id=\"cSu_et+al_2017_a\" href=\"#rSu_et+al_2017_a\">Su et al, 2017</a>), are changed to manipulate the class label.",
        "If a point x has an -adversarial example in this norm, it can be perturbed into a different class by modifying at most pixels.",
        "Tighter bounds can be obtained if we only guarantee that adversarial examples exist for some data points in a class, without bounding the probability of this event.",
        "If a classifier has the ability to say \u201cI don\u2019t know,\u201d rather than assign a label to every input, the region of the cube that is assigned class labels might be very small, and adversarial examples could be escaped even if the other assumptions of Theorem 4 are satisfied.",
        "The theorem below shows that these fundamental limits do not depend in a non-trivial way on the dimensionality of the images in big MNIST, and so the relationship between dimensionality and susceptibility in Figure 4a results from the weakness of the training process.",
        "Suppose and p are such that, for all MNIST classifiers, a random image from class c has an -adversarial example with probability at least p.",
        "Adjacent pixels in MNIST are very highly correlated, and images are concentrated near simple, low-dimensional manifolds, resulting in highly concentrated image classes with large Uc. Theory predicts that such highly concentrated datasets can be relatively safe from adversarial examples.",
        "An informal interpretation of Theorem 2 is that \u201chigh complexity\u201d image classes are fundamentally more susceptible to adversarial examples, and Figure 4 suggests that complexity is largely responsible for differences we observe in the effectiveness of adversarial training for different datasets.",
        "It is impossible to know the exact properties of real-world image distributions or the resulting fundamental limits of adversarial training for specific datasets."
    ],
    "headline": "For certain classes of problems, adversarial examples are inescapable",
    "reference_links": [
        {
            "id": "Abramowitz_1965_a",
            "entry": "Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55. Courier Corporation, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abramowitz%2C%20Milton%20Stegun%2C%20Irene%20A.%20Handbook%20of%20mathematical%20functions%3A%20with%20formulas%2C%20graphs%2C%20and%20mathematical%20tables%2C%20volume%2055%201965"
        },
        {
            "id": "Athalye_2017_a",
            "entry": "Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Bobkov_1997_a",
            "entry": "Sergey G Bobkov et al. An isoperimetric inequality on the discrete cube, and an elementary proof of the isoperimetric inequality in gauss space. The Annals of Probability, 25(1):206\u2013214, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bobkov%2C%20Sergey%20G.%20An%20isoperimetric%20inequality%20on%20the%20discrete%20cube%2C%20and%20an%20elementary%20proof%20of%20the%20isoperimetric%20inequality%20in%20gauss%20space%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bobkov%2C%20Sergey%20G.%20An%20isoperimetric%20inequality%20on%20the%20discrete%20cube%2C%20and%20an%20elementary%20proof%20of%20the%20isoperimetric%20inequality%20in%20gauss%20space%201997"
        },
        {
            "id": "Buckman_et+al_2018_a",
            "entry": "Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. OpenReview, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018"
        },
        {
            "id": "Carlini_2016_a",
            "entry": "Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04311"
        },
        {
            "id": "Carlini_0000_a",
            "entry": "Nicholas Carlini and David Wagner. Magnet and \u201cefficient defenses against adversarial attacks\u201d are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08478"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Das_et+al_2018_a",
            "entry": "Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li, Li Chen, Michael E Kounavis, and Duen Horng Chau. Shield: Fast, practical defense and vaccination for deep learning using JPEG compression. arXiv preprint arXiv:1802.06816, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06816"
        },
        {
            "id": "Dhillon_et+al_2018_a",
            "entry": "Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv preprint arXiv:1803.01442, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01442"
        },
        {
            "id": "Fawzi_et+al_2018_a",
            "entry": "Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08686"
        },
        {
            "id": "Gilmer_et+al_2018_a",
            "entry": "Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00117"
        },
        {
            "id": "Kolter_2017_a",
            "entry": "J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "Ledoux_2001_a",
            "entry": "Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc., 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20The%20concentration%20of%20measure%20phenomenon.%20Number%2089%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledoux%2C%20Michel%20The%20concentration%20of%20measure%20phenomenon.%20Number%2089%202001"
        },
        {
            "id": "Levy_1951_a",
            "entry": "Paul Levy and Franco Pellegrino. Problemes concrets d\u2019analyse fonctionnelle. Gauthier-Villars, Paris, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Paul%20Pellegrino%2C%20Franco%20Problemes%20concrets%20d%E2%80%99analyse%20fonctionnelle%201951"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Michael E Houle, Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02613"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Mahloujifar_et+al_2018_a",
            "entry": "Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. arXiv preprint arXiv:1809.03063, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.03063"
        },
        {
            "id": "Mcdiarmid_1989_a",
            "entry": "C McDiarmid. On the method of bounded differences. London Mathematical Society Lecture Notes, 141: 148\u2013188, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989"
        },
        {
            "id": "Meng_2017_a",
            "entry": "Dongyu Meng and Hao Chen. MagNet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Dongyu%20Chen%2C%20Hao%20MagNet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Dongyu%20Chen%2C%20Hao%20MagNet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017"
        },
        {
            "id": "Milman_1986_a",
            "entry": "Vitali D Milman and Gideon Schechtman. Asymptotic Theory of Finite Dimensional Normed Spaces. Springer-Verlag, Berlin, Heidelberg, 1986. ISBN 0-387-16769-2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Milman%2C%20Vitali%20D.%20Schechtman%2C%20Gideon%20Asymptotic%20Theory%20of%20Finite%20Dimensional%20Normed%20Spaces%201986"
        },
        {
            "id": "Osserman_1978_a",
            "entry": "Robert Osserman et al. The isoperimetric inequality. Bulletin of the American Mathematical Society, 84(6): 1182\u20131238, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osserman%2C%20Robert%20The%20isoperimetric%20inequality%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osserman%2C%20Robert%20The%20isoperimetric%20inequality%201978"
        },
        {
            "id": "Papernot_et+al_0000_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582\u2013597. IEEE, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "Ros_2001_a",
            "entry": "Antonio Ros. The isoperimetric problem. Global theory of minimal surfaces, 2:175\u2013209, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ros%2C%20Antonio%20The%20isoperimetric%20problem.%20Global%20theory%20of%20minimal%20surfaces%2C%202%202001"
        },
        {
            "id": "Samangouei_et+al_2018_a",
            "entry": "Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06605"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Shiwei Shen, Guoqing Jin, Ke Gao, and Yongdong Zhang. APE-GAN: Adversarial perturbation elimination with GAN. ICLR Submission, available on OpenReview, 4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Shiwei%20Jin%2C%20Guoqing%20Gao%2C%20Ke%20Zhang%2C%20Yongdong%20APE-GAN%3A%20Adversarial%20perturbation%20elimination%20with%20GAN%202017"
        },
        {
            "id": "Simon-Gabriel_et+al_2018_a",
            "entry": "Carl-Johann Simon-Gabriel, Yann Ollivier, Bernhard Scholkopf, Leon Bottou, and David Lopez-Paz. Adversarial vulnerability of neural networks increases with input dimension. arXiv preprint arXiv:1802.01421, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01421"
        },
        {
            "id": "Sinha_et+al_2018_a",
            "entry": "Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. OpenReview, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifying%20some%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifying%20some%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "Song_et+al_2017_a",
            "entry": "Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. PixelDefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10766"
        },
        {
            "id": "Su_et+al_2017_a",
            "entry": "Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08864"
        },
        {
            "id": "Sudakov_1974_a",
            "entry": "Vladimir Sudakov and Boris Tsirelson. Extremal properties of half-spaces for spherically invariant measures. J. Soviet Math., 41, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sudakov%2C%20Vladimir%20Tsirelson%2C%20Boris%20Extremal%20properties%20of%20half-spaces%20for%20spherically%20invariant%20measures%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sudakov%2C%20Vladimir%20Tsirelson%2C%20Boris%20Extremal%20properties%20of%20half-spaces%20for%20spherically%20invariant%20measures%201974"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Talagrand_1995_a",
            "entry": "Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications Mathematiques de l\u2019Institut des Hautes Etudes Scientifiques, 81(1):73\u2013205, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20Michel%20Concentration%20of%20measure%20and%20isoperimetric%20inequalities%20in%20product%20spaces.%20Publications%20Mathematiques%20de%20l%E2%80%99Institut%20des%20Hautes%20Etudes%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talagrand%2C%20Michel%20Concentration%20of%20measure%20and%20isoperimetric%20inequalities%20in%20product%20spaces.%20Publications%20Mathematiques%20de%20l%E2%80%99Institut%20des%20Hautes%20Etudes%201995"
        },
        {
            "id": "Talagrand_1996_a",
            "entry": "Michel Talagrand. A new look at independence. The Annals of probability, pp. 1\u201334, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20Michel%20A%20new%20look%20at%20independence.%20The%20Annals%20of%20probability%201996"
        },
        {
            "id": "Tramer_et+al_0000_a",
            "entry": "Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07204"
        },
        {
            "id": "Tramer_et+al_0000_b",
            "entry": "Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03453"
        },
        {
            "id": "Vershynin_2017_a",
            "entry": "Roman Vershynin. High-Dimensional Probability. Cambridge University Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20High-Dimensional%20Probability%202017"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. arXiv preprint arXiv:1711.01991, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01991"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01155"
        },
        {
            "id": "Zantedeschi_et+al_2017_a",
            "entry": "Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adversarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 39\u201349. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017"
        },
        {
            "id": "The_1997_a",
            "entry": "The following Lemma was first proved in Sudakov & Tsirelson (1974), and an elementary proof was given in Bobkov et al. (1997).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20following%20Lemma%20was%20first%20proved%20in%20Sudakov%20%20Tsirelson%201974%20and%20an%20elementary%20proof%20was%20given%20in%20Bobkov%20et%20al%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20following%20Lemma%20was%20first%20proved%20in%20Sudakov%20%20Tsirelson%201974%20and%20an%20elementary%20proof%20was%20given%20in%20Bobkov%20et%20al%201997"
        },
        {
            "id": "2\u03c0x_1965_a",
            "entry": "2\u03c0x which is valid for x > 0, and can be found in Abramowitz & Stegun (1965).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%CF%80x%20which%20is%20valid%20for%20x%20%200%20and%20can%20be%20found%20in%20Abramowitz%20%20Stegun%201965"
        },
        {
            "id": "Our_1995_a",
            "entry": "Our proof emulates the method of Talagrand, with minor modifications that extend the result to other p norms. We need the following standard inequality. Proof can be found in Talagrand (1995; 1996).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Our%20proof%20emulates%20the%20method%20of%20Talagrand%20with%20minor%20modifications%20that%20extend%20the%20result%20to%20other%20p%20norms%20We%20need%20the%20following%20standard%20inequality%20Proof%20can%20be%20found%20in%20Talagrand%201995%201996"
        },
        {
            "id": "Our_1995_b",
            "entry": "Our proof of Lemma 3 follows the three-step process of Talagrand illustrated in Talagrand (1995). We begin by proving the bound etf (x,A) dx",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Our%20proof%20of%20Lemma%203%20follows%20the%20threestep%20process%20of%20Talagrand%20illustrated%20in%20Talagrand%201995%20We%20begin%20by%20proving%20the%20bound%20etf%20xA%20dx",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Our%20proof%20of%20Lemma%203%20follows%20the%20threestep%20process%20of%20Talagrand%20illustrated%20in%20Talagrand%201995%20We%20begin%20by%20proving%20the%20bound%20etf%20xA%20dx"
        },
        {
            "id": "covers_0000_a",
            "entry": "covers at least half the cube, we can invoke Lemma 3. We have that vol[R(; h)] \u2265 1 \u2212 \u03b4, where \u03b4=",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=covers%20at%20least%20half%20the%20cube%20we%20can%20invoke%20Lemma%203%20We%20have%20that%20volR%20h%20%201%20%20%CE%B4%20where%20%CE%B4"
        },
        {
            "id": "Let_0000_b",
            "entry": "Let A denote the support of pc, and suppose that this support has measure vol[A] = \u03b7. We want to show that, for large enough, the expansion A(, dp) is larger than half the cube. Since class c occupies less than half the cube, this would imply that A(, dp) overlaps with other classes, and so there must be data points in A",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Let%20A%20denote%20the%20support%20of%20pc%20and%20suppose%20that%20this%20support%20has%20measure%20volA%20%20%CE%B7%20We%20want%20to%20show%20that%20for%20large%20enough%20the%20expansion%20A%20dp%20is%20larger%20than%20half%20the%20cube%20Since%20class%20c%20occupies%20less%20than%20half%20the%20cube%20this%20would%20imply%20that%20A%20dp%20overlaps%20with%20other%20classes%20and%20so%20there%20must%20be%20data%20points%20in%20A"
        },
        {
            "id": "Vol_0000_a",
            "entry": "vol[A(, dp)] \u2265 \u03a6(\u03b1 + ) \u2265 \u03a6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=vol%5BA.%20dp%29%5D%20%E2%89%A5%20%CE%A6%28%CE%B1%20%2B"
        },
        {
            "id": "In_0000_c",
            "entry": "In the case p = 0, we need to use equation 17 from the proof of Lemma 3 in Appendix B, which we restate here vol[A(, d0)] \u2265 1 \u2212 exp",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20the%20case%20p%20%200%20we%20need%20to%20use%20equation%2017%20from%20the%20proof%20of%20Lemma%203%20in%20Appendix%20B%20which%20we%20restate%20here%20volA%20d0%20%201%20%20exp"
        }
    ]
}
