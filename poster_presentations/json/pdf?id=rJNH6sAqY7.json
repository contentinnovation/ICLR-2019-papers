{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON COMPUTATION AND GENERALIZATION OF GENERATIVE ADVERSARIAL NETWORKS UNDER SPECTRUM CONTROL",
        "author": "Haoming Jiang, Zhehui Chen, Minshuo Chen & Tuo Zhao \u2217 School of Industrial and Systems Engineering Georgia Institute of Technology Atlanta, GA 30318, USA {jianghm, zhchen, mchen, tourzhao}@gatech.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJNH6sAqY7"
        },
        "abstract": "Generative Adversarial Networks (GANs), though powerful, is hard to train. Several recent works (<a class=\"ref-link\" id=\"cBrock_et+al_2016_a\" href=\"#rBrock_et+al_2016_a\">Brock et al., 2016</a>; Miyato et al., 2018) suggest that controlling the spectra of weight matrices in the discriminator can significantly improve the training of GANs. Motivated by their discovery, we propose a new framework for training GANs, which allows more flexible spectrum control (e.g., making the weight matrices of the discriminator have slow singular value decays). Specifically, we propose a new reparameterization approach for the weight matrices of the discriminator in GANs, which allows us to directly manipulate the spectra of the weight matrices through various regularizers and constraints, without intensively computing singular value decompositions. Theoretically, we further show that the spectrum control improves the generalization ability of GANs. Our experiments on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other methods, our proposed method is capable of generating images with competitive quality by utilizing spectral normalization and encouraging the slow singular value decay."
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "ERC": "Empirical Rademacher complexity",
        "SN": "Spectral Normalization"
    },
    "highlights": [
        "Many efforts have been recently devoted to studying Generative Adversarial Networks (GANs, Goodfellow et al (2014))",
        "Different from other unsupervised learning methods, which directly maximize the likelihood of deep generative models (e.g., Variational Auto-encoder, Nonlinear ICA, and Restricted Boltzmann Machine), Generative Adversarial Networks introduce a competition between two neural networks",
        "One neural network serves as the generator that yields artificial samples, and the other serves as the discriminator that distinguishes the artificial samples from the real data",
        "We show how the spectrum control benefits the generalization of Generative Adversarial Networks",
        "We propose a new SVD-type reparameterization for weight matrices of the discriminator in Generative Adversarial Networks, allowing us to efficiently manipulate the spectra of weight matrices",
        "We than establish a new generalization bound of Generative Adversarial Networks to justify the importance of spectrum control on weight matrices"
    ],
    "key_statements": [
        "Many efforts have been recently devoted to studying Generative Adversarial Networks (GANs, Goodfellow et al (2014))",
        "Generative Adversarial Networks provide a general unsupervised framework to learn a generative model from unlabeled real data",
        "Different from other unsupervised learning methods, which directly maximize the likelihood of deep generative models (e.g., Variational Auto-encoder, Nonlinear ICA, and Restricted Boltzmann Machine), Generative Adversarial Networks introduce a competition between two neural networks",
        "One neural network serves as the generator that yields artificial samples, and the other serves as the discriminator that distinguishes the artificial samples from the real data",
        "We provide theoretical analysis, which characterizes how the spectrum control benefits the generalization ability of Generative Adversarial Networks",
        "The rest of the paper is organized as follows: Section 2 introduces our proposed training framework in detail; Section 3 presents the generalization bound for Generative Adversarial Networks under spectrum control; Section 4 presents numerical experiments on CIFAR-10, STL-10, and ImageNet datasets",
        "We show how the spectrum control benefits the generalization of Generative Adversarial Networks",
        "We propose a new SVD-type reparameterization for weight matrices of the discriminator in Generative Adversarial Networks, allowing us to efficiently manipulate the spectra of weight matrices",
        "We than establish a new generalization bound of Generative Adversarial Networks to justify the importance of spectrum control on weight matrices"
    ],
    "summary": [
        "Many efforts have been recently devoted to studying Generative Adversarial Networks (GANs, Goodfellow et al (2014)).",
        "They further show that spectral normalization essentially controls the Lipschitz constant of the discriminator with respect to the input.",
        "Motivated by the spectral normalization, we propose a novel training framework, which provides more flexible and precise control over the spectra of weight matrices in the discriminator.",
        "CIFAR-10 and better performance than the spectral normalization and other competing approaches on STL-10 and ImageNet. Besides the empirical studies, we provide theoretical analysis, which characterizes how the spectrum control benefits the generalization ability of GANs. denote \u03bc as the underlying data distribution and \u03bdn as the distribution given by the well trained generator.",
        "The rest of the paper is organized as follows: Section 2 introduces our proposed training framework in detail; Section 3 presents the generalization bound for GANs under spectrum control; Section 4 presents numerical experiments on CIFAR-10, STL-10, and ImageNet datasets.",
        "Comparing to the orthogonal regularization, Miyato et al (2018) suggest that we should allow more flexibility by using spectral normalization, which only bounds the largest singular value.",
        "Note that the divergence regularizer requires the singular values in the interval [0, 1] and D-optimal regularizer cannot control the Lipschitz constant of the discriminator D.",
        "We incorporate the divergence regularizer with the spectrum constraint and combine the D-optimal regularizer with the spectral normalization to bound the Lipschitz constant.",
        "Recall that by (1), the training of GANs is essentially minimizing the F-distance with F being the collection of composite functions A(D(\u00b7)), where D(\u00b7) is the L-layer discriminator network defined by (2).",
        "Figure 4 shows that the singular value decays of weight matrices with two different methods: SN-GAN and Doptimal regularizer with spectral normalization.",
        "Such a slower decay improves the performance of GANs. Table 2 presents the inception scores and FIDs of our proposed methods as well as other methods on CIFAR-10 and STL-10.",
        "Compared with STL-10, CIFAR-10 is easy to learn, and GAN train- Figure 3: Inception scores on ImageNet. ing can only limitedly benefits from encouraging the slow We can see that our method outperforms singular value decay.",
        "Due to our computational resource limit, we only test the method of spectral normalization with D-optimal regularizer, which achieves the best performance on CNN experiments.",
        "We than establish a new generalization bound of GAN to justify the importance of spectrum control on weight matrices.",
        "Our experiments on CIFAR-10, STL-10, and ImageNet datasets support our proposed methods, theory, and discoveries"
    ],
    "headline": "We propose a new framework for training Generative Adversarial Networks, which allows more flexible spectrum control",
    "reference_links": [
        {
            "id": "Abadi_2016_a",
            "entry": "Mart\u00edn Abadi and David G Andersen. Learning to protect communications with adversarial neural cryptography. arXiv preprint arXiv:1610.06918, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06918"
        },
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "Barratt_2018_a",
            "entry": "Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Brock_et+al_2016_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07093"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Dowson_1982_a",
            "entry": "DC Dowson and BV Landau. The fr\u00e9chet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450\u2013455, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dowson%2C%20D.C.%20Landau%2C%20B.V.%20The%20fr%C3%A9chet%20distance%20between%20multivariate%20normal%20distributions%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dowson%2C%20D.C.%20Landau%2C%20B.V.%20The%20fr%C3%A9chet%20distance%20between%20multivariate%20normal%20distributions%201982"
        },
        {
            "id": "Gao_et+al_2017_a",
            "entry": "Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributional robustness and regularization in statistical learning. CoRR, abs/1712.06050, 2017. URL http://arxiv.org/abs/1712.06050.",
            "url": "http://arxiv.org/abs/1712.06050",
            "arxiv_url": "https://arxiv.org/pdf/1712.06050"
        },
        {
            "id": "Goodfellow_2016_a",
            "entry": "Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, and Bo Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv preprint arXiv:1709.06079, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06079"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Li_2009_a",
            "entry": "Gang Li and Dibyen Majumdar. Some results on d-optimal designs for nonlinear models with applications. Biometrika, 96(2):487\u2013493, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Gang%20Majumdar%2C%20Dibyen%20Some%20results%20on%20d-optimal%20designs%20for%20nonlinear%20models%20with%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Gang%20Majumdar%2C%20Dibyen%20Some%20results%20on%20d-optimal%20designs%20for%20nonlinear%20models%20with%20applications%202009"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06547"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards minimum hyperspherical energy. arXiv preprint arXiv:1805.09298, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09298"
        },
        {
            "id": "Mentre_et+al_1997_a",
            "entry": "France Mentre, Alain Mallet, and Doha Baccar. Optimal design in random-effects regression models. Biometrika, 84(2):429\u2013442, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mentre%2C%20France%20Mallet%2C%20Alain%20Baccar%2C%20Doha%20Optimal%20design%20in%20random-effects%20regression%20models%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mentre%2C%20France%20Mallet%2C%20Alain%20Baccar%2C%20Doha%20Optimal%20design%20in%20random-effects%20regression%20models%201997"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint arXiv:1802.05637, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05637"
        },
        {
            "id": "Miyato_et+al_2018_b",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5591\u20135600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Roth_et+al_2015_a",
            "entry": "Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems, pp. 2015\u20132025, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202015"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "Salimans_et+al_2016_b",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Shepard_et+al_2015_a",
            "entry": "Ron Shepard, Scott R Brozell, and Gergely Gidofalvi. The representation and parametrization of orthogonal matrices. The Journal of Physical Chemistry A, 119(28):7924\u20137939, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shepard%2C%20Ron%20Brozell%2C%20Scott%20R.%20Gidofalvi%2C%20Gergely%20The%20representation%20and%20parametrization%20of%20orthogonal%20matrices%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shepard%2C%20Ron%20Brozell%2C%20Scott%20R.%20Gidofalvi%2C%20Gergely%20The%20representation%20and%20parametrization%20of%20orthogonal%20matrices%202015"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. Cvpr, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Wu_2011_a",
            "entry": "CF Jeff Wu and Michael S Hamada. Experiments: planning, analysis, and optimization, volume 552. John Wiley & Sons, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20C.F.Jeff%20Hamada%2C%20Michael%20S.%20Experiments%3A%20planning%2C%20analysis%2C%20and%20optimization%2C%20volume%20552%202011"
        },
        {
            "id": "Xiang_2017_a",
            "entry": "Sitao Xiang and Hao Li. On the effects of batch and weight normalization in generative adversarial networks. stat, 1050:22, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiang%2C%20Sitao%20Li%2C%20Hao%20On%20the%20effects%20of%20batch%20and%20weight%20normalization%20in%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiang%2C%20Sitao%20Li%2C%20Hao%20On%20the%20effects%20of%20batch%20and%20weight%20normalization%20in%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Yang_et+al_2013_a",
            "entry": "Min Yang, Stefanie Biedermann, and Elina Tang. On optimal designs for nonlinear models: a general and efficient algorithm. Journal of the American Statistical Association, 108(504):1411\u20131420, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Min%20Biedermann%2C%20Stefanie%20Tang%2C%20Elina%20On%20optimal%20designs%20for%20nonlinear%20models%3A%20a%20general%20and%20efficient%20algorithm%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Min%20Biedermann%2C%20Stefanie%20Tang%2C%20Elina%20On%20optimal%20designs%20for%20nonlinear%20models%3A%20a%20general%20and%20efficient%20algorithm%202013"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07892"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02771"
        }
    ]
}
