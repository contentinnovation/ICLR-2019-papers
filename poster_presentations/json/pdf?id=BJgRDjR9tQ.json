{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ROBUST ESTIMATION AND GENERATIVE ADVERSARIAL NETWORKS",
        "author": "Chao Gao Department of Statistics University of Chicago Chicago, IL 60637 USA chaogao@galton.uchicago.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJgRDjR9tQ"
        },
        "abstract": "Robust estimation under Huber\u2019s -contamination model has become an important topic in statistics and theoretical computer science. Statistically optimal procedures such as Tukey\u2019s median and other estimators based on depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f -GANs and various depth functions through the lens of f -Learning. Similar to the derivation of f GANs, we show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. In particular, we show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GANs lead to statistically optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
    },
    "keywords": [
        {
            "term": "computer science",
            "url": "https://en.wikipedia.org/wiki/computer_science"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "maximum likelihood estimator",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimator"
        },
        {
            "term": "computational intractability",
            "url": "https://en.wikipedia.org/wiki/computational_intractability"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "robust estimator",
            "url": "https://en.wikipedia.org/wiki/robust_estimator"
        }
    ],
    "abbreviations": {
        "MLE": "maximum likelihood estimator",
        "TV-GAN": "total variation GAN",
        "JS-GAN": "Jensen-Shannon GAN",
        "ReLU": "rectified linear unit",
        "HKRGC": "Hong Kong Research Grant Council"
    },
    "highlights": [
        "In the setting of Huber\u2019s -contamination model (<a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\"><a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\">Huber, 1964</a></a>; 1965), one has i.i.d observations<br/><br/>X1, ..., Xn \u223c (1 \u2212 )P\u03b8 + Q, (1)<br/><br/>and the goal is to estimate the model parameter \u03b8",
        "The search for both statistically optimal and computationally feasible procedures has become a fundamental problem in areas including statistics and computer science",
        "Our key observation is that robust estimators that are maximizers of depth functions, including halfspace depth, regression depth and covariance matrix depth, can all be derived under the framework of f -GAN (<a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>)",
        "The following proposition shows that when QQ approaches to Q in some neighborhood, TV-Learning leads to robust estimators that are defined as the maximizers of various depth functions including Tukey\u2019s depth, regression depth, and covariance depth",
        "We further study the performance of Jensen-Shannon GAN with various structures of neural networks"
    ],
    "key_statements": [
        "In the setting of Huber\u2019s -contamination model (<a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\"><a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\">Huber, 1964</a></a>; 1965), one has i.i.d observations<br/><br/>X1, ..., Xn \u223c (1 \u2212 )P\u03b8 + Q, (1)<br/><br/>and the goal is to estimate the model parameter \u03b8",
        "The search for both statistically optimal and computationally feasible procedures has become a fundamental problem in areas including statistics and computer science",
        "Our key observation is that robust estimators that are maximizers of depth functions, including halfspace depth, regression depth and covariance matrix depth, can all be derived under the framework of f -GAN (<a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>)",
        "The following proposition shows that when QQ approaches to Q in some neighborhood, TV-Learning leads to robust estimators that are defined as the maximizers of various depth functions including Tukey\u2019s depth, regression depth, and covariance depth",
        "We further study the performance of Jensen-Shannon GAN with various structures of neural networks"
    ],
    "summary": [
        "In the setting of Huber\u2019s -contamination model (<a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\"><a class=\"ref-link\" id=\"cHuber_1964_a\" href=\"#rHuber_1964_a\">Huber, 1964</a></a>; 1965), one has i.i.d observations<br/><br/>X1, ..., Xn \u223c (1 \u2212 )P\u03b8 + Q, (1)<br/><br/>and the goal is to estimate the model parameter \u03b8.",
        "The depth-based procedures are adaptive to unknown nuisance parameters in the models such as covariance structures, contamination proportion, and error distributions (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a>; <a class=\"ref-link\" id=\"cGao_2017_a\" href=\"#rGao_2017_a\">Gao, 2017</a>).",
        "Our key observation is that robust estimators that are maximizers of depth functions, including halfspace depth, regression depth and covariance matrix depth, can all be derived under the framework of f -GAN (<a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>).",
        "Our theoretical results give insights on how to choose appropriate neural network classes that lead to minimax optimal robust estimation under Huber\u2019s -contamination model.",
        "The following proposition shows that when QQ approaches to Q in some neighborhood, TV-Learning leads to robust estimators that are defined as the maximizers of various depth functions including Tukey\u2019s depth, regression depth, and covariance depth.",
        "While f -Learning provides a unified perspective in understanding various depth-based procedures in robust estimation, we can step back into the more general f -GAN for its computational advantages, and to design efficient computational strategies.",
        "Unlike TV-GAN, our experiment results show that (14) with the logistic regression discriminator class (13) is not robust to contamination.",
        "If we replace (13) by a neural network class with one or more hidden layers, the estimator will be robust and will work very well numerically.",
        "An advantage of Tukey\u2019s median (9) is that it leads to optimal robust location estimation under general elliptical distributions such as Cauchy distribution whose mean does not exist.",
        "To achieve rate-optimality for robust location estimation under general elliptical distributions, the estimator (16) is different from (14) only in the generator class.",
        "We give extensive numerical studies of robust mean estimation via GAN.",
        "The generator for mean estimation is G\u03b7(Z) = Z + \u03b7 with Z \u223c N (0p, Ip); the discriminator D is a multilayer perceptron (MLP), where each layer consisting of a linear map and a sigmoid activation function and the number of nodes will vary in different experiments to be specified below.",
        "The robust mean estimator constructed through JS-GAN can be made adaptive to unknown covariance structure, which is a special case of (16).",
        "+ log 4, The estimator \u03b8, as a result, is rate-optimal even when the true covariance matrix is not necessarily identity and is unknown.",
        "We consider the estimation of the location parameter \u03b8 in elliptical distribution EC(\u03b8, \u03a3, h) by the JS-GAN defined in (16)."
    ],
    "headline": "Similar to the derivation of f GANs, we show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning",
    "reference_links": [
        {
            "id": "Ali_1966_a",
            "entry": "Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B (Methodological), pp. 131\u2013142, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ali%2C%20Syed%20Mumtaz%20Silvey%2C%20Samuel%20D.%20A%20general%20class%20of%20coefficients%20of%20divergence%20of%20one%20distribution%20from%20another%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ali%2C%20Syed%20Mumtaz%20Silvey%2C%20Samuel%20D.%20A%20general%20class%20of%20coefficients%20of%20divergence%20of%20one%20distribution%20from%20another%201966"
        },
        {
            "id": "Amenta_et+al_2000_a",
            "entry": "Nina Amenta, Marshall Bern, David Eppstein, and S-H Teng. Regression depth and center points. Discrete & Computational Geometry, 23(3):305\u2013323, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amenta%2C%20Nina%20Bern%2C%20Marshall%20Eppstein%2C%20David%20Teng%2C%20S.-H.%20Regression%20depth%20and%20center%20points%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amenta%2C%20Nina%20Bern%2C%20Marshall%20Eppstein%2C%20David%20Teng%2C%20S.-H.%20Regression%20depth%20and%20center%20points%202000"
        },
        {
            "id": "Bartlett_1997_a",
            "entry": "Peter L Bartlett. For valid generalization the size of the weights is more important than the size of the network. In Advances in neural information processing systems, pp. 134\u2013140, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20For%20valid%20generalization%20the%20size%20of%20the%20weights%20is%20more%20important%20than%20the%20size%20of%20the%20network%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20For%20valid%20generalization%20the%20size%20of%20the%20weights%20is%20more%20important%20than%20the%20size%20of%20the%20network%201997"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Chan_2004_a",
            "entry": "Timothy M Chan. An optimal randomized algorithm for maximum tukey depth. In Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 430\u2013436. Society for Industrial and Applied Mathematics, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20Timothy%20M.%20An%20optimal%20randomized%20algorithm%20for%20maximum%20tukey%20depth%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20Timothy%20M.%20An%20optimal%20randomized%20algorithm%20for%20maximum%20tukey%20depth%202004"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under hubers contamination model. The Annals of Statistics, 46(5):1932\u20131960, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Mengjie%20Gao%2C%20Chao%20Ren%2C%20Zhao%20Robust%20covariance%20and%20scatter%20matrix%20estimation%20under%20hubers%20contamination%20model%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Mengjie%20Gao%2C%20Chao%20Ren%2C%20Zhao%20Robust%20covariance%20and%20scatter%20matrix%20estimation%20under%20hubers%20contamination%20model%202018"
        },
        {
            "id": "Csiszar_1964_a",
            "entry": "Imre Csiszar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85\u2013108, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csiszar%2C%20Imre%20Eine%20informationstheoretische%20ungleichung%20und%20ihre%20anwendung%20auf%20beweis%20der%20ergodizitaet%20von%20markoffschen%20ketten.%20Magyer%20Tud.%20Akad%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csiszar%2C%20Imre%20Eine%20informationstheoretische%20ungleichung%20und%20ihre%20anwendung%20auf%20beweis%20der%20ergodizitaet%20von%20markoffschen%20ketten.%20Magyer%20Tud.%20Akad%201964"
        },
        {
            "id": "Devroye_2012_a",
            "entry": "Luc Devroye and Gabor Lugosi. Combinatorial methods in density estimation. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20Luc%20Lugosi%2C%20Gabor%20Combinatorial%20methods%20in%20density%20estimation%202012"
        },
        {
            "id": "Diakonikolas_et+al_2016_a",
            "entry": "Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 655\u2013664. IEEE, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20Ilias%20Kamath%2C%20Gautam%20Kane%2C%20Daniel%20M.%20Li%2C%20Jerry%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20Ilias%20Kamath%2C%20Gautam%20Kane%2C%20Daniel%20M.%20Li%2C%20Jerry%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016"
        },
        {
            "id": "Diakonikolas_et+al_0000_a",
            "entry": "Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Robust learning of fixed-structure bayesian networks. arXiv preprint arXiv:1606.07384, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07384"
        },
        {
            "id": "Diakonikolas_et+al_2017_a",
            "entry": "Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00893"
        },
        {
            "id": "Diakonikolas_et+al_0000_b",
            "entry": "Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02815"
        },
        {
            "id": "Diakonikolas_et+al_2018_a",
            "entry": "Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1047\u20131060. ACM, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20Ilias%20Kane%2C%20Daniel%20M.%20Stewart%2C%20Alistair%20List-decodable%20robust%20mean%20estimation%20and%20learning%20mixtures%20of%20spherical%20gaussians%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20Ilias%20Kane%2C%20Daniel%20M.%20Stewart%2C%20Alistair%20List-decodable%20robust%20mean%20estimation%20and%20learning%20mixtures%20of%20spherical%20gaussians%202018"
        },
        {
            "id": "Diakonikolas_et+al_0000_c",
            "entry": "Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. arXiv preprint arXiv:1806.00040, 2018c.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00040"
        },
        {
            "id": "Donoho_1992_a",
            "entry": "David L Donoho and Miriam Gasko. Breakdown properties of location estimates based on halfspace depth and projected outlyingness. The Annals of Statistics, 20(4):1803\u20131827, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Gasko%2C%20Miriam%20Breakdown%20properties%20of%20location%20estimates%20based%20on%20halfspace%20depth%20and%20projected%20outlyingness%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Gasko%2C%20Miriam%20Breakdown%20properties%20of%20location%20estimates%20based%20on%20halfspace%20depth%20and%20projected%20outlyingness%201992"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Simon S Du, Sivaraman Balakrishnan, and Aarti Singh. Computationally efficient robust estimation of sparse functionals. arXiv preprint arXiv:1702.07709, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07709"
        },
        {
            "id": "Fang_2017_a",
            "entry": "Kai Wang Fang. Symmetric Multivariate and Related Distributions: 0. Chapman and Hall/CRC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fang%2C%20Kai%20Wang%20Symmetric%20Multivariate%20and%20Related%20Distributions%3A%200.%20Chapman%20and%20Hall/CRC%202017"
        },
        {
            "id": "Gao_2017_a",
            "entry": "Chao Gao. Robust regression via mutivariate regression depth. arXiv preprint arXiv:1702.04656, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04656"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013 256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Huber_1964_a",
            "entry": "Peter J Huber. Robust estimation of a location parameter. The annals of mathematical statistics, 35(1):73\u2013101, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20Peter%20J.%20Robust%20estimation%20of%20a%20location%20parameter.%20The%20annals%20of%20mathematical%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huber%2C%20Peter%20J.%20Robust%20estimation%20of%20a%20location%20parameter.%20The%20annals%20of%20mathematical%201964"
        },
        {
            "id": "Huber_1965_a",
            "entry": "Peter J Huber. A robust version of the probability ratio test. The Annals of Mathematical Statistics, 36(6): 1753\u20131758, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20Peter%20J.%20A%20robust%20version%20of%20the%20probability%20ratio%20test%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huber%2C%20Peter%20J.%20A%20robust%20version%20of%20the%20probability%20ratio%20test%201965"
        },
        {
            "id": "Kothari_et+al_2018_a",
            "entry": "Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1035\u20131046. ACM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kothari%2C%20Pravesh%20K.%20Steinhardt%2C%20Jacob%20Steurer%2C%20David%20Robust%20moment%20estimation%20and%20improved%20clustering%20via%20sum%20of%20squares%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kothari%2C%20Pravesh%20K.%20Steinhardt%2C%20Jacob%20Steurer%2C%20David%20Robust%20moment%20estimation%20and%20improved%20clustering%20via%20sum%20of%20squares%202018"
        },
        {
            "id": "Kurach_et+al_2018_a",
            "entry": "Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The gan landscape: Losses, architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04720"
        },
        {
            "id": "Lai_et+al_2016_a",
            "entry": "Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 665\u2013674. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Kevin%20A.%20Rao%2C%20Anup%20B.%20Vempala%2C%20Santosh%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Kevin%20A.%20Rao%2C%20Anup%20B.%20Vempala%2C%20Santosh%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In Advances in Neural Information Processing Systems, pp. 5545\u20135553, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017"
        },
        {
            "id": "Lucic_et+al_2017_a",
            "entry": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "Mcdiarmid_1989_a",
            "entry": "Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148\u2013188, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDiarmid%2C%20Colin%20On%20the%20method%20of%20bounded%20differences%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDiarmid%2C%20Colin%20On%20the%20method%20of%20bounded%20differences%201989"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Mizera_2002_a",
            "entry": "Ivan Mizera. On depth and deep points: a calculus. The Annals of Statistics, 30(6):1681\u20131736, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mizera%2C%20Ivan%20On%20depth%20and%20deep%20points%3A%20a%20calculus%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mizera%2C%20Ivan%20On%20depth%20and%20deep%20points%3A%20a%20calculus%202002"
        },
        {
            "id": "Mizera_2004_a",
            "entry": "Ivan Mizera and Christine H Muller. Location\u2013scale depth. Journal of the American Statistical Association, 99(468):949\u2013966, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mizera%2C%20Ivan%20Muller%2C%20Christine%20H.%20Location%E2%80%93scale%20depth%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mizera%2C%20Ivan%20Muller%2C%20Christine%20H.%20Location%E2%80%93scale%20depth%202004"
        },
        {
            "id": "Mroueh_et+al_2017_a",
            "entry": "Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching gan. arXiv preprint arXiv:1702.08398, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08398"
        },
        {
            "id": "Nguyen_et+al_2010_a",
            "entry": "XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20XuanLong%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Estimating%20divergence%20functionals%20and%20the%20likelihood%20ratio%20by%20convex%20risk%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20XuanLong%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Estimating%20divergence%20functionals%20and%20the%20likelihood%20ratio%20by%20convex%20risk%20minimization%202010"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Paindaveine_2017_a",
            "entry": "Davy Paindaveine and Germain Van Bever. Halfspace depths for scatter, concentration and shape matrices. arXiv preprint arXiv:1704.06160, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06160"
        },
        {
            "id": "Pollard_2012_a",
            "entry": "David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pollard%2C%20David%20Convergence%20of%20stochastic%20processes%202012"
        },
        {
            "id": "Polyanskiy_2017_a",
            "entry": "Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyanskiy%2C%20Yury%20Wu%2C%20Yihong%20Lecture%20notes%20on%20information%20theory%202017"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Rousseeuw_1999_a",
            "entry": "Peter J Rousseeuw and Mia Hubert. Regression depth. Journal of the American Statistical Association, 94 (446):388\u2013402, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rousseeuw%2C%20Peter%20J.%20Hubert%2C%20Mia%20Regression%20depth%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rousseeuw%2C%20Peter%20J.%20Hubert%2C%20Mia%20Regression%20depth%201999"
        },
        {
            "id": "Rousseeuw_1998_a",
            "entry": "Peter J Rousseeuw and Anja Struyf. Computing location depth and regression depth in higher dimensions. Statistics and Computing, 8(3):193\u2013203, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rousseeuw%2C%20Peter%20J.%20Struyf%2C%20Anja%20Computing%20location%20depth%20and%20regression%20depth%20in%20higher%20dimensions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rousseeuw%2C%20Peter%20J.%20Struyf%2C%20Anja%20Computing%20location%20depth%20and%20regression%20depth%20in%20higher%20dimensions%201998"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Tukey_1975_a",
            "entry": "John W Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress of Mathematicians, Vancouver, 1975, volume 2, pp. 523\u2013531, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tukey%2C%20John%20W.%20Mathematics%20and%20the%20picturing%20of%20data%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tukey%2C%20John%20W.%20Mathematics%20and%20the%20picturing%20of%20data%201975"
        },
        {
            "id": "Van_et+al_1999_a",
            "entry": "Marc van Kreveld, Joseph SB Mitchell, Peter Rousseeuw, Micha Sharir, Jack Snoeyink, and Bettina Speckmann. Efficient algorithms for maximum regression depth. In Proceedings of the fifteenth annual symposium on Computational geometry, pp. 31\u201340. ACM, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Kreveld%2C%20Marc%20Mitchell%2C%20Joseph%20S.B.%20Rousseeuw%2C%20Peter%20Sharir%2C%20Micha%20Efficient%20algorithms%20for%20maximum%20regression%20depth%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Kreveld%2C%20Marc%20Mitchell%2C%20Joseph%20S.B.%20Rousseeuw%2C%20Peter%20Sharir%2C%20Micha%20Efficient%20algorithms%20for%20maximum%20regression%20depth%201999"
        },
        {
            "id": "Yatracos_1985_a",
            "entry": "Yannis G Yatracos. Rates of convergence of minimum distance estimators and kolmogorov\u2019s entropy. The Annals of Statistics, 13(2):768\u2013774, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yatracos%2C%20Yannis%20G.%20Rates%20of%20convergence%20of%20minimum%20distance%20estimators%20and%20kolmogorov%E2%80%99s%20entropy%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yatracos%2C%20Yannis%20G.%20Rates%20of%20convergence%20of%20minimum%20distance%20estimators%20and%20kolmogorov%E2%80%99s%20entropy%201985"
        },
        {
            "id": "Zhang_2002_a",
            "entry": "Jian Zhang. Some extensions of tukey\u2019s depth function. Journal of Multivariate Analysis, 82(1):134\u2013165, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jian%20Some%20extensions%20of%20tukey%E2%80%99s%20depth%20function%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jian%20Some%20extensions%20of%20tukey%E2%80%99s%20depth%20function%202002"
        },
        {
            "id": "activation_1997_a",
            "entry": "activation function. Since our goal is to learn a p-dimensional mean vector, a deep neural network discriminator without any regularization will certainly lead to overfitting. Therefore, it is crucial to design a network class with some appropriate regularizations. Inspired by the work of Bartlett (1997); Bartlett & Mendelson (2002), we consider a network class with 1 regularizations on all layers except for the second last layer with an 2 regularization. With G1H (B) = g(x) = ReLU(vT x): v 1 \u2264 B, a neural network class with l + 1 layers is defined as",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=activation%20function%20Since%20our%20goal%20is%20to%20learn%20a%20pdimensional%20mean%20vector%20a%20deep%20neural%20network%20discriminator%20without%20any%20regularization%20will%20certainly%20lead%20to%20overfitting%20Therefore%20it%20is%20crucial%20to%20design%20a%20network%20class%20with%20some%20appropriate%20regularizations%20Inspired%20by%20the%20work%20of%20Bartlett%201997%20Bartlett%20%20Mendelson%202002%20we%20consider%20a%20network%20class%20with%201%20regularizations%20on%20all%20layers%20except%20for%20the%20second%20last%20layer%20with%20an%202%20regularization%20With%20G1H%20B%20%20gx%20%20ReLUvT%20x%20v%201%20%20B%20a%20neural%20network%20class%20with%20l%20%201%20layers%20is%20defined%20as"
        }
    ]
}
