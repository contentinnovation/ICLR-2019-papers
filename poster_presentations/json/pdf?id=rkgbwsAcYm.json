{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NET-",
        "author": "Xingjian Li, Haoyi Xiong, Hanchao Wang,Yuxuan Rao, Liping Liu+, Jun Huan, \u2020 Big Data Lab, Baidu Reaseach \u2021 University of Illinois at Urbana-Champaign + Big Data Department, Baidu Inc. {lixingjian,xionghaoyi,wanghanchao,lipingliu,huanjun}@baidu.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkgbwsAcYm"
        },
        "abstract": "Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA aligns the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in a supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP . The experiment results show that our method outperforms these baselines with higher accuracy for new tasks."
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "fine tuning",
            "url": "https://en.wikipedia.org/wiki/fine_tuning"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "learning transfer",
            "url": "https://en.wikipedia.org/wiki/learning_transfer"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "DELTA",
            "url": "https://en.wikipedia.org/wiki/DELTA"
        }
    ],
    "abbreviations": {
        "SPAR": "start point as reference",
        "ReLU": "Rectified Linear Units"
    },
    "highlights": [
        "In many real-world applications, deep learning practitioners often have limited number of training instances",
        "The key contributions made in this paper include 1) rather than regularizing the distance between the parameters of source network and target network, DELTA constrains the L2-norm of the difference between their behaviors; and 2) the regularization term used in DELTA incorporates a supervised attention mechanism, which re-weights regularizers according to their performance gain/loss",
        "We find that on some datasets, fine-tuning using L2 normalization does not perform significantly better than directly using the pre-trained model as a feature extractor(L2-F E), while L2-SP outperforms the naive methods without start point as reference",
        "We studied a regularization technique that transfers the behaviors and semantics of the source network to the target one through constraining the difference between the feature maps generated by the convolution layers of source/target networks with attentions",
        "We designed a regularized learning algorithm DELTA that models the difference of feature maps with attentions between networks, where the attention models are obtained through supervised learning",
        "Our extensive experiments evaluated DELTA using several real-world datasets based on commonly used convolutional neural networks"
    ],
    "key_statements": [
        "In many real-world applications, deep learning practitioners often have limited number of training instances",
        "Through paying attention to discriminative parts of feature maps, DELTA characterizes the distance between source/target networks using their outer layer outputs, and incorporates such distance as the regularization term of the loss function",
        "Our work primarily focuses on inductive transfer learning for deep neural networks, where the label space of the target task differs from that of the source task",
        "The key contributions made in this paper include 1) rather than regularizing the distance between the parameters of source network and target network, DELTA constrains the L2-norm of the difference between their behaviors; and 2) the regularization term used in DELTA incorporates a supervised attention mechanism, which re-weights regularizers according to their performance gain/loss",
        "We find that the learning curve of DELTA is smoother than L2-SP and it is not sensitive to the learning rate decay happened at the 6000th iteration when using StepLR",
        "We find that on some datasets, fine-tuning using L2 normalization does not perform significantly better than directly using the pre-trained model as a feature extractor(L2-F E), while L2-SP outperforms the naive methods without start point as reference",
        "We studied a regularization technique that transfers the behaviors and semantics of the source network to the target one through constraining the difference between the feature maps generated by the convolution layers of source/target networks with attentions",
        "We designed a regularized learning algorithm DELTA that models the difference of feature maps with attentions between networks, where the attention models are obtained through supervised learning",
        "Our extensive experiments evaluated DELTA using several real-world datasets based on commonly used convolutional neural networks",
        "The experiment results show that DELTA is able to significantly outperform the state-of-the-art transfer learning methods"
    ],
    "summary": [
        "In many real-world applications, deep learning practitioners often have limited number of training instances.",
        "The generalization capacity could be improved through aligning the behaviors of the outer layers of the target network to the source one, which has been pre-trained using an extremely large dataset.",
        "Through paying attention to discriminative parts of feature maps, DELTA characterizes the distance between source/target networks using their outer layer outputs, and incorporates such distance as the regularization term of the loss function.",
        "The key contributions made in this paper include 1) rather than regularizing the distance between the parameters of source network and target network, DELTA constrains the L2-norm of the difference between their behaviors; and 2) the regularization term used in DELTA incorporates a supervised attention mechanism, which re-weights regularizers according to their performance gain/loss.",
        "The regularization terms used by the existing deep transfer learning approaches neither consider how the network with certain parameters would behave with the new data or leverages the supervision information from the labeled data to improve the transfer performance.",
        "Given a pre-trained parameter \u03c9\u2217 and any input image x, the regularizer \u03a9 (\u03c9, \u03c9\u2217, x) measures the distance between the behaviors of target network with parameter \u03c9 and the source one based on \u03c9\u2217.",
        "DELTA characterizes the outer layer output of the network model z based on input xi and parameter \u03c9 using a set of feature maps, such as FMj(z, \u03c9, xi) and 1 \u2264 j \u2264 N for the N filters in networks.",
        "Given the pre-trained parameter \u03c9\u2217 and an input image xi, DELTA sets the weight of the jth channel, using the gap between the empirical losses of the networks on the labeled sample with and without the jth channel, as follow, Wj(z, \u03c9\u2217, xi, yi) = softmax L(z, yi) \u2212 L(z, yi)",
        "We find that on some datasets, fine-tuning using L2 normalization does not perform significantly better than directly using the pre-trained model as a feature extractor(L2-F E), while L2-SP outperforms the naive methods without SPAR.",
        "We studied a regularization technique that transfers the behaviors and semantics of the source network to the target one through constraining the difference between the feature maps generated by the convolution layers of source/target networks with attentions.",
        "We designed a regularized learning algorithm DELTA that models the difference of feature maps with attentions between networks, where the attention models are obtained through supervised learning.",
        "The experiment results show that DELTA is able to significantly outperform the state-of-the-art transfer learning methods"
    ],
    "headline": "We propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention",
    "reference_links": [
        {
            "id": "Aygun_et+al_2017_a",
            "entry": "Mehmet Aygun, Yusuf Aytar, and Hazim Kemal Ekenel. Exploiting convolution filter patterns for transfer learning. In ICCV Workshops, pp. 2674\u20132680, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aygun%2C%20Mehmet%20Aytar%2C%20Yusuf%20Ekenel%2C%20Hazim%20Kemal%20Exploiting%20convolution%20filter%20patterns%20for%20transfer%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aygun%2C%20Mehmet%20Aytar%2C%20Yusuf%20Ekenel%2C%20Hazim%20Kemal%20Exploiting%20convolution%20filter%20patterns%20for%20transfer%20learning%202017"
        },
        {
            "id": "Bengio_et+al_2007_a",
            "entry": "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pp. 153\u2013160, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007"
        },
        {
            "id": "Caruana_1997_a",
            "entry": "Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Multitask%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Multitask%20learning%201997"
        },
        {
            "id": "Cui_et+al_2018_a",
            "entry": "Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale fine-grained categorization and domain-specific transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4109\u20134118, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20Yin%20Song%2C%20Yang%20Sun%2C%20Chen%20Howard%2C%20Andrew%20Large%20scale%20fine-grained%20categorization%20and%20domain-specific%20transfer%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cui%2C%20Yin%20Song%2C%20Yang%20Sun%2C%20Chen%20Howard%2C%20Andrew%20Large%20scale%20fine-grained%20categorization%20and%20domain-specific%20transfer%20learning%202018"
        },
        {
            "id": "Donahue_et+al_2014_a",
            "entry": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647\u2013655, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014"
        },
        {
            "id": "Ge_2017_a",
            "entry": "Weifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 10\u201319, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Weifeng%20Yu%2C%20Yizhou%20Borrowing%20treasures%20from%20the%20wealthy%3A%20Deep%20transfer%20learning%20through%20selective%20joint%20fine-tuning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Weifeng%20Yu%2C%20Yizhou%20Borrowing%20treasures%20from%20the%20wealthy%3A%20Deep%20transfer%20learning%20through%20selective%20joint%20fine-tuning%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming he, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=he%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=he%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Hinton_et+al_2006_a",
            "entry": "Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee-Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "Huh_2016_a",
            "entry": "Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08614"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. Thirty-fifth International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Xuhong%20Grandvalet%2C%20Yves%20Davoine%2C%20Franck%20Explicit%20inductive%20bias%20for%20transfer%20learning%20with%20convolutional%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Xuhong%20Grandvalet%2C%20Yves%20Davoine%2C%20Franck%20Explicit%20inductive%20bias%20for%20transfer%20learning%20with%20convolutional%20networks%202018"
        },
        {
            "id": "Li_2017_a",
            "entry": "Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202017"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Jiaming Liu, Yali Wang, and Yu Qiao. Sparse deep transfer learning for convolutional neural network. In AAAI, pp. 2245\u20132251, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Jiaming%20Wang%2C%20Yali%20Qiao%2C%20Yu%20Sparse%20deep%20transfer%20learning%20for%20convolutional%20neural%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Jiaming%20Wang%2C%20Yali%20Qiao%2C%20Yu%20Sparse%20deep%20transfer%20learning%20for%20convolutional%20neural%20network%202017"
        },
        {
            "id": "Mnih_et+al_2014_a",
            "entry": "Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances in neural information processing systems, pp. 2204\u20132212, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "Pan_2010_a",
            "entry": "Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Sinno%20Jialin%20Yang%2C%20Qiang%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Sinno%20Jialin%20Yang%2C%20Qiang%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "Romero_et+al_2014_a",
            "entry": "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6550"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048\u20132057, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 21\u201329, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016"
        },
        {
            "id": "Yim_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320\u20133328, 2014. Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. Yinghua Zhang, Yu Zhang, and Qiang Yang. Parameter transfer unit for deep neural networks. arXiv preprint arXiv:1804.08613, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03928"
        }
    ]
}
