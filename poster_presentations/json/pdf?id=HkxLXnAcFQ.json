{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A CLOSER LOOK AT FEW-SHOT CLASSIFICATION",
        "author": "Wei-Yu Chen Carnegie Mellon University weiyuc@andrew.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkxLXnAcFQ"
        },
        "abstract": "Few-shot classification aims to learn a classifier to recognize unseen classes during training with limited labeled examples. While significant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difficult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the miniImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic cross-domain evaluation setting, we show that a baseline method with a standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms."
    },
    "keywords": [
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "new class",
            "url": "https://en.wikipedia.org/wiki/new_class"
        },
        {
            "term": "classification algorithm",
            "url": "https://en.wikipedia.org/wiki/classification_algorithm"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Deep learning models have achieved state-of-the-art performance on visual recognition tasks such as image classification",
        "We address the few-shot classification problem under three scenarios: 1) generic object recognition, 2) fine-grained image classification, and 3) cross-domain adaptation",
        "We use the mini-ImageNet dataset commonly used in evaluating few-shot classification algorithms",
        "We have investigated the limits of the standard evaluation setting for few-shot classification",
        "Through comparing methods on a common ground, our results show that the Baseline++ model is competitive to state of art under standard conditions, and the Baseline model achieves competitive performance with recent state-of-the-art meta-learning algorithms on both CUB and mini-ImageNet benchmark datasets when using a deeper feature backbone",
        "By making our source code publicly available, we believe that community can benefit from the consistent comparative experiments and move forward to tackle the challenge of potential domain shifts in the context of few-shot learning"
    ],
    "key_statements": [
        "Deep learning models have achieved state-of-the-art performance on visual recognition tasks such as image classification",
        "We investigate a practical evaluation setting where base and novel classes are sampled from different domains",
        "We address the few-shot classification problem under three scenarios: 1) generic object recognition, 2) fine-grained image classification, and 3) cross-domain adaptation",
        "We use the mini-ImageNet dataset commonly used in evaluating few-shot classification algorithms",
        "While meta-learning methods learn to learn from the support set during the meta-training stage, they are not able to adapt to novel classes that are too different since all of the base support sets are within the same dataset",
        "We have investigated the limits of the standard evaluation setting for few-shot classification",
        "Through comparing methods on a common ground, our results show that the Baseline++ model is competitive to state of art under standard conditions, and the Baseline model achieves competitive performance with recent state-of-the-art meta-learning algorithms on both CUB and mini-ImageNet benchmark datasets when using a deeper feature backbone",
        "By making our source code publicly available, we believe that community can benefit from the consistent comparative experiments and move forward to tackle the challenge of potential domain shifts in the context of few-shot learning"
    ],
    "summary": [
        "Deep learning models have achieved state-of-the-art performance on visual recognition tasks such as image classification.",
        "Our results show that using a deep backbone shrinks the performance gap between different methods in the setting of limited domain differences between base and novel classes.",
        "To adapt the model to recognize novel classes in the fine-tuning stage, we fix the pre-trained network parameter \u03b8 in our feature extractor f\u03b8 and train a new classifier C(.|Wn) by minimizing Lpred using the few labeled of examples in the novel classes Xn. 3.2 BASELINE++",
        "We consider three distance metric learning based methods (MatchingNet Vinyals et al (2016), ProtoNet Snell et al Meta-training stage",
        "The objective is to train a classification model M that minimizes N-way prediction loss LN\u2212way of the samples in the query set Qb. Here, the classifier M is conditioned on provided support set Sb. By making prediction conditioned on the given support set, a meta-learning method can learn how to learn from limited labeled data through training from a collection of tasks.",
        "The MAML method <a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al (2017</a>) is an initialization based meta-learning algorithm, where each support set is used to adapt the initial model parameters using few gradient updates.",
        "We use a four-layer convolution backbone (Conv-4) with an input size of 84x84 as in <a class=\"ref-link\" id=\"cSnell_et+al_2017_a\" href=\"#rSnell_et+al_2017_a\">Snell et al (2017</a>) and perform 5-way classification for only novel classes during the fine-tuning or meta-testing stage.",
        "Our results show that the Baseline approach under 5-shot setting can be improved by a large margin since previous implementations of the Baseline do not include data augmentation in their training stage, thereby leads to over-fitting.",
        "To better understand the effect, below we discuss how domain differences between base and novel classes impact few-shot classification results.",
        "While meta-learning methods learn to learn from the support set during the meta-training stage, they are not able to adapt to novel classes that are too different since all of the base support sets are within the same dataset.",
        "Through comparing methods on a common ground, our results show that the Baseline++ model is competitive to state of art under standard conditions, and the Baseline model achieves competitive performance with recent state-of-the-art meta-learning algorithms on both CUB and mini-ImageNet benchmark datasets when using a deeper feature backbone.",
        "The Baseline compares favorably against all the evaluated meta-learning algorithms under a realistic scenario where there exists domain shift between the base and novel classes.",
        "By making our source code publicly available, we believe that community can benefit from the consistent comparative experiments and move forward to tackle the challenge of potential domain shifts in the context of few-shot learning"
    ],
    "headline": "We present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the miniImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms",
    "reference_links": [
        {
            "id": "Antoniou_et+al_2018_a",
            "entry": "Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. In Proceedings of the International Conference on Learning Representations Workshops (ICLR Workshops), 2018. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antoniou%2C%20Antreas%20Storkey%2C%20Amos%20Edwards%2C%20Harrison%20Data%20augmentation%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antoniou%2C%20Antreas%20Storkey%2C%20Amos%20Edwards%2C%20Harrison%20Data%20augmentation%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Bertinetto_et+al_2019_a",
            "entry": "Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. In Proceedings of the International Conference on Learning Representations (ICLR), 2019. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertinetto%2C%20Luca%20Henriques%2C%20Joao%20F.%20Torr%2C%20Philip%20H.S.%20Vedaldi%2C%20Andrea%20Meta-learning%20with%20differentiable%20closed-form%20solvers%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertinetto%2C%20Luca%20Henriques%2C%20Joao%20F.%20Torr%2C%20Philip%20H.S.%20Vedaldi%2C%20Andrea%20Meta-learning%20with%20differentiable%20closed-form%20solvers%202019"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andrevan Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017. 13",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "Davies_1979_a",
            "entry": "David L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1979. 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davies%2C%20David%20L.%20Bouldin%2C%20Donald%20W.%20A%20cluster%20separation%20measure%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davies%2C%20David%20L.%20Bouldin%2C%20Donald%20W.%20A%20cluster%20separation%20measure%201979"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Nanqing_2018_a",
            "entry": "Nanqing Dong and Eric P Xing. Domain adaption in one-shot learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018. 3, 12, 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nanqing%20Dong%20and%20Eric%20P%20Xing%20Domain%20adaption%20in%20oneshot%20learning%20In%20Joint%20European%20Conference%20on%20Machine%20Learning%20and%20Knowledge%20Discovery%20in%20Databases%20Springer%202018%203%2012%2013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nanqing%20Dong%20and%20Eric%20P%20Xing%20Domain%20adaption%20in%20oneshot%20learning%20In%20Joint%20European%20Conference%20on%20Machine%20Learning%20and%20Knowledge%20Discovery%20in%20Databases%20Springer%202018%203%2012%2013"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the International Conference on Machine Learning (ICML), 2017. 1, 2, 5, 7, 12",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Finn_et+al_2018_a",
            "entry": "Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Advances in Neural Information Processing Systems (NIPS), 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Xu%2C%20Kelvin%20Levine%2C%20Sergey%20Probabilistic%20model-agnostic%20meta-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Xu%2C%20Kelvin%20Levine%2C%20Sergey%20Probabilistic%20model-agnostic%20meta-learning%202018"
        },
        {
            "id": "Ganin_2015_a",
            "entry": "Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the International Conference on Machine Learning (ICML), 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Lempitsky%2C%20Victor%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Lempitsky%2C%20Victor%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015"
        },
        {
            "id": "Garcia_2018_a",
            "entry": "Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia%2C%20Victor%20Bruna%2C%20Joan%20Few-shot%20learning%20with%20graph%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia%2C%20Victor%20Bruna%2C%20Joan%20Few-shot%20learning%20with%20graph%20neural%20networks%202018"
        },
        {
            "id": "Gidaris_2018_a",
            "entry": "Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 2, 3, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20Spyros%20Komodakis%2C%20Nikos%20Dynamic%20few-shot%20visual%20learning%20without%20forgetting%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20Spyros%20Komodakis%2C%20Nikos%20Dynamic%20few-shot%20visual%20learning%20without%20forgetting%202018"
        },
        {
            "id": "Hariharan_2017_a",
            "entry": "Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating features. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hariharan%2C%20Bharath%20Girshick%2C%20Ross%20Low-shot%20visual%20recognition%20by%20shrinking%20and%20hallucinating%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hariharan%2C%20Bharath%20Girshick%2C%20Ross%20Low-shot%20visual%20recognition%20by%20shrinking%20and%20hallucinating%20features%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hilliard_et+al_2018_a",
            "entry": "Nathan Hilliard, Lawrence Phillips, Scott Howland, Artem Yankov, Courtney D Corley, and Nathan O Hodas. Few-shot learning with metric-agnostic conditional embeddings. arXiv preprint arXiv:1802.04376, 2018. 6",
            "arxiv_url": "https://arxiv.org/pdf/1802.04376"
        },
        {
            "id": "Hsu_et+al_2018_a",
            "entry": "Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across domains and tasks. 2018. 3, 12",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Yen-Chang%20Lv%2C%20Zhaoyang%20Kira%2C%20Zsolt%20Learning%20to%20cluster%20in%20order%20to%20transfer%20across%20domains%20and%20tasks%202018"
        },
        {
            "id": "Hu_et+al_2015_a",
            "entry": "Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Junlin%20Lu%2C%20Jiwen%20Tan%2C%20Yap-Peng%20Deep%20transfer%20metric%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Junlin%20Lu%2C%20Jiwen%20Tan%2C%20Yap-Peng%20Deep%20transfer%20metric%20learning%202015"
        },
        {
            "id": "Koch_et+al_2015_a",
            "entry": "Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In Proceedings of the International Conference on Machine Learning Workshops (ICML Workshops), 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "Lake_et+al_2011_a",
            "entry": "Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Cogsci, 2011. 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011"
        },
        {
            "id": "Mensink_et+al_2012_a",
            "entry": "Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. In Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2012. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensink%2C%20Thomas%20Verbeek%2C%20Jakob%20Perronnin%2C%20Florent%20Csurka%2C%20Gabriela%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mensink%2C%20Thomas%20Verbeek%2C%20Jakob%20Perronnin%2C%20Florent%20Csurka%2C%20Gabriela%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012"
        },
        {
            "id": "George_1995_a",
            "entry": "George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=George%20A%20Miller%20Wordnet%20a%20lexical%20database%20for%20english%20Communications%20of%20the%20ACM%201995%208",
            "oa_query": "https://api.scholarcy.com/oa_version?query=George%20A%20Miller%20Wordnet%20a%20lexical%20database%20for%20english%20Communications%20of%20the%20ACM%201995%208"
        },
        {
            "id": "Motiian_et+al_2017_a",
            "entry": "Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems (NIPS), 2017. 12",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Motiian%2C%20Saeid%20Jones%2C%20Quinn%20Iranmanesh%2C%20Seyed%20Doretto%2C%20Gianfranco%20Few-shot%20adversarial%20domain%20adaptation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Motiian%2C%20Saeid%20Jones%2C%20Quinn%20Iranmanesh%2C%20Seyed%20Doretto%2C%20Gianfranco%20Few-shot%20adversarial%20domain%20adaptation%202017"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Conference on Machine Learning (ICML), 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munkhdalai%2C%20Tsendsuren%20Yu%2C%20Hong%20Meta%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munkhdalai%2C%20Tsendsuren%20Yu%2C%20Hong%20Meta%20networks%202017"
        },
        {
            "id": "Nichol_2018_a",
            "entry": "Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018. 2",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Pan_2010_a",
            "entry": "Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering (TKDE), 2010. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Sinno%20Jialin%20Yang%2C%20Qiang%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Sinno%20Jialin%20Yang%2C%20Qiang%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "Qi_et+al_2018_a",
            "entry": "Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 2, 3, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20Hang%20Brown%2C%20Matthew%20Lowe%2C%20David%20G.%20Low-shot%20learning%20with%20imprinted%20weights%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20Hang%20Brown%2C%20Matthew%20Lowe%2C%20David%20G.%20Low-shot%20learning%20with%20imprinted%20weights%202018"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2017. 1, 2, 6, 12, 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Rusu_et+al_2019_a",
            "entry": "Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2019. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusu%2C%20Andrei%20A.%20Rao%2C%20Dushyant%20Sygnowski%2C%20Jakub%20Vinyals%2C%20Oriol%20Meta-learning%20with%20latent%20embedding%20optimization%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusu%2C%20Andrei%20A.%20Rao%2C%20Dushyant%20Sygnowski%2C%20Jakub%20Vinyals%2C%20Oriol%20Meta-learning%20with%20latent%20embedding%20optimization%202019"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems (NIPS), 2017. 1, 3, 4, 5, 6, 7, 12, 13, 16",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Sung_et+al_2018_a",
            "entry": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 5, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems (NIPS), 2016. 1, 3, 4, 5, 6, 7, 8, 12, 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Wah_et+al_2011_a",
            "entry": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20Catherine%20Branson%2C%20Steve%20Welinder%2C%20Peter%20Perona%2C%20Pietro%20The%20caltech-ucsd%20birds-200-2011%20dataset%202011"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 12",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yu-Xiong%20Girshick%2C%20Ross%20Hebert%2C%20Martial%20Hariharan%2C%20Bharath%20Low-shot%20learning%20from%20imaginary%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yu-Xiong%20Girshick%2C%20Ross%20Hebert%2C%20Martial%20Hariharan%2C%20Bharath%20Low-shot%20learning%20from%20imaginary%20data%202018"
        }
    ]
}
