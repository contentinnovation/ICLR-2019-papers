{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "What do Neural Machine Translation Models Learn about Morphology?",
        "author": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass",
        "date": 2017,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1z-PsR5KX",
            "doi": "10.18653/v1/p17-1080"
        },
        "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "abstract": "Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons."
    },
    "keywords": [
        {
            "term": "Neural machine translation",
            "url": "https://en.wikipedia.org/wiki/Neural_machine_translation"
        },
        {
            "term": "Linguistics",
            "url": "https://en.wikipedia.org/wiki/Linguistics"
        },
        {
            "term": "Computational Linguistics",
            "url": "https://en.wikipedia.org/wiki/Computational_Linguistics"
        },
        {
            "term": "Qatar Computing Research Institute",
            "url": "https://en.wikipedia.org/wiki/Qatar_Computing_Research_Institute"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "United Nations",
            "url": "https://en.wikipedia.org/wiki/United_Nations"
        },
        {
            "term": "Gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/Gaussian_mixture_model"
        }
    ],
    "abbreviations": {
        "NMT": "Neural machine translation",
        "MaxCorr": "maximum correlation",
        "MinCorr": "minimum correlation",
        "LinReg": "linear regression",
        "SVCCA": "Singular vector canonical correlation analysis",
        "GMM": "Gaussian mixture model",
        "UN": "United Nations",
        "charCNN": "character convolutional neural network",
        "QCRI": "Qatar Computing Research Institute"
    },
    "highlights": [
        "Neural machine translation (NMT) systems achieve state-of-the-art results by learning from large amounts of example translations, typically without additional linguistic information",
        "Recent studies have shown that representations learned by Neural machine translation models contain a non-trivial amount of linguistic information on multiple levels: morphological (<a class=\"ref-link\" id=\"cBelinkov_et+al_2017_a\" href=\"#rBelinkov_et+al_2017_a\">Belinkov et al, 2017a</a>; <a class=\"ref-link\" id=\"cDalvi_et+al_2017_a\" href=\"#rDalvi_et+al_2017_a\">Dalvi et al, 2017</a>), syntactic (<a class=\"ref-link\" id=\"cShi_et+al_2016_b\" href=\"#rShi_et+al_2016_b\">Shi et al, 2016b</a>), and semantic (<a class=\"ref-link\" id=\"cHill_et+al_2017_a\" href=\"#rHill_et+al_2017_a\">Hill et al, 2017</a>)",
        "We develop several unsupervised methods for ranking neurons according to their importance to an Neural machine translation model",
        "Comparing erasure with different rankings, we find similar patterns with maximum correlation, minimum correlation, and linear regression: erasing the top ranked 10% (50 neurons) degrades BLEU by 15-20 points, while erasing the bottom 10% neurons only hurts by 2-3 points",
        "We developed unsupervised methods for finding important neurons in Neural machine translation, and evaluated how these neurons impact translation quality",
        "We analyzed several linguistic properties that are captured by individual neurons using quantitative prediction tasks and qualitative visualizations"
    ],
    "key_statements": [
        "Neural machine translation (NMT) systems achieve state-of-the-art results by learning from large amounts of example translations, typically without additional linguistic information",
        "Recent studies have shown that representations learned by Neural machine translation models contain a non-trivial amount of linguistic information on multiple levels: morphological (<a class=\"ref-link\" id=\"cBelinkov_et+al_2017_a\" href=\"#rBelinkov_et+al_2017_a\">Belinkov et al, 2017a</a>; <a class=\"ref-link\" id=\"cDalvi_et+al_2017_a\" href=\"#rDalvi_et+al_2017_a\">Dalvi et al, 2017</a>), syntactic (<a class=\"ref-link\" id=\"cShi_et+al_2016_b\" href=\"#rShi_et+al_2016_b\">Shi et al, 2016b</a>), and semantic (<a class=\"ref-link\" id=\"cHill_et+al_2017_a\" href=\"#rHill_et+al_2017_a\">Hill et al, 2017</a>)",
        "We develop several unsupervised methods for ranking neurons according to their importance to an Neural machine translation model",
        "Our work indicates that not all information is distributed in Neural machine translation models, and that many humaninterpretable grammatical and structural properties are captured by individual neurons",
        "We propose intrinsic unsupervised methods for detecting important neurons based on correlations between independently trained models",
        "Much recent work on analyzing Neural machine translation relies on supervised learning, where Neural machine translation representations are used as features for predicting linguistic annotations",
        "In other cases we found it useful to estimate a Gaussian mixture model (GMM) for predicting a label and measure its prediction quality",
        "Bottom. This confirms our hypothesis that neurons ranked higher by our methods have a larger impact on translation quality",
        "Comparing erasure with different rankings, we find similar patterns with maximum correlation, minimum correlation, and linear regression: erasing the top ranked 10% (50 neurons) degrades BLEU by 15-20 points, while erasing the bottom 10% neurons only hurts by 2-3 points",
        "Other Properties We found many more linguistic properties by visualizing top neurons ranked by our methods, especially with maximum correlation",
        "Our unsupervised methods are flexible enough to find any neurons deemed important by the Neural machine translation model, without constraining the analysis to properties for which we have supervised annotations.\n6 CONTROLLING TRANSLATIONS",
        "We developed unsupervised methods for finding important neurons in Neural machine translation, and evaluated how these neurons impact translation quality",
        "We analyzed several linguistic properties that are captured by individual neurons using quantitative prediction tasks and qualitative visualizations"
    ],
    "summary": [
        "Neural machine translation (NMT) systems achieve state-of-the-art results by learning from large amounts of example translations, typically without additional linguistic information.",
        "We make initial progress towards addressing these limitations by developing unsupervised methods for analyzing the contribution of individual neurons to NMT models.",
        "We develop several unsupervised methods for ranking neurons according to their importance to an NMT model.",
        "Our work indicates that not all information is distributed in NMT models, and that many humaninterpretable grammatical and structural properties are captured by individual neurons.",
        "Modifying the activations of individual neurons allows controlling the translation output according to specified linguistic properties.",
        "We want to verify that neurons ranked highly by the unsupervised methods are important for the NMT models.",
        "In order to study both word and sub-word properties, we use a word representation based on a character convolutional neural network as input to both encoder and decoder, which was shown to learn morphology in language modeling and NMT (<a class=\"ref-link\" id=\"cKim_et+al_2015_a\" href=\"#rKim_et+al_2015_a\">Kim et al, 2015</a>; <a class=\"ref-link\" id=\"cBelinkov_et+al_2017_a\" href=\"#rBelinkov_et+al_2017_a\">Belinkov et al, 2017a</a>).3 While we focus here on recurrent NMT, our approach can be applied to other models like the Transformer (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a>), which we leave for future work.",
        "Neurons that activate on specific tokens or capture position in the sentence are important in some of the methods, as shown in the previous section.",
        "Neurons that detect parentheses were ranked highly in most models by the MaxCorr method, indicating that they capture important patterns in multiple networks.",
        "Our unsupervised methods are flexible enough to find any neurons deemed important by the NMT model, without constraining the analysis to properties for which we have supervised annotations.",
        "We conjecture that if a given neuron matters to the model, we can control the translation in predictable ways by modifying its activations.",
        "3. For every neuron in the encoder, predict the target property on the word aligned to its source word activations using a supervised GMM model.6",
        "Number Table 4a shows translation control results for a number neuron from an English-Spanish model, which activates negatively/positively on plural/singular nouns.",
        "Gender Table 4b shows examples of controlling gender translation for a gender neuron from the same model, which activates negatively/positively on masculine/feminine nouns.",
        "We are able to change the translation from past to present by modifying the activation of the tense neurons from the previous section (Table 3).",
        "We developed unsupervised methods for finding important neurons in NMT, and evaluated how these neurons impact translation quality.",
        "We designed a protocol for controlling translations by modifying neurons that capture desired properties"
    ],
    "headline": "We develop unsupervised methods for discovering important neurons in Neural machine translation models",
    "reference_links": [
        {
            "id": "Adi_et+al_2016_a",
            "entry": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. arXiv preprint arXiv:1608.04207, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04207"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Bau_et+al_2017_a",
            "entry": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017"
        },
        {
            "id": "Belinkov_2019_a",
            "entry": "Yonatan Belinkov and James Glass. Analysis Methods in Neural Language Processing: A Survey. Transactions of the Association for Computational Linguistics (TACL), 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belinkov%2C%20Yonatan%20Glass%2C%20James%20Analysis%20Methods%20in%20Neural%20Language%20Processing%3A%20A%20Survey.%20Transactions%20of%20the%20Association%20for%20Computational%20Linguistics%202019"
        },
        {
            "id": "Belinkov_et+al_2017_a",
            "entry": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do Neural Machine Translation Models Learn about Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 861\u2013872. Association for Computational Linguistics, 2017a. doi: 10.18653/v1/P17-1080. URL http://www.aclweb.org/anthology/P17-1080.",
            "crossref": "https://dx.doi.org/10.18653/v1/P17-1080",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/P17-1080"
        },
        {
            "id": "Belinkov_et+al_2017_b",
            "entry": "Yonatan Belinkov, Llu\u0131s Marquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1\u201310, Taipei, Taiwan, November 2017b. Asian Federation of Natural Language Processing. URL http://www.aclweb.org/anthology/I171001.",
            "url": "http://www.aclweb.org/anthology/I171001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belinkov%2C%20Yonatan%20Marquez%2C%20Llu%C4%B1s%20Sajjad%2C%20Hassan%20Durrani%2C%20Nadir%20Evaluating%20layers%20of%20representation%20in%20neural%20machine%20translation%20on%20part-of-speech%20and%20semantic%20tagging%20tasks%202017-11"
        },
        {
            "id": "Brunner_et+al_2018_a",
            "entry": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, and Michael Weigelt. Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations. arXiv preprint arXiv:1801.06024, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.06024"
        },
        {
            "id": "Dalvi_et+al_2017_a",
            "entry": "Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, and Stephan Vogel. Understanding and improving morphological learning in the neural machine translation decoder. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 142\u2013151, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL http://www.aclweb.org/anthology/I17-1015.",
            "url": "http://www.aclweb.org/anthology/I17-1015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalvi%2C%20Fahim%20Durrani%2C%20Nadir%20Sajjad%2C%20Hassan%20Belinkov%2C%20Yonatan%20Understanding%20and%20improving%20morphological%20learning%20in%20the%20neural%20machine%20translation%20decoder%202017-11"
        },
        {
            "id": "Dalvi_et+al_2019_a",
            "entry": "Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, D. Anthony Bau, and James Glass. What is one Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), January 2019a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalvi%2C%20Fahim%20Durrani%2C%20Nadir%20Sajjad%2C%20Hassan%20Yonatan%20Belinkov%2C%20D.Anthony%20Bau%20What%20is%20one%20Grain%20of%20Sand%20in%20the%20Desert%3F%20Analyzing%20Individual%20Neurons%20in%20Deep%20NLP%20Models%202019-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalvi%2C%20Fahim%20Durrani%2C%20Nadir%20Sajjad%2C%20Hassan%20Yonatan%20Belinkov%2C%20D.Anthony%20Bau%20What%20is%20one%20Grain%20of%20Sand%20in%20the%20Desert%3F%20Analyzing%20Individual%20Neurons%20in%20Deep%20NLP%20Models%202019-01"
        },
        {
            "id": "Dalvi_et+al_2019_b",
            "entry": "Fahim Dalvi, Avery Nortonsmith, D. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, and James Glass. NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Honolulu, USA, January 2019b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalvi%2C%20Fahim%20Avery%20Nortonsmith%2C%20D.Anthony%20Bau%20Belinkov%2C%20Yonatan%20Sajjad%2C%20Hassan%20NeuroX%3A%20A%20Toolkit%20for%20Analyzing%202019-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalvi%2C%20Fahim%20Avery%20Nortonsmith%2C%20D.Anthony%20Bau%20Belinkov%2C%20Yonatan%20Sajjad%2C%20Hassan%20NeuroX%3A%20A%20Toolkit%20for%20Analyzing%202019-01"
        },
        {
            "id": "Chris_2013_a",
            "entry": "Chris Dyer, Victor Chahuneau, and Noah A. Smith. A Simple, Fast, and Effective Reparameterization of IBM Model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 644\u2013648. Association for Computational Linguistics, 2013. URL http://www.aclweb.org/anthology/N13-1073.",
            "url": "http://www.aclweb.org/anthology/N13-1073",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chris%20Dyer%2C%20Victor%20Chahuneau%20Noah%20A.%20Smith.%20A%20Simple%2C%20Fast%2C%20and%20Effective%20Reparameterization%20of%20IBM%20Model%202%202013"
        },
        {
            "id": "Elman_1991_a",
            "entry": "Jeffrey L. Elman. Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2-3):195\u2013225, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elman%2C%20Jeffrey%20L.%20Distributed%20representations%2C%20simple%20recurrent%20networks%2C%20and%20grammatical%20structure%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elman%2C%20Jeffrey%20L.%20Distributed%20representations%2C%20simple%20recurrent%20networks%2C%20and%20grammatical%20structure%201991"
        },
        {
            "id": "Ganesh_et+al_2017_a",
            "entry": "J. Ganesh, Manish Gupta, and Vasudeva Varma. Interpretation of Semantic Tweet Representations. In Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017, ASONAM \u201917, pp. 95\u2013102, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4993-2. doi: 10.1145/3110025.3110083. URL http://doi.acm.org/10.1145/3110025.3110083.",
            "crossref": "https://dx.doi.org/10.1145/3110025.3110083",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3110025.3110083"
        },
        {
            "id": "Gayler_2011_a",
            "entry": "Ross W. Gayler and Simon D. Levy. Compositional connectionism in cognitive science ii: the localist/distributed dimension. Connection Science, 23(2):85\u201389, 2011. doi: 10.1080/ 09540091.2011.587505. URL https://doi.org/10.1080/09540091.2011.587505.",
            "crossref": "https://dx.doi.org/10.1080/09540091.2011.587505",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/09540091.2011.587505"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional Sequence to Sequence Learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1243\u20131252, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/gehring17a.html.",
            "url": "http://proceedings.mlr.press/v70/gehring17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20Sequence%20to%20Sequence%20Learning%202017-08"
        },
        {
            "id": "Giulianelli_et+al_2018_a",
            "entry": "Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 240\u2013248. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/W18-5426.",
            "url": "http://aclweb.org/anthology/W18-5426",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giulianelli%2C%20Mario%20Harding%2C%20Jack%20Mohnert%2C%20Florian%20Hupkes%2C%20Dieuwke%20Under%20the%20Hood%3A%20Using%20Diagnostic%20Classifiers%20to%20Investigate%20and%20Improve%20how%20Language%20Models%20Track%20Agreement%20Information%202018"
        },
        {
            "id": "Hill_et+al_2017_a",
            "entry": "Felix Hill, Kyunghyun Cho, Sebastien Jean, and Yoshua Bengio. The representational geometry of word meanings acquired by neural machine translation models. Machine Translation, 31(1):3\u2013 18, Jun 2017. ISSN 1573-0573. doi: 10.1007/s10590-017-9194-2. URL https://doi.org/10.1007/s10590-017-9194-2.",
            "crossref": "https://dx.doi.org/10.1007/s10590-017-9194-2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10590-017-9194-2"
        },
        {
            "id": "Kadar_et+al_2016_a",
            "entry": "Akos Kadar, Grzegorz Chrupa\u0142a, and Afra Alishahi. Representation of linguistic form and function in recurrent neural networks. arXiv preprint arXiv:1602.08952, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.08952"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Kim_et+al_2015_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware Neural Language Models. arXiv preprint arXiv:1508.06615, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.06615"
        },
        {
            "id": "Kohn_2015_a",
            "entry": "Arne Kohn. What\u2019s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2067\u20132073, Lisbon, Portugal, September 2015. Association for Computational Linguistics. URL http://aclweb.org/anthology/D15-1246.",
            "url": "http://aclweb.org/anthology/D15-1246",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohn%2C%20Arne%20What%E2%80%99s%20in%20an%20Embedding%3F%20Analyzing%20Word%20Embeddings%20through%20Multilingual%20Evaluation%202015-09"
        },
        {
            "id": "Li_et+al_0000_a",
            "entry": "Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding Neural Networks through Representation Erasure. arXiv preprint arXiv:1612.08220, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08220"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent Learning: Do different neural networks learn the same representations? In International Conference for Learning Representations (ICLR), 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20Learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20Learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202016"
        },
        {
            "id": "Lipton_2016_a",
            "entry": "Zachary C Lipton. The Mythos of Model Interpretability. In ICML Workshop on Human Interpretability in Machine Learning (WHI), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lipton%2C%20Zachary%20C.%20The%20Mythos%20of%20Model%20Interpretability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lipton%2C%20Zachary%20C.%20The%20Mythos%20of%20Model%20Interpretability%202016"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1iuQjxCZ.",
            "url": "https://openreview.net/forum?id=r1iuQjxCZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20importance%20of%20single%20directions%20for%20generalization%202018"
        },
        {
            "id": "Qian_et+al_2016_a",
            "entry": "Peng Qian, Xipeng Qiu, and Xuanjing Huang. Analyzing Linguistic Knowledge in Sequential Model of Sentence. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 826\u2013835, Austin, Texas, November 2016a. Association for Computational Linguistics. URL https://aclweb.org/anthology/D16-1079.",
            "url": "https://aclweb.org/anthology/D16-1079",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20Peng%20Qiu%2C%20Xipeng%20Huang%2C%20Xuanjing%20Analyzing%20Linguistic%20Knowledge%20in%20Sequential%20Model%20of%20Sentence%202016-11"
        },
        {
            "id": "Qian_et+al_2016_b",
            "entry": "Peng Qian, Xipeng Qiu, and Xuanjing Huang. Investigating Language Universal and Specific Properties in Word Embeddings. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1478\u20131488, Berlin, Germany, August 2016b. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P16-1140.",
            "url": "http://www.aclweb.org/anthology/P16-1140",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20Peng%20Qiu%2C%20Xipeng%20Huang%2C%20Xuanjing%20Investigating%20Language%20Universal%20and%20Specific%20Properties%20in%20Word%20Embeddings%202016-08"
        },
        {
            "id": "Radford_et+al_2017_a",
            "entry": "Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01444"
        },
        {
            "id": "Raghu_et+al_2017_a",
            "entry": "Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6078\u20136087. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7188-svcca-singular-vectorcanonical-correlation-analysis-for-deep-learning-dynamics-andinterpretability.pdf.",
            "url": "http://papers.nips.cc/paper/7188-svcca-singular-vectorcanonical-correlation-analysis-for-deep-learning-dynamics-andinterpretability.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20SVCCA%3A%20Singular%20Vector%20Canonical%20Correlation%20Analysis%20for%20Deep%20Learning%20Dynamics%202017"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715\u20131725. Association for Computational Linguistics, 2016. doi: 10.18653/v1/P16-1162. URL http://www.aclweb.org/anthology/P161162.",
            "crossref": "https://dx.doi.org/10.18653/v1/P16-1162",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/P16-1162"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Xing Shi, Kevin Knight, and Deniz Yuret. Why Neural Translations are the Right Length. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2278\u20132282. Association for Computational Linguistics, 2016a. doi: 10.18653/v1/D16-1248. URL http://www.aclweb.org/anthology/D16-1248.",
            "crossref": "https://dx.doi.org/10.18653/v1/D16-1248",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/D16-1248"
        },
        {
            "id": "Shi_et+al_2016_b",
            "entry": "Xing Shi, Inkit Padhi, and Kevin Knight. Does String-Based Neural MT Learn Source Syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1526\u20131534, Austin, Texas, November 2016b. Association for Computational Linguistics. URL https://aclweb.org/anthology/D16-1159.",
            "url": "https://aclweb.org/anthology/D16-1159",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Xing%20Padhi%2C%20Inkit%20Knight%2C%20Kevin%20Association%20for%20Computational%20Linguistics%202016-11"
        },
        {
            "id": "Su_et+al_2018_a",
            "entry": "Jinsong Su, Shan Wu, Deyi Xiong, Yaojie Lu, Xianpei Han, and Biao Zhang. Variational Recurrent Neural Machine Translation. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jinsong%20Su%20Shan%20Wu%20Deyi%20Xiong%20Yaojie%20Lu%20Xianpei%20Han%20and%20Biao%20Zhang%20Variational%20Recurrent%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%20ThirtySecond%20AAAI%20Conference%20on%20Artificial%20Intelligence%20AAAI18%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jinsong%20Su%20Shan%20Wu%20Deyi%20Xiong%20Yaojie%20Lu%20Xianpei%20Han%20and%20Biao%20Zhang%20Variational%20Recurrent%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%20ThirtySecond%20AAAI%20Conference%20on%20Artificial%20Intelligence%20AAAI18%202018"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5998\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.",
            "url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81%20ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20All%20you%20Need%20In%20I%20Guyon%20U%20V%20Luxburg%20S%20Bengio%20H%20Wallach%20R%20Fergus%20S%20Vishwanathan%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20pp%2059986008%20Curran%20Associates%20Inc%202017%20URL%20httppapersnipsccpaper7181attentionisallyouneedpdf"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Biao Zhang, Deyi Xiong, jinsong su, Hong Duan, and Min Zhang. Variational Neural Machine",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Biao%20Zhang%20Deyi%20Xiong%20jinsong%20su%20Hong%20Duan%20and%20Min%20Zhang%20Variational%20Neural%20Machine",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Biao%20Zhang%20Deyi%20Xiong%20jinsong%20su%20Hong%20Duan%20and%20Min%20Zhang%20Variational%20Neural%20Machine"
        },
        {
            "id": "Translation_2016_a",
            "entry": "Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 521\u2013530. Association for Computational Linguistics, 2016. doi: 10.18653/v1/ D16-1050. URL http://www.aclweb.org/anthology/D16-1050.",
            "crossref": "https://dx.doi.org/10.18653/v1/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "B. Zhou, A. Khosla, Lapedriza. A., A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR, 2016. Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Revisiting the Importance of Individual Units in CNNs via Ablation. arXiv preprint arXiv:1806.02891, 2018. Micha\u0142 Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations Parallel Corpus v1.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Paris, France, may 2016. European Language Resources Association (ELRA). ISBN 978-2-9517408-9-1.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02891"
        }
    ]
}
