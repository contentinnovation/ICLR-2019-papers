{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "KERNEL RNN LEARNING (KERNL)",
        "author": "Christopher Roth[1],[2], Ingmar Kanitscheider[2],[3], and Ila Fiete[2],[4] [1] Department of Physics, University of Texas at Austin, Austin, TX, 78712 [2] Department of Neuroscience, University of Texas at Austin, Austin, TX, 78712 [3] OpenAI, San Francisco CA, 94110 [4] Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, 02139",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryGfnoC5KQ"
        },
        "abstract": "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks. The approximation replaces a rank-4 gradient learning tensor, which describes how past hidden unit activations affect the current state, by a simple reduced-rank product of a sensitivity weight and a temporal eligibility trace. In this structured approximation motivated by node perturbation, the sensitivity weights and eligibility kernel time scales are themselves learned by applying perturbations. The rule represents another step toward biologically plausible or neurally inspired ML, with lower complexity in terms of relaxed architectural requirements (no symmetric return weights), a smaller memory demand (no unfolding and storage of states over time), and a shorter feedback time. Finally, we show that KeRNL can learn long time-scales more efficiently than BPTT in an online setting."
    },
    "keywords": [
        {
            "term": "backpropagation through time",
            "url": "https://en.wikipedia.org/wiki/backpropagation_through_time"
        },
        {
            "term": "long term",
            "url": "https://en.wikipedia.org/wiki/long_term"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "long time",
            "url": "https://en.wikipedia.org/wiki/Long_Time"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "KeRNL": "Kernel RNN Learning",
        "BPTT": "backpropagation through time",
        "RNNs": "recurrent neural networks"
    },
    "highlights": [
        "Animals and humans excel at learning tasks that involve long-term temporal dependencies",
        "We describe Kernel recurrent neural networks Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to backpropagation through time on long time-dependence tasks",
        "We implemented batch learning with Kernel recurrent neural networks Learning and backpropagation through time on two tasks: the adding problem ( <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber (1997</a>); <a class=\"ref-link\" id=\"cHochreiter_et+al_2001_a\" href=\"#rHochreiter_et+al_2001_a\">Hochreiter et al (2001</a>)) and pixel-by-pixel MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al (1998</a>))",
        "To test how computation time for truncated-backpropagation through time and Kernel recurrent neural networks Learning compare in the online setting, we implemented a dummy recurrent neural networks, where the required tensor operations were performed using a random vector for both the input data and the error signal (Table 6, both algorithms were implemented in Python for uniformity; Details in Appendix A)",
        "Kernel recurrent neural networks Learning is faster than truncated backpropagation through time beyond very short truncation lengths",
        "In this paper we show that Kernel recurrent neural networks Learning, a reduced-rank and forward-running approximation to backpropagation in recurrent neural networks, is able to perform roughly comparably to backpropagation through time on a range of hard recurrent neural networks tasks with long time-dependencies"
    ],
    "key_statements": [
        "Animals and humans excel at learning tasks that involve long-term temporal dependencies",
        "We describe Kernel recurrent neural networks Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to backpropagation through time on long time-dependence tasks",
        "The network does not have to recompute the entire gradient at each update, as the time-scales of the eligibility trace and the sensitivity weight are learned over time.\n2 RELATED WORK",
        "We implemented batch learning with Kernel recurrent neural networks Learning and backpropagation through time on two tasks: the adding problem ( <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber (1997</a>); <a class=\"ref-link\" id=\"cHochreiter_et+al_2001_a\" href=\"#rHochreiter_et+al_2001_a\">Hochreiter et al (2001</a>)) and pixel-by-pixel MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al (1998</a>))",
        "To test how computation time for truncated-backpropagation through time and Kernel recurrent neural networks Learning compare in the online setting, we implemented a dummy recurrent neural networks, where the required tensor operations were performed using a random vector for both the input data and the error signal (Table 6, both algorithms were implemented in Python for uniformity; Details in Appendix A)",
        "Kernel recurrent neural networks Learning is faster than truncated backpropagation through time beyond very short truncation lengths",
        "In this paper we show that Kernel recurrent neural networks Learning, a reduced-rank and forward-running approximation to backpropagation in recurrent neural networks, is able to perform roughly comparably to backpropagation through time on a range of hard recurrent neural networks tasks with long time-dependencies",
        "We present limited evidence that Kernel recurrent neural networks Learning may combat the vanishing gradient problem with tanh units by imposing a prior of long time-dependencies through the eligibility",
        "While we show empirically that Kernel recurrent neural networks Learning performs hill-climbing, there is no guarantee that the gradients computed by Kernel recurrent neural networks Learning are unbiased"
    ],
    "summary": [
        "Animals and humans excel at learning tasks that involve long-term temporal dependencies.",
        "We describe Kernel RNN Learning (KeRNL), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs) that gives competitive performance to BPTT on long time-dependence tasks.",
        "The network does not have to recompute the entire gradient at each update, as the time-scales of the eligibility trace and the sensitivity weight are learned over time.",
        "When these parameters are not learned, KeRNL is still able to perform the task for the shorter 200-length sequence (Table 2) implying that a feedback-alignment-like mechanism (<a class=\"ref-link\" id=\"cLillicrap_et+al_2016_a\" href=\"#rLillicrap_et+al_2016_a\">Lillicrap et al (2016</a>), N\u00f8kland (2016)) may be enabling learning even when the error signals are not delivered along the instantaneous gradients.",
        "Right: Examining the relative importance of learnable parameters in KeRNL: Performance on BPTT and various versions of KeRNL using a tanh RNN after 7 \u00d7 104 minibatches: fixing the sensitivities, \u03b2, while learning the inverse timescales, \u03b3, is better than doing the reverse.",
        "Untruncated BPTT requires information sent back T steps in time for each weight update, the wallclock speed of computation of the gradients at each weight update in online learning scales as T , and the total scaling is of order T 2.",
        "We tested the performance of online KeRNL against UORO, another online learning algorithm, and online BPTT on the An, Bn task, where the network must predict the character in a stream of letters.",
        "To test how computation time for truncated-BPTT and KeRNL compare in the online setting, we implemented a dummy RNN, where the required tensor operations were performed using a random vector for both the input data and the error signal (Table 6, both algorithms were implemented in Python for uniformity; Details in Appendix A).",
        "In this paper we show that KeRNL, a reduced-rank and forward-running approximation to backpropagation in RNNs, is able to perform roughly comparably to BPTT on a range of hard RNN tasks with long time-dependencies.",
        "One may view KeRNL as imposing a strong prior on the way in which neural activity from the past should be assigned credit for current performance, through the choice of the temporal kernels K in the eligibility trace, and the choice of the sensitivity weights \u03b2.",
        "We hope that the present work shows how, with the use of reduced-rank tensor products and eligibility traces, to construct entire nested families of relaxed approximations to gradient learning in RNNs. 4This is not an entirely unreasonable assumption since we assume some static set of weights Wk\u2217i can produce the desirable time-varying trajectory on the task."
    ],
    "headline": "We describe Kernel recurrent neural networks Learning, a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time for training recurrent neural networks that gives competitive performance to backpropagation through time on long time-dependence tasks",
    "reference_links": [
        {
            "id": "Bengio_et+al_2015_a",
            "entry": "Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. Stdp as presynaptic activity times rate of change of postsynaptic activity. arXiv preprint arXiv:1509.05936, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.05936"
        },
        {
            "id": "Fiete_2006_a",
            "entry": "Ila R Fiete and H Sebastian Seung. Gradient learning in spiking neural networks by dynamic perturbation of conductances. Physical review letters, 97(4):048104, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiete%2C%20Ila%20R.%20Seung%2C%20H.Sebastian%20Gradient%20learning%20in%20spiking%20neural%20networks%20by%20dynamic%20perturbation%20of%20conductances%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fiete%2C%20Ila%20R.%20Seung%2C%20H.Sebastian%20Gradient%20learning%20in%20spiking%20neural%20networks%20by%20dynamic%20perturbation%20of%20conductances%202006"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013 256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Gruslys_et+al_2016_a",
            "entry": "Audrunas Gruslys, R\u00e9mi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient backpropagation through time. In Advances in Neural Information Processing Systems, pp. 4125\u20134133, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gruslys%2C%20Audrunas%20Munos%2C%20R%C3%A9mi%20Danihelka%2C%20Ivo%20Lanctot%2C%20Marc%20Memory-efficient%20backpropagation%20through%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gruslys%2C%20Audrunas%20Munos%2C%20R%C3%A9mi%20Danihelka%2C%20Ivo%20Lanctot%2C%20Marc%20Memory-efficient%20backpropagation%20through%20time%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hochreiter_et+al_2001_a",
            "entry": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J\u00fcrgen Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Bengio%2C%20Yoshua%20Frasconi%2C%20Paolo%20Schmidhuber%2C%20J%C3%BCrgen%20Gradient%20flow%20in%20recurrent%20nets%3A%20the%20difficulty%20of%20learning%20long-term%20dependencies%202001"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.05343"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7:13276, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016"
        },
        {
            "id": "N_2016_a",
            "entry": "Arild N\u00f8kland. Direct feedback alignment provides learning in deep neural networks. In Advances in neural information processing systems, pp. 1037\u20131045, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Ollivier_et+al_2015_a",
            "entry": "Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without backtracking. arXiv preprint arXiv:1507.07680, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.07680"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Hinton_1985_a",
            "entry": "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%20Williams%2C%20Ronald%20J.%20Learning%20internal%20representations%20by%20error%20propagation%201985"
        },
        {
            "id": "Scellier_2017_a",
            "entry": "Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energy-based%20models%20and%20backpropagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energy-based%20models%20and%20backpropagation%202017"
        },
        {
            "id": "Tallec_2017_a",
            "entry": "Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. arXiv preprint arXiv:1702.05043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05043"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "St_2011_a",
            "entry": "St\u00e9fan van der Walt, S Chris Colbert, and Gael Varoquaux. The numpy array: a structure for efficient numerical computation. Computing in Science & Engineering, 13(2):22\u201330, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=St%C3%A9fan%20van%20der%20Walt%2C%20S.Chris%20Colbert%20Varoquaux%2C%20Gael%20The%20numpy%20array%3A%20a%20structure%20for%20efficient%20numerical%20computation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=St%C3%A9fan%20van%20der%20Walt%2C%20S.Chris%20Colbert%20Varoquaux%2C%20Gael%20The%20numpy%20array%3A%20a%20structure%20for%20efficient%20numerical%20computation%202011"
        },
        {
            "id": "Werbos_1990_a",
            "entry": "Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10): 1550\u20131560, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20what%20it%20does%20and%20how%20to%20do%20it%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20what%20it%20does%20and%20how%20to%20do%20it%201990"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Williams_1989_a",
            "entry": "Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270\u2013280, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Zipser%2C%20David%20A%20learning%20algorithm%20for%20continually%20running%20fully%20recurrent%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Zipser%2C%20David%20A%20learning%20algorithm%20for%20continually%20running%20fully%20recurrent%20neural%20networks%201989"
        },
        {
            "id": "For_2014_a",
            "entry": "For the adding problem and pixel-by-pixel MNIST, we tested performance by varying \u03b7 and gc over several orders of magnitude: \u03b7 = {1e \u2212 03, 1e \u2212 04, 1e \u2212 05, 1e \u2212 06, 1e \u2212 07}, gc = {1, 10, 100}, using both Adam (Kingma & Ba (2014)) and RMSProp (Tieleman & Hinton (2012)). We then varied \u03b7m = {1e \u2212 03, 1e \u2212",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20adding%20problem%20and%20pixelbypixel%20MNIST%20we%20tested%20performance%20by%20varying%20%CE%B7%20and%20gc%20over%20several%20orders%20of%20magnitude%20%CE%B7%20%201e%20%2003%201e%20%2004%201e%20%2005%201e%20%2006%201e%20%2007%20gc%20%201%2010%20100%20using%20both%20Adam%20Kingma%20%20Ba%202014%20and%20RMSProp%20Tieleman%20%20Hinton%202012%20We%20then%20varied%20%CE%B7m%20%201e%20%2003%201e",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20the%20adding%20problem%20and%20pixelbypixel%20MNIST%20we%20tested%20performance%20by%20varying%20%CE%B7%20and%20gc%20over%20several%20orders%20of%20magnitude%20%CE%B7%20%201e%20%2003%201e%20%2004%201e%20%2005%201e%20%2006%201e%20%2007%20gc%20%201%2010%20100%20using%20both%20Adam%20Kingma%20%20Ba%202014%20and%20RMSProp%20Tieleman%20%20Hinton%202012%20We%20then%20varied%20%CE%B7m%20%201e%20%2003%201e"
        },
        {
            "id": "Besides_2010_a",
            "entry": "Besides the recurrent weights of the IRNN, all other weight matrices were initialized using Xavier initialization (Glorot & Bengio (2010)). We initialized \u03b2 with Xavier initialization for the tanh RNN, and to the identity for the",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Besides%20the%20recurrent%20weights%20of%20the%20IRNN%20all%20other%20weight%20matrices%20were%20initialized%20using%20Xavier%20initialization%20Glorot%20%20Bengio%202010%20We%20initialized%20%CE%B2%20with%20Xavier%20initialization%20for%20the%20tanh%20RNN%20and%20to%20the%20identity%20for%20the",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Besides%20the%20recurrent%20weights%20of%20the%20IRNN%20all%20other%20weight%20matrices%20were%20initialized%20using%20Xavier%20initialization%20Glorot%20%20Bengio%202010%20We%20initialized%20%CE%B2%20with%20Xavier%20initialization%20for%20the%20tanh%20RNN%20and%20to%20the%20identity%20for%20the"
        },
        {
            "id": "Finally_2011_a",
            "entry": "Finally we used the alternative cost function described in footnote 2. We trained on both of these tasks using the Python numpy (Walt et al. (2011)) package.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finally%20we%20used%20the%20alternative%20cost%20function%20described%20in%20footnote%202%20We%20trained%20on%20both%20of%20these%20tasks%20using%20the%20Python%20numpy%20Walt%20et%20al%202011%20package",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finally%20we%20used%20the%20alternative%20cost%20function%20described%20in%20footnote%202%20We%20trained%20on%20both%20of%20these%20tasks%20using%20the%20Python%20numpy%20Walt%20et%20al%202011%20package"
        },
        {
            "id": "For_2011_b",
            "entry": "For the dummy RNN, we used the Python numpy package (Walt et al. (2011)) to perform matrix algebra on a RNN with 100 hidden nodes, 100 input nodes and a tanh nonlinearity. We called \u201cmatmul\u201d for matrix multiplication and \u201ceinsum\u201d for other tensor operations. We used the \u201ctanh\u201d and \u201ccosh\u201d functions to compute the nonlinearity and its derivatives.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20dummy%20RNN%20we%20used%20the%20Python%20numpy%20package%20Walt%20et%20al%202011%20to%20perform%20matrix%20algebra%20on%20a%20RNN%20with%20100%20hidden%20nodes%20100%20input%20nodes%20and%20a%20tanh%20nonlinearity%20We%20called%20matmul%20for%20matrix%20multiplication%20and%20einsum%20for%20other%20tensor%20operations%20We%20used%20the%20tanh%20and%20cosh%20functions%20to%20compute%20the%20nonlinearity%20and%20its%20derivatives",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20the%20dummy%20RNN%20we%20used%20the%20Python%20numpy%20package%20Walt%20et%20al%202011%20to%20perform%20matrix%20algebra%20on%20a%20RNN%20with%20100%20hidden%20nodes%20100%20input%20nodes%20and%20a%20tanh%20nonlinearity%20We%20called%20matmul%20for%20matrix%20multiplication%20and%20einsum%20for%20other%20tensor%20operations%20We%20used%20the%20tanh%20and%20cosh%20functions%20to%20compute%20the%20nonlinearity%20and%20its%20derivatives"
        },
        {
            "id": "In_1997_a",
            "entry": "In this section we describe how to implement KeRNL on an LSTM Hochreiter & Schmidhuber (1997) in more detail. The dynamics of the LSTM (without peepholes) are as follows it = \u03c3(W iixt + W ihht\u22121 + bi)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20section%20we%20describe%20how%20to%20implement%20KeRNL%20on%20an%20LSTM%20Hochreiter%20%20Schmidhuber%201997%20in%20more%20detail%20The%20dynamics%20of%20the%20LSTM%20without%20peepholes%20are%20as%20follows%20it%20%20%CF%83W%20iixt%20%20W%20ihht1%20%20bi"
        },
        {
            "id": "Published_2019_b",
            "entry": "Published as a conference paper at ICLR 2019 where netjt represents the presynaptic input to fjt. The other gradients can be calculated in an analogous manner.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20where%20netjt%20represents%20the%20presynaptic%20input%20to%20fjt%20The%20other%20gradients%20can%20be%20calculated%20in%20an%20analogous%20manner"
        }
    ]
}
