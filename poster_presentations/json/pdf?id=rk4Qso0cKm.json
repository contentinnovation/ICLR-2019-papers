{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AD V-BNN: IMPROVED ADVERSARIAL DEFENSE THROUGH ROBUST BAYESIAN NEURAL NETWORK",
        "author": "Xuanqing Liu, Yao Li, Chongruo Wu,\u2217& Cho-Jui Hsieh, 1: Department of Computer Science, UCLA Los Angeles, CA 90095, UCLA {xqliu,choheish}@cs.ucla.edu 2: Department of Statistics, UC Davis 3: Department of Computer Science, UC Davis Davis, CA 95616, USA {crwu,yaoli}@ucdavis.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rk4Qso0cKm"
        },
        "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al., 2017</a></a>), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural network. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al., 2017</a>) and random self-ensemble (<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al., 2017</a></a>) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet1."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "physical world",
            "url": "https://en.wikipedia.org/wiki/physical_world"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "BNN": "BAYESIAN NEURAL NETWORKS",
        "RSE": "Random Self-Ensemble",
        "ELBO": "evidence lower bound"
    },
    "highlights": [
        "Deep neural networks have demonstrated state-of-the-art performances on many difficult machine learning tasks",
        "Despite the fundamental breakthroughs in various tasks, deep neural networks have been shown to be utterly vulnerable to adversarial attacks (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a>)",
        "Algorithms seek to find such perturbations are denoted as adversarial attacks (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a>; <a class=\"ref-link\" id=\"cCarlini_2017_b\" href=\"#rCarlini_2017_b\">Carlini & Wagner, 2017b</a>; <a class=\"ref-link\" id=\"cPapernot_et+al_2017_a\" href=\"#rPapernot_et+al_2017_a\">Papernot et al, 2017</a>), and some attacks are still effective in the physical world (<a class=\"ref-link\" id=\"cKurakin_et+al_2017_a\" href=\"#rKurakin_et+al_2017_a\">Kurakin et al, 2017</a>; <a class=\"ref-link\" id=\"cEvtimov_et+al_2017_a\" href=\"#rEvtimov_et+al_2017_a\">Evtimov et al, 2017</a>)",
        "We propose a new mini-max formulation to combine adversarial training with BAYESIAN NEURAL NETWORKS, and show the problem can be solved by alternating between projected gradient descent and SGD",
        "Our method is inspired by adversarial training (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>) and BAYESIAN NEURAL NETWORKS (<a class=\"ref-link\" id=\"cBlundell_et+al_2015_a\" href=\"#rBlundell_et+al_2015_a\">Blundell et al, 2015</a>), so these two methods are natural baselines",
        "We find that the Bayesian neural network has no defense functionality, when combined with adversarial training, its robustness against adversarial attack increases significantly. This method can be regarded as a non-trivial combination of BAYESIAN NEURAL NETWORKS and the adversarial training: robust classification relies on the controlled local Lipschitz value, while adversarial training does not generalize this property well enough to the test set; if we train the BAYESIAN NEURAL NETWORKS with adversarial examples, the robustness increases by a large margin"
    ],
    "key_statements": [
        "Deep neural networks have demonstrated state-of-the-art performances on many difficult machine learning tasks",
        "Despite the fundamental breakthroughs in various tasks, deep neural networks have been shown to be utterly vulnerable to adversarial attacks (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a>)",
        "Algorithms seek to find such perturbations are denoted as adversarial attacks (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a>; <a class=\"ref-link\" id=\"cCarlini_2017_b\" href=\"#rCarlini_2017_b\">Carlini & Wagner, 2017b</a>; <a class=\"ref-link\" id=\"cPapernot_et+al_2017_a\" href=\"#rPapernot_et+al_2017_a\">Papernot et al, 2017</a>), and some attacks are still effective in the physical world (<a class=\"ref-link\" id=\"cKurakin_et+al_2017_a\" href=\"#rKurakin_et+al_2017_a\">Kurakin et al, 2017</a>; <a class=\"ref-link\" id=\"cEvtimov_et+al_2017_a\" href=\"#rEvtimov_et+al_2017_a\">Evtimov et al, 2017</a>)",
        "We propose a new mini-max formulation to combine adversarial training with BAYESIAN NEURAL NETWORKS, and show the problem can be solved by alternating between projected gradient descent and SGD",
        "We propose to incorporate adversarial training in Bayesian neural network to achieve better robustness",
        "Our Adv-BAYESIAN NEURAL NETWORKS method trains an arbitrary Bayesian neural network with the min-max robust optimization, which is similar to <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al (2017</a>)",
        "Our method is inspired by adversarial training (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>) and BAYESIAN NEURAL NETWORKS (<a class=\"ref-link\" id=\"cBlundell_et+al_2015_a\" href=\"#rBlundell_et+al_2015_a\">Blundell et al, 2015</a>), so these two methods are natural baselines",
        "We would like to compare our method with Random Self-Ensemble (<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>), another strong defense algorithm relying on randomization",
        "Likewise, {Random Self-Ensemble, Adv-BAYESIAN NEURAL NETWORKS, Adv.Train} constitute the other group, yet the affinity is not very strong (\u03c1 \u2248 0.5\u223c0.6), meaning these three methods are all robust to the black box attack to some extent.\n4.3",
        "We find that the Bayesian neural network has no defense functionality, when combined with adversarial training, its robustness against adversarial attack increases significantly. This method can be regarded as a non-trivial combination of BAYESIAN NEURAL NETWORKS and the adversarial training: robust classification relies on the controlled local Lipschitz value, while adversarial training does not generalize this property well enough to the test set; if we train the BAYESIAN NEURAL NETWORKS with adversarial examples, the robustness increases by a large margin"
    ],
    "summary": [
        "Deep neural networks have demonstrated state-of-the-art performances on many difficult machine learning tasks.",
        "Attack: Most algorithms generate adversarial examples based on the gradient of loss function with respect to the inputs.",
        "It is essentially a data augmentation method, which trains the deep neural networks on adversarial examples until the loss converges.",
        "We combine the idea of adversarial training (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>) with Bayesian neural network, hoping that the randomness in the weights w provides stronger protection for our model.",
        "Our Adv-BNN method trains an arbitrary Bayesian neural network with the min-max robust optimization, which is similar to <a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al (2017</a>).",
        "Both randomized network and adversarial training can be viewed as different ways for controlling local Lipschitz constants of the loss surface around the image manifold, and it is non-trivial to see whether combining those two techniques can lead to better robustness.",
        "We test the performance of our robust Bayesian neural networks (Adv-BNN) with strong baselines on a wide variety of datasets.",
        "From Fig. 2 and Tab. 1 we can observe that BNN itself does not increase the robustness of the model, when combined with the adversarial training method, it dramatically increase the testing accuracy for \u223c10% on a variety of datasets.",
        "It was proposed as a black box attack algorithm: when the attacker has no access to the target model, one can instead train a similar model from scratch, and generate adversarial samples with source model.",
        "Where \u03c1A\u2192B measures the success rate using source model A and target model B, Acc[B] denotes the accuracy of model B without attack, Acc[B|A] means the accuracy under adversarial samples generated by model A.",
        "Likewise, {RSE, Adv-BNN, Adv.Train} constitute the other group, yet the affinity is not very strong (\u03c1 \u2248 0.5\u223c0.6), meaning these three methods are all robust to the black box attack to some extent.",
        "We find that the Bayesian neural network has no defense functionality, when combined with adversarial training, its robustness against adversarial attack increases significantly.",
        "This method can be regarded as a non-trivial combination of BNN and the adversarial training: robust classification relies on the controlled local Lipschitz value, while adversarial training does not generalize this property well enough to the test set; if we train the BNN with adversarial examples, the robustness increases by a large margin.",
        "Our method is still far from the ideal case, and it is still an open problem on what the optimal defense solution will be"
    ],
    "headline": "We present a new algorithm to train a robust neural network against adversarial attacks",
    "reference_links": [
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Blei_et+al_2017_a",
            "entry": "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec \u201917, pp. 3\u201314, New York, NY, USA, 2017a. ACM. ISBN 978-1-4503-52024. doi: 10.1145/3128572.3140444. URL http://doi.acm.org/10.1145/3128572.3140444.",
            "crossref": "https://dx.doi.org/10.1145/3128572.3140444",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3128572.3140444"
        },
        {
            "id": "Carlini_2017_b",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202018"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Engstrom_et+al_2018_a",
            "entry": "Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.10272"
        },
        {
            "id": "Evtimov_et+al_2017_a",
            "entry": "Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "Feinman_et+al_2017_a",
            "entry": "Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00410"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6572.",
            "url": "http://arxiv.org/abs/1412.6572",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Huang_et+al_2015_a",
            "entry": "Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adversary. arXiv preprint arXiv:1511.03034, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03034"
        },
        {
            "id": "Ilyas_et+al_2018_a",
            "entry": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. arXiv preprint arXiv:1804.08598, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08598"
        },
        {
            "id": "Hemant_2005_a",
            "entry": "Hemant Ishwaran, J Sunil Rao, et al. Spike and slab variable selection: frequentist and bayesian strategies. The Annals of Statistics, 33(2):730\u2013773, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hemant%20Ishwaran%2C%20J.Sunil%20Rao%20Spike%20and%20slab%20variable%20selection%3A%20frequentist%20and%20bayesian%20strategies%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hemant%20Ishwaran%2C%20J.Sunil%20Rao%20Spike%20and%20slab%20variable%20selection%3A%20frequentist%20and%20bayesian%20strategies%202005"
        },
        {
            "id": "Kingma_2017_a",
            "entry": "Diederik P Kingma. Variational inference & deep learning: A new synthesis. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Variational%20inference%20%26%20deep%20learning%3A%20A%20new%20synthesis%202017"
        },
        {
            "id": "Kingma_et+al_2015_a",
            "entry": "Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Kurakin_et+al_2017_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In International Conference of Learning Representation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20machine%20learning%20at%20scale%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20machine%20learning%20at%20scale%202017"
        },
        {
            "id": "Kurakin_et+al_2018_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, et al. Adversarial attacks and defences competition. arXiv preprint arXiv:1804.00097, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00097"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Chunyuan Li, Changyou Chen, David E Carlson, and Lawrence Carin. Preconditioned stochastic gradient langevin dynamics for deep neural networks. In AAAI, volume 2, pp. 4, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carlson%2C%20David%20E.%20Carin%2C%20Lawrence%20Preconditioned%20stochastic%20gradient%20langevin%20dynamics%20for%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carlson%2C%20David%20E.%20Carin%2C%20Lawrence%20Preconditioned%20stochastic%20gradient%20langevin%20dynamics%20for%20deep%20neural%20networks%202016"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. arXiv preprint arXiv:1703.02914, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02914"
        },
        {
            "id": "Liao_et+al_2017_a",
            "entry": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against adversarial attacks using high-level representation guided denoiser. arXiv preprint arXiv:1712.02976, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02976"
        },
        {
            "id": "Xuanqing_2018_a",
            "entry": "Xuanqing Liu and Cho-Jui Hsieh. From adversarial training to generative adversarial networks. arXiv preprint arXiv:1807.10454, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.10454"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via random self-ensemble. arXiv preprint arXiv:1712.00673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00673"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "Takeru Miyato and Masanori Koyama. cgans with projection discriminator. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cgans%20with%20projection%20discriminator%202018"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582\u2013597. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Schmidt_et+al_2018_a",
            "entry": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detection. arXiv preprint arXiv:1803.08533, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08533"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Welling_2011_a",
            "entry": "Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681\u2013688, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. arXiv preprint arXiv:1711.01991, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01991"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pp. 2048\u20132057, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "Ye_et+al_2018_a",
            "entry": "Nanyang Ye and Zhanxing Zhu. Bayesian adversarial learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 6892\u20136901. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf.",
            "url": "http://papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nanyang%20Ye%20and%20Zhanxing%20Zhu%20Bayesian%20adversarial%20learning%20In%20S%20Bengio%20H%20Wallach%20H%20Larochelle%20K%20Grauman%20N%20CesaBianchi%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2031%20pp%2068926901%20Curran%20Associates%20Inc%202018%20URL%20httppapersnipsccpaper7921bayesianadversariallearningpdf"
        },
        {
            "id": "Zantedeschi_et+al_2017_a",
            "entry": "Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adversarial attacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 39\u201349. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zantedeschi%2C%20Valentina%20Nicolae%2C%20Maria-Irina%20Rawat%2C%20Ambrish%20Efficient%20defenses%20against%20adversarial%20attacks%202017"
        },
        {
            "id": "Specifically_0000_a",
            "entry": "(2018). Specifically, we derive the algorithm for white box attack to random networks denoted as f (w; ), where w is the (fixed) network parameters and is the random vector. Many random neural networks can be reparameterized to this form, where each forward propagation returns different results. In particular, this framework includes our Adv-BNN model by setting w = (\u03bc, s). Recall the prediction is made through \u201cmajority voting\u201d: y = arg min E f (x; w, ), y.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Specifically%20we%20derive%20the%20algorithm%20for%20white%20box%20attack%20to%20random%20networks%20denoted%20as%20f%20%28w%3B%20%29%2C%20where%20w%20is%20the%20%28fixed%29%20network%20parameters%20and%20is%20the%20random%20vector.%20Many%20random%20neural%20networks%20can%20be%20reparameterized%20to%20this%20form%2C%20where%20each%20forward%20propagation%20returns%20different%20results.%20In%20particular%2C%20this%20framework%20includes%20our%20Adv-BNN%20model%20by%20setting%20w%20%3D%20%28%CE%BC%2C%20s%29.%20Recall%20the%20prediction%20is%20made%20through%20%E2%80%9Cmajority%20voting%E2%80%9D"
        }
    ]
}
