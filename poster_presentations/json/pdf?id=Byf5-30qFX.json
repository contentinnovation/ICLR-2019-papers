{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DHER: HINDSIGHT EXPERIENCE REPLAY FOR DYNAMIC GOALS",
        "author": "Meng Fang,Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu, Tong Zhang Tencent AI Lab",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Byf5-30qFX"
        },
        "abstract": "Dealing with sparse rewards is one of the most important challenges in reinforcement learning (RL), especially when a goal is dynamic (e.g., to grasp a moving object). Hindsight experience replay (HER) has been shown an effective solution to handling sparse rewards with fixed goals. However, it does not account for dynamic goals in its vanilla form and, as a result, even degrades the performance of existing off-policy RL algorithms when the goal is changing over time. In this paper, we present Dynamic Hindsight Experience Replay (DHER), a novel approach for tasks with dynamic goals in the presence of sparse rewards. DHER automatically assembles successful experiences from two relevant failures and can be used to enhance an arbitrary off-policy RL algorithm when the tasks\u2019 goals are dynamic. We evaluate DHER on tasks of robotic manipulation and moving object tracking, and transfer the polices from simulation to physical robots. Extensive comparison and ablation studies demonstrate the superiority of our approach, showing that DHER is a crucial ingredient to enable RL to solve tasks with dynamic goals in manipulation and grid world domains."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "Generative Adversarial Network",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "HER": "Hindsight experience replay",
        "DHER": "Dynamic Hindsight Experience Replay",
        "GAN": "Generative Adversarial Network"
    },
    "highlights": [
        "Deep reinforcement learning has been shown an effective framework for solving a rich repertoire of complex control problems",
        "Assumes the goal is fixed. This assumption impedes the learning of reinforcement learning agents in the environments of moving goals. We address this challenge with a new method, Dynamic Hindsight Experience Replay (DHER), for accomplishing tasks with moving goals",
        "Our results demonstrate that Dynamic Hindsight Experience Replay is clearly better than others for these tasks",
        "We summarize our main contributions as follows: (1) We demonstrate that, both in simulation and real worlds, Dynamic Hindsight Experience Replay succeeds in continuous control with a moving target",
        "Our proposed algorithm called Dynamic Hindsight Experience Replay ( Dynamic Hindsight Experience Replay) is able to address the tasks with sparse rewards and dynamic goals",
        "Our technique can be combined with an arbitrary off-policy reinforcement learning algorithm and we experimentally demonstrated that with DQN and DDPG"
    ],
    "key_statements": [
        "Deep reinforcement learning has been shown an effective framework for solving a rich repertoire of complex control problems",
        "Assumes the goal is fixed. This assumption impedes the learning of reinforcement learning agents in the environments of moving goals. We address this challenge with a new method, Dynamic Hindsight Experience Replay (DHER), for accomplishing tasks with moving goals",
        "Dynamic Hindsight Experience Replay uses replay buffers to allow the agent to learn from a couple of failures by assembling new \u2018experience\u2019 from different episodes",
        "We evaluate our method along with the state-of-the-art baselines on new environments and manipulation tasks, which have sparse rewards and dynamic goals",
        "Our results demonstrate that Dynamic Hindsight Experience Replay is clearly better than others for these tasks",
        "We summarize our main contributions as follows: (1) We demonstrate that, both in simulation and real worlds, Dynamic Hindsight Experience Replay succeeds in continuous control with a moving target",
        "(3) We design and implement a set of new environments and continuous tasks with dynamic goals, which would be of interest to researchers at the intersection of robotics and reinforcement learning.1\n1Our code and environments are available at https://github.com/mengf1/Dynamic Hindsight Experience Replay. 2\n2 RELATED WORK",
        "Our approach assembles successful experience from two failures. Comparing with these methods, which do not consider dynamic goals, our approach uses a series of goals to assemble successful experience",
        "Our method is able to accomplish the tasks with sparse rewards and dynamic goals",
        "Hindsight experience replay is a simple and effective method of manipulating the replay buffer used in off-policy reinforcement learning algorithms that allows it to learn policies more efficiently from sparse rewards",
        "Hindsight experience replay assumes that the mechanism of reaching the new goal helps the agent learn for the original goal.\n3.1",
        "Vanilla DDPG is slightly better than the version with Hindsight experience replay",
        "Comparing Figure 3a and Figure 3b with Figure 3c, we find that Dynamic Hindsight Experience Replay learns faster in Dy-Reaching and Dy-Circling than Dy-Pushing probably because Dy-Reaching and Dy-Circling are easier tasks than Dy-Pushing",
        "We develop a direct extension of Hindsight experience replay, called Hindsight experience replay+, that modifies desired goals at every timestep based on the law of the motion of the food to create successful experience",
        "We introduced a novel technique that assembles successful experience from a couple of failures",
        "Our proposed algorithm called Dynamic Hindsight Experience Replay ( Dynamic Hindsight Experience Replay) is able to address the tasks with sparse rewards and dynamic goals",
        "Our technique can be combined with an arbitrary off-policy reinforcement learning algorithm and we experimentally demonstrated that with DQN and DDPG"
    ],
    "summary": [
        "Deep reinforcement learning has been shown an effective framework for solving a rich repertoire of complex control problems.",
        "Hindsight experience replay (HER) is very effective for improving the performance of off-policy RL algorithms in solving goal-based tasks with sparse rewards (<a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\"><a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\">Andrychowicz et al, 2017</a></a>).",
        "We address this challenge with a new method, Dynamic Hindsight Experience Replay (DHER), for accomplishing tasks with moving goals.",
        "DHER uses replay buffers to allow the agent to learn from a couple of failures by assembling new \u2018experience\u2019 from different episodes.",
        "We evaluate our method along with the state-of-the-art baselines on new environments and manipulation tasks, which have sparse rewards and dynamic goals.",
        "(3) We design and implement a set of new environments and continuous tasks with dynamic goals, which would be of interest to researchers at the intersection of robotics and reinforcement learning.1",
        "Comparing with these methods, which do not consider dynamic goals, our approach uses a series of goals to assemble successful experience.",
        "Our method is able to accomplish the tasks with sparse rewards and dynamic goals.",
        "HER is a simple and effective method of manipulating the replay buffer used in off-policy RL algorithms that allows it to learn policies more efficiently from sparse rewards.",
        "We shall compose a new dynamic goal gdynamic = {sot0bj, sot1bj, \u00b7 \u00b7 \u00b7 , stoTbj } from a failed episode s0, s1, \u00b7 \u00b7 \u00b7 , sT in order to apply HER (<a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\"><a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\">Andrychowicz et al, 2017</a></a>) to our problem setting.",
        "Per the discussion above, this new dynamic goal gdynamic may be inadmissible by the environment, leading to no positive feedback to the agent at all.",
        "We reuse the failed experience from the replay buffer with inverse simulation to create successful rewards for the agent, as shown in Algorithm 1.",
        "Unlike HER, our DHER needs to search all failed experiences to compose a \u201cimagined\u201d goal trajectory.",
        "A good reward shaping may give rise to better performance by carefully tuning it for the task of dynamic goals.",
        "The more failed experiences the agent encounters, the better change our algorithm is able to identify relevant episodes from them for assembling useful dynamic goals.",
        "We develop a direct extension of HER, called HER+, that modifies desired goals at every timestep based on the law of the motion of the food to create successful experience.",
        "Our proposed algorithm called DHER ( Dynamic Hindsight Experience Replay) is able to address the tasks with sparse rewards and dynamic goals.",
        "As far as we know, it is the first time that an agent is allowed to learn from assembled experience from two failures"
    ],
    "headline": "We present Dynamic Hindsight Experience Replay, a novel approach for tasks with dynamic goals in the presence of sparse rewards",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI. Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013. ISSN 1076-9757.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th International Conference on Machine Learning, pp. 41\u201348. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20Jrme%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20Jrme%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "Rocco_et+al_2013_a",
            "entry": "Maurizio Di Rocco, Federico Pecora, Prasanna Kumar Sivakumar, and Alessandro Saffiotti. Configuration planning with multiple dynamic goals. In AAAI Spring Symposium: Designing Intelligent Robots, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocco%2C%20Maurizio%20Di%20Pecora%2C%20Federico%20Sivakumar%2C%20Prasanna%20Kumar%20Saffiotti%2C%20Alessandro%20Configuration%20planning%20with%20multiple%20dynamic%20goals.%20In%20AAAI%20Spring%20Symposium%3A%20Designing%20Intelligent%20Robots%202013"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1329\u20131338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Florensa_et+al_2018_a",
            "entry": "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 1515\u20131528, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Florensa%2C%20Carlos%20Held%2C%20David%20Geng%2C%20Xinyang%20Abbeel%2C%20Pieter%20Automatic%20goal%20generation%20for%20reinforcement%20learning%20agents%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Florensa%2C%20Carlos%20Held%2C%20David%20Geng%2C%20Xinyang%20Abbeel%2C%20Pieter%20Automatic%20goal%20generation%20for%20reinforcement%20learning%20agents%202018"
        },
        {
            "id": "Heess_et+al_2017_a",
            "entry": "Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "Kaelbling_0000_a",
            "entry": "Leslie Pack Kaelbling. Learning to achieve goals. In Proceedings of the 13th International Joint Conference on Artificial Intelligence, pp. 1094\u20131099.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20Leslie%20Pack%20Learning%20to%20achieve%20goals",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20Leslie%20Pack%20Learning%20to%20achieve%20goals"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Lin_1992_a",
            "entry": "Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293\u2013321, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Long-Ji%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Long-Ji%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Narvekar_et+al_2017_a",
            "entry": "Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized curriculum design in reinforcement learning. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, volume 147, pp. 149, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narvekar%2C%20Sanmit%20Sinapov%2C%20Jivko%20Stone%2C%20Peter%20Autonomous%20task%20sequencing%20for%20customized%20curriculum%20design%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narvekar%2C%20Sanmit%20Sinapov%2C%20Jivko%20Stone%2C%20Peter%20Autonomous%20task%20sequencing%20for%20customized%20curriculum%20design%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the 16th International Conference on Machine Learning, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1312\u20131320, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20value%20function%20approximators%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20value%20function%20approximators%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015a"
        },
        {
            "id": "Schaul_et+al_0000_a",
            "entry": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015b.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05952"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05477"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01224"
        }
    ]
}
