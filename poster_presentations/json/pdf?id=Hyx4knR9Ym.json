{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GENERALIZABLE ADVERSARIAL TRAINING VIA SPECTRAL NORMALIZATION",
        "author": "Farzan Farnia, Jesse M. Zhang, David N. Tse Department of Electrical Engineering Stanford University {farnia,jessez,dntse}@stanford.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hyx4knR9Ym"
        },
        "abstract": "Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN\u2019s weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "convolutional layer",
            "url": "https://en.wikipedia.org/wiki/convolutional_layer"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "ERM": "empirical risk minimization",
        "SN": "spectral normalization",
        "GANs": "generative adversarial networks",
        "FGM": "Fast Gradient Method",
        "PGM": "Projected Gradient Method",
        "WRM": "Wasserstein Risk Minimization",
        "FGSM": "fast gradient sign method",
        "MLPs": "multi layer perceptrons",
        "SVM": "support vector machine",
        "CSoI": "Center for Science of Information"
    },
    "highlights": [
        "Despite their impressive performance on many supervised learning tasks, deep neural networks (DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>; Goodfellow et al, 2014b)",
        "We propose using spectral normalization (SN) (<a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al, 2018</a>) as a computationallyefficient and statistically-powerful regularization scheme for adversarial training of deep neural networks",
        "The bounds motivate regularizing these spectral norms in order to limit the deep neural networks\u2019s capacity and improve its generalization performance under adversarial attacks",
        "We prove three separate adversarial generalization error bounds for Fast Gradient Method, Projected Gradient Method, and Wasserstein Risk Minimization attacks",
        "We show that spectral normalization improves both test accuracy and generalization for a variety of adversarial training schemes, datasets, and network architectures",
        "Figure 4 shows how the Fast Gradient Method, Projected Gradient Method, or Wasserstein Risk Minimization adversarial training schemes only slightly delay the rate at which AlexNet fits random labels on CIFAR10, and the generalization gap can be quite large without proper regularization"
    ],
    "key_statements": [
        "Despite their impressive performance on many supervised learning tasks, deep neural networks (DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>; Goodfellow et al, 2014b)",
        "Adversarial attack studies have mainly focused on developing effective attack and defense schemes",
        "Existing defense methods result in considerably better performance compared to standard training methods, the improved performance can still be far below the performance in non-adversarial settings (<a class=\"ref-link\" id=\"cAthalye_et+al_2018_a\" href=\"#rAthalye_et+al_2018_a\">Athalye et al, 2018</a>; <a class=\"ref-link\" id=\"cSchmidt_et+al_2018_a\" href=\"#rSchmidt_et+al_2018_a\">Schmidt et al, 2018</a>)",
        "We propose using spectral normalization (SN) (<a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al, 2018</a>) as a computationallyefficient and statistically-powerful regularization scheme for adversarial training of deep neural networks",
        "We extend the standard notion of margin loss to adversarial settings",
        "We show that spectral normalization can significantly improve the test performance of adversarially-trained deep neural networks",
        "Similar to <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al (2017a</a>), we evaluate a deep neural networks\u2019s generalization performance using its expected margin loss defined for margin parameter \u03b3 > 0 as",
        "To evaluate the adversarial generalization performance, we extend the notion of margin loss defined earlier in (1) to adversarial training settings by defining the adversarial margin loss as",
        "The bounds motivate regularizing these spectral norms in order to limit the deep neural networks\u2019s capacity and improve its generalization performance under adversarial attacks",
        "We prove three separate adversarial generalization error bounds for Fast Gradient Method, Projected Gradient Method, and Wasserstein Risk Minimization attacks",
        "We extend our adversarial generalization analysis to Wasserstein Risk Minimization attacks",
        "We provide an array of empirical experiments to validate both the bounds we derived in Section 3 and our implementation of spectral normalization described in section 4",
        "We show that spectral normalization improves both test accuracy and generalization for a variety of adversarial training schemes, datasets, and network architectures",
        "We provide a comparison of our method to that of <a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al (2018</a>) in Appendix A.1, empirically demonstrating that Miyato et al.\u2019s method does not properly control the spectral norm of convolutional layers, resulting in worse generalization performance",
        "We see that even without further correction, empirical risk minimization training with spectral normalization allows AlexNet to have distinguishable performance between the two datasets. This observation suggests that spectral normalization as a regularization scheme enforces the generalization error bounds shown for spectrally-normalized deep neural networks by <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al (2017</a>) and <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al (2017a</a>)",
        "Figure 4 shows how the Fast Gradient Method, Projected Gradient Method, or Wasserstein Risk Minimization adversarial training schemes only slightly delay the rate at which AlexNet fits random labels on CIFAR10, and the generalization gap can be quite large without proper regularization",
        "We observe that training schemes regularized with spectral normalization result in networks more robust to adversarial attacks",
        "We demonstrate the power of regularization via spectral normalization on several combinations of datasets, network architectures, and adversarial training schemes",
        "We provide Table 2 in the Appendix which shows the proportional increase in training time after introducing spectral normalization with our TensorFlow implementation",
        "While our work does not provide sample complexity lower-bounds, we study the broader function class of deep neural networks where we provide upper-bounds on adversarial generalization error and suggest an explicit regularization scheme for adversarial training over deep neural networks"
    ],
    "summary": [
        "Despite their impressive performance on many supervised learning tasks, deep neural networks (DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>; Goodfellow et al, 2014b).",
        "Figure 1 shows the training and validation performance for AlexNet fit on the CIFAR10 dataset using FGM, PGM, and WRM, resulting in adversarial test accuracy improvements of 9, 11, and 4 percent, respectively.",
        "We use L\u03b3fgm, L\u03b3pgm, and Lw\u03b3 rm to denote the adversarial margin losses with FGM (5), PGM (6), and WRM (7) attacks, respectively.",
        "We provide generalization bounds for DNN classifiers under adversarial attacks in terms of the spectral norms of the trained DNN\u2019s weight matrices.",
        "The bounds motivate regularizing these spectral norms in order to limit the DNN\u2019s capacity and improve its generalization performance under adversarial attacks.",
        "For neural net class Fnn and training loss satisfying Theorem 2\u2019s assumptions, consider a WRM attack with Lagrangian coefficient \u03bb and Euclidean norm \u00b7 2.",
        "We show that spectral normalization improves both test accuracy and generalization for a variety of adversarial training schemes, datasets, and network architectures.",
        "We provide a comparison of our method to that of <a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al (2018</a>) in Appendix A.1, empirically demonstrating that Miyato et al.\u2019s method does not properly control the spectral norm of convolutional layers, resulting in worse generalization performance.",
        "The first column of Figure 3 shows that, as observed by <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al (2017</a></a>), AlexNet trained using ERM generates similar margin distributions for both random and true labels on CIFAR10 unless we normalize the margins appropriately.",
        "This observation suggests that SN as a regularization scheme enforces the generalization error bounds shown for spectrally-normalized DNNs by <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al (2017</a></a>) and <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al (2017a</a>).",
        "Figure 4 shows how the FGM, PGM, or WRM adversarial training schemes only slightly delay the rate at which AlexNet fits random labels on CIFAR10, and the generalization gap can be quite large without proper regularization.",
        "We observe that training schemes regularized with SN result in networks more robust to adversarial attacks.",
        "In a recent related work, <a class=\"ref-link\" id=\"cSchmidt_et+al_2018_a\" href=\"#rSchmidt_et+al_2018_a\">Schmidt et al (2018</a>) numerically shows the wide generalization gap in PGM adversarial training and theoretically establishes lower-bounds on the sample complexity of linear classifiers in Gaussian settings.",
        "While our work does not provide sample complexity lower-bounds, we study the broader function class of DNNs where we provide upper-bounds on adversarial generalization error and suggest an explicit regularization scheme for adversarial training over DNNs. Generalization in deep learning has been a topic of great interest in machine learning (Zhang et al, 2016)."
    ],
    "headline": "We extend the notion of margin loss to adversarial settings and bound the generalization error for deep neural networks trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the deep neural networks\u2019s weight matrices",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "Anthony_2009_a",
            "entry": "Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, pp. 274\u2013283, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athalye%2C%20Anish%20Carlini%2C%20Nicholas%20Wagner%2C%20David%20Obfuscated%20gradients%20give%20a%20false%20sense%20of%20security%3A%20Circumventing%20defenses%20to%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athalye%2C%20Anish%20Carlini%2C%20Nicholas%20Wagner%2C%20David%20Obfuscated%20gradients%20give%20a%20false%20sense%20of%20security%3A%20Circumventing%20defenses%20to%20adversarial%20examples%202018"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Carlini_2016_a",
            "entry": "Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04311"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Cisse_et+al_2017_a",
            "entry": "Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08847"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M Roy. Entropy-sgd optimizes the prior of a pac-bayes bound: Datadependent pac-bayes priors via differential privacy. arXiv preprint arXiv:1712.09376, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09376"
        },
        {
            "id": "Fawzi_et+al_2016_a",
            "entry": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems, pp. 1632\u20131640, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016"
        },
        {
            "id": "Fawzi_et+al_2018_a",
            "entry": "Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08686"
        },
        {
            "id": "Gilmer_et+al_2018_a",
            "entry": "Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goodfellow_et+al_0000_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014b.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Gouk_et+al_2018_a",
            "entry": "Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04368"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "Madry_et+al_2018_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "Mcallester_1999_a",
            "entry": "David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pp. 164\u2013170. ACM, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Pac-bayesian%20model%20averaging%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Pac-bayesian%20model%20averaging%201999"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Dezfooli_et+al_2016_a",
            "entry": "Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In COLT, pp. 1376\u20131401, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015"
        },
        {
            "id": "Neyshabur_et+al_0000_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949\u20135958, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582\u2013597. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Schmidt_et+al_2018_a",
            "entry": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "Sedghi_et+al_2018_a",
            "entry": "Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10408"
        },
        {
            "id": "Sinha_et+al_2018_a",
            "entry": "Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1): 1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tram_et+al_2018_a",
            "entry": "Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018"
        },
        {
            "id": "Tropp_2012_a",
            "entry": "Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012"
        },
        {
            "id": "Tsuzuku_et+al_2018_a",
            "entry": "Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks. arXiv preprint arXiv:1802.04034, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04034"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial examples. arXiv preprint arXiv:1706.03922, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03922"
        },
        {
            "id": "Zhang_et+al_2009_a",
            "entry": "Published as a conference paper at ICLR 2019 Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10(Jul):1485\u20131510, 2009. Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10941"
        }
    ]
}
