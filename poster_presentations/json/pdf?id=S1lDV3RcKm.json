{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MISGAN: LEARNING FROM INCOMPLETE DATA WITH GENERATIVE ADVERSARIAL NETWORKS",
        "author": "Steven Cheng-Xian Li University of Massachusetts Amherst cxl@cs.umass.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lDV3RcKm"
        },
        "journal": "Following Little",
        "abstract": "Generative adversarial networks (GANs) have been shown to provide an effective way to model complex distributions and have obtained impressive results on various challenging tasks. However, typical GANs require fully-observed data during training. In this paper, we present a GAN-based framework for learning from complex, high-dimensional incomplete data. The proposed framework learns a complete data generator along with a mask generator that models the missing data distribution. We further demonstrate how to impute missing data by equipping our framework with an adversarially trained imputer. We evaluate the proposed framework using a series of experiments with several types of missing data processes under the missing completely at random assumption.1"
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "Missing at random",
            "url": "https://en.wikipedia.org/wiki/Missing_at_random"
        },
        {
            "term": "missing completely at random",
            "url": "https://en.wikipedia.org/wiki/missing_completely_at_random"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial networks",
        "MCAR": "missing completely at random",
        "MAR": "Missing at random",
        "NMAR": "Not missing at random",
        "ConvAC": "convolutional arithmetic circuit",
        "GAIN": "Generative Adversarial Imputation Network"
    },
    "highlights": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) provide a powerful modeling framework for learning complex high-dimensional distributions",
        "In Section 2 we described how the discriminator Dx in MisGAN takes as input the masked samples using (3) without knowing what value \u03c4 is used or which entries in the input vector are missing",
        "Baseline We compare MisGAN to a baseline model that is capable of learning from large-scale incomplete data: the generative convolutional arithmetic circuit (ConvAC) (<a class=\"ref-link\" id=\"cSharir_et+al_2016_a\" href=\"#rSharir_et+al_2016_a\">Sharir et al, 2016</a>)",
        "We find that training MisGAN is more stable than training Generative Adversarial Imputation Network across all scenarios in the experiments",
        "We only focus on the missing completely at random case in this work, MisGAN can be extended to cases where the output of the data generator is provided to the mask generator",
        "The question of learnability requires further investigation as the analysis in Section 3 no longer holds due to dependence between the transition matrix and the data distribution under Missing at random and Not missing at random"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) provide a powerful modeling framework for learning complex high-dimensional distributions",
        "We focus on the missing completely at random (MCAR) case, where the two generators are independent of each other and have their own noise distributions pz and p\u03b5",
        "In Section 2 we described how the discriminator Dx in MisGAN takes as input the masked samples using (3) without knowing what value \u03c4 is used or which entries in the input vector are missing",
        "Baseline We compare MisGAN to a baseline model that is capable of learning from large-scale incomplete data: the generative convolutional arithmetic circuit (ConvAC) (<a class=\"ref-link\" id=\"cSharir_et+al_2016_a\" href=\"#rSharir_et+al_2016_a\">Sharir et al, 2016</a>)",
        "Results Figures 3 and 4 show the generated data samples as well as the learned mask samples produced by Conv-MisGAN and FC-MisGAN under the square observation and independent dropout missing mechanisms",
        "Samples generated by convolutional arithmetic circuit are shown in Figure 18 in Appendix G",
        "We quantitatively evaluate Conv-MisGAN, FC-MisGAN and convolutional arithmetic circuit under two missing patterns with missing rates from 10% to 90% with a step of 10%",
        "We found Generative Adversarial Imputation Network training to be quite unstable for the block missingness",
        "We find that training MisGAN is more stable than training Generative Adversarial Imputation Network across all scenarios in the experiments",
        "We only focus on the missing completely at random case in this work, MisGAN can be extended to cases where the output of the data generator is provided to the mask generator",
        "The question of learnability requires further investigation as the analysis in Section 3 no longer holds due to dependence between the transition matrix and the data distribution under Missing at random and Not missing at random"
    ],
    "summary": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) provide a powerful modeling framework for learning complex high-dimensional distributions.",
        "Following Little & Rubin (2014), the generative process for incompletely observed data can be described as shown below where x \u2208 Rn is a complete data vector and m \u2208 {0, 1}n is a binary mask2 that determines which entries in x to reveal: x \u223c p\u03b8(x), m \u223c p\u03c6(m|x).",
        "The masks are used to \u201cmask\u201d generated complete data by filling the indicated missing entries with a constant value.",
        "In Section 2 we described how the discriminator Dx in MisGAN takes as input the masked samples using (3) without knowing what value \u03c4 is used or which entries in the input vector are missing.",
        "That is, estimating the data generating process in the presence of missing data based on the masking scheme used in MisGAN is equivalent to solving the linear system",
        "The following theorem justifies the training objective (6) of MisGAN for the missing data problem.",
        "We show how to impute missing data according to p by equipping MisGAN with an imputer Gi accompanied by a corresponding discriminator Di. The imputer is a function of the incomplete example (x, m) and a random vector \u03c9 drawn from a noise distribution p\u03c9.",
        "Min max Lm(Dm, Gm) + \u03b1Lx(Dx, Gx, Gm), where we use \u03b2 = 0.1 in the experiments when optimizing Gx. This encourages the generated complete data to match the distribution of the imputed real data in addition to having the masked generated data match the masked real data.",
        "Baseline We compare MisGAN to a baseline model that is capable of learning from large-scale incomplete data: the generative convolutional arithmetic circuit (ConvAC) (<a class=\"ref-link\" id=\"cSharir_et+al_2016_a\" href=\"#rSharir_et+al_2016_a\">Sharir et al, 2016</a>).",
        "Results Figures 3 and 4 show the generated data samples as well as the learned mask samples produced by Conv-MisGAN and FC-MisGAN under the square observation and independent dropout missing mechanisms.",
        "We note that if we modify (11) to train the imputer together with the data generator from scratch without the mask generator/discriminator, it fails most of the time for a similar reason to why AmbientGAN fails.",
        "Evaluation of imputation We impute all of the incomplete examples in the training set and use the FID between the imputed data and the original fully-observed data as the evaluation metric.5 Architecture We use convolutional generators and discriminators for MisGAN for all experiments .",
        "This work presents and evaluates a highly flexible framework for learning standard GAN data generators in the presence of missing data."
    ],
    "headline": "We present a Generative adversarial networks-based framework for learning from complex, high-dimensional incomplete data",
    "reference_links": [
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Berthelot_et+al_2017_a",
            "entry": "David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "Bora_et+al_2018_a",
            "entry": "Ashish Bora, Eric Price, and Alexandros G Dimakis. AmbientGAN: Generative models from lossy measurements. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bora%2C%20Ashish%20Price%2C%20Eric%20Dimakis%2C%20Alexandros%20G.%20AmbientGAN%3A%20Generative%20models%20from%20lossy%20measurements%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bora%2C%20Ashish%20Price%2C%20Eric%20Dimakis%2C%20Alexandros%20G.%20AmbientGAN%3A%20Generative%20models%20from%20lossy%20measurements%202018"
        },
        {
            "id": "Bruckstein_et+al_2008_a",
            "entry": "Alfred M Bruckstein, Michael Elad, and Michael Zibulevsky. On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations. IEEE Transactions on Information Theory, 54(11):4813\u20134820, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Zibulevsky%2C%20Michael%20On%20the%20uniqueness%20of%20nonnegative%20sparse%20solutions%20to%20underdetermined%20systems%20of%20equations%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruckstein%2C%20Alfred%20M.%20Elad%2C%20Michael%20Zibulevsky%2C%20Michael%20On%20the%20uniqueness%20of%20nonnegative%20sparse%20solutions%20to%20underdetermined%20systems%20of%20equations%202008"
        },
        {
            "id": "Ghahramani_1994_a",
            "entry": "Zoubin Ghahramani and Michael I Jordan. Supervised learning from incomplete data via an em approach. In Advances in neural information processing systems, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Supervised%20learning%20from%20incomplete%20data%20via%20an%20em%20approach%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Supervised%20learning%20from%20incomplete%20data%20via%20an%20em%20approach%201994"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6629\u20136640, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Little_2014_a",
            "entry": "Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 333. John Wiley & Sons, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Little%2C%20Roderick%20J.A.%20Rubin%2C%20Donald%20B.%20Statistical%20analysis%20with%20missing%20data%2C%20volume%20333%202014"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Mohamed_2016_a",
            "entry": "Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03483"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Kr\u00e4henb\u00fchl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Poon_2011_a",
            "entry": "Hoifung Poon and Pedro Domingos. Sum-Product Networks: a new deep architecture. In Uncertainty in Artificail Intelligence (UAI), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poon%2C%20Hoifung%20Domingos%2C%20Pedro%20Sum-Product%20Networks%3A%20a%20new%20deep%20architecture%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poon%2C%20Hoifung%20Domingos%2C%20Pedro%20Sum-Product%20Networks%3A%20a%20new%20deep%20architecture%202011"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Sharir_et+al_2016_a",
            "entry": "Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tractable generative convolutional arithmetic circuits. arXiv preprint arXiv:1610.04167, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04167"
        },
        {
            "id": "Yeh_et+al_2017_a",
            "entry": "Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G Schwing, Mark Hasegawa-Johnson, and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "Yoon_et+al_2018_a",
            "entry": "Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: Missing data imputation using generative adversarial nets. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoon%2C%20Jinsung%20Jordon%2C%20James%20van%20der%20Schaar%2C%20Mihaela%20GAIN%3A%20Missing%20data%20imputation%20using%20generative%20adversarial%20nets%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoon%2C%20Jinsung%20Jordon%2C%20James%20van%20der%20Schaar%2C%20Mihaela%20GAIN%3A%20Missing%20data%20imputation%20using%20generative%20adversarial%20nets%202018"
        }
    ]
}
