{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE RELATIVISTIC DISCRIMINATOR: A KEY ELEMENT MISSING FROM STANDARD GAN",
        "author": "Alexia Jolicoeur-Martineau Lady Davis Institute MILA, Universit\u00e9 de Montr\u00e9al Montreal, Canada alexia.jolicoeur-martineau@mail.mcgill.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1erHoR5t7"
        },
        "abstract": "In standard generative adversarial network (SGAN), the discriminator D estimates the probability that the input data is real. The generator G is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) SGAN would be more similar to integral probability metric (IPM) GANs. We show that this property can be induced by using a \u201crelativistic discriminator\u201d which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization. The code is freely available on https://github.com/AlexiaJM/RelativisticGAN."
    },
    "keywords": [
        {
            "term": "input data",
            "url": "https://en.wikipedia.org/wiki/input_data"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "standard deviation",
            "url": "https://en.wikipedia.org/wiki/standard_deviation"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "Jensen\u2013Shannon divergence",
            "url": "https://en.wikipedia.org/wiki/Jensen_Shannon_divergence"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial networks",
        "SGAN": "standard GAN",
        "JSD": "Jensen\u2013Shannon divergence",
        "IPMs": "Integral probability metrics",
        "RSGAN": "Relativistic SGAN",
        "RGANs": "Relativistic GANs",
        "FID": "Fr\u00e9chet Inception Distance",
        "LSGAN": "least-squares GAN",
        "RaSGAN": "Relativistic average SGAN",
        "RaLSGAN": "Relativistic average LSGAN",
        "SD": "standard deviation",
        "BN": "batch norm"
    },
    "highlights": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cHong_et+al_2017_a\" href=\"#rHong_et+al_2017_a\"><a class=\"ref-link\" id=\"cHong_et+al_2017_a\" href=\"#rHong_et+al_2017_a\">Hong et al, 2017</a></a>) form a broad class of generative models in which a game is played between two competing neural networks, the discriminator D and the generator G",
        "When D is optimal, the loss function of standard GAN is approximately equal to the Jensen\u2013Shannon divergence (JSD) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>)",
        "We report the Fr\u00e9chet Inception Distance (FID) (<a class=\"ref-link\" id=\"cHeusel_et+al_2017_a\" href=\"#rHeusel_et+al_2017_a\">Heusel et al, 2017</a>), a measure that is generally better correlated with data quality than the Inception Distance (Salimans et al, 2016) (<a class=\"ref-link\" id=\"cBorji_2018_a\" href=\"#rBorji_2018_a\">Borji, 2018</a>); lower Fr\u00e9chet Inception Distance means that the generated images are of better quality",
        "Relativistic average LSGAN performed on par with least-squares GAN, albeit sightly worse",
        "The resulting Fr\u00e9chet Inception Distance of 25.60 is on par with the lowest Fr\u00e9chet Inception Distance obtained for this architecture using spectral normalization, as reported by <a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al (2018</a>) (25.5)",
        "We proposed the relativistic discriminator as a way to fix and improve on standard Generative adversarial networks"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cHong_et+al_2017_a\" href=\"#rHong_et+al_2017_a\">Hong et al, 2017</a>) form a broad class of generative models in which a game is played between two competing neural networks, the discriminator D and the generator G",
        "When D is optimal, the loss function of standard GAN is approximately equal to the Jensen\u2013Shannon divergence (JSD) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>)",
        "We argue that non-Integral probability metrics-based Generative adversarial networks are missing a key ingredient, a relativistic discriminator, which Integral probability metrics-based Generative adversarial networks already possess",
        "We show that a relativistic discriminator is necessary to make Generative adversarial networks analogous to divergence minimization and produce sensible predictions based on the a priori knowledge that half of the samples in the mini-batch are fake",
        "Generative adversarial networks with the saturating loss are such that g1=\u2212f1 and g2=\u2212f2, while Generative adversarial networks with the non-saturating loss are such that g1=f2 and g2=f1",
        "We argue that the key missing property of standard GAN is that the probability of real data being real (D) should decrease as the probability of fake data being real (D) increase",
        "We can generalize this approach to work with any Generative adversarial networks loss function using the following formulation: LDRaGAN = Exr\u223cP f1 C \u2212 Exf \u223cQC ) + Exf \u223cQ [f2 (C \u2212 Exr\u223cPC)] . (15)",
        "We report the Fr\u00e9chet Inception Distance (FID) (<a class=\"ref-link\" id=\"cHeusel_et+al_2017_a\" href=\"#rHeusel_et+al_2017_a\">Heusel et al, 2017</a>), a measure that is generally better correlated with data quality than the Inception Distance (Salimans et al, 2016) (<a class=\"ref-link\" id=\"cBorji_2018_a\" href=\"#rBorji_2018_a\">Borji, 2018</a>); lower Fr\u00e9chet Inception Distance means that the generated images are of better quality",
        "Relativistic average LSGAN performed on par with least-squares GAN, albeit sightly worse",
        "The resulting Fr\u00e9chet Inception Distance of 25.60 is on par with the lowest Fr\u00e9chet Inception Distance obtained for this architecture using spectral normalization, as reported by <a class=\"ref-link\" id=\"cMiyato_et+al_2018_a\" href=\"#rMiyato_et+al_2018_a\">Miyato et al (2018</a>) (25.5)",
        "We proposed the relativistic discriminator as a way to fix and improve on standard Generative adversarial networks",
        "More experiments are required to determine which relativistic Generative adversarial networks loss function is best over a wide-range of datasets and hyper-parameters"
    ],
    "summary": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cHong_et+al_2017_a\" href=\"#rHong_et+al_2017_a\"><a class=\"ref-link\" id=\"cHong_et+al_2017_a\" href=\"#rHong_et+al_2017_a\">Hong et al, 2017</a></a>) form a broad class of generative models in which a game is played between two competing neural networks, the discriminator D and the generator G.",
        "When D is optimal, the loss function of SGAN is approximately equal to the Jensen\u2013Shannon divergence (JSD) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>).",
        "SGAN has two variants for the generator loss functions: saturating and non-saturating.",
        "To improve on SGAN, many GAN variants have been suggested using different loss functions and discriminators that are not classifiers (e.g., LSGAN (<a class=\"ref-link\" id=\"cMao_et+al_2017_a\" href=\"#rMao_et+al_2017_a\">Mao et al, 2017</a>), WGAN (Arjovsky et al, 2017)).",
        "Non-saturating GANs can be thought as optimizing the same loss function, but swapping real data with fake data.",
        "It can be observed that both discriminator and generator loss functions are unbounded and would diverge to \u2212\u221e if optimized directly.",
        "In SGAN, when optimized, we have that the discriminator loss function is equal to the",
        "To bring SGAN closer to divergence minimization, training the generator should not only increase D but",
        "It can be shown that the gradients of the discriminator and generator in IPM-based GANs are respectively:",
        "A simple way to make discriminator relativistic is to sample from real/fake data pairs x = and define it as D(x) = sigmoid(C \u2212 C).",
        "We can interpret this modification in the following way: the discriminator estimates the probability that the given real data is more realistic than a randomly sampled fake data.",
        "The discriminator and generator loss functions of the Relativistic Standard GAN (RSGAN) can be written as: LDRSGAN = \u2212E\u223c(P,Q) [log \u2212 C))] .",
        "We can generalize this approach to work with any GAN loss function using the following formulation: LDRaGAN = Exr\u223cP f1 C \u2212 Exf \u223cQC ) + Exf \u223cQ [f2 (C \u2212 Exr\u223cPC)] .",
        "RasGAN-GP performed poorly; RSGAN-GP performed better than all other loss functions using only one discriminator update per generator update.",
        "These results show that using a relativistic discriminator generally improve data generation quality and that RSGAN works very well in conjunction with gradient penalty to obtain state-of-the-art results.",
        "The CAT dataset is challenging due to its small sample size and high-resolution images; this makes it perfect for testing the stability of different GAN loss functions.",
        "We observe lower minimum FID, maximum FID, mean and standard deviation for RGANs and RaGANs than their non-relativistic counterparts (SGAN, LSGAN, RaLSGAN).",
        "100k generator iterations on the CAT dataset with different GAN loss functions.",
        "More experiments are required to determine which relativistic GAN loss function is best over a wide-range of datasets and hyper-parameters.",
        "Note: A missing number imply that the model did not converge and became stuck in the first few iterations"
    ],
    "headline": "We argue that it should simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) standard GAN would be more similar to integral probability metric Generative adversarial networks",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "Martin Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Borji_2018_a",
            "entry": "Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03446"
        },
        {
            "id": "Fedus_et+al_2017_a",
            "entry": "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease adivergence at every step. arXiv preprint arXiv:1710.08446, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08446"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672\u20132680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.",
            "url": "http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5767\u20135777. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf.",
            "url": "http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "Hong_et+al_2017_a",
            "entry": "Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, and Sungroh Yoon. How generative adversarial nets and its variants work: An overview of gan. arXiv preprint arXiv:1711.05914, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05914"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Jolicoeur-Martineau_2018_a",
            "entry": "Alexia Jolicoeur-Martineau. Gans beyond divergence minimization. arXiv preprint arXiv:1809.02145, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.02145"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04086"
        },
        {
            "id": "Lucic_et+al_2017_a",
            "entry": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Mroueh_et+al_2017_a",
            "entry": "Youssef Mroueh and Tom Sercu. Fisher gan. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2513\u20132523. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6845-fisher-gan.pdf.",
            "url": "http://papers.nips.cc/paper/6845-fisher-gan.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Youssef%20Mroueh%20and%20Tom%20Sercu%20Fisher%20gan%20In%20I%20Guyon%20U%20V%20Luxburg%20S%20Bengio%20H%20Wallach%20R%20Fergus%20S%20Vishwanathan%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20pp%2025132523%20Curran%20Associates%20Inc%202017%20URL%20httppapersnipsccpaper6845fisherganpdf"
        },
        {
            "id": "Mroueh_et+al_2017_b",
            "entry": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev gan. arXiv preprint arXiv:1711.04894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04894"
        },
        {
            "id": "Mueller_1997_a",
            "entry": "Alfred M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%BCller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 271\u2013279. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6066-f-gan-training-generativeneural-samplers-using-variational-divergence-minimization.pdf.",
            "url": "http://papers.nips.cc/paper/6066-f-gan-training-generativeneural-samplers-using-variational-divergence-minimization.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Published_2015_a",
            "entry": "Published as a conference paper at ICLR 2019 Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Chen_2016_a",
            "entry": "Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 2234\u20132242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6125improved-techniques-for-training-gans.pdf. Weiwei Zhang, Jian Sun, and Xiaoou Tang. Cat head detection-how to effectively exploit shape and texture features. In European Conference on Computer Vision, pp.802\u2013816.",
            "url": "http://papers.nips.cc/paper/6125improved-techniques-for-training-gans.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Improved%20techniques%20for%20training%20gans%202016"
        }
    ]
}
