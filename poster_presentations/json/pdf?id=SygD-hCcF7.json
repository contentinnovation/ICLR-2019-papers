{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DIMENSIONALITY REDUCTION FOR REPRESENTING THE KNOWLEDGE OF PROBABILISTIC MODELS",
        "author": "Marc T. Law & Jake Snell University of Toronto, Canada Vector Institute, Canada",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SygD-hCcF7"
        },
        "journal": "Bregman Soft Clustering problem",
        "abstract": "Most deep learning models rely on expressive high-dimensional representations to achieve good performance on tasks such as classification. However, the high dimensionality of these representations makes them difficult to interpret and prone to over-fitting. We propose a simple, intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification. When applying our framework to visualization, our representations more accurately reflect inter-class distances than standard visualization techniques such as t-SNE. We experimentally show that our framework improves generalization performance to unseen categories in zero-shot learning. We also provide a finite sample error upper bound guarantee for the method."
    },
    "keywords": [
        {
            "term": "Intelligence Advanced Research Projects Activity",
            "url": "https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity"
        },
        {
            "term": "Locally Linear Embedding",
            "url": "https://en.wikipedia.org/wiki/Locally_Linear_Embedding"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "t-SNE",
            "url": "https://en.wikipedia.org/wiki/t-SNE"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        }
    ],
    "abbreviations": {
        "DRPR": "Dimensionality Reduction of Probabilistic Representations",
        "BSCP": "Bregman Soft Clustering problem",
        "LLE": "Locally Linear Embedding",
        "CUB": "Caltech-UCSD Birds",
        "MLP": "multilayer perceptrons",
        "IARPA": "Intelligence Advanced Research Projects Activity",
        "DoI/IBC": "Department of Interior/Interior Business Center"
    },
    "highlights": [
        "Dimensionality reduction is an important problem in machine learning tasks to increase classification performance of learned models, improve computational efficiency, or perform visualization",
        "This paper introduces a dimensionality reduction method that represents the relations in a dataset that has probabilistic interpretation",
        "We provide in the supplementary material the visualizations obtained by t-SNE when replacing the 2-norm in the input space by the KL-divergence and the Jensen-Shannon divergence to compare probabilistic representations that Dimensionality Reduction of Probabilistic Representations uses as input",
        "Results: We report the performance of our approach on the test categories of the Caltech-UCSD Birds and Flowers datasets in Tables 3 and 4, respectively",
        "We have proposed a dimensionality reduction approach such that the soft clustering scores obtained in the low-dimensional space are similar to those given as input",
        "We experimentally show that our approach improves generalization performance in zero-shot learning on challenging datasets"
    ],
    "key_statements": [
        "Dimensionality reduction is an important problem in machine learning tasks to increase classification performance of learned models, improve computational efficiency, or perform visualization",
        "<a class=\"ref-link\" id=\"cSchulz_et+al_2015_a\" href=\"#rSchulz_et+al_2015_a\">Schulz et al (2015</a>) proposed a visualization technique that represents examples based on their predicted category only. None of these techniques exploit the fact that deep models have soft probabilistic interpretations",
        "We experimentally demonstrate that the soft probability representations learned by a neural network reveal key structure about the learned model",
        "We propose a dimensionality reduction framework that transforms probability representations into a low-dimensional space for easy visualization",
        "Proposed approach: We propose to exploit as input representations the probability scores generated by a high-dimensional pre-trained model, called the teacher model or target, in order to train a lower-dimensional representation, called the student",
        "We summarize in Section 2.1 the soft clustering algorithm that is used by Dimensionality Reduction of Probabilistic Representations in the low-dimensional space, the algorithm is detailed in <a class=\"ref-link\" id=\"cBanerjee_et+al_2005_a\" href=\"#rBanerjee_et+al_2005_a\">Banerjee et al (2005</a>)",
        "We describe how to learn F so that the soft clustering scores predicted by the Bregman Soft Clustering problem match those of the target",
        "Our di- Figure 1: Dimensionality Reduction of Probabilistic Representations learns low-dimensional representations mensionality reduction problem is given that reflect the uncertainties of a pre-trained classifier",
        "This paper introduces a dimensionality reduction method that represents the relations in a dataset that has probabilistic interpretation",
        "We propose to exploit probability classification scores as input of our dimensionality reduction framework",
        "We provide in the supplementary material the visualizations obtained by t-SNE when replacing the 2-norm in the input space by the KL-divergence and the Jensen-Shannon divergence to compare probabilistic representations that Dimensionality Reduction of Probabilistic Representations uses as input",
        "We report the scores obtained with the logit representations which are not dimensionality reduction representations but provide an estimate of the behavior of the original dataset",
        "Results: We report the performance of our approach on the test categories of the Caltech-UCSD Birds and Flowers datasets in Tables 3 and 4, respectively",
        "We have proposed a dimensionality reduction approach such that the soft clustering scores obtained in the low-dimensional space are similar to those given as input",
        "We experimentally show that our approach improves generalization performance in zero-shot learning on challenging datasets",
        "Real-world applications that can be used with Dimensionality Reduction of Probabilistic Representations include distillation",
        "When the teacher model is too large to store on a device with small memory, the student model which has a smaller memory footprint is used instead"
    ],
    "summary": [
        "Dimensionality reduction is an important problem in machine learning tasks to increase classification performance of learned models, improve computational efficiency, or perform visualization.",
        "By learning low-dimensional representations that match the classification scores of a high-dimensional pre-trained model, our approach takes into account inter-class similarities and generalizes better to unseen categories than standard approaches.",
        "Our approach learns low-dimensional student representations of examples such that, when applying a specific soft clustering algorithm on the student representations, the predicted clustering scores are similar to the target probability scores.",
        "(2) By exploiting the probability representations generated by a pre-trained model, our approach reflects the learned semantic structure better than standard visualization approaches.",
        "Our di- Figure 1: DRPR learns low-dimensional representations mensionality reduction problem is given that reflect the uncertainties of a pre-trained classifier.",
        "DRPR considers that each class c \u2208 {1, \u00b7 \u00b7 \u00b7 , k} is represented by one cluster prototype \u03bcc \u2208 Rd. Terminology: In our experiments, the target is the assignment matrix Y \u2208 Yn\u00d7k that contains the probability scores generated by a pre-trained neural network.",
        "The goal is to learn a representation such that applying a hard clustering algorithm on the training dataset will return the desired assignment matrix Y .",
        "Dimensionality reduction: Learning models by exploiting soft probability scores predicted by another pre-trained model as supervision was proposed in <a class=\"ref-link\" id=\"cBa_2014_a\" href=\"#rBa_2014_a\">Ba & Caruana (2014</a>) for classification.",
        "The first learns low-dimensional representations for visualization to better interpret pre-trained deep models.",
        "The second experiment exploits the probability scores generated by a pre-trained classifier in the zero-shot learning context; these probability scores are used as supervision to improve performance on novel categories.",
        "To generate target soft clustering probability scores in the 3D space, we compute the relative distances of the examples to the different cluster centers and normalize them to obtain responsibilities as done in Eq (3).",
        "We evaluate our approach on the test sets of the MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>), STL (<a class=\"ref-link\" id=\"cCoates_et+al_2011_a\" href=\"#rCoates_et+al_2011_a\">Coates et al, 2011</a>), CIFAR 10 and CIFAR 100 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton, 2009</a>) datasets with pre-trained models that are publicly available and optimized for cross entropy.2 The dimensionality of the high-dimensional representations is equal to the number of categories in the respective datasets (i.e. 10 except for CIFAR 100 that contains 100 categories).",
        "Both of them take as input the image and category representations used to create the target soft assignment matrix when its probability scores Y2 are used as supervision, and the representations learned by <a class=\"ref-link\" id=\"cSnell_et+al_2017_a\" href=\"#rSnell_et+al_2017_a\">Snell et al (2017</a>) otherwise).",
        "We have proposed a dimensionality reduction approach such that the soft clustering scores obtained in the low-dimensional space are similar to those given as input.",
        "Future work includes applying our approach to the task of distillation in the standard classification task where training categories are test categories"
    ],
    "headline": "Intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification",
    "reference_links": [
        {
            "id": "Akata_et+al_2015_a",
            "entry": "Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 2927\u20132936. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akata%2C%20Zeynep%20Reed%2C%20Scott%20Walter%2C%20Daniel%20Lee%2C%20Honglak%20Evaluation%20of%20output%20embeddings%20for%20fine-grained%20image%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akata%2C%20Zeynep%20Reed%2C%20Scott%20Walter%2C%20Daniel%20Lee%2C%20Honglak%20Evaluation%20of%20output%20embeddings%20for%20fine-grained%20image%20classification%202015"
        },
        {
            "id": "Ba_2014_a",
            "entry": "Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information Processing Systems (NeurIPS - 27), pp. 2654\u20132662. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "Banerjee_et+al_2005_a",
            "entry": "Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. Journal of Machine Learning Research (JMLR), 6(Oct):1705\u20131749, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20with%20Bregman%20divergences%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20with%20Bregman%20divergences%202005"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research (JMLR), 3:463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Bartlett_et+al_2005_a",
            "entry": "Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexities. The Annals of Statistics, 33(4):1497\u20131537, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20Rademacher%20complexities%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20Rademacher%20complexities%202005"
        },
        {
            "id": "Bregman_1967_a",
            "entry": "Lev M Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR computational mathematics and mathematical physics, 7(3):200\u2013217, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bregman%2C%20Lev%20M.%20The%20relaxation%20method%20of%20finding%20the%20common%20point%20of%20convex%20sets%20and%20its%20application%20to%20the%20solution%20of%20problems%20in%20convex%20programming.%20USSR%20computational%20mathematics%20and%20mathematical%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bregman%2C%20Lev%20M.%20The%20relaxation%20method%20of%20finding%20the%20common%20point%20of%20convex%20sets%20and%20its%20application%20to%20the%20solution%20of%20problems%20in%20convex%20programming.%20USSR%20computational%20mathematics%20and%20mathematical%201967"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Doukhan_1994_a",
            "entry": "Paul Doukhan. Mixing: Properties and Examples, volume 85 of Lecture Notes in Statistics. SpringerVerlag, Berlin, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doukhan%2C%20Paul%20Mixing%3A%20Properties%20and%20Examples%2C%20volume%2085%20of%20Lecture%20Notes%20in%20Statistics%201994"
        },
        {
            "id": "Farahmand_2012_a",
            "entry": "Amir-massoud Farahmand and Csaba Szepesv\u00e1ri. Regularized least-squares regression: Learning from a \u03b2-mixing sequence. Journal of Statistical Planning and Inference, 142(2):493 \u2013 505, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Szepesv%C3%A1ri%2C%20Csaba%20Regularized%20least-squares%20regression%3A%20Learning%20from%20a%20%CE%B2-mixing%20sequence%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Szepesv%C3%A1ri%2C%20Csaba%20Regularized%20least-squares%20regression%3A%20Learning%20from%20a%20%CE%B2-mixing%20sequence%202012"
        },
        {
            "id": "Gin_2015_a",
            "entry": "Evarist Gin\u00e9 and Richard Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge University Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gin%C3%A9%2C%20Evarist%20Nickl%2C%20Richard%20Mathematical%20Foundations%20of%20Infinite-Dimensional%20Statistical%20Models%202015"
        },
        {
            "id": "Gyoerfi_et+al_2002_a",
            "entry": "L\u00e1szl\u00f3 Gy\u00f6rfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer Verlag, New York, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gy%C3%B6rfi%2C%20L%C3%A1szl%C3%B3%20Kohler%2C%20Michael%20Krzyzak%2C%20Adam%20Walk%2C%20Harro%20A%20Distribution-Free%20Theory%20of%20Nonparametric%20Regression%202002"
        },
        {
            "id": "Hinton_2006_a",
            "entry": "Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lajugie_et+al_2014_a",
            "entry": "R\u00e9mi Lajugie, Sylvain Arlot, and Francis Bach. Large-margin metric learning for constrained partitioning problems. In ICML, pp. 297\u2013305, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lajugie%2C%20R%C3%A9mi%20Arlot%2C%20Sylvain%20Bach%2C%20Francis%20Large-margin%20metric%20learning%20for%20constrained%20partitioning%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lajugie%2C%20R%C3%A9mi%20Arlot%2C%20Sylvain%20Bach%2C%20Francis%20Large-margin%20metric%20learning%20for%20constrained%20partitioning%20problems%202014"
        },
        {
            "id": "Law_et+al_2016_a",
            "entry": "Marc T. Law, Yaoliang Yu, Matthieu Cord, and Eric P. Xing. Closed-form training of Mahalanobis distance for supervised clustering. In CVPR. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Law%2C%20Marc%20T.%20Yu%2C%20Yaoliang%20Cord%2C%20Matthieu%20Xing%2C%20Eric%20P.%20Closed-form%20training%20of%20Mahalanobis%20distance%20for%20supervised%20clustering.%20In%20CVPR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Law%2C%20Marc%20T.%20Yu%2C%20Yaoliang%20Cord%2C%20Matthieu%20Xing%2C%20Eric%20P.%20Closed-form%20training%20of%20Mahalanobis%20distance%20for%20supervised%20clustering.%20In%20CVPR%202016"
        },
        {
            "id": "Law_et+al_1985_a",
            "entry": "Marc T. Law, Raquel Urtasun, and Richard S. Zemel. Deep spectral clustering learning. In International Conference on Machine Learning, pp. 1985\u20131994, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Law%2C%20Marc%20T.%20Urtasun%2C%20Raquel%20Zemel%2C%20Richard%20S.%20Deep%20spectral%20clustering%20learning%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Law%2C%20Marc%20T.%20Urtasun%2C%20Raquel%20Zemel%2C%20Richard%20S.%20Deep%20spectral%20clustering%20learning%201985"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Long_et+al_2015_a",
            "entry": "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u20133440, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "Meir_2000_a",
            "entry": "Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning, 39(1):5\u201334, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meir%2C%20Ron%20Nonparametric%20time%20series%20prediction%20through%20adaptive%20model%20selection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meir%2C%20Ron%20Nonparametric%20time%20series%20prediction%20through%20adaptive%20model%20selection%202000"
        },
        {
            "id": "Mensink_et+al_2012_a",
            "entry": "Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. Computer Vision\u2013ECCV 2012, pp. 488\u2013501, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensink%2C%20Thomas%20Verbeek%2C%20Jakob%20Perronnin%2C%20Florent%20Csurka%2C%20Gabriela%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mensink%2C%20Thomas%20Verbeek%2C%20Jakob%20Perronnin%2C%20Florent%20Csurka%2C%20Gabriela%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012"
        },
        {
            "id": "Mohri_2009_a",
            "entry": "Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In Advances in Neural Information Processing Systems 21, pp. 1097\u20131104. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Rademacher%20complexity%20bounds%20for%20non-i.i.d.%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Rademacher%20complexity%20bounds%20for%20non-i.i.d.%20processes%202009"
        },
        {
            "id": "Mohri_2010_a",
            "entry": "Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary \u03c6-mixing and \u03b2-mixing processes. Journal of Machine Learning Research (JMLR), 11:789\u2013814, 2010. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Stability%20bounds%20for%20stationary%20%CF%86-mixing%20and%20%CE%B2-mixing%20processes%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Stability%20bounds%20for%20stationary%20%CF%86-mixing%20and%20%CE%B2-mixing%20processes%202010"
        },
        {
            "id": "Nilsback_2008_a",
            "entry": "Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilsback%2C%20Maria-Elena%20Zisserman%2C%20Andrew%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilsback%2C%20Maria-Elena%20Zisserman%2C%20Andrew%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%202008-12"
        },
        {
            "id": "Oquab_et+al_2014_a",
            "entry": "Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1717\u20131724. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oquab%2C%20Maxime%20Bottou%2C%20Leon%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Learning%20and%20transferring%20mid-level%20image%20representations%20using%20convolutional%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oquab%2C%20Maxime%20Bottou%2C%20Leon%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Learning%20and%20transferring%20mid-level%20image%20representations%20using%20convolutional%20neural%20networks%202014"
        },
        {
            "id": "Reed_et+al_2016_a",
            "entry": "Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 49\u201358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Lee%2C%20Honglak%20Schiele%2C%20Bernt%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20Akata%2C%20Zeynep%20Lee%2C%20Honglak%20Schiele%2C%20Bernt%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%202016"
        },
        {
            "id": "Roweis_2000_a",
            "entry": "Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roweis%2C%20Sam%20T.%20Saul%2C%20Lawrence%20K.%20Nonlinear%20dimensionality%20reduction%20by%20locally%20linear%20embedding%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roweis%2C%20Sam%20T.%20Saul%2C%20Lawrence%20K.%20Nonlinear%20dimensionality%20reduction%20by%20locally%20linear%20embedding%202000"
        },
        {
            "id": "Salakhutdinov_2007_a",
            "entry": "Ruslan Salakhutdinov and Geoff Hinton. Learning a nonlinear embedding by preserving class neighbourhood structure. In Artificial Intelligence and Statistics, pp. 412\u2013419, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20Ruslan%20Hinton%2C%20Geoff%20Learning%20a%20nonlinear%20embedding%20by%20preserving%20class%20neighbourhood%20structure%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20Ruslan%20Hinton%2C%20Geoff%20Learning%20a%20nonlinear%20embedding%20by%20preserving%20class%20neighbourhood%20structure%202007"
        },
        {
            "id": "Schulz_et+al_2015_a",
            "entry": "Alexander Schulz, Andrej Gisbrecht, and Barbara Hammer. Using discriminative dimensionality reduction to visualize classifiers. Neural Processing Letters, 42(1):27\u201354, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulz%2C%20Alexander%20Gisbrecht%2C%20Andrej%20Hammer%2C%20Barbara%20Using%20discriminative%20dimensionality%20reduction%20to%20visualize%20classifiers%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulz%2C%20Alexander%20Gisbrecht%2C%20Andrej%20Hammer%2C%20Barbara%20Using%20discriminative%20dimensionality%20reduction%20to%20visualize%20classifiers%202015"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS - 30). 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20S.%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20S.%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Steinwart_2008_a",
            "entry": "Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008. Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. In Advances in",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20Vector%20Machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20Vector%20Machines%202008"
        }
    ]
}
