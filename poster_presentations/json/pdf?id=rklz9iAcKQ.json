{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DEEP GRAPH INFOMAX",
        "author": "Petar Velickovic, Department of Computer Science and Technology University of Cambridge petar.velickovic@cst.cam.ac.uk",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rklz9iAcKQ"
        },
        "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs\u2014both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning."
    },
    "keywords": [
        {
            "term": "protein-protein interaction",
            "url": "https://en.wikipedia.org/wiki/protein-protein_interaction"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "random walk",
            "url": "https://en.wikipedia.org/wiki/random_walk"
        }
    ],
    "abbreviations": {
        "DGI": "Deep Graph Infomax",
        "DIM": "Deep InfoMax",
        "BCE": "binary cross-entropy",
        "PPI": "protein-protein interaction",
        "GCN": "Graph Convolutional Network",
        "LP": "Label Propagation"
    },
    "highlights": [
        "Generalizing neural networks to graph-structured inputs is one of the current major challenges of machine learning (Bronstein et al, 2017; Hamilton et al, 2017b; <a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\"><a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a></a>)",
        "We propose an alternative objective for unsupervised graph learning that is based upon mutual information, rather than random walks",
        "We report the mean classification accuracy on the test nodes of our method after 50 runs of training, and reuse the metrics already reported in Kipf & Welling (2016a) for the performance of DeepWalk and Graph Convolutional Network, as well as Label Propagation (LP) (<a class=\"ref-link\" id=\"cZhu_et+al_2003_a\" href=\"#rZhu_et+al_2003_a\">Zhu et al, 2003</a>) and Planetoid (<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>)\u2014a representative supervised random walk method",
        "We assume that these benefits stem from the fact that, indirectly, the Deep Graph Infomax approach allows for every node to have access to structural properties of the entire graph, whereas the supervised Graph Convolutional Network is limited to only two-layer neighborhoods",
        "We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data",
        "By leveraging local mutual information maximization across the graph\u2019s patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph"
    ],
    "key_statements": [
        "Generalizing neural networks to graph-structured inputs is one of the current major challenges of machine learning (Bronstein et al, 2017; Hamilton et al, 2017b; <a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\"><a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a></a>)",
        "We propose an alternative objective for unsupervised graph learning that is based upon mutual information, rather than random walks",
        "Scalable estimation of mutual information was made both possible and practical through Mutual Information Neural Estimation (MINE, Belghazi et al, 2018), which relies on training a statistics network as a classifier of samples coming from the joint distribution of two random variables and their product of marginals",
        "Deep InfoMax trains an encoder model to maximize the mutual information between a high-level \u201cglobal\u201d representation and \u201clocal\u201d parts of the input",
        "We adapt ideas from Deep InfoMax to the graph domain, which can be thought of as having a more general type of structure than the ones captured by convolutional neural networks",
        "We demonstrate Deep Graph Infomax is stable to other choices of corruption functions in Appendix C, but we find those that preserve the graph structure result in the strongest features",
        "While we have found this readout to perform the best across all our experiments, we assume that its power will diminish with the increase in graph size, and in those cases, more sophisticated readout architectures such as set2vec (<a class=\"ref-link\" id=\"cVinyals_et+al_2015_a\" href=\"#rVinyals_et+al_2015_a\">Vinyals et al, 2015</a>) or DiffPool (Ying et al, 2018b) are likely to be more appropriate",
        "We report the mean classification accuracy on the test nodes of our method after 50 runs of training, and reuse the metrics already reported in Kipf & Welling (2016a) for the performance of DeepWalk and Graph Convolutional Network, as well as Label Propagation (LP) (<a class=\"ref-link\" id=\"cZhu_et+al_2003_a\" href=\"#rZhu_et+al_2003_a\">Zhu et al, 2003</a>) and Planetoid (<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>)\u2014a representative supervised random walk method",
        "We provide results for training the logistic regression on raw input features, as well as DeepWalk with the input features concatenated.\n3 A reference Deep Graph Infomax implementation may be found at https://github.com/PetarV-/Deep Graph Infomax",
        "We note that the Deep Graph Infomax approach is competitive with the results reported for the Graph Convolutional Network model with the supervised loss, even exceeding its performance on the Cora and Citeseer datasets",
        "We assume that these benefits stem from the fact that, indirectly, the Deep Graph Infomax approach allows for every node to have access to structural properties of the entire graph, whereas the supervised Graph Convolutional Network is limited to only two-layer neighborhoods",
        "We further observe that the Deep Graph Infomax method successfully outperformed all the competing unsupervised GraphSAGE approaches on the Reddit and protein-protein interaction datasets\u2014 verifying the potential of methods based on local mutual information maximization in the inductive node classification domain",
        "We have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised representations on graph-structured data",
        "By leveraging local mutual information maximization across the graph\u2019s patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph"
    ],
    "summary": [
        "Generalizing neural networks to graph-structured inputs is one of the current major challenges of machine learning (Bronstein et al, 2017; Hamilton et al, 2017b; <a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\"><a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a></a>).",
        "3.2 LOCAL-GLOBAL MUTUAL INFORMATION MAXIMIZATION Our approach to learning the encoder relies on maximizing local mutual information\u2014that is, we seek to obtain node representations that capture the global information content of the entire graph, represented by a summary vector, s.",
        "We provide some intuition that connects the classification error of our discriminator to mutual information maximization on graph representations.",
        "We have assessed the benefits of the representation learnt by the DGI encoder on a variety of node classification tasks, obtaining competitive results.",
        "For the transductive learning tasks (Cora, Citeseer and Pubmed), our encoder is a one-layer Graph Convolutional Network (GCN) model (Kipf & Welling, 2016a), with the following propagation rule: E(X, A) = \u03c3",
        "The corruption function used in this setting is designed to encourage the representations to properly encode structural similarities of different nodes in the graph; for this purpose, C preserves the original adjacency matrix (A = A), whereas the corrupted features, X, are obtained by row-wise shuffling of X.",
        "All models are initialized using Glorot initialization (Glorot & Bengio, 2010) and trained to maximize the mutual information provided in Equation 1 on the available nodes using the Adam SGD optimizer (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Ba, 2014</a>) with an initial learning rate of 0.001.",
        "We report the mean classification accuracy on the test nodes of our method after 50 runs of training, and reuse the metrics already reported in Kipf & Welling (2016a) for the performance of DeepWalk and GCN, as well as Label Propagation (LP) (<a class=\"ref-link\" id=\"cZhu_et+al_2003_a\" href=\"#rZhu_et+al_2003_a\">Zhu et al, 2003</a>) and Planetoid (<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>)\u2014a representative supervised random walk method.",
        "We note that the DGI approach is competitive with the results reported for the GCN model with the supervised loss, even exceeding its performance on the Cora and Citeseer datasets.",
        "We assume that these benefits stem from the fact that, indirectly, the DGI approach allows for every node to have access to structural properties of the entire graph, whereas the supervised GCN is limited to only two-layer neighborhoods.",
        "We further observe that the DGI method successfully outperformed all the competing unsupervised GraphSAGE approaches on the Reddit and PPI datasets\u2014 verifying the potential of methods based on local mutual information maximization in the inductive node classification domain.",
        "By leveraging local mutual information maximization across the graph\u2019s patch representations, obtained by powerful graph convolutional architectures, we are able to obtain node embeddings that are mindful of the global structural properties of the graph.",
        "This enables competitive performance across a variety of both transductive and inductive classification tasks, at times even outperforming relevant supervised architectures"
    ],
    "headline": "We present Deep Graph Infomax, a general approach for learning node representations within graph-structured data in an unsupervised manner",
    "reference_links": [
        {
            "id": "Battaglia_et+al_2018_a",
            "entry": "Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01261"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, ICML\u20192018, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04062"
        },
        {
            "id": "Bojchevski_2018_a",
            "entry": "Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1ZdKJ-0W.",
            "url": "https://openreview.net/forum?id=r1ZdKJ-0W",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojchevski%2C%20Aleksandar%20Gunnemann%2C%20Stephan%20Deep%20gaussian%20embedding%20of%20graphs%3A%20Unsupervised%20inductive%20learning%20via%20ranking%202018"
        },
        {
            "id": "Gutmann_2010_a",
            "entry": "Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297\u2013304, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gutmann%2C%20Michael%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%3A%20A%20new%20estimation%20principle%20for%20unnormalized%20statistical%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gutmann%2C%20Michael%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%3A%20A%20new%20estimation%20principle%20for%20unnormalized%20statistical%20models%202010"
        },
        {
            "id": "Hamilton_et+al_2017_a",
            "entry": "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024\u20131034, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "Hamilton_et+al_0000_a",
            "entry": "William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1709.05584"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hjelm_et+al_2018_a",
            "entry": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.06670"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Jeh_0000_a",
            "entry": "Glen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th International Conference on the World Wide Web, pp. 271\u2013279.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jeh%2C%20Glen%20Widom%2C%20Jennifer%20Scaling%20personalized%20web%20search",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jeh%2C%20Glen%20Widom%2C%20Jennifer%20Scaling%20personalized%20web%20search"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kipf_0000_a",
            "entry": "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02907"
        },
        {
            "id": "Kipf_0000_b",
            "entry": "Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07308"
        },
        {
            "id": "Van_2008_a",
            "entry": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mnih_2013_a",
            "entry": "Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in neural information processing systems, pp. 2265\u20132273, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Andriy%20Kavukcuoglu%2C%20Koray%20Learning%20word%20embeddings%20efficiently%20with%20noise-contrastive%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Andriy%20Kavukcuoglu%2C%20Koray%20Learning%20word%20embeddings%20efficiently%20with%20noise-contrastive%20estimation%202013"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Perozzi_et+al_2014_a",
            "entry": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701\u2013710. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perozzi%2C%20Bryan%20Al-Rfou%2C%20Rami%20Skiena%2C%20Steven%20Deepwalk%3A%20Online%20learning%20of%20social%20representations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perozzi%2C%20Bryan%20Al-Rfou%2C%20Rami%20Skiena%2C%20Steven%20Deepwalk%3A%20Online%20learning%20of%20social%20representations%202014"
        },
        {
            "id": "Leonardo_2017_a",
            "entry": "Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node representations from structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 385\u2013394. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leonardo%20FR%20Ribeiro%2C%20Pedro%20HP%20Saverese%20Figueiredo%2C%20Daniel%20R%20struc2vec%3A%20Learning%20node%20representations%20from%20structural%20identity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leonardo%20FR%20Ribeiro%2C%20Pedro%20HP%20Saverese%20Figueiredo%2C%20Daniel%20R%20struc2vec%3A%20Learning%20node%20representations%20from%20structural%20identity%202017"
        },
        {
            "id": "Rousseeuw_1987_a",
            "entry": "Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53\u201365, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rousseeuw%2C%20Peter%20J.%20Silhouettes%3A%20a%20graphical%20aid%20to%20the%20interpretation%20and%20validation%20of%20cluster%20analysis%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rousseeuw%2C%20Peter%20J.%20Silhouettes%3A%20a%20graphical%20aid%20to%20the%20interpretation%20and%20validation%20of%20cluster%20analysis%201987"
        },
        {
            "id": "Sen_et+al_2008_a",
            "entry": "Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sen%2C%20Prithviraj%20Namata%2C%20Galileo%20Bilgic%2C%20Mustafa%20Getoor%2C%20Lise%20Collective%20classification%20in%20network%20data%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sen%2C%20Prithviraj%20Namata%2C%20Galileo%20Bilgic%2C%20Mustafa%20Getoor%2C%20Lise%20Collective%20classification%20in%20network%20data%202008"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Subramanian_et+al_2005_a",
            "entry": "Aravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert, Michael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proceedings of the National Academy of Sciences, 102(43):15545\u201315550, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Subramanian%2C%20Aravind%20Tamayo%2C%20Pablo%20Mootha%2C%20Vamsi%20K.%20Mukherjee%2C%20Sayan%20Gene%20set%20enrichment%20analysis%3A%20a%20knowledge-based%20approach%20for%20interpreting%20genome-wide%20expression%20profiles%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Subramanian%2C%20Aravind%20Tamayo%2C%20Pablo%20Mootha%2C%20Vamsi%20K.%20Mukherjee%2C%20Sayan%20Gene%20set%20enrichment%20analysis%3A%20a%20knowledge-based%20approach%20for%20interpreting%20genome-wide%20expression%20profiles%202005"
        },
        {
            "id": "Tang_2015_a",
            "entry": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Largescale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067\u20131077. International World Wide Web Conferences Steering Committee, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Jian%20Qu%2C%20Meng%20Line%3A%20Largescale%20information%20network%20embedding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Jian%20Qu%2C%20Meng%20Line%3A%20Largescale%20information%20network%20embedding%202015"
        },
        {
            "id": "Velickovic_et+al_2018_a",
            "entry": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.",
            "url": "https://openreview.net/forum?id=rJXMpikCZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petar%20Velickovic%20Guillem%20Cucurull%20Arantxa%20Casanova%20Adriana%20Romero%20Pietro%20Lio%20and%20Yoshua%20Bengio%20Graph%20Attention%20Networks%20International%20Conference%20on%20Learning%20Representations%202018%20URL%20httpsopenreviewnetforumidrJXMpikCZ"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06391"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1225\u20131234. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Daixin%20Cui%2C%20Peng%20Zhu%2C%20Wenwu%20Structural%20deep%20network%20embedding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Daixin%20Cui%2C%20Peng%20Zhu%2C%20Wenwu%20Structural%20deep%20network%20embedding%202016"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community preserving network embedding. In AAAI, pp. 203\u2013209, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiao%20Cui%2C%20Peng%20Wang%2C%20Jing%20Pei%2C%20Jian%20Community%20preserving%20network%20embedding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiao%20Cui%2C%20Peng%20Wang%2C%20Jing%20Pei%2C%20Jian%20Community%20preserving%20network%20embedding%202017"
        },
        {
            "id": "Weisfeiler_1968_a",
            "entry": "Boris Weisfeiler and AA Lehman. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weisfeiler%2C%20Boris%20Lehman%2C%20A.A.%20A%20reduction%20of%20a%20graph%20to%20a%20canonical%20form%20and%20an%20algebra%20arising%20during%20this%20reduction%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weisfeiler%2C%20Boris%20Lehman%2C%20A.A.%20A%20reduction%20of%20a%20graph%20to%20a%20canonical%20form%20and%20an%20algebra%20arising%20during%20this%20reduction%201968"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08861"
        },
        {
            "id": "Ying_et+al_0000_a",
            "entry": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. arXiv preprint arXiv:1806.01973, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01973"
        },
        {
            "id": "Ying_et+al_0000_b",
            "entry": "Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. arXiv preprint arXiv:1806.08804, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08804"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07294"
        },
        {
            "id": "Zhu_et+al_2003_a",
            "entry": "Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912\u2013919, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003"
        },
        {
            "id": "Zitnik_2017_a",
            "entry": "Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33(14):i190\u2013i198, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zitnik%2C%20Marinka%20Leskovec%2C%20Jure%20Predicting%20multicellular%20function%20through%20multi-layer%20tissue%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zitnik%2C%20Marinka%20Leskovec%2C%20Jure%20Predicting%20multicellular%20function%20through%20multi-layer%20tissue%20networks%202017"
        }
    ]
}
