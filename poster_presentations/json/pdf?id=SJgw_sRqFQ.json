{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE UNUSUAL EFFECTIVENESS OF AVERAGING IN GAN TRAINING",
        "author": "Yasin Yaz\u0131c\u0131, Nanyang Technological University (NTU)",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJgw_sRqFQ"
        },
        "abstract": "We examine two different techniques for parameter averaging in GAN training. Moving Average (MA) computes the time-average of parameters, whereas Exponential Moving Average (EMA) computes an exponentially discounted sum. Whilst MA is known to lead to convergence in bilinear settings, we provide the \u2013 to our knowledge \u2013 first theoretical arguments in support of EMA. We show that EMA converges to limit cycles around the equilibrium with vanishing amplitude as the discount parameter approaches one for simple bilinear games and also enhances the stability of general GAN training. We establish experimentally that both techniques are strikingly effective in the non-convex-concave GAN setting as well. Both improve inception and FID scores on different architectures and for different GAN objectives. We provide comprehensive experimental results across a range of datasets \u2013 mixture of Gaussians, CIFAR-10, STL-10, CelebA and ImageNet \u2013 to demonstrate its effectiveness. We achieve state-of-the-art results on CIFAR-10 and produce clean CelebA face images.1"
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "moving average",
            "url": "https://en.wikipedia.org/wiki/moving_average"
        },
        {
            "term": "time scale",
            "url": "https://en.wikipedia.org/wiki/time_scale"
        },
        {
            "term": "exponential moving average",
            "url": "https://en.wikipedia.org/wiki/exponential_moving_average"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        }
    ],
    "abbreviations": {
        "MA": "moving average",
        "EMA": "exponential moving average",
        "GANs": "Generative Adversarial Networks",
        "OMD": "Optimistic Adam",
        "CO": "Consensus Optimization",
        "Zero-GP": "Zero Centered Gradient Penalty",
        "ADSC": "Advanced Digital Sciences Center",
        "ROSE": "Rich Object Search"
    },
    "highlights": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a>) are two-player zero-sum games",
        "Our contributions are the following: (i) We show theoretically that exponential moving average does not converge to equilibrium, even in bilinear games, it helps to stabilize cyclic behavior by shrinking its amplitude",
        "Optimistic Adam, Consensus Optimization and Zero Centered Gradient Penalty perform better than the baseline, they seem to be less stable across different iterations than exponential moving average results",
        "We have explored the effect of two different techniques for averaging parameters outside of the Generative Adversarial Networks training loop, moving average (MA) and exponential moving average (EMA)",
        "We have shown that both techniques significantly improve the quality of generated images on various datasets, network architectures and Generative Adversarial Networks objectives",
        "In the case of the exponential moving average technique, we have provided the first theoretical analysis of its implications, showing that even in simple bilinear settings it converges to stable limit cycles of small amplitude around the solution of the saddle problem"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a>) are two-player zero-sum games",
        "Our contributions are the following: (i) We show theoretically that exponential moving average does not converge to equilibrium, even in bilinear games, it helps to stabilize cyclic behavior by shrinking its amplitude",
        "In non-bilinear settings it preserves the stability of locally stable fixed points. We demonstrate that both averaging techniques consistently improve results for several different datasets, network architectures, and Generative Adversarial Networks objectives. (i) We compare it with several other methods that try to alleviate the cycling or non-convergence problem and demonstrate its unusual effectiveness",
        "Optimistic Adam, Consensus Optimization and Zero Centered Gradient Penalty perform better than the baseline, they seem to be less stable across different iterations than exponential moving average results",
        "We have observed improvements when the starting point of moving average is pushed later in training, for convenience and ease of experimentation our main results are based on the exponential moving average method.\n5.3",
        "Even though quantitative scores are better for exponential moving average, we do not see as clear visual improvements as in previous results but rather small changes",
        "We have explored the effect of two different techniques for averaging parameters outside of the Generative Adversarial Networks training loop, moving average (MA) and exponential moving average (EMA)",
        "We have shown that both techniques significantly improve the quality of generated images on various datasets, network architectures and Generative Adversarial Networks objectives",
        "In the case of the exponential moving average technique, we have provided the first theoretical analysis of its implications, showing that even in simple bilinear settings it converges to stable limit cycles of small amplitude around the solution of the saddle problem"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a>) are two-player zero-sum games.",
        "We demonstrate that both averaging techniques consistently improve results for several different datasets, network architectures, and GAN objectives.",
        "There have been attempts to improve stability and alleviate cycling issues arising in GANs. <a class=\"ref-link\" id=\"cSalimans_et+al_2016_a\" href=\"#rSalimans_et+al_2016_a\">Salimans et al (2016</a>) use historical averaging of both generator and discriminator parameters as a regularization term in the objective function: each model deviating from its time average is penalized to improve stability.",
        "In case of convex-concave min-max games, it is known that the average of generator/discriminator parameters is an optimal solution (<a class=\"ref-link\" id=\"cFreund_1999_a\" href=\"#rFreund_1999_a\">Freund & Schapire, 1999</a>), but there is no such guarantee for a non-convex/concave setting.",
        "In addition to these experiments, we compare the averaging methods, with Consensus Optimization (<a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\">Mescheder et al, 2017</a>), Optimistic Adam (<a class=\"ref-link\" id=\"cDaskalakis_et+al_2018_a\" href=\"#rDaskalakis_et+al_2018_a\">Daskalakis et al, 2018</a>) and Zero Centered Gradient Penalty (<a class=\"ref-link\" id=\"cMescheder_et+al_2018_a\" href=\"#rMescheder_et+al_2018_a\">Mescheder et al, 2018</a>) on the Mixture of Gaussians data set and CIFAR-10.",
        "Original GAN (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is used as the objective function with Gradient Penalty from <a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al (2017</a>), with 1:1 discriminator/generator update and alternative update rule.",
        "We compare this baseline with Optimistic Adam (OMD), Consensus Optimization (CO), and Zero Centered Gradient Penalty (Zero-GP) by using the same architecture, and following the original implementations as closely as possible.",
        "OMD, CO and Zero-GP perform better than the baseline, they seem to be less stable across different iterations than EMA results.",
        "We have observed improvements when the starting point of MA is pushed later in training, for convenience and ease of experimentation our main results are based on the EMA method.",
        "Our intuition is that this phenomenon occurs because generator iterates does not stay in the same parameter region and averaging over parameters of different regions produces worse results.",
        "5.4 CELEBA Figure 6 shows the same 10 samples from the prior fed to the non-averaged generator, the EMA generator with various \u03b2 values, and the MA generator at 250k iterations in the training.",
        "5.5 STL-10 & IMAGENET Figure 8 shows images generated with and w/o EMA for STL-10 and ImageNet. Even though quantitative scores are better for EMA, we do not see as clear visual improvements as in previous results but rather small changes.",
        "We have explored the effect of two different techniques for averaging parameters outside of the GAN training loop, moving average (MA) and exponential moving average (EMA).",
        "In the case of the EMA technique, we have provided the first theoretical analysis of its implications, showing that even in simple bilinear settings it converges to stable limit cycles of small amplitude around the solution of the saddle problem."
    ],
    "headline": "We examine two different techniques for parameter averaging in Generative Adversarial Networks training",
    "reference_links": [
        {
            "id": "Mart_2017_a",
            "entry": "Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proc. 34th International Conference on Machine Learning (ICML), pp. 214\u2013223, 2017. URL http://proceedings.mlr.press/v70/arjovsky17a.html.",
            "url": "http://proceedings.mlr.press/v70/arjovsky17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Athiwaratkun_et+al_1806_a",
            "entry": "B. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson. Improving consistency-based semisupervised learning with weight averaging. ArXiv e-prints, abs/1806.05594, June 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05594"
        },
        {
            "id": "James_2018_a",
            "entry": "James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In Proc. ACM Conference on Economics and Computation (EC), pp. 321\u2013338. ACM, 2018. doi: 10.1145/3219166.3219235.",
            "crossref": "https://dx.doi.org/10.1145/3219166.3219235",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3219166.3219235"
        },
        {
            "id": "Bailey_2019_a",
            "entry": "James P Bailey and Georgios Piliouras. Multi-agent learning in network zero-sum games is a Hamiltonian system. In Int. Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bailey%2C%20James%20P.%20Piliouras%2C%20Georgios%20Multi-agent%20learning%20in%20network%20zero-sum%20games%20is%20a%20Hamiltonian%20system%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bailey%2C%20James%20P.%20Piliouras%2C%20Georgios%20Multi-agent%20learning%20in%20network%20zero-sum%20games%20is%20a%20Hamiltonian%20system%202019"
        },
        {
            "id": "Balduzzi_et+al_2018_a",
            "entry": "David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In Proc. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balduzzi%2C%20David%20Racaniere%2C%20Sebastien%20Martens%2C%20James%20Foerster%2C%20Jakob%20and%20Thore%20Graepel.%20The%20mechanics%20of%20n-player%20differentiable%20games%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20David%20Racaniere%2C%20Sebastien%20Martens%2C%20James%20Foerster%2C%20Jakob%20and%20Thore%20Graepel.%20The%20mechanics%20of%20n-player%20differentiable%20games%202018"
        },
        {
            "id": "Barratt_1801_a",
            "entry": "S. Barratt and R. Sharma. A note on the Inception score. ArXiv e-prints, abs/1801.01973, January 2018. URL https://arxiv.org/abs/1801.01973.",
            "url": "https://arxiv.org/abs/1801.01973",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "A. Coates, H. Lee, and A.Y. Ng. An analysis of single-layer networks in unsupervised feature learning. In Proc. 14th International Conference on Artificial Intelligence and Statistics, volume 15 of JMLR Workshop and Conference Proceedings, pp. 215\u2013223, 2011. URL http://jmlr.csail.mit.edu/proceedings/papers/v15/coates11a.html.",
            "url": "http://jmlr.csail.mit.edu/proceedings/papers/v15/coates11a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20A.%20Lee%2C%20H.%20Ng%2C%20A.Y.%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Daskalakis_et+al_2018_a",
            "entry": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20Constantinos%20Ilyas%2C%20Andrew%20Syrgkanis%2C%20Vasilis%20Zeng%2C%20Haoyang%20Training%20GANs%20with%20optimism%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20Constantinos%20Ilyas%2C%20Andrew%20Syrgkanis%2C%20Vasilis%20Zeng%2C%20Haoyang%20Training%20GANs%20with%20optimism%202018"
        },
        {
            "id": "Flammarion_1504_a",
            "entry": "N. Flammarion and F. Bach. From averaging to acceleration, there is only a step-size. ArXiv e-prints, abs/1504.01577, April 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.01577"
        },
        {
            "id": "Freund_1999_a",
            "entry": "Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1-2):79\u2013103, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20Adaptive%20game%20playing%20using%20multiplicative%20weights%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20Adaptive%20game%20playing%20using%20multiplicative%20weights%201999"
        },
        {
            "id": "Ge_et+al_2018_a",
            "entry": "Hao Ge, Yin Xia, Xu Chen, Randall Berry, and Ying Wu. Fictitious GAN: Training GANs with historical models. CoRR, abs/1803.08647, 2018. URL http://arxiv.org/abs/1803.08647.",
            "url": "http://arxiv.org/abs/1803.08647",
            "arxiv_url": "https://arxiv.org/pdf/1803.08647"
        },
        {
            "id": "Gidel_et+al_1807_a",
            "entry": "G. Gidel, R. Askari Hemmat, M. Pezeshki, R. Lepriol, G. Huang, S. Lacoste-Julien, and I. Mitliagkas. Negative momentum for improved game dynamics. ArXiv e-prints, abs/1807.04740, July 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04740"
        },
        {
            "id": "Gidel_et+al_2018_a",
            "entry": "Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial nets. CoRR, abs/1802.10551, 2018. URL http://arxiv.org/abs/1802.10551.",
            "url": "http://arxiv.org/abs/1802.10551",
            "arxiv_url": "https://arxiv.org/pdf/1802.10551"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672\u20132680, 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.",
            "url": "http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goodfellow_2017_a",
            "entry": "Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160, 2017. URL http://arxiv.org/abs/1701.00160.",
            "url": "http://arxiv.org/abs/1701.00160",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 5769\u20135779, December 2017. URL https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.arxiv:1704.00028.",
            "url": "https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.arxiv:1704.00028",
            "arxiv_url": "https://arxiv.org/pdf/1704.00028"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. CoRR, abs/1706.08500, 2017. URL http://arxiv.org/abs/1706.08500.",
            "url": "http://arxiv.org/abs/1706.08500",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "Izmailov_et+al_2018_a",
            "entry": "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. CoRR, abs/1803.05407, 2018. URL http://arxiv.org/abs/1803.05407.",
            "url": "http://arxiv.org/abs/1803.05407",
            "arxiv_url": "https://arxiv.org/pdf/1803.05407"
        },
        {
            "id": "Karras_et+al_2017_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. URL http://arxiv.org/abs/1710.10196.",
            "url": "http://arxiv.org/abs/1710.10196",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_et+al_2009_a",
            "entry": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research) dataset, 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html.",
            "url": "http://www.cs.toronto.edu/~kriz/cifar.html"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proc. International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Mertikopoulos_et+al_2018_a",
            "entry": "Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In Proc. 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 2703\u20132717, 2018. URL http://dl.acm.org/citation.cfm?id=3174304.3175476.",
            "url": "http://dl.acm.org/citation.cfm?id=3174304.3175476",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mertikopoulos%2C%20Panayotis%20Papadimitriou%2C%20Christos%20Piliouras%2C%20Georgios%20Cycles%20in%20adversarial%20regularized%20learning%202018"
        },
        {
            "id": "Mertikopoulos_et+al_2019_a",
            "entry": "Panayotis Mertikopoulos, Houssam Zenati, Bruno Lecouat, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mertikopoulos%2C%20Panayotis%20Zenati%2C%20Houssam%20Lecouat%2C%20Bruno%20Foo%2C%20Chuan-Sheng%20Optimistic%20mirror%20descent%20in%20saddle-point%20problems%3A%20Going%20the%20extra%20%28gradient%29%20mile%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mertikopoulos%2C%20Panayotis%20Zenati%2C%20Houssam%20Lecouat%2C%20Bruno%20Foo%2C%20Chuan-Sheng%20Optimistic%20mirror%20descent%20in%20saddle-point%20problems%3A%20Going%20the%20extra%20%28gradient%29%20mile%202019"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017"
        },
        {
            "id": "Mescheder_et+al_2018_a",
            "entry": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Proc. International Conference on Machine learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. URL http://arxiv.org/abs/1802.05957.",
            "url": "http://arxiv.org/abs/1802.05957",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5585\u20135595, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "Christos_2018_a",
            "entry": "Christos Papadimitriou and Georgios Piliouras. From nash equilibria to chain recurrent sets: An algorithmic solution concept for game theory. Entropy, 20(10), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christos%20Papadimitriou%20and%20Georgios%20Piliouras%20From%20nash%20equilibria%20to%20chain%20recurrent%20sets%20An%20algorithmic%20solution%20concept%20for%20game%20theory%20Entropy%202010%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christos%20Papadimitriou%20and%20Georgios%20Piliouras%20From%20nash%20equilibria%20to%20chain%20recurrent%20sets%20An%20algorithmic%20solution%20concept%20for%20game%20theory%20Entropy%202010%202018"
        },
        {
            "id": "Polyak_1992_a",
            "entry": "B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control and Optimization, 30(4):838\u2013855, July 1992. doi: 10.1137/0330046.",
            "crossref": "https://dx.doi.org/10.1137/0330046",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/0330046"
        },
        {
            "id": "Radford_et+al_1511_a",
            "entry": "A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ArXiv e-prints, abs/1511.06434, November 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. CoRR, abs/1606.03498, 2016. URL http://arxiv.org/abs/1606.03498.",
            "url": "http://arxiv.org/abs/1606.03498",
            "arxiv_url": "https://arxiv.org/pdf/1606.03498"
        },
        {
            "id": "Van_et+al_2017_a",
            "entry": "Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, et al. Parallel WaveNet: Fast high-fidelity speech synthesis. CoRR, abs/1711.10433, 2017. URL http://arxiv.org/abs/1711.10433.",
            "url": "http://arxiv.org/abs/1711.10433",
            "arxiv_url": "https://arxiv.org/pdf/1711.10433"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu. Activation maximization generative adversarial nets. In Proc. International Conference on Learning Representations (ICLR), 2018. URL https://openreview.net/forum?id= HyyP33gAZ.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Zhiming%20Cai%2C%20Han%20Rong%2C%20Shu%20Song%2C%20Yuxuan%20Activation%20maximization%20generative%20adversarial%20nets%202018"
        }
    ]
}
