{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THREE MECHANISMS OF WEIGHT DECAY REGULARIZATION",
        "author": "Guodong Zhang, Chaoqi Wang, Bowen Xu, Roger Grosse University of Toronto, Vector Institute {gdzhang, cqwang, bowenxu, rgrosse}@cs.toronto.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1lz-3Rct7"
        },
        "journal": "Loshchilov",
        "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of L2 regularization. Literal weight decay has been shown to outperform L2 regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the inputoutput Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "learning rate",
            "url": "https://en.wikipedia.org/wiki/learning_rate"
        },
        {
            "term": "l2 regularization",
            "url": "https://en.wikipedia.org/wiki/l2_regularization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        }
    ],
    "abbreviations": {
        "BN": "Batch Normalization",
        "K-FAC": "Kronecker-factored approximate curvature",
        "SGD": "stochastic gradient descent"
    },
    "highlights": [
        "Weight decay has long been a standard trick to improve the generalization performance of neural networks (<a class=\"ref-link\" id=\"cKrogh_1992_a\" href=\"#rKrogh_1992_a\"><a class=\"ref-link\" id=\"cKrogh_1992_a\" href=\"#rKrogh_1992_a\">Krogh & Hertz, 1992</a></a>; <a class=\"ref-link\" id=\"cBos_1996_a\" href=\"#rBos_1996_a\"><a class=\"ref-link\" id=\"cBos_1996_a\" href=\"#rBos_1996_a\">Bos & Chug, 1996</a></a>) by encouraging the weights to be small in magnitude",
        "In order to better understand the effect of weight decay, we experimented with both weight decay and L2 regularization applied to image classifiers using three different optimization algorithms: stochastic gradient descent, Adam, and Kronecker-Factored Approximate Curvature (K-FAC) (<a class=\"ref-link\" id=\"cMartens_2015_b\" href=\"#rMartens_2015_b\">Martens & Grosse, 2015</a>)",
        "In our experiments with first-order optimization methods (SGD and Adam) on networks with Batch Normalization, we found that it acts by way of the effective learning rate",
        "Figure 3 shows the effective learning rate over time for two Batch Normalization networks trained with stochastic gradient descent, one with weight decay and one without it",
        "We conclude that for Batch Normalization networks trained with stochastic gradient descent or Adam, weight decay achieves its regularization effect primarily through the effective learning rate.\n4.2",
        "Our experiments highlight that the dynamics of the norms of weights and curvature matrices, and their interaction with optimization hyperparameters, can have a substantial impact on generalization"
    ],
    "key_statements": [
        "Weight decay has long been a standard trick to improve the generalization performance of neural networks (<a class=\"ref-link\" id=\"cKrogh_1992_a\" href=\"#rKrogh_1992_a\">Krogh & Hertz, 1992</a>; <a class=\"ref-link\" id=\"cBos_1996_a\" href=\"#rBos_1996_a\">Bos & Chug, 1996</a>) by encouraging the weights to be small in magnitude",
        "Weight decay is widely used in networks with Batch Normalization (BN) (<a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a>)",
        "In order to better understand the effect of weight decay, we experimented with both weight decay and L2 regularization applied to image classifiers using three different optimization algorithms: stochastic gradient descent, Adam, and Kronecker-Factored Approximate Curvature (K-FAC) (<a class=\"ref-link\" id=\"cMartens_2015_b\" href=\"#rMartens_2015_b\">Martens & Grosse, 2015</a>)",
        "In our experiments with first-order optimization methods (SGD and Adam) on networks with Batch Normalization, we found that it acts by way of the effective learning rate",
        "We show that when Kronecker-factored approximate curvature is applied to a linear network using the Gauss-Newton metric (K-FAC-G), weight decay is equivalent to regularizing the squared Frobenius norm of the input-output Jacobian to improve generalization)",
        "We have identified three distinct mechanisms by which weight decay improves generalization, depending on the optimization algorithm and network architecture",
        "While more analysis and experimentation is needed to understand how broadly each of our three mechanisms applies, our work provides a starting point for understanding practical regularization effects in neural network training",
        "We find that weight decay can work in unexpected ways, especially in the presence of Batch Normalization",
        "<a class=\"ref-link\" id=\"cVan_2017_a\" href=\"#rVan_2017_a\">Van Laarhoven (2017</a>) observed that L2 regularization has an influence on the effective learning rate in gradient descent. We extend this result to first-order optimizers that weight decay increases the effective learning rate by reducing the scale of the weights",
        "Figure 3 shows the effective learning rate over time for two Batch Normalization networks trained with stochastic gradient descent, one with weight decay and one without it",
        "We show that the effective learning rate schedule 0.8 explains nearly the entire generalization effect of weight decay",
        "We conclude that for Batch Normalization networks trained with stochastic gradient descent or Adam, weight decay achieves its regularization effect primarily through the effective learning rate.\n4.2",
        "In Section 3, we observed that when Batch Normalization is disabled, weight decay has the strongest regularization effect when Kronecker-factored approximate curvature is used as the optimizer",
        "Our experiments highlight that the dynamics of the norms of weights and curvature matrices, and their interaction with optimization hyperparameters, can have a substantial impact on generalization"
    ],
    "summary": [
        "Weight decay has long been a standard trick to improve the generalization performance of neural networks (<a class=\"ref-link\" id=\"cKrogh_1992_a\" href=\"#rKrogh_1992_a\"><a class=\"ref-link\" id=\"cKrogh_1992_a\" href=\"#rKrogh_1992_a\">Krogh & Hertz, 1992</a></a>; <a class=\"ref-link\" id=\"cBos_1996_a\" href=\"#rBos_1996_a\"><a class=\"ref-link\" id=\"cBos_1996_a\" href=\"#rBos_1996_a\">Bos & Chug, 1996</a></a>) by encouraging the weights to be small in magnitude.",
        "In order to better understand the effect of weight decay, we experimented with both weight decay and L2 regularization applied to image classifiers using three different optimization algorithms: SGD, Adam, and Kronecker-Factored Approximate Curvature (K-FAC) (<a class=\"ref-link\" id=\"cMartens_2015_b\" href=\"#rMartens_2015_b\">Martens & Grosse, 2015</a>).",
        "2. We show that when K-FAC is applied to a linear network using the Gauss-Newton metric (K-FAC-G), weight decay is equivalent to regularizing the squared Frobenius norm of the input-output Jacobian to improve generalization).",
        "Figure 3 shows the effective learning rate over time for two BN networks trained with SGD, one with weight decay and one without it.",
        "We observe that whether weight decay was applied to the top layer did not have a Figure 4: The curves of test accuracies of significant impact; whether it was applied to the reamining ResNet32 on CIFAR-100.",
        "As shown in Figure 4, this effective learning rate transfer scheme eliminates almost the entire generalization gap; it is fully closed by adding weight decay to the top layer.",
        "We conclude that for BN networks trained with SGD or Adam, weight decay achieves its regularization effect primarily through the effective learning rate.",
        "In Section 3, we observed that when BN is disabled, weight decay has the strongest regularization effect when K-FAC is used as the optimizer.",
        "We show that in a certain idealized setting, K-FAC with weight decay regularizes the input-output Jacobian of the network.",
        "The Jacobian regularization mechanism from Section 4.2 does not apply in this case, since rescaling the weights results in an equivalent network, and does not affect the input-output Jacobian.",
        "If the network is trained with K-FAC, the effective learning rate mechanism from Section 4.1 does not apply because the K-FAC update is invariant to affine reparameterization (<a class=\"ref-link\" id=\"cLuk_2018_a\" href=\"#rLuk_2018_a\">Luk & Grosse, 2018</a>) and not affected by the scaling of the weights.",
        "Figure 7 shows the norms of the Fisher matrix F and GN matrix G of the normalized weights for the first layer of a CIFAR-10 network throughout training.",
        "The reason is that weight decay reduces the effective damping, helping K-FAC to retain its second-order properties.",
        "We\u2019ve identified three distinct mechanisms by which weight decay improves generalization, depending on the architecture and optimization algorithm: increasing the effective learning rate, reducing the Jacobian norm, and reducing the effective damping parameter."
    ],
    "headline": "We empirically investigate weight decay for three optimization algorithms and a variety of network architectures",
    "reference_links": [
        {
            "id": "Amari_1998_a",
            "entry": "Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kroneckerfactored approximations. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Grosse%2C%20Roger%20Martens%2C%20James%20Distributed%20second-order%20optimization%20using%20kroneckerfactored%20approximations%202016"
        },
        {
            "id": "Bernacchia_et+al_2018_a",
            "entry": "Alberto Bernacchia, M\u00e1t\u00e9 Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear networks and application to the nonlinear case. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bernacchia%2C%20Alberto%20Lengyel%2C%20M%C3%A1t%C3%A9%20Hennequin%2C%20Guillaume%20Exact%20natural%20gradient%20in%20deep%20linear%20networks%20and%20application%20to%20the%20nonlinear%20case%202018"
        },
        {
            "id": "Bos_1996_a",
            "entry": "Siegfried Bos and E Chug. Using weight decay to optimize the generalization ability of a perceptron. In Neural Networks, 1996., IEEE International Conference on, volume 1, pp. 241\u2013246. IEEE, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bos%2C%20Siegfried%20Chug%2C%20E.%20Using%20weight%20decay%20to%20optimize%20the%20generalization%20ability%20of%20a%20perceptron%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bos%2C%20Siegfried%20Chug%2C%20E.%20Using%20weight%20decay%20to%20optimize%20the%20generalization%20ability%20of%20a%20perceptron%201996"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heskes_2000_a",
            "entry": "Tom Heskes. On \u201cnatural\u201d learning and pruning in multilayered perceptrons. Neural Computation, 12(4):881\u2013901, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heskes%2C%20Tom%20On%20%E2%80%9Cnatural%E2%80%9D%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heskes%2C%20Tom%20On%20%E2%80%9Cnatural%E2%80%9D%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000"
        },
        {
            "id": "Hoffer_et+al_2017_a",
            "entry": "Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1731\u20131741, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "Hoffer_et+al_2018_a",
            "entry": "Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate normalization schemes in deep networks. arXiv preprint arXiv:1803.01814, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01814"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Jastrzebski_et+al_2017_a",
            "entry": "Stanis\u0142aw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04623"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Krogh_1992_a",
            "entry": "Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950\u2013957, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Liang_et+al_2017_a",
            "entry": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01530"
        },
        {
            "id": "Loshchilov_2017_a",
            "entry": "Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05101"
        },
        {
            "id": "Luk_2018_a",
            "entry": "Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. arXiv preprint arXiv:1808.10340, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.10340"
        },
        {
            "id": "Martens_2010_a",
            "entry": "James Martens. Deep learning via hessian-free optimization. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20James%20Deep%20learning%20via%20hessian-free%20optimization%202010"
        },
        {
            "id": "Martens_2014_a",
            "entry": "James Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.1193"
        },
        {
            "id": "Martens_2015_a",
            "entry": "James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408\u20132417, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20James%20Grosse%2C%20Roger%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20James%20Grosse%2C%20Roger%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%202015"
        },
        {
            "id": "Neelakantan_et+al_2015_a",
            "entry": "Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06807"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha SohlDickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08760"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Van_2017_a",
            "entry": "Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05350"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5279\u20135288, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as variational inference. arXiv preprint arXiv:1712.02390, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02390"
        },
        {
            "id": "Throughout_2009_a",
            "entry": "Throughout the paper, we perform experiments on image classification with three different datasets, MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). For MNIST, we use simple fully-connected networks with different depth and width. For CIFAR-10 and CIFAR100, we use VGG16 (Simonyan & Zisserman, 2014) and ResNet32 (He et al., 2016). To make the network more flexible, we widen all convolutional layers in ResNet32 by a factor of 4, according to Zagoruyko & Komodakis (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Throughout%20the%20paper%20we%20perform%20experiments%20on%20image%20classification%20with%20three%20different%20datasets%20MNIST%20LeCun%20et%20al%201998%20CIFAR10%20and%20CIFAR100%20Krizhevsky%20%20Hinton%202009%20For%20MNIST%20we%20use%20simple%20fullyconnected%20networks%20with%20different%20depth%20and%20width%20For%20CIFAR10%20and%20CIFAR100%20we%20use%20VGG16%20Simonyan%20%20Zisserman%202014%20and%20ResNet32%20He%20et%20al%202016%20To%20make%20the%20network%20more%20flexible%20we%20widen%20all%20convolutional%20layers%20in%20ResNet32%20by%20a%20factor%20of%204%20according%20to%20Zagoruyko%20%20Komodakis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Throughout%20the%20paper%20we%20perform%20experiments%20on%20image%20classification%20with%20three%20different%20datasets%20MNIST%20LeCun%20et%20al%201998%20CIFAR10%20and%20CIFAR100%20Krizhevsky%20%20Hinton%202009%20For%20MNIST%20we%20use%20simple%20fullyconnected%20networks%20with%20different%20depth%20and%20width%20For%20CIFAR10%20and%20CIFAR100%20we%20use%20VGG16%20Simonyan%20%20Zisserman%202014%20and%20ResNet32%20He%20et%20al%202016%20To%20make%20the%20network%20more%20flexible%20we%20widen%20all%20convolutional%20layers%20in%20ResNet32%20by%20a%20factor%20of%204%20according%20to%20Zagoruyko%20%20Komodakis%202016"
        },
        {
            "id": "We_2015_a",
            "entry": "We investigate three different optimization methods, including Stochastic Gradient Descent (SGD), Adam (Kingma & Ba, 2014) and K-FAC (Martens & Grosse, 2015). In K-FAC, two different curvature matrices are studied, including Fisher information matrix and Gauss-Newton matrix.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20investigate%20three%20different%20optimization%20methods%20including%20Stochastic%20Gradient%20Descent%20SGD%20Adam%20Kingma%20%20Ba%202014%20and%20KFAC%20Martens%20%20Grosse%202015%20In%20KFAC%20two%20different%20curvature%20matrices%20are%20studied%20including%20Fisher%20information%20matrix%20and%20GaussNewton%20matrix",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20investigate%20three%20different%20optimization%20methods%20including%20Stochastic%20Gradient%20Descent%20SGD%20Adam%20Kingma%20%20Ba%202014%20and%20KFAC%20Martens%20%20Grosse%202015%20In%20KFAC%20two%20different%20curvature%20matrices%20are%20studied%20including%20Fisher%20information%20matrix%20and%20GaussNewton%20matrix"
        },
        {
            "id": "In_2017_a",
            "entry": "In default, batch size 128 is used unless stated otherwise. In SGD and Adam, we train the networks with a budge of 200 epochs and decay the learning rate by a factor of 10 every 60 epochs for batch sizes of 128 and 640, and every 80 epochs for the batch size of 2K. Whereas we train the networks only with 100 epochs and decay the learning rate every 40 epochs in K-FAC. Additionally, the curvature matrix is updated by running average with re-estimation every 10 iterations and the inverse operator is amortized to 100 iterations. For K-FAC, we use fixed damping term 1e\u22123 unless state otherwise. For each algorithm, best hyperparameters (learning rate and regularization factor) are selected using grid search on held-out 5k validation set. For the large batch setting, we adopt the same strategies in Hoffer et al. (2017) for adjusting the search range of hyperparameters. Finally, we retrain the model with both training data and validation data.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20default%20batch%20size%20128%20is%20used%20unless%20stated%20otherwise%20In%20SGD%20and%20Adam%20we%20train%20the%20networks%20with%20a%20budge%20of%20200%20epochs%20and%20decay%20the%20learning%20rate%20by%20a%20factor%20of%2010%20every%2060%20epochs%20for%20batch%20sizes%20of%20128%20and%20640%20and%20every%2080%20epochs%20for%20the%20batch%20size%20of%202K%20Whereas%20we%20train%20the%20networks%20only%20with%20100%20epochs%20and%20decay%20the%20learning%20rate%20every%2040%20epochs%20in%20KFAC%20Additionally%20the%20curvature%20matrix%20is%20updated%20by%20running%20average%20with%20reestimation%20every%2010%20iterations%20and%20the%20inverse%20operator%20is%20amortized%20to%20100%20iterations%20For%20KFAC%20we%20use%20fixed%20damping%20term%201e3%20unless%20state%20otherwise%20For%20each%20algorithm%20best%20hyperparameters%20learning%20rate%20and%20regularization%20factor%20are%20selected%20using%20grid%20search%20on%20heldout%205k%20validation%20set%20For%20the%20large%20batch%20setting%20we%20adopt%20the%20same%20strategies%20in%20Hoffer%20et%20al%202017%20for%20adjusting%20the%20search%20range%20of%20hyperparameters%20Finally%20we%20retrain%20the%20model%20with%20both%20training%20data%20and%20validation%20data",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20default%20batch%20size%20128%20is%20used%20unless%20stated%20otherwise%20In%20SGD%20and%20Adam%20we%20train%20the%20networks%20with%20a%20budge%20of%20200%20epochs%20and%20decay%20the%20learning%20rate%20by%20a%20factor%20of%2010%20every%2060%20epochs%20for%20batch%20sizes%20of%20128%20and%20640%20and%20every%2080%20epochs%20for%20the%20batch%20size%20of%202K%20Whereas%20we%20train%20the%20networks%20only%20with%20100%20epochs%20and%20decay%20the%20learning%20rate%20every%2040%20epochs%20in%20KFAC%20Additionally%20the%20curvature%20matrix%20is%20updated%20by%20running%20average%20with%20reestimation%20every%2010%20iterations%20and%20the%20inverse%20operator%20is%20amortized%20to%20100%20iterations%20For%20KFAC%20we%20use%20fixed%20damping%20term%201e3%20unless%20state%20otherwise%20For%20each%20algorithm%20best%20hyperparameters%20learning%20rate%20and%20regularization%20factor%20are%20selected%20using%20grid%20search%20on%20heldout%205k%20validation%20set%20For%20the%20large%20batch%20setting%20we%20adopt%20the%20same%20strategies%20in%20Hoffer%20et%20al%202017%20for%20adjusting%20the%20search%20range%20of%20hyperparameters%20Finally%20we%20retrain%20the%20model%20with%20both%20training%20data%20and%20validation%20data"
        },
        {
            "id": "f\u03b8(x)_2018_a",
            "entry": "f\u03b8(x) 2, we note that kronecker-product is exact under the condition that the network is linear (Bernacchia et al., 2018), which means GK\u2212FAC is the diagonal block version of Gauss-Newton matrix G. Therefore, we have \u03b8 = 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=f%CE%B8x%202%20we%20note%20that%20kroneckerproduct%20is%20exact%20under%20the%20condition%20that%20the%20network%20is%20linear%20Bernacchia%20et%20al%202018%20which%20means%20GKFAC%20is%20the%20diagonal%20block%20version%20of%20GaussNewton%20matrix%20G%20Therefore%20we%20have%20%CE%B8%20%202"
        },
        {
            "id": "Martens_2015_b",
            "entry": "Martens & Grosse (2015) proposed K-FAC for performing efficient natural gradient optimization in deep neural networks. Following on that work, K-FAC has been adopted in many tasks (Wu et al., 2017; Zhang et al., 2017) to gain optimization benefits, and was shown to be amendable to distributed computation (Ba et al., 2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%20Grosse%20proposed%20K-FAC%20for%20performing%20efficient%20natural%20gradient%20optimization%20in%20deep%20neural%20networks.%20Following%20on%20that%20work%2C%20K-FAC%20has%20been%20adopted%20in%20many%20tasks%20%28Wu%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%20Grosse%20proposed%20K-FAC%20for%20performing%20efficient%20natural%20gradient%20optimization%20in%20deep%20neural%20networks.%20Following%20on%20that%20work%2C%20K-FAC%20has%20been%20adopted%20in%20many%20tasks%20%28Wu%202015"
        },
        {
            "id": "As_2018_a",
            "entry": "As shown by Luk & Grosse (2018), K-FAC can be applied to general pullback metric, including Fisher metric and the Gauss-Newton metric. For convenience, we introduce K-FAC here using the Fisher metric.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=As%20shown%20by%20Luk%20%20Grosse%202018%20KFAC%20can%20be%20applied%20to%20general%20pullback%20metric%20including%20Fisher%20metric%20and%20the%20GaussNewton%20metric%20For%20convenience%20we%20introduce%20KFAC%20here%20using%20the%20Fisher%20metric",
            "oa_query": "https://api.scholarcy.com/oa_version?query=As%20shown%20by%20Luk%20%20Grosse%202018%20KFAC%20can%20be%20applied%20to%20general%20pullback%20metric%20including%20Fisher%20metric%20and%20the%20GaussNewton%20metric%20For%20convenience%20we%20introduce%20KFAC%20here%20using%20the%20Fisher%20metric"
        },
        {
            "id": "Ba_2015_a",
            "entry": "It has been shown that K-FAC scales very favorably to larger mini-batches compared to SGD, enjoying a nearly linear relationship between mini-batch size and per-iteration progress for medium-to-large sized mini-batches (Martens & Grosse, 2015; Ba et al., 2016). However, Keskar et al. (2016) showed that large-batch methods converge to sharp minima and generalize worse. In this subsection, we measure the generalization performance of K-FAC with large batch training and analyze the effect of weight decay. In Table 3, we compare K-FAC with SGD using different batch sizes. In particular, we interpolate between small-batch (BS128) and large-batch (BS2000). We can see that in accordance with previous works (Keskar et al., 2016; Hoffer et al., 2017) the move from a small-batch to a large-batch indeed incurs a substantial generalization gap. However, adding weight decay regularization to K-FAC almost close the gap on CIFAR-10 and cause much of the gap diminish on CIFAR-100.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%20It%20has%20been%20shown%20that%20K-FAC%20scales%20very%20favorably%20to%20larger%20mini-batches%20compared%20to%20SGD%2C%20enjoying%20a%20nearly%20linear%20relationship%20between%20mini-batch%20size%20and%20per-iteration%20progress%20for%20medium-to-large%20sized%20mini-batches%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%20It%20has%20been%20shown%20that%20K-FAC%20scales%20very%20favorably%20to%20larger%20mini-batches%20compared%20to%20SGD%2C%20enjoying%20a%20nearly%20linear%20relationship%20between%20mini-batch%20size%20and%20per-iteration%20progress%20for%20medium-to-large%20sized%20mini-batches%202015"
        }
    ]
}
