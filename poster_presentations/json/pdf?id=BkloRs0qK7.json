{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Improving deep neural networks for LVCSR using rectified linear units and dropout",
        "author": "George E. Dahl, Tara N. Sainath, Geoffrey E. Hinton",
        "date": 2013,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkloRs0qK7",
            "doi": "10.1109/icassp.2013.6639346"
        },
        "journal": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
        "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models."
    },
    "keywords": [
        {
            "term": "incremental learning",
            "url": "https://en.wikipedia.org/wiki/incremental_learning"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "fisher information matrix",
            "url": "https://en.wikipedia.org/wiki/fisher_information_matrix"
        },
        {
            "term": "catastrophic forgetting",
            "url": "https://en.wikipedia.org/wiki/Catastrophic_Forgetting"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "CF": "catastrophic forgetting\u201d",
        "DNN": "Deep Neural Network",
        "SLTs": "Sequential Learning Tasks",
        "DNNs": "Deep Neural Networks",
        "IMM": "Incremental Moment Matching technique",
        "CNN": "convolutional neural network",
        "EWC": "Elastic Weight Consolidation",
        "LWTA": "Local Winner Takes All"
    },
    "highlights": [
        "This article is in the context of sequential or incremental learning in Deep Neural Networks (DNNs)",
        "A related approach is pursued by the Incremental Moment Matching technique (IMM)), where weights from Deep Neural Network trained on a current and a past sub-tasks are \u201cmerged\u201d using the Fisher information matrix",
        "We investigate the incremental learning capacity of various Deep Neural Network approaches (Dropout, Local Winner Takes All, Elastic Weight Consolidation and Incremental Moment Matching technique) using the largest number of qualitatively different classification datasets so far described",
        "The results of the experiment described in Sec. 3 are summarized in Tab. 3, and in Tab. 4 for Incremental Moment Matching technique",
        "They lead us to the following principal conclusions: Permutation-based Sequential Learning Tasks should be used with caution We find that DP10-10, the Sequential Learning Tasks based on permutation, does not show catastrophic forgetting\u201d for any model and dataset, which is exemplary visualized for the FC model in Fig. 2 which fails completely for all other Sequential Learning Tasks",
        "The primary conclusion from the results in Sec. 4 is that catastrophic forgetting\u201d still represents a major problem when training Deep Neural Network. This is true if Deep Neural Network training happens under application constraints as outlined in Sec. 1.2"
    ],
    "key_statements": [
        "This article is in the context of sequential or incremental learning in Deep Neural Networks (DNNs)",
        "Neural networks have long been known to suffer from a problem termed \u201ccatastrophic forgetting\u201d(CF) (e.g., <a class=\"ref-link\" id=\"cFrench_1999_a\" href=\"#rFrench_1999_a\">French (1999</a>)) which denotes the abrupt and near-complete loss of knowledge from previous subtasks D1, . . . , Dk\u22121 after only a few training iterations on the current sub-task Dk",
        "We focus on Sequential Learning Tasks from the visual domain with two sub-tasks each, as Deep Neural Network show pronounced catastrophic forgetting\u201d behavior even when only two sub-tasks are involved",
        "A related approach is pursued by the Incremental Moment Matching technique (IMM)), where weights from Deep Neural Network trained on a current and a past sub-tasks are \u201cmerged\u201d using the Fisher information matrix",
        "We propose a training and evaluation paradigm for incremental learning in Deep Neural Network that enforces typical application constraints, see Sec. 1.2",
        "We investigate the incremental learning capacity of various Deep Neural Network approaches (Dropout, Local Winner Takes All, Elastic Weight Consolidation and Incremental Moment Matching technique) using the largest number of qualitatively different classification datasets so far described",
        "We collect a large number of visual classification datasets, from each of which we construct Sequential Learning Tasks according to a common scheme, and compare several recent Deep Neural Network models using these Sequential Learning Tasks",
        "The results of the experiment described in Sec. 3 are summarized in Tab. 3, and in Tab. 4 for Incremental Moment Matching technique",
        "They lead us to the following principal conclusions: Permutation-based Sequential Learning Tasks should be used with caution We find that DP10-10, the Sequential Learning Tasks based on permutation, does not show catastrophic forgetting\u201d for any model and dataset, which is exemplary visualized for the FC model in Fig. 2 which fails completely for all other Sequential Learning Tasks",
        "The primary conclusion from the results in Sec. 4 is that catastrophic forgetting\u201d still represents a major problem when training Deep Neural Network. This is true if Deep Neural Network training happens under application constraints as outlined in Sec. 1.2",
        "The primary conclusion from the results in Sec. 4 is that catastrophic forgetting\u201d still represents a major problem when training Deep Neural Network. This is true if Deep Neural Network training happens under application constraints as outlined in Sec. 1.2. Some of these constraints may be relaxed depending on the concrete application: if some prior knowledge about future sub-task exists, it can be used to simplify model selection wtIMM\n1.0 mode max. baseline",
        "In general application scenarios without prior knowledge or extra resources, an essential conclusion we draw from Sec. 4 is that model selection must form an integral part of training a Deep Neural Network on Sequential Learning Tasks",
        "We propose and implement such a procedure, and as a consequence claim that catastrophic forgetting\u201d is still very much of a problem for Deep Neural Network"
    ],
    "summary": [
        "This article is in the context of sequential or incremental learning in Deep Neural Networks (DNNs).",
        "We propose a training and evaluation paradigm for incremental learning in DNNs that enforces typical application constraints, see Sec. 1.2.",
        "We investigate the incremental learning capacity of various DNN approaches (Dropout, LWTA, EWC and IMM) using the largest number of qualitatively different classification datasets so far described.",
        "We collect a large number of visual classification datasets, from each of which we construct SLTs according to a common scheme, and compare several recent DNN models using these SLTs. The experimental protocol is such that application constraints, see Sec. 1.2, are enforced.",
        "For a given model m and an SLT (D1 and D2), the first step is to determine the best hyper-parameter vector p\u2217 for sub-task D1 only, which determines the model mp\u2217 used for re-training.",
        "Data: model m, SLT with sub-tasks D1, D2, hyper-parameter value set P Result: incremental learning quality for model with hyper-parameters p\u2217: qp\u2217",
        "For the D5-5 type SLTs, a modest incremental learning quality is attained, which is quite far away from the baseline accuracy, even for MNIST-derived SLTs. This is in contrast to the results reported in <a class=\"ref-link\" id=\"cLee_et+al_2017_a\" href=\"#rLee_et+al_2017_a\">Lee et al (2017</a>) for MNIST: we attribute this discrepancy to the application-oriented model selection procedure using only D1 that we perform.",
        "Fig. 7 and Fig. 8 give a visual impression of training an IMM model on D9-1 and D5-5 type SLTs, again illustrating basic feasibility, but the variability of the \u201ctuning curves\u201d we use to determine the optimal balancing parameter \u03b1.",
        "Comparable both in the number of tested models and benchmarks, <a class=\"ref-link\" id=\"cSerra_et+al_2018_a\" href=\"#rSerra_et+al_2018_a\">Serra et al (2018</a>) uses a different evaluation methodology imposing softer constraints than ours, which is strongly focused on application scenarios.",
        "In general application scenarios without prior knowledge or extra resources, an essential conclusion we draw from Sec. 4 is that model selection must form an integral part of training a DNN on SLTs. a wrong choice of hyper-parameters based on D1 can be disastrous for the remaining sub-tasks, which is why application scenarios require DNN variants that do not have extreme dependencies on hyper-parameters such as layer number and layer sizes.",
        "If model selection is addressed, a small subset of D1 may be kept in memory for both methods: to determine optimal values of \u03b1 for IMM and to determine when to stop re-training for EWC.",
        "Some of these constraints may be relaxed depending on the concrete application: if some prior knowledge about future sub-task exists, it can be used to simplify model selection CIFAR10 Fruits"
    ],
    "headline": "We present a large-scale empirical study of catastrophic forgetting in modern Deep Neural Network models that perform sequential learning",
    "reference_links": [
        {
            "id": "Sherif_0000_a",
            "entry": "Abdelazeem Sherif and El-Sherif Ezzat. AHDBase. URL http://datacenter.aucegypt.edu/shazeem/.",
            "url": "http://datacenter.aucegypt.edu/shazeem/"
        },
        {
            "id": "Acharya_et+al_2015_a",
            "entry": "Shailesh Acharya, Ashok Kumar Pant, and Prashnna Kumar Gyawali. Deep learning based large scale handwritten devanagari character recognition. In Software, Knowledge, Information Management and Applications (SKIMA), 2015 9th International Conference on, pp. 1\u20136. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Acharya%2C%20Shailesh%20Pant%2C%20Ashok%20Kumar%20Gyawali%2C%20Prashnna%20Kumar%20Deep%20learning%20based%20large%20scale%20handwritten%20devanagari%20character%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Acharya%2C%20Shailesh%20Pant%2C%20Ashok%20Kumar%20Gyawali%2C%20Prashnna%20Kumar%20Deep%20learning%20based%20large%20scale%20handwritten%20devanagari%20character%20recognition%202015"
        },
        {
            "id": "Aljundi_et+al_2018_a",
            "entry": "Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selfless sequential learning. arXiv preprint arXiv:1806.05421, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05421"
        },
        {
            "id": "Yaroslav_0000_a",
            "entry": "Bulatov Yaroslav. Machine Learning, etc: notMNIST dataset. URL http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html.",
            "url": "http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bulatov%20Yaroslav%20Machine%20Learning%20etc%20notMNIST%20dataset%20URL%20httpyaroslavvbblogspotcom201109notmnistdatasethtml"
        },
        {
            "id": "Cirean_et+al_2011_a",
            "entry": "Dan C. Cirean, Ueli Meier, Jonathan Masci, Luca M. Gambardella, and Jurgen Schmidhuber. Flexible, high performance convolutional neural networks for image classification. IJCAI International Joint Conference on Artificial Intelligence, pp. 1237\u20131242, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cirean%2C%20Dan%20C.%20Meier%2C%20Ueli%20Masci%2C%20Jonathan%20Gambardella%2C%20Luca%20M.%20Flexible%2C%20high%20performance%20convolutional%20neural%20networks%20for%20image%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cirean%2C%20Dan%20C.%20Meier%2C%20Ueli%20Masci%2C%20Jonathan%20Gambardella%2C%20Luca%20M.%20Flexible%2C%20high%20performance%20convolutional%20neural%20networks%20for%20image%20classification%202011"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to handwritten letters. Proceedings of the International Joint Conference on Neural Networks, pp. 2921\u20132926, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Gregory%20Afshar%2C%20Saeed%20Tapson%2C%20Jonathan%20Schaik%2C%20Andre%20Van%20EMNIST%3A%20Extending%20MNIST%20to%20handwritten%20letters%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Gregory%20Afshar%2C%20Saeed%20Tapson%2C%20Jonathan%20Schaik%2C%20Andre%20Van%20EMNIST%3A%20Extending%20MNIST%20to%20handwritten%20letters%202017"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "French_1999_a",
            "entry": "Robert French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3 (4):128\u2013135, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20Robert%20Catastrophic%20forgetting%20in%20connectionist%20networks.%20Trends%20in%20Cognitive%20Sciences%2C%203%201999"
        },
        {
            "id": "Gepperth_2016_a",
            "entry": "Alexander Gepperth and Barbara Hammer. Incremental learning algorithms and applications. European Symposium on Artificial Neural Networks (ESANN), (April):357\u2013368, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gepperth%2C%20Alexander%20Hammer%2C%20Barbara%20Incremental%20learning%20algorithms%20and%20applications%202016-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gepperth%2C%20Alexander%20Hammer%2C%20Barbara%20Incremental%20learning%20algorithms%20and%20applications%202016-04"
        },
        {
            "id": "Gepperth_2016_b",
            "entry": "Alexander Gepperth and Cem Karaoguz. A bio-inspired incremental learning architecture for applied perceptual problems. Cognitive Computation, 8(5):924\u2013934, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gepperth%2C%20Alexander%20Karaoguz%2C%20Cem%20A%20bio-inspired%20incremental%20learning%20architecture%20for%20applied%20perceptual%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gepperth%2C%20Alexander%20Karaoguz%2C%20Cem%20A%20bio-inspired%20incremental%20learning%20architecture%20for%20applied%20perceptual%20problems%202016"
        },
        {
            "id": "Goodfellow_et+al_2013_a",
            "entry": "Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6211"
        },
        {
            "id": "Kemker_2017_a",
            "entry": "Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. arXiv preprint arXiv:1711.10563, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10563"
        },
        {
            "id": "Kemker_et+al_2018_a",
            "entry": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kemker%2C%20Ronald%20McClure%2C%20Marc%20Abitino%2C%20Angelina%20Hayes%2C%20Tyler%20L.%20Measuring%20catastrophic%20forgetting%20in%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kemker%2C%20Ronald%20McClure%2C%20Marc%20Abitino%2C%20Angelina%20Hayes%2C%20Tyler%20L.%20Measuring%20catastrophic%20forgetting%20in%20neural%20networks%202018"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Hyo-Eun Kim, Seungwook Kim, and Jaehwan Lee. Keep and learn: Continual learning by constraining the latent space for knowledge preservation in neural networks. arXiv preprint arXiv:1805.10784, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10784"
        },
        {
            "id": "Kirkpatrick_et+al_2017_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Science Department, University of Toronto, Tech., pp. 1\u201360, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pp. 4652\u20134662, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017"
        },
        {
            "id": "Murean_2017_a",
            "entry": "Horea Murean and Mihai Oltean. Fruit recognition from images using deep learning. arXiv preprint arXiv:1712.00580, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00580"
        },
        {
            "id": "Netzer_et+al_2011_a",
            "entry": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Parisi_et+al_2018_a",
            "entry": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. arXiv preprint arXiv:1802.07569, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07569"
        },
        {
            "id": "Rebuffi_et+al_2017_a",
            "entry": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCARL: Incremental classifier and representation learning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5533\u20135542. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20Lampert%2C%20Christoph%20H.%20iCARL%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20Lampert%2C%20Christoph%20H.%20iCARL%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "Ren_et+al_2017_a",
            "entry": "Boya Ren, Hongzhi Wang, Jianzhong Li, and Hong Gao. Life-long learning based on dynamic combination model. Applied Soft Computing Journal, 56:398\u2013404, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Boya%20Wang%2C%20Hongzhi%20Li%2C%20Jianzhong%20Gao%2C%20Hong%20Life-long%20learning%20based%20on%20dynamic%20combination%20model%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Boya%20Wang%2C%20Hongzhi%20Li%2C%20Jianzhong%20Gao%2C%20Hong%20Life-long%20learning%20based%20on%20dynamic%20combination%20model%202017"
        },
        {
            "id": "Serra_et+al_2018_a",
            "entry": "Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Proceedings of the 35th International Conference on Machine Learning, pp. 4548\u20134557. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serra%2C%20Joan%20Suris%2C%20Didac%20Miron%2C%20Marius%20Karatzoglou%2C%20Alexandros%20Overcoming%20catastrophic%20forgetting%20with%20hard%20attention%20to%20the%20task%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serra%2C%20Joan%20Suris%2C%20Didac%20Miron%2C%20Marius%20Karatzoglou%2C%20Alexandros%20Overcoming%20catastrophic%20forgetting%20with%20hard%20attention%20to%20the%20task%202018"
        },
        {
            "id": "Shin_et+al_2017_a",
            "entry": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990\u20132999, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017"
        },
        {
            "id": "Srivastava_et+al_2013_a",
            "entry": "Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jurgen Schmidhuber. Compete to Compute. In Advances in Neural Information Processing Systems, pp. 2310\u20132318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rupesh%20Kumar%20Srivastava%20Jonathan%20Masci%20Sohrob%20Kazerounian%20Faustino%20Gomez%20and%20Jurgen%20Schmidhuber%20Compete%20to%20Compute%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2023102318%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rupesh%20Kumar%20Srivastava%20Jonathan%20Masci%20Sohrob%20Kazerounian%20Faustino%20Gomez%20and%20Jurgen%20Schmidhuber%20Compete%20to%20Compute%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2023102318%202013"
        },
        {
            "id": "Sutskever_et+al_2010_a",
            "entry": "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, (2010):8609\u20138613, 2013. doi: 10.1109/ICASSP. 2013.6639346.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2013.6639346",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2013.6639346"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, pp. 1\u20136, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        }
    ]
}
