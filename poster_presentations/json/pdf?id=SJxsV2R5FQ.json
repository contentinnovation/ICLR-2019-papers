{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING SPARSE RELATIONAL TRANSITION MODELS",
        "author": "Victoria Xia, Zi Wang, Kelsey Allen Tom Silver Leslie Pack Kaelbling",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJxsV2R5FQ"
        },
        "abstract": "We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects\u2019 properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects on a cluttered table."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "SPARE": "sparse relational transition model",
        "NN": "Neural network"
    },
    "highlights": [
        "Many complex domains are appropriately described in terms of sets of objects, properties of those objects, and relations among them",
        "In many important domains, ranging from interacting with physical objects to managing the operations of an airline, actions have localized effects: they may change the state of the object(s) being directly operated on, as well as some objects that are related to those objects in important ways, but will generally not affect the vast majority of other objects",
        "We present a strategy for learning state-transition models that embodies these assumptions",
        "Bidirectional edges connect every node in the graph",
        "The graph Neural network consists of encoders for the nodes and edges, propagation networks for message passing, and a node decoder to convert back to predict the mean and variance of the state of each object.\n5.3",
        "Our EM approach is able to concentrate to the 4-block case as shown in Fig. 5(a). Conclusion These results demonstrate the power of combining relational abstraction with neural networks, to learn probabilistic state transition models for an important class of domains from very little training data"
    ],
    "key_statements": [
        "Many complex domains are appropriately described in terms of sets of objects, properties of those objects, and relations among them",
        "In many important domains, ranging from interacting with physical objects to managing the operations of an airline, actions have localized effects: they may change the state of the object(s) being directly operated on, as well as some objects that are related to those objects in important ways, but will generally not affect the vast majority of other objects",
        "We present a strategy for learning state-transition models that embodies these assumptions",
        "Bidirectional edges connect every node in the graph",
        "The graph Neural network consists of encoders for the nodes and edges, propagation networks for message passing, and a node decoder to convert back to predict the mean and variance of the state of each object.\n5.3",
        "Our EM approach is able to concentrate to the 4-block case as shown in Fig. 5(a). Conclusion These results demonstrate the power of combining relational abstraction with neural networks, to learn probabilistic state transition models for an important class of domains from very little training data"
    ],
    "summary": [
        "Many complex domains are appropriately described in terms of sets of objects, properties of those objects, and relations among them.",
        "We structure our model in terms of rules, each of which only depends on and affects the properties and relations among a small number of objects in the domain, and only very few of which may apply for characterizing the effects of any given action.",
        "Returning to the definition of a transition rule, we can see informally that if the parameters of action template A are instantiated to actual objects in a problem instance, \u0393 and \u2206 can be used to determine lists of input and output objects.",
        "A transition rule T = (A, \u0393, \u2206, \u03c6\u03b8, vdefault) applies to a particular state-action (s, a) pair if a is an instance of A and if none of the elements of the input or output object lists is empty.",
        "On in action instance a, and successively computing references \u03b3i \u2208 \u0393 based on the previously selected objects, applying the definition of the deictic reference F in each \u03b3i to the actual values of the properties as specified in the state s.",
        "For a particular transition rule T with associated action template A, once \u0393 and \u2206 have been specified, we can extract input and output features x and y from a given set of experience samples E.",
        "Instead we first learn a single transition rule T = (A, \u0393, \u2206, \u03c6\u03b8, vdefault) from E using the algorithm in Section 4.2, use the resulting \u0393 and \u2206 to transform E into x and y, and run k-means clustering on the concatenation of x, y, and values of the loss function when T is used to predict each of the samples.",
        "Whereas a probabilistic transition rule has been defined as T = (A, \u0393, \u2206, \u03c6\u03b8, vdefault), a mixture rule is T = (A, \u03c0\u0393, \u03c0\u2206, \u03a6), where \u03c0\u0393 represents a distribution over all possible lists of input references \u0393, of which there are a finite number, since the set of available reference functions F is finite, and there is an upper bound N\u0393 on the maximum number of references \u0393 may contain.",
        "Conclusion These results demonstrate the power of combining relational abstraction with neural networks, to learn probabilistic state transition models for an important class of domains from very little training data.",
        "The structural nature of the learned models will allow them to be used in factored search-based planning methods that can take advantage of sparsity of effects to plan efficiently"
    ],
    "headline": "We present a representation for describing transition models in complex uncertain domains using relational rules",
    "reference_links": [
        {
            "id": "Agre_1987_a",
            "entry": "Philip E Agre and David Chapman. Pengi: An implementation of a theory of activity. In AAAI, volume 87, pp. 286\u2013272, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agre%2C%20Philip%20E.%20Chapman%2C%20David%20Pengi%3A%20An%20implementation%20of%20a%20theory%20of%20activity%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agre%2C%20Philip%20E.%20Chapman%2C%20David%20Pengi%3A%20An%20implementation%20of%20a%20theory%20of%20activity%201987"
        },
        {
            "id": "Battaglia_et+al_2018_a",
            "entry": "P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and graph networks. ArXiv e-prints, June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20P.W.%20Hamrick%2C%20J.B.%20Bapst%2C%20V.%20Sanchez-Gonzalez%2C%20A.%20Relational%20inductive%20biases%2C%20deep%20learning%2C%20and%20graph%20networks.%20ArXiv%20e-prints%202018-06"
        },
        {
            "id": "Benson_1997_a",
            "entry": "Scott S Benson. Learning action models for reactive autonomous agents. Technical report, Stanford University, Stanford, CA, USA, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benson%2C%20Scott%20S.%20Learning%20action%20models%20for%20reactive%20autonomous%20agents%201997"
        },
        {
            "id": "Coumans_2016_a",
            "entry": "Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016\u20132018.",
            "url": "http://pybullet.org"
        },
        {
            "id": "Drescher_1991_a",
            "entry": "Gary L Drescher. Made-up minds: a constructivist approach to artificial intelligence. MIT press, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drescher%2C%20Gary%20L.%20Made-up%20minds%3A%20a%20constructivist%20approach%20to%20artificial%20intelligence%201991"
        },
        {
            "id": "Kansky_et+al_2017_a",
            "entry": "Ken Kansky, Tom Silver, David A. Mely, Mohamed Eldawy, Miguel Lazaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, D. Scott Phoenix, and Dileep George. Schema networks: Zeroshot transfer with a generative causal model of intuitive physics. In Proceedings of the 34th International Conference on Machine Learning, pp. 1809\u20131818, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kansky%2C%20Ken%20Silver%2C%20Tom%20Mely%2C%20David%20A.%20Eldawy%2C%20Mohamed%20Schema%20networks%3A%20Zeroshot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kansky%2C%20Ken%20Silver%2C%20Tom%20Mely%2C%20David%20A.%20Eldawy%2C%20Mohamed%20Schema%20networks%3A%20Zeroshot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017"
        },
        {
            "id": "Lang_2010_a",
            "entry": "Tobias Lang and Marc Toussaint. Planning with noisy probabilistic relational rules. Journal of Artificial Intelligence Research, 39:1\u201349, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Planning%20with%20noisy%20probabilistic%20relational%20rules%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Planning%20with%20noisy%20probabilistic%20relational%20rules%202010"
        },
        {
            "id": "Marom_2018_a",
            "entry": "Ofir Marom and Benjamin Rosman. Zero-shot transfer with deictic object-oriented representation in reinforcement learning. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marom%2C%20Ofir%20Rosman%2C%20Benjamin%20Zero-shot%20transfer%20with%20deictic%20object-oriented%20representation%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marom%2C%20Ofir%20Rosman%2C%20Benjamin%20Zero-shot%20transfer%20with%20deictic%20object-oriented%20representation%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "Mourao_2014_a",
            "entry": "Kira Mourao. Learning probabilistic planning operators from noisy observations. In Proceedings of the Workshop of the UK Planning and Scheduling Special Interest Group, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mourao%2C%20Kira%20Learning%20probabilistic%20planning%20operators%20from%20noisy%20observations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mourao%2C%20Kira%20Learning%20probabilistic%20planning%20operators%20from%20noisy%20observations%202014"
        },
        {
            "id": "Mourao_et+al_2012_a",
            "entry": "Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, and Mark Steedman. Learning STRIPS operators from noisy and incomplete observations. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 614\u2013623, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mourao%2C%20Kira%20Zettlemoyer%2C%20Luke%20S.%20Petrick%2C%20Ronald%20P.A.%20Steedman%2C%20Mark%20Learning%20STRIPS%20operators%20from%20noisy%20and%20incomplete%20observations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mourao%2C%20Kira%20Zettlemoyer%2C%20Luke%20S.%20Petrick%2C%20Ronald%20P.A.%20Steedman%2C%20Mark%20Learning%20STRIPS%20operators%20from%20noisy%20and%20incomplete%20observations%202012"
        },
        {
            "id": "Pasula_et+al_2007_a",
            "entry": "Hanna M Pasula, Luke S Zettlemoyer, and Leslie Pack Kaelbling. Learning symbolic models of stochastic domains. Journal of Artificial Intelligence Research, 29:309\u2013352, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pasula%2C%20Hanna%20M.%20Zettlemoyer%2C%20Luke%20S.%20Kaelbling%2C%20Leslie%20Pack%20Learning%20symbolic%20models%20of%20stochastic%20domains%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pasula%2C%20Hanna%20M.%20Zettlemoyer%2C%20Luke%20S.%20Kaelbling%2C%20Leslie%20Pack%20Learning%20symbolic%20models%20of%20stochastic%20domains%202007"
        }
    ]
}
