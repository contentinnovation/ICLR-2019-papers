{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNIVERSAL SUCCESSOR FEATURES APPROXIMATORS",
        "author": "Diana Borsa, Andre Barreto, John Quan, Daniel Mankowitz, Remi Munos Hado van Hasselt, David Silver, Tom Schaul DeepMind London, UK {borsa,andrebarreto,johnquan,dmankowitz,munos, hado,davidsilver,schaul}@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1VWjiRcKX"
        },
        "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "dynamic programming",
            "url": "https://en.wikipedia.org/wiki/dynamic_programming"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "Reinforcement learning",
        "UVFAs": "universal valuefunction approximators",
        "GPI": "generalised policy improvement",
        "SFs": "successor features",
        "USFAs": "universal successor features approximators",
        "DP": "dynamic programming",
        "UVF": "universal value function",
        "GVFs": "general value functions",
        "USFA": "universal successor features approximator"
    },
    "highlights": [
        "Reinforcement learning (RL) provides a general framework to model sequential decision-making problems with sparse evaluative feedback in the form of rewards",
        "When we look at the relative performance of the agents, it is clear that universal successor features approximators perform considerably better than the unstructured universal valuefunction approximators. This is true even for the case where C = {w }, in which universal successor features approximators essentially reduce to a structured universal valuefunction approximators that was trained by decoupling tasks and policies",
        "We described the choices involved in training and evaluating a universal successor features approximators and discussed the trade-offs associated with these alternatives",
        "We show that universal successor features approximators can leverage the sort of parametric generalisation provided by universal valuefunction approximators and even improve on it, due to its decoupled training regime and the use of generalised policy improvement in areas where the approximation is not quite perfect",
        "Our second example is in some sense a reciprocal one, where we know from previous work that the generalisation provided via generalised policy improvement can be very effective even on a small set of policies, while generalising in the space of optimal policies, like universal valuefunction approximators do, seems to require a lot more data",
        "We believe universal successor features approximators are a powerful model that can exploit the available structure effectively: i) the structure induced by the shared dynamics, ii) the structure in the policy space and iii) the structure in the Reinforcement learning problem itself, and could potentially be useful across a wide range of Reinforcement learning applications that exhibit these properties"
    ],
    "key_statements": [
        "Reinforcement learning (RL) provides a general framework to model sequential decision-making problems with sparse evaluative feedback in the form of rewards",
        "We consider the usual Reinforcement learning framework: an agent interacts with an environment and selects actions in order to maximise the expected amount of reward received in the long run (<a class=\"ref-link\" id=\"cSutton_1998_a\" href=\"#rSutton_1998_a\">Sutton and Barto, 1998</a>)",
        "We show how in this example universal successor features approximators can effectively leverage both",
        "We show the learning curve of a universal valuefunction approximators",
        "Once an agent was trained on M we evaluated it on a separate set of unseen tasks, M , using the generalised policy improvement policy (3) over different sets of policies C",
        "One thing that immediately stands out in the figure is the fact that all architectures generalise quite well to the test tasks",
        "This is a surprisingly good result when we consider the difficulty of the scenario considered: recall that the agents are solving the test tasks without any learning taking place. This performance is even more impressive when we note that some test tasks contain negative rewards, something never experienced by the agents during training",
        "When we look at the relative performance of the agents, it is clear that universal successor features approximators perform considerably better than the unstructured universal valuefunction approximators. This is true even for the case where C = {w }, in which universal successor features approximators essentially reduce to a structured universal valuefunction approximators that was trained by decoupling tasks and policies",
        "In this paper we presented universal successor features approximators, a generalisation of universal valuefunction approximators through successor features",
        "The combination of universal successor features approximators and generalised policy improvement results in a powerful model capable of exploiting the same types of regularity exploited by its precursors: structure in the value function, like universal valuefunction approximators, and structure in the problem itself, like successor features&generalised policy improvement",
        "We described the choices involved in training and evaluating a universal successor features approximators and discussed the trade-offs associated with these alternatives",
        "To make the discussion concrete, we presented two examples aimed to illustrate different regimes of operation",
        "We show that universal successor features approximators can leverage the sort of parametric generalisation provided by universal valuefunction approximators and even improve on it, due to its decoupled training regime and the use of generalised policy improvement in areas where the approximation is not quite perfect",
        "Our second example is in some sense a reciprocal one, where we know from previous work that the generalisation provided via generalised policy improvement can be very effective even on a small set of policies, while generalising in the space of optimal policies, like universal valuefunction approximators do, seems to require a lot more data",
        "We show that universal successor features approximators can recover the type of generalisation provided by successor features when appropriate",
        "This example highlights some of the complexities involved in training at scale and shows how universal successor features approximators are readily applicable to this scenario",
        "We believe universal successor features approximators are a powerful model that can exploit the available structure effectively: i) the structure induced by the shared dynamics, ii) the structure in the policy space and iii) the structure in the Reinforcement learning problem itself, and could potentially be useful across a wide range of Reinforcement learning applications that exhibit these properties"
    ],
    "summary": [
        "Reinforcement learning (RL) provides a general framework to model sequential decision-making problems with sparse evaluative feedback in the form of rewards.",
        "<a class=\"ref-link\" id=\"cBarreto_et+al_2017_a\" href=\"#rBarreto_et+al_2017_a\">Barreto et al (2017</a>) provide theoretical guarantees on the performance of SF&GPI applied to any task w \u2208 M based on a fixed set of SFs. Below we state a slightly more general version of this result that highlights the two types of generalisation promoted by USFAs Theorem 2).",
        "Contrary to vanilla SF&GPI, USFA can discover and exploit the rich structure in the policyspace, enjoying the same generalisation properties as UVFAs but enhanced by the combination of the off-policy and off-task training regime.",
        "The evaluation on the test tasks M was done by \u201cfreezing\u201d the agent at different stages of the learning process and using the GPI policy (3) to select actions.",
        "SF&GPI on the training set M seems to provide a more effective way of generalising, as compared to UVFAs, even when the latter has a structure specialised to (1).",
        "Note that this can be done without changing M and is not possible with conventional UVFAs. One of the strengths of USFAs is exactly that: by disentangling tasks and policies, one can learn about the latter without ever having to try them out in the environment.",
        "This shows that USFAs can operate in two regimes: i) with limited coverage of the policy space, GPI over M will provide a reliable generalisation; ii) with a broader coverage of the space structured UVFAs will do increasingly better.1",
        "Solutions to this problem can result in better performance on the training set (<a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al, 2018</a>), can improve data efficiency (<a class=\"ref-link\" id=\"cTeh_et+al_2017_a\" href=\"#rTeh_et+al_2017_a\">Teh et al, 2017</a>) and enable generalisation to new tasks.",
        "In this paper we presented USFAs, a generalisation of UVFAs through SFs. The combination of USFAs and GPI results in a powerful model capable of exploiting the same types of regularity exploited by its precursors: structure in the value function, like UVFAs, and structure in the problem itself, like SF&GPI.",
        "We show that USFAs can leverage the sort of parametric generalisation provided by UVFAs and even improve on it, due to its decoupled training regime and the use of GPI in areas where the approximation is not quite perfect.",
        "Our second example is in some sense a reciprocal one, where we know from previous work that the generalisation provided via GPI can be very effective even on a small set of policies, while generalising in the space of optimal policies, like UVFAs do, seems to require a lot more data.",
        "We believe USFAs are a powerful model that can exploit the available structure effectively: i) the structure induced by the shared dynamics, ii) the structure in the policy space and iii) the structure in the RL problem itself, and could potentially be useful across a wide range of RL applications that exhibit these properties"
    ],
    "headline": "We focus on one aspect in particular, namely the ability to generalise to unseen tasks",
    "reference_links": [
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy sketches. arXiv preprint arXiv:1611.01796, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01796"
        },
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Ashar_1994_a",
            "entry": "R. Ashar. Hierarchical learning in stochastic domains. PhD thesis, Citeseer, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashar%2C%20R.%20Hierarchical%20learning%20in%20stochastic%20domains%201994"
        },
        {
            "id": "Barreto_et+al_2017_a",
            "entry": "A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20A.%20Dabney%2C%20W.%20Munos%2C%20R.%20Hunt%2C%20J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20A.%20Dabney%2C%20W.%20Munos%2C%20R.%20Hunt%2C%20J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "Barreto_et+al_2018_a",
            "entry": "A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In Proceedings of the International Conference on Machine Learning (ICML), pages 501\u2013510, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20A.%20Borsa%2C%20D.%20Quan%2C%20J.%20Schaul%2C%20T.%20Transfer%20in%20deep%20reinforcement%20learning%20using%20successor%20features%20and%20generalised%20policy%20improvement%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20A.%20Borsa%2C%20D.%20Quan%2C%20J.%20Schaul%2C%20T.%20Transfer%20in%20deep%20reinforcement%20learning%20using%20successor%20features%20and%20generalised%20policy%20improvement%202018"
        },
        {
            "id": "Beattie_et+al_2016_a",
            "entry": "C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Kuttler, A. Lefrancq, S. Green, V. Valdes, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03801"
        },
        {
            "id": "Devin_et+al_2017_a",
            "entry": "C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 2169\u20132176. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devin%2C%20C.%20Gupta%2C%20A.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devin%2C%20C.%20Gupta%2C%20A.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. CoRR, abs/1703.03400, 2017. URL http://arxiv.org/abs/1703.03400.",
            "url": "http://arxiv.org/abs/1703.03400",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "Heess_et+al_2016_a",
            "entry": "N. Heess, G. Wayne, Y. Tassa, T. Lillicrap, M. Riedmiller, and D. Silver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05182"
        },
        {
            "id": "Hermann_et+al_2017_a",
            "entry": "K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06551"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016. URL http://arxiv.org/abs/1612.00796.",
            "url": "http://arxiv.org/abs/1612.00796",
            "arxiv_url": "https://arxiv.org/pdf/1612.00796"
        },
        {
            "id": "Kulkarni_et+al_2016_a",
            "entry": "T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02396"
        },
        {
            "id": "Lazaric_2012_a",
            "entry": "A. Lazaric. Transfer in Reinforcement Learning: A Framework and a Survey, pages 143\u2013173. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaric%2C%20A.%20Transfer%20in%20Reinforcement%20Learning%3A%20A%20Framework%20and%20a%20Survey%202012"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "C. Ma, J. Wen, and Y. Bengio. Universal successor representations for transfer reinforcement learning. arXiv preprint arXiv:1804.03758, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03758"
        },
        {
            "id": "Mankowitz_et+al_2018_a",
            "entry": "D. J. Mankowitz, A. Z\u0131dek, A. Barreto, D. Horgan, M. Hessel, J. Quan, J. Oh, H. van Hasselt, D. Silver, and T. Schaul. Unicorn: Continual learning with a universal, off-policy agent. arXiv preprint arXiv:1802.08294, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08294"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "J. Oh, S. P. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. CoRR, abs/1706.05064, 2017. URL http://arxiv.org/abs/1706.05064.",
            "url": "http://arxiv.org/abs/1706.05064",
            "arxiv_url": "https://arxiv.org/pdf/1706.05064"
        },
        {
            "id": "Puterman_1994_a",
            "entry": "M. L. Puterman. Markov Decision Processes\u2014Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20M.L.%20Markov%20Decision%20Processes%E2%80%94Discrete%20Stochastic%20Dynamic%20Programming%201994"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016. URL http://arxiv.org/abs/1606.04671.",
            "url": "http://arxiv.org/abs/1606.04671",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal Value Function Approximators. In International Conference on Machine Learning (ICML), pages 1312\u20131320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20Value%20Function%20Approximators%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%20pages%2013121320%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20Value%20Function%20Approximators%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%20pages%2013121320%202015"
        },
        {
            "id": "Schmidhuber_1996_a",
            "entry": "J. Schmidhuber. A general method for incremental self-improvement and multi-agent learning in unrestricted environments. In Evolutionary Computation: Theory and Applications. Scientific Publishing Company, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20A%20general%20method%20for%20incremental%20self-improvement%20and%20multi-agent%20learning%20in%20unrestricted%20environments.%20In%20Evolutionary%20Computation%3A%20Theory%20and%20Applications%201996"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998. URL http://www-anw.cs.umass.edu/\u0303rich/book/the-book.html.",
            "url": "http://www-anw.cs.umass.edu/\u0303rich/book/the-book.html"
        },
        {
            "id": "Sutton_et+al_2011_a",
            "entry": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In International Conference on Autonomous Agents and Multiagent Systems, pages 761\u2013768, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Modayil%2C%20J.%20Delp%2C%20M.%20Degris%2C%20T.%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Modayil%2C%20J.%20Delp%2C%20M.%20Degris%2C%20T.%20Horde%3A%20A%20scalable%20real-time%20architecture%20for%20learning%20knowledge%20from%20unsupervised%20sensorimotor%20interaction%202011"
        },
        {
            "id": "Szepesvari_2010_a",
            "entry": "C. Szepesvari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesvari%2C%20C.%20Algorithms%20for%20Reinforcement%20Learning.%20Synthesis%20Lectures%20on%20Artificial%20Intelligence%20and%20Machine%20Learning%202010"
        },
        {
            "id": "Taylor_2009_a",
            "entry": "M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(1):1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009"
        },
        {
            "id": "Teh_et+al_2017_a",
            "entry": "Y. W. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 4499\u20134509, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.W.%20Bapst%2C%20V.%20Czarnecki%2C%20W.M.%20Quan%2C%20J.%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.W.%20Bapst%2C%20V.%20Czarnecki%2C%20W.M.%20Quan%2C%20J.%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. FeUdal networks for hierarchical reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 3540\u20133549, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Watkins_1989_a",
            "entry": "C. Watkins. Learning from Delayed Rewards. PhD thesis, University of Cambridge, England, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watkins%2C%20C.%20Learning%20from%20Delayed%20Rewards%201989"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard. Deep reinforcement learning with successor features for navigation across similar environments. CoRR, abs/1612.05533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.05533"
        }
    ]
}
