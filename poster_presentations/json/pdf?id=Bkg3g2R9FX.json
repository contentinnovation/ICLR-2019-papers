{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF LEARNING RATE",
        "author": "Liangchen Luo, Yuanhao Xiong, Yan Liu\u00a7, Xu Sun,\u00b6 \u2020MOE Key Lab of Computational Linguistics, School of EECS, Peking University \u2021College of Information Science and Electronic Engineering, Zhejiang University \u00a7Department of Computer Science, University of Southern California \u00b6Center for Data Science, Beijing Institute of Big Data Research, Peking University \u2020{luolc,xusun}@pku.edu.cn \u2021xiongyh@zju.edu.cn \u00a7yanliu.cs@usc.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bkg3g2R9FX"
        },
        "abstract": "Adaptive optimization methods such as ADAGRAD, RMSPROP and ADAM have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGRAD to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of ADAM and AMSGRAD, called ADABOUND and AMSBOUND respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound."
    },
    "keywords": [
        {
            "term": "ADAM",
            "url": "https://en.wikipedia.org/wiki/ADAM"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "learning rate",
            "url": "https://en.wikipedia.org/wiki/learning_rate"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "LSTM": "Long Short-Term Memory"
    },
    "highlights": [
        "There has been tremendous progress in first-order optimization algorithms for training deep neural networks",
        "Recent work has proposed a variety of adaptive methods that scale the gradient by square roots of some form of the average of the squared values of past gradients",
        "We further provide an example of a simple convex optimization problem to elucidate how tiny learning rates of adaptive methods can lead to undesirable non-convergence",
        "Based on the above analysis, we propose new variants of ADAM and AMSGRAD, named ADABOUND and AMSBOUND, which do not suffer from the negative impact of extreme learning rates",
        "We show that undesirable convergence behavior for ADAM and RMSPROP can be caused by extremely small learning rates, and in some cases no matter how large the initial step size \u03b1 is, ADAM will still fail to find the right path and converge to some highly suboptimal points",
        "We investigate existing adaptive algorithms and find that extremely large or small learning rates can result in the poor convergence behavior"
    ],
    "key_statements": [
        "There has been tremendous progress in first-order optimization algorithms for training deep neural networks",
        "Recent work has proposed a variety of adaptive methods that scale the gradient by square roots of some form of the average of the squared values of past gradients",
        "ADAM in particular has become the default algorithm leveraged across many deep learning frameworks due to its rapid training speed (<a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a>)",
        "We further provide an example of a simple convex optimization problem to elucidate how tiny learning rates of adaptive methods can lead to undesirable non-convergence",
        "Based on the above analysis, we propose new variants of ADAM and AMSGRAD, named ADABOUND and AMSBOUND, which do not suffer from the negative impact of extreme learning rates",
        "We show that undesirable convergence behavior for ADAM and RMSPROP can be caused by extremely small learning rates, and in some cases no matter how large the initial step size \u03b1 is, ADAM will still fail to find the right path and converge to some highly suboptimal points",
        "stochastic gradient descent performs slightly better than adaptive methods ADAM and AMSGRAD",
        "Despite the relative bad generalization ability of adaptive methods, our proposed methods overcome this drawback by allocating bounds for their learning rates and obtain almost the best accuracy on the test set for both DenseNet and ResNet on CIFAR-10.\n5.4",
        "We investigate existing adaptive algorithms and find that extremely large or small learning rates can result in the poor convergence behavior"
    ],
    "summary": [
        "There has been tremendous progress in first-order optimization algorithms for training deep neural networks.",
        "We employ dynamic bounds on learning rates in these adaptive methods, where the lower and upper bound are initialized as zero and infinity respectively, and they both smoothly converge to a constant final step size.",
        "There is an online convex optimization problem where for any initial step size \u03b1, ADAM has non-zero average regret i.e., RT /T 0 as T \u2192 \u221e.",
        "The following result shows that for any constant \u03b21 and \u03b22 with \u03b21 < \u03b22, there exists an example where ADAM has non-zero average regret asymptotically regardles\u221as of the initial step size \u03b1.",
        "For any constant \u03b21, \u03b22 \u2208 [0, 1) such that \u03b21 < \u03b22, there is an online convex optimization problem where for any initial step size \u03b1, ADAM has non-zero average regret i.e., RT /T 0 as T \u2192 \u221e.",
        "We turn to an empirical study of different models to compare new variants with popular optimization methods including SGD(M), ADAGRAD, ADAM, and AMSGRAD.",
        "SGD performs slightly better than adaptive methods ADAM and AMSGRAD.",
        "As for our methods, ADABOUND and AMSBOUND, they converge as fast as adaptive ones and achieve a bit higher accuracy than SGDM on the test set at the end of training.",
        "Despite the relative bad generalization ability of adaptive methods, our proposed methods overcome this drawback by allocating bounds for their learning rates and obtain almost the best accuracy on the test set for both DenseNet and ResNet on CIFAR-10.",
        "SGDM AdaGrad Adam AMSGrad AdaBound AMSBound 25 50 75 100 125 150 175 200 Epoch (c) Training Accuracy for ResNet-34",
        "Their variants, ADABOUND and AMSBOUND, on the other hand, demonstrate a fast speed of convergence compared with SGD while they exceed two original methods greatly with respect to test accuracy at the end of training.",
        "Extreme learning rates may appear more frequently in complex models such as ResNet. As our algorithms are proposed to avoid them, the greater enhancement of performance in complex architectures can be explained intuitively.",
        "Last but not least, applying dynamic bounds on learning rates is only one particular way to conduct gradual transformation from adaptive methods to SGD.",
        "We investigate existing adaptive algorithms and find that extremely large or small learning rates can result in the poor convergence behavior.",
        "ADABOUND and AMSBOUND, which employ dynamic bounds on their learning rates, achieve a smooth transition to SGD.",
        "They show the great efficacy on several standard benchmarks while maintaining advantageous properties of adaptive methods such as rapid initial progress and hyperparameter insensitivity"
    ],
    "headline": "We demonstrate that extreme learning rates can lead to poor performance",
    "reference_links": [
        {
            "id": "Cesa-Bianchi_et+al_2002_a",
            "entry": "Nico Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. In Advances in Neural Information Processing Systems 14 (NIPS), pp. 359\u2013 366, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nico%20Conconi%2C%20Alex%20Gentile%2C%20Claudio%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nico%20Conconi%2C%20Alex%20Gentile%2C%20Claudio%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202002"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type algorithms for non-convex optimization. CoRR, abs/1808.02941, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02941"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research (JMLR), 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u2013 1780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Keskar_2017_a",
            "entry": "Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from Adam to SGD. CoRR, abs/1712.07628, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07628"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pp. 2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Honglak_2009_a",
            "entry": "Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pp. 609\u2013616, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Honglak%20Lee%20Roger%20Grosse%20Rajesh%20Ranganath%20and%20Andrew%20Y%20Ng%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%20In%20Proceedings%20of%20the%2026th%20Annual%20International%20Conference%20on%20Machine%20Learning%20ICML%20pp%20609616%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Honglak%20Lee%20Roger%20Grosse%20Rajesh%20Ranganath%20and%20Andrew%20Y%20Ng%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%20In%20Proceedings%20of%20the%2026th%20Annual%20International%20Conference%20on%20Machine%20Learning%20ICML%20pp%20609616%202009"
        },
        {
            "id": "Luo_et+al_2019_a",
            "entry": "Liangchen Luo, Wenhao Huang, Qi Zeng, Zaiqing Nie, and Xu Sun. Learning personalized end-toend goal-oriented dialog. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI), 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Liangchen%20Huang%2C%20Wenhao%20Zeng%2C%20Qi%20Nie%2C%20Zaiqing%20Learning%20personalized%20end-toend%20goal-oriented%20dialog%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Liangchen%20Huang%2C%20Wenhao%20Zeng%2C%20Qi%20Nie%2C%20Zaiqing%20Learning%20personalized%20end-toend%20goal-oriented%20dialog%202019"
        },
        {
            "id": "Marcus_et+al_1993_a",
            "entry": "Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Comput. Linguist., 19(2):313\u2013330, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993"
        },
        {
            "id": "Mcmahan_2010_a",
            "entry": "H Brendan Mcmahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the 23rd Annual Conference On Learning Theory (COLT), pp. 244\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mcmahan%2C%20H.Brendan%20Streeter%2C%20Matthew%20Adaptive%20bound%20optimization%20for%20online%20convex%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mcmahan%2C%20H.Brendan%20Streeter%2C%20Matthew%20Adaptive%20bound%20optimization%20for%20online%20convex%20optimization%202010"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "Sashank J. Reddi, Stayen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Kale%2C%20Stayen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Kale%2C%20Stayen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. RMSprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20RMSprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20RMSprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Wilson_et+al_2017_a",
            "entry": "Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems 30 (NIPS), pp. 4148\u20134158, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20NIPS%20pp%2041484158%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20NIPS%20pp%2041484158%202017"
        },
        {
            "id": "Wu_2018_a",
            "entry": "Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the 15th European Conference on Computer Vision (ECCV), pp. 3\u201319, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuxin%20He%2C%20Kaiming%20Group%20normalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuxin%20He%2C%20Kaiming%20Group%20normalization%202018"
        }
    ]
}
