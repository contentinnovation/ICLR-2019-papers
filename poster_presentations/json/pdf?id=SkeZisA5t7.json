{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADAPTIVE ESTIMATORS SHOW INFORMATION COMPRESSION IN DEEP NEURAL NETWORKS",
        "author": "Ivan Chelombiev, Conor J. Houghton & Cian O\u2019Donnell \u2217 Department of Computer Science University of Bristol Bristol, UK {ic,conor.houghton,cian.odonnell}@bristol.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkeZisA5t7"
        },
        "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "kernel density estimation",
            "url": "https://en.wikipedia.org/wiki/kernel_density_estimation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "IB": "information bottleneck",
        "KDE": "kernel density estimation",
        "EBAB": "entropy-based adaptive binning",
        "aKDE": "adaptive KDE"
    },
    "highlights": [
        "Deep learning) has produced astonishing advances in machine learning (<a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\">Silver et al, 2017</a></a>), a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.<br/><br/>According to the information bottleneck (IB) theory of deep learning (<a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\"><a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\">Tishby & Zaslavsky, 2015</a></a>; <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\"><a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby, 2017</a></a>) the ability of deep neural networks to generalize can be seen as a type of representation compression",
        "In this paper we used a fully connected network consisting of 5 hidden layers with 10-7-5-4-3 units, similar to <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby (2017</a>) and some of the networks described in <a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\">Saxe et al (2018</a>)",
        "To individual ReLU initializations, when compression was compared with the average accuracy of networks, there was no significant correlation, as shown in Figure 9b. These findings suggest that the information bottleneck is not implemented by the whole network, and generalization can be achieved without compression in the hidden layers",
        "In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of deep neural networks",
        "We found that deep neural networks implemented with L2 regularization strongly compress information, forcing layers to forget information about the input",
        "The clustering of mutual information to a single point on the information plane has never been reported previously. This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization (<a class=\"ref-link\" id=\"cAchille_2018_a\" href=\"#rAchille_2018_a\">Achille & Soatto, 2018</a>), as well as linking information compression to memorization in neural networks (Zhang et al, 2016)"
    ],
    "key_statements": [
        "Deep learning) has produced astonishing advances in machine learning (<a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\">Silver et al, 2017</a></a>), a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.<br/><br/>According to the information bottleneck (IB) theory of deep learning (<a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\"><a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\">Tishby & Zaslavsky, 2015</a></a>; <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\"><a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby, 2017</a></a>) the ability of deep neural networks to generalize can be seen as a type of representation compression",
        "In this paper we used a fully connected network consisting of 5 hidden layers with 10-7-5-4-3 units, similar to <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby (2017</a>) and some of the networks described in <a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\">Saxe et al (2018</a>)",
        "To individual ReLU initializations, when compression was compared with the average accuracy of networks, there was no significant correlation, as shown in Figure 9b. These findings suggest that the information bottleneck is not implemented by the whole network, and generalization can be achieved without compression in the hidden layers",
        "In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of deep neural networks",
        "We found that deep neural networks implemented with L2 regularization strongly compress information, forcing layers to forget information about the input",
        "The clustering of mutual information to a single point on the information plane has never been reported previously. This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization (<a class=\"ref-link\" id=\"cAchille_2018_a\" href=\"#rAchille_2018_a\">Achille & Soatto, 2018</a>), as well as linking information compression to memorization in neural networks (Zhang et al, 2016)",
        "Overall we have three main observations: first, compression is not restricted to saturating activation functions, second, L2 regularization induces compression, and third, generalization accuracy is positively correlated with the degree of compression only in the last layer and is not significantly affected by compression of hidden layers"
    ],
    "summary": [
        "Deep learning) has produced astonishing advances in machine learning (<a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2017_a\" href=\"#rSilver_et+al_2017_a\">Silver et al, 2017</a></a>), a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.<br/><br/>According to the information bottleneck (IB) theory of deep learning (<a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\"><a class=\"ref-link\" id=\"cTishby_2015_a\" href=\"#rTishby_2015_a\">Tishby & Zaslavsky, 2015</a></a>; <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\"><a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby, 2017</a></a>) the ability of DNNs to generalize can be seen as a type of representation compression.",
        "When this study was replicated by <a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\"><a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\">Saxe et al (2018</a></a>) with networks using ReLU (<a class=\"ref-link\" id=\"cNair_2010_a\" href=\"#rNair_2010_a\">Nair & Hinton, 2010</a>) activation function instead of tanh, the compression phase did not happen, and the information planes only showed fitting throughout the whole training process.",
        "To estimate mutual information, <a class=\"ref-link\" id=\"cShwartz-Ziv_2017_a\" href=\"#rShwartz-Ziv_2017_a\">Shwartz-Ziv & Tishby (2017</a>) and <a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\"><a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\">Saxe et al (2018</a></a>) primarily relied on binning hidden activity.",
        "With non-saturating functions, and the resulting unbounded hidden activity, the level of noise brought by the estimation procedure has to be proportional and consistent, adapting to the state of every layer of the network at a particular epoch.",
        "It shows that L2 regularization leads to more compression and clusters all layers to the same value of mutual information.",
        "In <a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\"><a class=\"ref-link\" id=\"cSaxe_et+al_2018_a\" href=\"#rSaxe_et+al_2018_a\">Saxe et al (2018</a></a>) for networks with ReLU units in their hidden layer, all hidden activity was binned using a single range, going from zero to m, where m is the maximum value any ReLU unit has achieved throughout the training.",
        "Information planes of the same network in Figure 3 using ReLU analyzed with different estimators show how non-adaptive binning consistently underestimates compression.",
        "After deriving adaptive estimation framework, we investigated the occurrence of compression in networks with non-saturating activation functions.",
        "We generated 50 networks with different random initializations for every activation function to investigate the average behaviour.",
        "To individual ReLU initializations, when compression was compared with the average accuracy of networks, there was no significant correlation, as shown in Figure 9b.",
        "These findings suggest that the information bottleneck is not implemented by the whole network, and generalization can be achieved without compression in the hidden layers.",
        "In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of DNNs. These adaptive approaches allowed us to compare behaviour of different activation functions and to observe compression in DNNs with non-saturating activation functions.",
        "This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization (<a class=\"ref-link\" id=\"cAchille_2018_a\" href=\"#rAchille_2018_a\">Achille & Soatto, 2018</a>), as well as linking information compression to memorization in neural networks (Zhang et al, 2016).",
        "Overall we have three main observations: first, compression is not restricted to saturating activation functions, second, L2 regularization induces compression, and third, generalization accuracy is positively correlated with the degree of compression only in the last layer and is not significantly affected by compression of hidden layers."
    ],
    "headline": "In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions",
    "reference_links": [
        {
            "id": "Achille_2018_a",
            "entry": "Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018"
        },
        {
            "id": "Chechik_2003_a",
            "entry": "Gal Chechik and Naftali Tishby. Extracting relevant structures with side information. In Advances in Neural Information Processing Systems, pp. 881\u2013888, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chechik%2C%20Gal%20Tishby%2C%20Naftali%20Extracting%20relevant%20structures%20with%20side%20information%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chechik%2C%20Gal%20Tishby%2C%20Naftali%20Extracting%20relevant%20structures%20with%20side%20information%202003"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Elfwing_et+al_2018_a",
            "entry": "Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elfwing%2C%20Stefan%20Uchibe%2C%20Eiji%20Doya%2C%20Kenji%20Sigmoid-weighted%20linear%20units%20for%20neural%20network%20function%20approximation%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elfwing%2C%20Stefan%20Uchibe%2C%20Eiji%20Doya%2C%20Kenji%20Sigmoid-weighted%20linear%20units%20for%20neural%20network%20function%20approximation%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 315\u2013323, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%202011"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kohavi_1996_a",
            "entry": "Ron Kohavi and Mehran Sahami. Error-based and entropy-based discretization of continuous features. In KDD, pp. 114\u2013119, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohavi%2C%20Ron%20Sahami%2C%20Mehran%20Error-based%20and%20entropy-based%20discretization%20of%20continuous%20features%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohavi%2C%20Ron%20Sahami%2C%20Mehran%20Error-based%20and%20entropy-based%20discretization%20of%20continuous%20features%201996"
        },
        {
            "id": "Kolchinsky_2017_a",
            "entry": "Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolchinsky%2C%20Artemy%20Tracey%2C%20Brendan%20D.%20Estimating%20mixture%20entropy%20with%20pairwise%20distances%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolchinsky%2C%20Artemy%20Tracey%2C%20Brendan%20D.%20Estimating%20mixture%20entropy%20with%20pairwise%20distances%202017"
        },
        {
            "id": "Mishkin_et+al_2017_a",
            "entry": "Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of convolution neural network advances on the imagenet. Computer Vision and Image Understanding, 161:11\u201319, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishkin%2C%20Dmytro%20Sergievskiy%2C%20Nikolay%20Matas%2C%20Jiri%20Systematic%20evaluation%20of%20convolution%20neural%20network%20advances%20on%20the%20imagenet%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishkin%2C%20Dmytro%20Sergievskiy%2C%20Nikolay%20Matas%2C%20Jiri%20Systematic%20evaluation%20of%20convolution%20neural%20network%20advances%20on%20the%20imagenet%202017"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06959"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Ramachandran_et+al_2018_a",
            "entry": "Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05941"
        },
        {
            "id": "Saxe_et+al_2018_a",
            "entry": "Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20Michael%20Bansal%2C%20Yamini%20Dapello%2C%20Joel%20Advani%2C%20Madhu%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20Michael%20Bansal%2C%20Yamini%20Dapello%2C%20Joel%20Advani%2C%20Madhu%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018"
        },
        {
            "id": "Schmidhuber_2015_a",
            "entry": "Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In Information Theory Workshop (ITW), 2015 IEEE, pp. 1\u20135. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "Tishby_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        }
    ]
}
