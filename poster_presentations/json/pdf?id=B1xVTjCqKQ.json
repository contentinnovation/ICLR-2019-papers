{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A DATA-DRIVEN AND DISTRIBUTED APPROACH TO SPARSE SIGNAL REPRESENTATION AND RECOVERY",
        "author": "Ali Mousavi Google AI alimous@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1xVTjCqKQ"
        },
        "abstract": "In this paper, we focus on two challenges which offset the promise of sparse signal representation, sensing, and recovery. First, real-world signals can seldom be described as perfectly sparse vectors in a known basis, and traditionally used random measurement schemes are seldom optimal for sensing them. Second, existing signal recovery algorithms are usually not fast enough to make them applicable to real-time problems. In this paper, we address these two challenges by presenting a novel framework based on deep learning. For the first challenge, we cast the problem of finding informative measurements by using a maximum likelihood (ML) formulation and show how we can build a data-driven dimensionality reduction protocol for sensing signals using convolutional architectures. For the second challenge, we discuss and analyze a novel parallelization scheme and show it significantly speeds-up the signal recovery process. We demonstrate the significant improvement our method obtains over competing methods through a series of experiments."
    },
    "keywords": [
        {
            "term": "inverse problem",
            "url": "https://en.wikipedia.org/wiki/inverse_problem"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "compressive sensing",
            "url": "https://en.wikipedia.org/wiki/compressive_sensing"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        },
        {
            "term": "linear inverse problem",
            "url": "https://en.wikipedia.org/wiki/linear_inverse_problem"
        },
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "signal recovery",
            "url": "https://en.wikipedia.org/wiki/signal_recovery"
        },
        {
            "term": "compressed sensing",
            "url": "https://en.wikipedia.org/wiki/compressed_sensing"
        }
    ],
    "abbreviations": {
        "ML": "maximum likelihood",
        "CS": "compressive sensing",
        "SRA": "sparse recovery autoencoder",
        "DCN": "deep convolutional networks",
        "MSE": "mean squared error",
        "ANN": "approximate nearest neighbors"
    },
    "highlights": [
        "High-dimensional inverse problems and low-dimensional embeddings play a key role in a wide range of applications in machine learning and signal processing",
        "We first study the quality of the linear embeddings produced by DeepSSRR and its comparison with two other linear algorithms \u2013 NuMax (<a class=\"ref-link\" id=\"cHegde_et+al_2015_a\" href=\"#rHegde_et+al_2015_a\">Hegde et al, 2015</a>) and random Gaussian projections",
        "We train DeepSSRR and deep convolutional networks according to Algorithm 1 by using filters of size 5 \u00d7 5",
        "In this paper we introduced DeepSSRR, a framework that can learn both near-optimal sensing schemes, and fast signal recovery procedures",
        "Our findings set the stage for several directions for future exploration including the incorporation of adversarial training and its comparison with other methods (<a class=\"ref-link\" id=\"cBora_et+al_2017_a\" href=\"#rBora_et+al_2017_a\">Bora et al, 2017</a>; <a class=\"ref-link\" id=\"cDumoulin_et+al_2016_a\" href=\"#rDumoulin_et+al_2016_a\">Dumoulin et al, 2016</a>; <a class=\"ref-link\" id=\"cDonahue_et+al_2016_a\" href=\"#rDonahue_et+al_2016_a\">Donahue et al, 2016</a>)",
        "A major question arising from our work is quantifying the generalizability of a DeepSSRR-learned model based on the richness of training data"
    ],
    "key_statements": [
        "High-dimensional inverse problems and low-dimensional embeddings play a key role in a wide range of applications in machine learning and signal processing",
        "We show that the simultaneous learning of dimensionality reduction and reconstruction function using this formulation gives a lower-bound of the objective functions that needs to be optimized in learning the dimensionality reduction",
        "We show that our framework can learn dimensionality reductions that preserve specific geometric properties",
        "We show that our framework can outperform state-of-the-art signal recovery methods such as DAMP",
        "<a class=\"ref-link\" id=\"cShi_et+al_2017_a\" href=\"#rShi_et+al_2017_a\">Shi et al (2017</a>) and <a class=\"ref-link\" id=\"cKulkarni_et+al_2016_a\" href=\"#rKulkarni_et+al_2016_a\">Kulkarni et al (2016</a>) use an extra denoiser (e.g. BM3D, deep convolutional networks) for denoising the final reconstruction while our framework does not use any extra denoiser and yet outperforms state-of-the-art results as we show later",
        "The computational complexity of calculating the outputs of layers in DeepSSRR is O(M ) which is much less than the one for typical iterative and unrolled algorithms O(M N ) (e.g. DAMP and LDAMP (<a class=\"ref-link\" id=\"cMetzler_et+al_2016_a\" href=\"#rMetzler_et+al_2016_a\">Metzler et al, 2016</a>; 2017)) or previous recovery algorithms based on deep learning O(N ) (e.g",
        "We first study the quality of the linear embeddings produced by DeepSSRR and its comparison with two other linear algorithms \u2013 NuMax (<a class=\"ref-link\" id=\"cHegde_et+al_2015_a\" href=\"#rHegde_et+al_2015_a\">Hegde et al, 2015</a>) and random Gaussian projections",
        "We train DeepSSRR and deep convolutional networks according to Algorithm 1 by using filters of size 5 \u00d7 5",
        "We have considered two separate embedding problems: First M = 65 for random embedding and NuMax and M = 64 for deep convolutional networks and DeepSSRR\u2019s low-dimensional embedding",
        "In this paper we introduced DeepSSRR, a framework that can learn both near-optimal sensing schemes, and fast signal recovery procedures",
        "Our findings set the stage for several directions for future exploration including the incorporation of adversarial training and its comparison with other methods (<a class=\"ref-link\" id=\"cBora_et+al_2017_a\" href=\"#rBora_et+al_2017_a\">Bora et al, 2017</a>; <a class=\"ref-link\" id=\"cDumoulin_et+al_2016_a\" href=\"#rDumoulin_et+al_2016_a\">Dumoulin et al, 2016</a>; <a class=\"ref-link\" id=\"cDonahue_et+al_2016_a\" href=\"#rDonahue_et+al_2016_a\">Donahue et al, 2016</a>)",
        "A major question arising from our work is quantifying the generalizability of a DeepSSRR-learned model based on the richness of training data"
    ],
    "summary": [
        "High-dimensional inverse problems and low-dimensional embeddings play a key role in a wide range of applications in machine learning and signal processing.",
        "Our framework for sensing and recovering sparse signals can be considered as a variant of a convolutional autoencoder where the encoder is linear and the decoder is nonlinear and designed for CS application.",
        "In addition to image compression, there have been previous works (<a class=\"ref-link\" id=\"cShi_et+al_2017_a\" href=\"#rShi_et+al_2017_a\">Shi et al, 2017</a>; <a class=\"ref-link\" id=\"cKulkarni_et+al_2016_a\" href=\"#rKulkarni_et+al_2016_a\">Kulkarni et al, 2016</a>) to jointly learn the signal sensing and reconstruction algorithm in CS using convolutional networks.",
        "The computational complexity of calculating the outputs of layers in DeepSSRR is O(M ) which is much less than the one for typical iterative and unrolled algorithms O(M N ) (e.g. DAMP and LDAMP (<a class=\"ref-link\" id=\"cMetzler_et+al_2016_a\" href=\"#rMetzler_et+al_2016_a\"><a class=\"ref-link\" id=\"cMetzler_et+al_2016_a\" href=\"#rMetzler_et+al_2016_a\">Metzler et al, 2016</a></a>; 2017)) or previous recovery algorithms based on deep learning O(N ) (e.g. DeepInverse (<a class=\"ref-link\" id=\"cMousavi_2017_a\" href=\"#rMousavi_2017_a\">Mousavi & Baraniuk, 2017</a>)).",
        "According to (2), in the asymptotic setting, the measurement matrix which maximizes the probability of training data given its measurements, maximizes the mutual information between the input signal and undersampled measurements as well.",
        "EP(X)[log(q(X|Y = \u03a6X; \u039b))] \u2264 EP(X)[log(P(X|Y = \u03a6X; \u039b))], meaning that learning a parametric distribution for reconstructing X from Y is equivalent to maximizing a lower-bound of true conditional entropy and mutual information between the input signal X and undersampled measurements Y .",
        "The sensing part of DeepSSRR is a linear low-dimensional embedding that we can apply it to learn a mapping from a subset of RN to RM (M < N ) that is a near-isometry, i.e., a mapping that nearly preserves all inter-point distances.",
        "Algorithm 1 demonstrates the use of the low-dimensional embedding matrix \u03a6 of DeepSSRR to construct a near-isometric embedding.",
        "For DeepSSRR, depending on the size of the embedding we use five to seven layers to learn \u03a6 in Algorithm 1.",
        "Table 1 shows the isometry constant value of DeepSSRR\u2019s low-dimensional embedding with different number of layers and different filter sizes.",
        "We study the compressive image recovery problem by comparing DeepSSRR with two state-of-the-art algorithms DAMP (<a class=\"ref-link\" id=\"cMetzler_et+al_2016_a\" href=\"#rMetzler_et+al_2016_a\"><a class=\"ref-link\" id=\"cMetzler_et+al_2016_a\" href=\"#rMetzler_et+al_2016_a\">Metzler et al, 2016</a></a>) and LDAMP (<a class=\"ref-link\" id=\"cMetzler_et+al_2017_a\" href=\"#rMetzler_et+al_2017_a\">Metzler et al, 2017</a>).",
        "DeepSSRR uses only 7 convolutional layers to recover the Man image which is significantly smaller compared to LDAMP\u2019s number of layers.",
        "Iterative recovery algorithms and their unrolled versions such as DAMP and LDAMP typically involve a matrix vector multiplication in every iteration or layer, and their computational complexity is O(M N ).",
        "In this paper we introduced DeepSSRR, a framework that can learn both near-optimal sensing schemes, and fast signal recovery procedures."
    ],
    "headline": "We focus on two challenges which offset the promise of sparse signal representation, sensing, and recovery",
    "reference_links": [
        {
            "id": "Bah_et+al_2013_a",
            "entry": "B. Bah, A. Sadeghian, and V. Cevher. Energy-aware adaptive bi-Lipschitz embeddings. arXiv:1307.3457, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1307.3457"
        },
        {
            "id": "Baraniuk_2007_a",
            "entry": "R. G. Baraniuk. Compressive sensing. IEEE Signal Processing Mag., 24(4):118\u2013121, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baraniuk%2C%20R.G.%20Compressive%20sensing%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baraniuk%2C%20R.G.%20Compressive%20sensing%202007"
        },
        {
            "id": "Bora_et+al_2017_a",
            "entry": "A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. arXiv:1703.03208, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03208"
        },
        {
            "id": "Borgerding_2016_a",
            "entry": "M. Borgerding and P. Schniter. Onsager-corrected deep networks for sparse linear inverse problems. arXiv:1612.01183, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01183"
        },
        {
            "id": "Broomhead_2005_a",
            "entry": "D. Broomhead and M. Kirby. Dimensionality reduction using secant-based projection methods: The induced dynamics in projected systems. Nonlin. Dyn., 41(1):47\u201367, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Broomhead%2C%20D.%20Kirby%2C%20M.%20Dimensionality%20reduction%20using%20secant-based%20projection%20methods%3A%20The%20induced%20dynamics%20in%20projected%20systems%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Broomhead%2C%20D.%20Kirby%2C%20M.%20Dimensionality%20reduction%20using%20secant-based%20projection%20methods%3A%20The%20induced%20dynamics%20in%20projected%20systems%202005"
        },
        {
            "id": "Broomhead_2001_a",
            "entry": "D. S. Broomhead and M. J. Kirby. The Whitney reduction network: A method for computing autoassociative graphs. Neural Comput., 13(11):2595\u20132616, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Broomhead%2C%20D.S.%20Kirby%2C%20M.J.%20The%20Whitney%20reduction%20network%3A%20A%20method%20for%20computing%20autoassociative%20graphs%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Broomhead%2C%20D.S.%20Kirby%2C%20M.J.%20The%20Whitney%20reduction%20network%3A%20A%20method%20for%20computing%20autoassociative%20graphs%202001"
        },
        {
            "id": "E_2006_a",
            "entry": "E. Cand\u00e8s, J. Romberg,, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, Feb. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E.%20Cand%C3%A8s%2C%20J.%20Romberg%20Tao%2C%20T.%20Robust%20uncertainty%20principles%3A%20Exact%20signal%20reconstruction%20from%20highly%20incomplete%20frequency%20information%202006-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E.%20Cand%C3%A8s%2C%20J.%20Romberg%20Tao%2C%20T.%20Robust%20uncertainty%20principles%3A%20Exact%20signal%20reconstruction%20from%20highly%20incomplete%20frequency%20information%202006-02"
        },
        {
            "id": "Cand_2005_a",
            "entry": "E. J. Cand\u00e8s and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory, 51(12):4203 \u2013 4215, Dec. 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%202005-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%202005-12"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "J.H. Chang, C. Li, B. Poczos, V. Kumar, and A. C. Sankaranarayanan. One network to solve them all\u2014solving linear inverse problems using deep projection models. arXiv:1703.09912, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09912"
        },
        {
            "id": "Daubechies_et+al_2004_a",
            "entry": "I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. on Pure and Applied Math., 75:1412\u20131457, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daubechies%2C%20I.%20Defrise%2C%20M.%20Mol%2C%20C.De%20An%20iterative%20thresholding%20algorithm%20for%20linear%20inverse%20problems%20with%20a%20sparsity%20constraint%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daubechies%2C%20I.%20Defrise%2C%20M.%20Mol%2C%20C.De%20An%20iterative%20thresholding%20algorithm%20for%20linear%20inverse%20problems%20with%20a%20sparsity%20constraint%202004"
        },
        {
            "id": "Donahue_et+al_2016_a",
            "entry": "J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell. Adversarial feature learning. arXiv:1605.09782, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09782"
        },
        {
            "id": "Dong_et+al_2016_a",
            "entry": "C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. IEEE Trans. Pattern Anal. Machine Intell., 38(2):295\u2013307, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20C.%20Loy%2C%20C.%20He%2C%20K.%20Tang%2C%20X.%20Image%20super-resolution%20using%20deep%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20C.%20Loy%2C%20C.%20He%2C%20K.%20Tang%2C%20X.%20Image%20super-resolution%20using%20deep%20convolutional%20networks%202016"
        },
        {
            "id": "Donoho_2006_a",
            "entry": "D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, Apr. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.L.%20Compressed%20sensing%202006-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.L.%20Compressed%20sensing%202006-04"
        },
        {
            "id": "Donoho_et+al_2009_a",
            "entry": "D. L. Donoho, A. Maleki, and A. Montanari. Message passing algorithms for compressed sensing. Proc. Natl. Acad. Sci., 106(45):18914\u201318919, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.L.%20Maleki%2C%20A.%20Montanari%2C%20A.%20Message%20passing%20algorithms%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.L.%20Maleki%2C%20A.%20Montanari%2C%20A.%20Message%20passing%20algorithms%20for%20compressed%20sensing%202009"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "Friedman_et+al_2010_a",
            "entry": "J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.%20Hastie%2C%20T.%20Tibshirani%2C%20R.%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20J.%20Hastie%2C%20T.%20Tibshirani%2C%20R.%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010"
        },
        {
            "id": "Grant_et+al_2013_a",
            "entry": "E. Grant, C. Hegde, and P. Indyk. Nearly optimal linear embeddings into very low dimensions. In Global Conference on Signal and Information Processing (GlobalSIP), 2013 IEEE, pp. 973\u2013976. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grant%2C%20E.%20Hegde%2C%20C.%20Indyk%2C%20P.%20Nearly%20optimal%20linear%20embeddings%20into%20very%20low%20dimensions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grant%2C%20E.%20Hegde%2C%20C.%20Indyk%2C%20P.%20Nearly%20optimal%20linear%20embeddings%20into%20very%20low%20dimensions%202013"
        },
        {
            "id": "Gregor_2010_a",
            "entry": "K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 399\u2013406, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20K.%20LeCun%2C%20Y.%20Learning%20fast%20approximations%20of%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20K.%20LeCun%2C%20Y.%20Learning%20fast%20approximations%20of%20sparse%20coding%202010"
        },
        {
            "id": "Hegde_et+al_2015_a",
            "entry": "C. Hegde, A. C. Sankaranarayanan, W. Yin, and R. G. Baraniuk. Numax: A convex approach for learning near-isometric linear embeddings. IEEE Trans. Signal Processing, 63(22):6109\u20136121, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hegde%2C%20C.%20Sankaranarayanan%2C%20A.C.%20Yin%2C%20W.%20Baraniuk%2C%20R.G.%20Numax%3A%20A%20convex%20approach%20for%20learning%20near-isometric%20linear%20embeddings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hegde%2C%20C.%20Sankaranarayanan%2C%20A.C.%20Yin%2C%20W.%20Baraniuk%2C%20R.G.%20Numax%3A%20A%20convex%20approach%20for%20learning%20near-isometric%20linear%20embeddings%202015"
        },
        {
            "id": "Jiang_et+al_2017_a",
            "entry": "F. Jiang, W. Tao, S. Liu, J. Ren, X. Guo, and D. Zhao. An end-to-end compression framework based on convolutional neural networks. IEEE Trans. on Circuits and Systems for Video Technology, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20F.%20Tao%2C%20W.%20Liu%2C%20S.%20Ren%2C%20J.%20An%20end-to-end%20compression%20framework%20based%20on%20convolutional%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20F.%20Tao%2C%20W.%20Liu%2C%20S.%20Ren%2C%20J.%20An%20end-to-end%20compression%20framework%20based%20on%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "Kamilov_2016_a",
            "entry": "U. S. Kamilov and H. Mansour. Learning optimal nonlinearities for iterative thresholding algorithms. IEEE Signal Process. Lett., 23(5):747\u2013751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamilov%2C%20U.S.%20Mansour%2C%20H.%20Learning%20optimal%20nonlinearities%20for%20iterative%20thresholding%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamilov%2C%20U.S.%20Mansour%2C%20H.%20Learning%20optimal%20nonlinearities%20for%20iterative%20thresholding%20algorithms%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kulkarni_et+al_2016_a",
            "entry": "K. Kulkarni, S. Lohit, P. Turaga, R. Kerviche, and A. Ashok. Reconnet: Non-iterative reconstruction of images from compressively sensed random measurements. arXiv:1601.06892, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06892"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "H. Li, Z. Xu, G. Taylor, and T. Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09913"
        },
        {
            "id": "Linsker_1989_a",
            "entry": "Ralph Linsker. An application of the principle of maximum information preservation to linear systems. In Proc. Adv. in Neural Processing Systems (NIPS), pp. 186\u2013194, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linsker%2C%20Ralph%20An%20application%20of%20the%20principle%20of%20maximum%20information%20preservation%20to%20linear%20systems%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linsker%2C%20Ralph%20An%20application%20of%20the%20principle%20of%20maximum%20information%20preservation%20to%20linear%20systems%201989"
        },
        {
            "id": "Metzler_et+al_2017_a",
            "entry": "C. Metzler, A. Mousavi, and R. Baraniuk. Learned D-AMP: Principled neural network based compressive image recovery. In Proc. Adv. in Neural Processing Systems (NIPS), pp. 1770\u20131781, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzler%2C%20C.%20Mousavi%2C%20A.%20Baraniuk%2C%20R.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzler%2C%20C.%20Mousavi%2C%20A.%20Baraniuk%2C%20R.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017"
        },
        {
            "id": "Metzler_et+al_2016_a",
            "entry": "C. A. Metzler, A. Maleki, and R. G. Baraniuk. From denoising to compressed sensing. IEEE Transactions on Information Theory, 62(9):5117\u20135144, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzler%2C%20C.A.%20Maleki%2C%20A.%20Baraniuk%2C%20R.G.%20From%20denoising%20to%20compressed%20sensing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzler%2C%20C.A.%20Maleki%2C%20A.%20Baraniuk%2C%20R.G.%20From%20denoising%20to%20compressed%20sensing%202016"
        },
        {
            "id": "Mousavi_2017_a",
            "entry": "A. Mousavi and R. G. Baraniuk. Learning to invert: Signal recovery via deep convolutional networks. In Proc. IEEE Int. Conf. Acoust., Speech, and Signal Processing (ICASSP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mousavi%2C%20A.%20Baraniuk%2C%20R.G.%20Learning%20to%20invert%3A%20Signal%20recovery%20via%20deep%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mousavi%2C%20A.%20Baraniuk%2C%20R.G.%20Learning%20to%20invert%3A%20Signal%20recovery%20via%20deep%20convolutional%20networks%202017"
        },
        {
            "id": "Mousavi_et+al_2015_a",
            "entry": "A. Mousavi, A. B. Patel, and R. G. Baraniuk. A deep learning approach to structured signal recovery. In Proc. Allerton Conf. Communication, Control, and Computing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mousavi%2C%20A.%20Patel%2C%20A.B.%20Baraniuk%2C%20R.G.%20A%20deep%20learning%20approach%20to%20structured%20signal%20recovery%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mousavi%2C%20A.%20Patel%2C%20A.B.%20Baraniuk%2C%20R.G.%20A%20deep%20learning%20approach%20to%20structured%20signal%20recovery%202015"
        },
        {
            "id": "Needell_2009_a",
            "entry": "D. Needell and J. A. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Appl. Comput. Harmon. Anal., 26(3):301\u2013321, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Needell%2C%20D.%20Tropp%2C%20J.A.%20Cosamp%3A%20Iterative%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20samples%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Needell%2C%20D.%20Tropp%2C%20J.A.%20Cosamp%3A%20Iterative%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20samples%202009"
        },
        {
            "id": "Nelson_2013_a",
            "entry": "J. Nelson and H. Nguyen. Sparsity lower bounds for dimensionality reducing maps. In Proc. ACM Symp. Theory of Comput., pp. 101\u2013110. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20J.%20Nguyen%2C%20H.%20Sparsity%20lower%20bounds%20for%20dimensionality%20reducing%20maps%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20J.%20Nguyen%2C%20H.%20Sparsity%20lower%20bounds%20for%20dimensionality%20reducing%20maps%202013"
        },
        {
            "id": "Ol_et+al_2015_a",
            "entry": "Ol. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein. Imagenet large scale visual recognition challenge. Int. J. Computer Vision, 115 (3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ol.%20Russakovsky%2C%20J.Deng%20Su%2C%20H.%20Krause%2C%20J.%20Satheesh%2C%20S.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ol.%20Russakovsky%2C%20J.Deng%20Su%2C%20H.%20Krause%2C%20J.%20Satheesh%2C%20S.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Shaw_2007_a",
            "entry": "B. Shaw and T. Jebara. Minimum volume embedding. In Proc. Int. Conf. Art. Intell. Stat. (AISTATS), pp. 460\u2013467, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaw%2C%20B.%20Jebara%2C%20T.%20Minimum%20volume%20embedding%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaw%2C%20B.%20Jebara%2C%20T.%20Minimum%20volume%20embedding%202007"
        },
        {
            "id": "W_2016_a",
            "entry": "W. Shi, J. Caballero, F. Husz\u00e1r, J. Totz, A. P Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proc. IEEE Int. Conf. Comp. Vision, and Pattern Recognition, pp. 1874\u20131883, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=W%20Shi%20J%20Caballero%20F%20Husz%C3%A1r%20J%20Totz%20A%20P%20Aitken%20R%20Bishop%20D%20Rueckert%20and%20Z%20Wang%20Realtime%20single%20image%20and%20video%20superresolution%20using%20an%20efficient%20subpixel%20convolutional%20neural%20network%20In%20Proc%20IEEE%20Int%20Conf%20Comp%20Vision%20and%20Pattern%20Recognition%20pp%2018741883%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=W%20Shi%20J%20Caballero%20F%20Husz%C3%A1r%20J%20Totz%20A%20P%20Aitken%20R%20Bishop%20D%20Rueckert%20and%20Z%20Wang%20Realtime%20single%20image%20and%20video%20superresolution%20using%20an%20efficient%20subpixel%20convolutional%20neural%20network%20In%20Proc%20IEEE%20Int%20Conf%20Comp%20Vision%20and%20Pattern%20Recognition%20pp%2018741883%202016"
        },
        {
            "id": "Shi_et+al_2017_a",
            "entry": "W. Shi, F. Jiang, S. Zhang, and D. Zhao. Deep networks for compressed image sensing. In Multimedia and Expo (ICME), 2017 IEEE Intl. Conf. on, pp. 877\u2013882. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Jiang%2C%20F.%20Zhang%2C%20S.%20Zhao%2C%20D.%20Deep%20networks%20for%20compressed%20image%20sensing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Jiang%2C%20F.%20Zhang%2C%20S.%20Zhao%2C%20D.%20Deep%20networks%20for%20compressed%20image%20sensing%202017"
        },
        {
            "id": "_0000_a",
            "entry": "https://www.healthcare.",
            "url": "https://www.healthcare"
        },
        {
            "id": "Compressed-Sensing_2017_a",
            "entry": "compressed-sensing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=compressedsensing%202017"
        },
        {
            "id": "Tenenbaum_et+al_2000_a",
            "entry": "J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20J.B.%20Silva%2C%20V.De%20Langford%2C%20J.C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20J.B.%20Silva%2C%20V.De%20Langford%2C%20J.C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000"
        },
        {
            "id": "Tibshirani_1996_a",
            "entry": "R. Tibshirani. Regression shrinkage and selection via the LASSO. J. Roy. Stat. Soc., Series A, 58(1): 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20LASSO%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20LASSO%201996"
        },
        {
            "id": "Verma_2013_a",
            "entry": "N. Verma. Distance preserving embeddings for general n-dimensional manifolds. J. Mach. Learn. Res., 14(1):2415\u20132448, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verma%2C%20N.%20Distance%20preserving%20embeddings%20for%20general%20n-dimensional%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verma%2C%20N.%20Distance%20preserving%20embeddings%20for%20general%20n-dimensional%20manifolds%202013"
        },
        {
            "id": "Vincent_et+al_2010_a",
            "entry": "P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Research, 11(Dec):3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20P.%20Larochelle%2C%20H.%20Lajoie%2C%20I.%20Bengio%2C%20Y.%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20P.%20Larochelle%2C%20H.%20Lajoie%2C%20I.%20Bengio%2C%20Y.%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "Weinberger_2006_a",
            "entry": "K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semidefinite programming. Int. J. of Comput. Vis., 70(1):77\u201390, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202006"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "S. Wu, A. G Dimakis, S. Sanghavi, F. X Yu, D. Holtmann-Rice, D. Storcheus, A. Rostamizadeh, and S. Kumar. The sparse recovery autoencoder. arXiv preprint arXiv:1806.10175, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10175"
        },
        {
            "id": "Yao_et+al_2017_a",
            "entry": "H. Yao, F. Dai, D. Zhang, Y. Ma, S. Zhang, and Y. Zhang. DR2-net: Deep residual reconstruction network for image compressive sensing. arXiv:1702.05743, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05743"
        }
    ]
}
