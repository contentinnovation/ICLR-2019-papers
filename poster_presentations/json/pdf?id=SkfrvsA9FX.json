{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2018,
        "title": "Reward Constrained Policy Optimization",
        "author": "Chen Tessler, Daniel J. Mankowitz, and Shie Mannor",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkfrvsA9FX"
        },
        "journal": "RCPO",
        "abstract": "Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work, we present a novel multi-timescale approach for constrained policy optimization, called \u2018Reward Constrained Policy Optimization\u2019 (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies."
    },
    "keywords": [
        {
            "term": "stochastic approximation",
            "url": "https://en.wikipedia.org/wiki/stochastic_approximation"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        }
    ],
    "abbreviations": {
        "RCPO": "Reward Constrained Policy Optimization\u2019",
        "RL": "Reinforcement Learning",
        "MDP": "Markov Decision Process",
        "CMDP": "Constrained Markov Decision Process"
    },
    "highlights": [
        "Reward Constrained Policy Optimization\u2019 incorporates the constraint as a penalty signal into the reward function",
        "We propose the \u2018Reward Constrained Policy Optimization\u2019 (RCPO) algorithm",
        "In Reinforcement Learning, the value (JR\u03c0) increases as training progresses, this suggests that a non-adaptive approach is prone to converge to sub-optimal solutions; when the penalty is large, it is plausible that at the beginning of training the agent will only focus on constraint satisfaction and ignore the underlying reward signal, quickly converging to a local minima",
        "We introduced a novel constrained actor-critic approach, named \u2018Reward Constrained Policy Optimization\u2019 (RCPO)",
        "Reward Constrained Policy Optimization\u2019 uses a multi-timescale approach; on the fast timescale an alternative, discounted, objective is estimated using a TD-critic; on the intermediate timescale the policy is learned using policy gradient methods; and on the slow timescale the penalty coefficient \u03bb is learned by ascending on the original constraint",
        "An exciting extension of this work is the combination of Reward Constrained Policy Optimization\u2019 with CPO (<a class=\"ref-link\" id=\"cAchiam_et+al_2017_a\" href=\"#rAchiam_et+al_2017_a\">Achiam et al, 2017</a>)"
    ],
    "key_statements": [
        "Reward Constrained Policy Optimization\u2019 incorporates the constraint as a penalty signal into the reward function",
        "We propose the \u2018Reward Constrained Policy Optimization\u2019 (RCPO) algorithm",
        "In Reinforcement Learning, the value (JR\u03c0) increases as training progresses, this suggests that a non-adaptive approach is prone to converge to sub-optimal solutions; when the penalty is large, it is plausible that at the beginning of training the agent will only focus on constraint satisfaction and ignore the underlying reward signal, quickly converging to a local minima",
        "We introduced a novel constrained actor-critic approach, named \u2018Reward Constrained Policy Optimization\u2019 (RCPO)",
        "Reward Constrained Policy Optimization\u2019 uses a multi-timescale approach; on the fast timescale an alternative, discounted, objective is estimated using a TD-critic; on the intermediate timescale the policy is learned using policy gradient methods; and on the slow timescale the penalty coefficient \u03bb is learned by ascending on the original constraint",
        "An exciting extension of this work is the combination of Reward Constrained Policy Optimization\u2019 with CPO (<a class=\"ref-link\" id=\"cAchiam_et+al_2017_a\" href=\"#rAchiam_et+al_2017_a\">Achiam et al, 2017</a>)"
    ],
    "summary": [
        "RCPO incorporates the constraint as a penalty signal into the reward function.",
        "This penalty signal guides the policy towards a constraint satisfying solution.",
        "We prove that RCPO converges almost surely, under mild assumptions, to a constraint satisfying solution (Theorem 2).",
        "Assumption 2 is the minimal requirement in order to ensure convergence, given a general constraint, of a gradient algorithm to a feasible solution.",
        "Set of feasible solutions and the set of the \u2018Reward Constrained Policy localOptimization\u2019 (RCPO) algorithm converges almost surely to a fixed point (\u03b8\u2217(\u03bb\u2217, v\u2217), v\u2217(\u03bb\u2217), \u03bb\u2217)",
        "If such a policy is reachable from any \u03b8, this is enough in order to provide a theoretical guarantee such that JC\u03b3 may be used as a guiding signal in order to converge to a fixed-point, which is a feasible solution.",
        "The experiments show that, Table 2: Comparison between RCPO and reward shaping with a torque constraint < 25%.",
        "RCPO finds a feasible solution, and, besides the Walker2d-v2 domain, exhibits superior performance when compared to the relevant reward shaping variants.",
        "2. Selecting a constant coefficient \u03bb such that the policy satisfies the constraint is not a trivial task, resulting in different results across domains (<a class=\"ref-link\" id=\"cAchiam_et+al_2017_a\" href=\"#rAchiam_et+al_2017_a\">Achiam et al, 2017</a>).",
        "5.2.4 The Drawbacks of Reward Shaping When performing reward shaping, the experiments show that in domains where the agent attains a high value, the penalty coefficient is required to be larger in order for the solution to satisfy the constraints.",
        "In domains where the agent attains a relatively low value, the same penalty coefficients can lead to drastically different behavior - often with severely sub-optimal solutions (e.g. Ant-v2 compared to Swimmer-v2).",
        "In RL, the value (JR\u03c0) increases as training progresses, this suggests that a non-adaptive approach is prone to converge to sub-optimal solutions; when the penalty is large, it is plausible that at the beginning of training the agent will only focus on constraint satisfaction and ignore the underlying reward signal, quickly converging to a local minima.",
        "RCPO uses a multi-timescale approach; on the fast timescale an alternative, discounted, objective is estimated using a TD-critic; on the intermediate timescale the policy is learned using policy gradient methods; and on the slow timescale the penalty coefficient \u03bb is learned by ascending on the original constraint.",
        "We validate our approach using simulations on both grid-world and robotics domains and show that RCPO converges in a stable and sample efficient manner to a constraint satisfying policy."
    ],
    "headline": "We present a novel multi-timescale approach for constrained policy optimization, called \u2018Reward Constrained Policy Optimization\u2019, which uses an alternative penalty signal to guide the policy towards a constraint satisfying one",
    "reference_links": [
        {
            "id": "Achiam_et+al_2017_a",
            "entry": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv preprint arXiv:1705.10528, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10528"
        },
        {
            "id": "Altman_1999_a",
            "entry": "Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altman%2C%20Eitan%20Constrained%20Markov%20decision%20processes%2C%20volume%207%201999"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47: 253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Bertesekas_1999_a",
            "entry": "D Bertesekas. Nonlinear programming. athena scientific. Belmont, Massachusetts, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertesekas%2C%20D.%20Nonlinear%20programming.%20athena%20scientific%201999"
        },
        {
            "id": "Bhatnagar_2012_a",
            "entry": "Shalabh Bhatnagar and K Lakshmanan. An online actor\u2013critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688\u2013708, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatnagar%2C%20Shalabh%20Lakshmanan%2C%20K.%20An%20online%20actor%E2%80%93critic%20algorithm%20with%20function%20approximation%20for%20constrained%20markov%20decision%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhatnagar%2C%20Shalabh%20Lakshmanan%2C%20K.%20An%20online%20actor%E2%80%93critic%20algorithm%20with%20function%20approximation%20for%20constrained%20markov%20decision%20processes%202012"
        },
        {
            "id": "Borkar_2005_a",
            "entry": "Vivek S Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207\u2013213, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20Vivek%20S.%20An%20actor-critic%20algorithm%20for%20constrained%20markov%20decision%20processes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borkar%2C%20Vivek%20S.%20An%20actor-critic%20algorithm%20for%20constrained%20markov%20decision%20processes%202005"
        },
        {
            "id": "Borkar_2008_a",
            "entry": "Vivek S Borkar et al. Stochastic approximation. Cambridge Books, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20Vivek%20S.%20Stochastic%20approximation%202008"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "Chow_et+al_2015_a",
            "entry": "Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-making: a cvar optimization approach. In Advances in Neural Information Processing Systems, pages 1522\u20131530, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20Yinlam%20Tamar%2C%20Aviv%20Mannor%2C%20Shie%20Pavone%2C%20Marco%20Risk-sensitive%20and%20robust%20decision-making%3A%20a%20cvar%20optimization%20approach%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20Yinlam%20Tamar%2C%20Aviv%20Mannor%2C%20Shie%20Pavone%2C%20Marco%20Risk-sensitive%20and%20robust%20decision-making%3A%20a%20cvar%20optimization%20approach%202015"
        },
        {
            "id": "Dalal_et+al_2018_a",
            "entry": "Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08757"
        },
        {
            "id": "Gu_et+al_2017_a",
            "entry": "Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3389\u20133396. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017"
        },
        {
            "id": "Hou_2017_a",
            "entry": "Chen Hou and Qianchuan Zhao. Optimization of web service-based control system for balance between network traffic and delay. IEEE Transactions on Automation Science and Engineering, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hou%2C%20Chen%20Zhao%2C%20Qianchuan%20Optimization%20of%20web%20service-based%20control%20system%20for%20balance%20between%20network%20traffic%20and%20delay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hou%2C%20Chen%20Zhao%2C%20Qianchuan%20Optimization%20of%20web%20service-based%20control%20system%20for%20balance%20between%20network%20traffic%20and%20delay%202017"
        },
        {
            "id": "Kakade_2002_a",
            "entry": "Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pages 267\u2013274, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002"
        },
        {
            "id": "Kostrikov_2018_a",
            "entry": "Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github.com/ikostrikov/pytorch-a2c-ppo-acktr, 2018.",
            "url": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"
        },
        {
            "id": "Koutsopoulos_2011_a",
            "entry": "Iordanis Koutsopoulos and Leandros Tassiulas. Control and optimization meet the smart power grid: Scheduling of power demands for optimal energy management. In Proceedings of the 2nd International Conference on Energy-efficient Computing and Networking, pages 41\u201350. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koutsopoulos%2C%20Iordanis%20Tassiulas%2C%20Leandros%20Control%20and%20optimization%20meet%20the%20smart%20power%20grid%3A%20Scheduling%20of%20power%20demands%20for%20optimal%20energy%20management%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koutsopoulos%2C%20Iordanis%20Tassiulas%2C%20Leandros%20Control%20and%20optimization%20meet%20the%20smart%20power%20grid%3A%20Scheduling%20of%20power%20demands%20for%20optimal%20energy%20management%202011"
        },
        {
            "id": "Krokhmal_et+al_2002_a",
            "entry": "Pavlo Krokhmal, Jonas Palmquist, and Stanislav Uryasev. Portfolio optimization with conditional value-at-risk objective and constraints. Journal of risk, 4:43\u201368, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krokhmal%2C%20Pavlo%20Palmquist%2C%20Jonas%20Uryasev%2C%20Stanislav%20Portfolio%20optimization%20with%20conditional%20value-at-risk%20objective%20and%20constraints%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krokhmal%2C%20Pavlo%20Palmquist%2C%20Jonas%20Uryasev%2C%20Stanislav%20Portfolio%20optimization%20with%20conditional%20value-at-risk%20objective%20and%20constraints%202002"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07406"
        },
        {
            "id": "Leike_et+al_2017_a",
            "entry": "Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09883"
        },
        {
            "id": "Levine_2013_a",
            "entry": "Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1\u20139, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013"
        },
        {
            "id": "Mania_et+al_2018_a",
            "entry": "Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07055"
        },
        {
            "id": "Mannor_2004_a",
            "entry": "Shie Mannor and Nahum Shimkin. A geometric approach to multi-criterion reinforcement learning. Journal of machine learning research, 5(Apr):325\u2013360, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20A%20geometric%20approach%20to%20multi-criterion%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20A%20geometric%20approach%20to%20multi-criterion%20reinforcement%20learning%202004"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Roboschool_2017_a",
            "entry": "OpenAI. Roboschool. https://github.com/openai/roboschool, 2017.",
            "url": "https://github.com/openai/roboschool"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Peng_et+al_2018_a",
            "entry": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Prashanth_2016_a",
            "entry": "LA Prashanth and Mohammad Ghavamzadeh. Variance-constrained actor-critic algorithms for discounted and average reward mdps. Machine Learning, 105(3):367\u2013417, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prashanth%2C%20L.A.%20Ghavamzadeh%2C%20Mohammad%20Variance-constrained%20actor-critic%20algorithms%20for%20discounted%20and%20average%20reward%20mdps%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prashanth%2C%20L.A.%20Ghavamzadeh%2C%20Mohammad%20Variance-constrained%20actor-critic%20algorithms%20for%20discounted%20and%20average%20reward%20mdps%202016"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_0000_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "Tamar_2013_a",
            "entry": "Aviv Tamar and Shie Mannor. Variance adjusted actor critic algorithms. arXiv preprint arXiv:1310.3697, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1310.3697"
        },
        {
            "id": "Tamar_et+al_2012_a",
            "entry": "Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy gradients with variance related risk criteria. In Proceedings of the twenty-ninth international conference on machine learning, pages 387\u2013396, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tamar%2C%20Aviv%20Castro%2C%20Dotan%20Di%20Mannor%2C%20Shie%20Policy%20gradients%20with%20variance%20related%20risk%20criteria%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tamar%2C%20Aviv%20Castro%2C%20Dotan%20Di%20Mannor%2C%20Shie%20Policy%20gradients%20with%20variance%20related%20risk%20criteria%202012"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for modelbased control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20modelbased%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20modelbased%20control%202012"
        },
        {
            "id": "Moffaert_2014_a",
            "entry": "Kristof Van Moffaert and Ann Nowe. Multi-objective reinforcement learning using sets of pareto dominating policies. The Journal of Machine Learning Research, 15(1):3483\u20133512, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moffaert%2C%20Kristof%20Van%20Nowe%2C%20Ann%20Multi-objective%20reinforcement%20learning%20using%20sets%20of%20pareto%20dominating%20policies%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moffaert%2C%20Kristof%20Van%20Nowe%2C%20Ann%20Multi-objective%20reinforcement%20learning%20using%20sets%20of%20pareto%20dominating%20policies%202014"
        },
        {
            "id": "In_2002_a",
            "entry": "In order to avoid the issue of exploration in this domain, we employ a linearly decaying random restart (Kakade and Langford, 2002). \u03bc, the initial state distribution, follows the following rule: \u03bc=",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20order%20to%20avoid%20the%20issue%20of%20exploration%20in%20this%20domain%20we%20employ%20a%20linearly%20decaying%20random%20restart%20Kakade%20and%20Langford%202002%20%CE%BC%20the%20initial%20state%20distribution%20follows%20the%20following%20rule%20%CE%BC"
        },
        {
            "id": "For_2018_a",
            "entry": "For these experiments we used a PyTorch (Paszke et al., 2017) implementation of PPO (Kostrikov, 2018). Notice that as in each domain the state represents the location and velocity of each joint, the number of inputs differs between domains. The network is as follows: Layer 1 2 3 LR",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20these%20experiments%20we%20used%20a%20PyTorch%20Paszke%20et%20al%202017%20implementation%20of%20PPO%20Kostrikov%202018%20Notice%20that%20as%20in%20each%20domain%20the%20state%20represents%20the%20location%20and%20velocity%20of%20each%20joint%20the%20number%20of%20inputs%20differs%20between%20domains%20The%20network%20is%20as%20follows%20Layer%201%202%203%20LR",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20these%20experiments%20we%20used%20a%20PyTorch%20Paszke%20et%20al%202017%20implementation%20of%20PPO%20Kostrikov%202018%20Notice%20that%20as%20in%20each%20domain%20the%20state%20represents%20the%20location%20and%20velocity%20of%20each%20joint%20the%20number%20of%20inputs%20differs%20between%20domains%20The%20network%20is%20as%20follows%20Layer%201%202%203%20LR"
        },
        {
            "id": "The_0000_b",
            "entry": "The simulations were run using Generalized Advantage Estimation (Schulman et al., 2015b) with coefficient \u03c4 = 0.95 and discount factor \u03b3 = 0.99.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20simulations%20were%20run%20using%20Generalized%20Advantage%20Estimation%20Schulman%20et%20al%202015b%20with%20coefficient%20%CF%84%20%20095%20and%20discount%20factor%20%CE%B3%20%20099"
        },
        {
            "id": "We_2008_a",
            "entry": "We provide a brief proof for clarity. We refer the reader to Chapter 6 of Borkar et al. (2008) for a full proof of convergence for two-timescale stochastic approximation processes.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20provide%20a%20brief%20proof%20for%20clarity%20We%20refer%20the%20reader%20to%20Chapter%206%20of%20Borkar%20et%20al%202008%20for%20a%20full%20proof%20of%20convergence%20for%20twotimescale%20stochastic%20approximation%20processes",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20provide%20a%20brief%20proof%20for%20clarity%20We%20refer%20the%20reader%20to%20Chapter%206%20of%20Borkar%20et%20al%202008%20for%20a%20full%20proof%20of%20convergence%20for%20twotimescale%20stochastic%20approximation%20processes"
        },
        {
            "id": "2",
            "entry": "2. Convergence of \u03bb-recursion: This step is similar to earlier analysis for constrained MDPs. In particular, we show that \u03bb-recursion in (4) converges and the overall convergence of (\u03b8k, \u03bbk) is to a local saddle point (\u03b8\u2217(\u03bb\u2217, \u03bb\u2217) of L(\u03bb, \u03b8). Thus (6) can be seen as a discretization of the ODE (12). Finally, using the standard stochastic approximation arguments from Borkar et al. (2008) concludes step 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Convergence%20of%20%CE%BBrecursion%20This%20step%20is%20similar%20to%20earlier%20analysis%20for%20constrained%20MDPs%20In%20particular%20we%20show%20that%20%CE%BBrecursion%20in%204%20converges%20and%20the%20overall%20convergence%20of%20%CE%B8k%20%CE%BBk%20is%20to%20a%20local%20saddle%20point%20%CE%B8%CE%BB%20%CE%BB%20of%20L%CE%BB%20%CE%B8%20Thus%206%20can%20be%20seen%20as%20a%20discretization%20of%20the%20ODE%2012%20Finally%20using%20the%20standard%20stochastic%20approximation%20arguments%20from%20Borkar%20et%20al%202008%20concludes%20step%201",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Convergence%20of%20%CE%BBrecursion%20This%20step%20is%20similar%20to%20earlier%20analysis%20for%20constrained%20MDPs%20In%20particular%20we%20show%20that%20%CE%BBrecursion%20in%204%20converges%20and%20the%20overall%20convergence%20of%20%CE%B8k%20%CE%BBk%20is%20to%20a%20local%20saddle%20point%20%CE%B8%CE%BB%20%CE%BB%20of%20L%CE%BB%20%CE%B8%20Thus%206%20can%20be%20seen%20as%20a%20discretization%20of%20the%20ODE%2012%20Finally%20using%20the%20standard%20stochastic%20approximation%20arguments%20from%20Borkar%20et%20al%202008%20concludes%20step%201"
        },
        {
            "id": "As_2008_c",
            "entry": "As shown in Borkar et al. (2008) chapter 6, (\u03bbn, \u03b8n) converges to the internally chain transitive invariant sets of the ODE (13), \u03b8t = 0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=As%20shown%20in%20Borkar%20et%20al%202008%20chapter%206%20%CE%BBn%20%CE%B8n%20converges%20to%20the%20internally%20chain%20transitive%20invariant%20sets%20of%20the%20ODE%2013%20%CE%B8t%20%200",
            "oa_query": "https://api.scholarcy.com/oa_version?query=As%20shown%20in%20Borkar%20et%20al%202008%20chapter%206%20%CE%BBn%20%CE%B8n%20converges%20to%20the%20internally%20chain%20transitive%20invariant%20sets%20of%20the%20ODE%2013%20%CE%B8t%20%200"
        },
        {
            "id": "Finally_2008_a",
            "entry": "Finally, as seen in Theorem 2 of Chapter 2 of Borkar et al. (2008), \u03b8n \u2192 \u03b8\u2217 a.s. then \u03bbn \u2192 \u03bb(\u03b8\u2217) a.s. which completes the proof.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finally%20as%20seen%20in%20Theorem%202%20of%20Chapter%202%20of%20Borkar%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finally%20as%20seen%20in%20Theorem%202%20of%20Chapter%202%20of%20Borkar%202008"
        },
        {
            "id": "The_2017_a",
            "entry": "The proof is obtained by a simple extension to that of Theorem 1. Assumption 2 states that any local minima \u03c0\u03b8 of 2 satisfies the constraints, e.g. JC\u03c0\u03b8 \u2264 \u03b1; additionally, Lee et al. (2017) show that first order methods such as gradient descent, converge almost surely to a local minima (avoiding saddle points and local maxima). Hence for \u03bbmax = \u221e (unbounded Lagrange multiplier), the process converges to a fixed point (\u03b8\u2217(\u03bb\u2217), \u03bb\u2217) which is a feasible solution.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20proof%20is%20obtained%20by%20a%20simple%20extension%20to%20that%20of%20Theorem%201%20Assumption%202%20states%20that%20any%20local%20minima%20%CF%80%CE%B8%20of%202%20satisfies%20the%20constraints%20eg%20JC%CF%80%CE%B8%20%20%CE%B1%20additionally%20Lee%20et%20al%202017%20show%20that%20first%20order%20methods%20such%20as%20gradient%20descent%20converge%20almost%20surely%20to%20a%20local%20minima%20avoiding%20saddle%20points%20and%20local%20maxima%20Hence%20for%20%CE%BBmax%20%20%20unbounded%20Lagrange%20multiplier%20the%20process%20converges%20to%20a%20fixed%20point%20%CE%B8%CE%BB%20%CE%BB%20which%20is%20a%20feasible%20solution",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20proof%20is%20obtained%20by%20a%20simple%20extension%20to%20that%20of%20Theorem%201%20Assumption%202%20states%20that%20any%20local%20minima%20%CF%80%CE%B8%20of%202%20satisfies%20the%20constraints%20eg%20JC%CF%80%CE%B8%20%20%CE%B1%20additionally%20Lee%20et%20al%202017%20show%20that%20first%20order%20methods%20such%20as%20gradient%20descent%20converge%20almost%20surely%20to%20a%20local%20minima%20avoiding%20saddle%20points%20and%20local%20maxima%20Hence%20for%20%CE%BBmax%20%20%20unbounded%20Lagrange%20multiplier%20the%20process%20converges%20to%20a%20fixed%20point%20%CE%B8%CE%BB%20%CE%BB%20which%20is%20a%20feasible%20solution"
        },
        {
            "id": "As_2016_a",
            "entry": "As opposed to Theorem 1, in this case we are considering a three-timescale stochastic approximation scheme (the previous Theorem considered two-timescales). The proof is similar in essence to that of Prashanth and Ghavamzadeh (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=As%20opposed%20to%20Theorem%201%20in%20this%20case%20we%20are%20considering%20a%20threetimescale%20stochastic%20approximation%20scheme%20the%20previous%20Theorem%20considered%20twotimescales%20The%20proof%20is%20similar%20in%20essence%20to%20that%20of%20Prashanth%20and%20Ghavamzadeh%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=As%20opposed%20to%20Theorem%201%20in%20this%20case%20we%20are%20considering%20a%20threetimescale%20stochastic%20approximation%20scheme%20the%20previous%20Theorem%20considered%20twotimescales%20The%20proof%20is%20similar%20in%20essence%20to%20that%20of%20Prashanth%20and%20Ghavamzadeh%202016"
        },
        {
            "id": "Step_2016_c",
            "entry": "Step 3: As shown previously (and in Prashanth and Ghavamzadeh (2016)), (\u03bbn, \u03b8n, vn) \u2192 (\u03bb(\u03b8\u2217), \u03b8\u2217, v(\u03b8\u2217)) a.s.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Step%203%20As%20shown%20previously%20and%20in%20Prashanth%20and%20Ghavamzadeh%202016%20%CE%BBn%20%CE%B8n%20vn%20%20%CE%BB%CE%B8%20%CE%B8%20v%CE%B8%20as"
        },
        {
            "id": "Published_2019_d",
            "entry": "Published as a conference paper at ICLR 2019 We finish by providing intuition regarding the behavior in case the assumptions do not hold.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20We%20finish%20by%20providing%20intuition%20regarding%20the%20behavior%20in%20case%20the%20assumptions%20do%20not%20hold"
        },
        {
            "id": "1",
            "entry": "1. Assumption 2 does not hold: As gradient descent algorithms descend until reaching a (local) stationary point. In such a scenario, the algorithm is only ensured to converge to some stationary solution, yet said solution is not necessarily a feasible one. As such we can only treat the constraint as a regularizing term for the policy in which \u03bbmax defines the maximal regularization allowed.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Assumption%202%20does%20not%20hold%20As%20gradient%20descent%20algorithms%20descend%20until%20reaching%20a%20local%20stationary%20point%20In%20such%20a%20scenario%20the%20algorithm%20is%20only%20ensured%20to%20converge%20to%20some%20stationary%20solution%20yet%20said%20solution%20is%20not%20necessarily%20a%20feasible%20one%20As%20such%20we%20can%20only%20treat%20the%20constraint%20as%20a%20regularizing%20term%20for%20the%20policy%20in%20which%20%CE%BBmax%20defines%20the%20maximal%20regularization%20allowed"
        },
        {
            "id": "2",
            "entry": "2. Assumption 4 does not hold: In this case, it is not safe to assume that the gradient of (2) may be used as a guide for solving (3). A Monte-Carlo approach may be used (as seen in Section 5.1) to approximate the gradients, however this does not enjoy the benefits of reduced variance and smaller samples (due to the lack of a critic). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Assumption%204%20does%20not%20hold%20In%20this%20case%20it%20is%20not%20safe%20to%20assume%20that%20the%20gradient%20of%202%20may%20be%20used%20as%20a%20guide%20for%20solving%203%20A%20MonteCarlo%20approach%20may%20be%20used%20as%20seen%20in%20Section%2051%20to%20approximate%20the%20gradients%20however%20this%20does%20not%20enjoy%20the%20benefits%20of%20reduced%20variance%20and%20smaller%20samples%20due%20to%20the%20lack%20of%20a%20critic"
        }
    ]
}
