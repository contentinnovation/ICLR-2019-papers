{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GRADIENT DESCENT ALIGNS THE LAYERS OF DEEP",
        "author": "LINEAR NETWORKS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJflg30qKX"
        },
        "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment \u2014 a form of implicit regularization \u2014 of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized ith weight matrix asymptotically equals its rank-1 approximation uivi ; (iii) these rank-1 matrices are aligned across layers, meaning |vi+1ui| \u2192 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network \u2014 the product of its weight matrices \u2014 converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "gradient flow",
            "url": "https://en.wikipedia.org/wiki/gradient_flow"
        },
        {
            "term": "frobenius norm",
            "url": "https://en.wikipedia.org/wiki/frobenius_norm"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Efforts to explain the effectiveness of gradient descent in deep learning have uncovered an exciting possibility: it not only finds solutions with low error, but biases the search for low complexity solutions which generalize well (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\"><a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al, 2017</a></a>; <a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\"><a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\">Gunasekar et al, 2018</a></a>).<br/><br/>This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear networks and linearly separable data",
        "This paper considers gradient flow and gradient descent, where gradient flow W (t) t \u2265 0, t \u2208 R can be interpreted as gradient descent with infinitesimal step sizes",
        "Impractical when compared with gradient descent, gradient flow can simplify the analysis and highlight proof ideas",
        "This paper rigorously proves that, for deep linear networks on linearly separable data, gradient flow and gradient descent minimize the risk to 0, align adjacent weight matrices, and align the first right singular vector of the first layer to the maximum margin solution determined by the data",
        "This paper only proves asymptotic convergence, for adaptive step sizes depending on the current weight matrix norms",
        "A refined analysis with rates for practical step sizes would allow the algorithm to be compared to other methods which globally optimize this objective, would suggest ways to improve step sizes and initialization, and ideally even exhibit a sensitivity to the network architecture and suggest how it could be improved"
    ],
    "key_statements": [
        "Efforts to explain the effectiveness of gradient descent in deep learning have uncovered an exciting possibility: it not only finds solutions with low error, but biases the search for low complexity solutions which generalize well (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\"><a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al, 2017</a></a>; <a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\"><a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\">Gunasekar et al, 2018</a></a>).<br/><br/>This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear networks and linearly separable data",
        "This paper considers gradient flow and gradient descent, where gradient flow W (t) t \u2265 0, t \u2208 R can be interpreted as gradient descent with infinitesimal step sizes",
        "Impractical when compared with gradient descent, gradient flow can simplify the analysis and highlight proof ideas",
        "One key property of gradient flow which is used in the previous proofs is that it never increases the risk, which is not necessarily true for gradient descent",
        "This paper rigorously proves that, for deep linear networks on linearly separable data, gradient flow and gradient descent minimize the risk to 0, align adjacent weight matrices, and align the first right singular vector of the first layer to the maximum margin solution determined by the data",
        "This paper only proves asymptotic convergence, for adaptive step sizes depending on the current weight matrix norms",
        "A refined analysis with rates for practical step sizes would allow the algorithm to be compared to other methods which globally optimize this objective, would suggest ways to improve step sizes and initialization, and ideally even exhibit a sensitivity to the network architecture and suggest how it could be improved"
    ],
    "summary": [
        "Efforts to explain the effectiveness of gradient descent in deep learning have uncovered an exciting possibility: it not only finds solutions with low error, but biases the search for low complexity solutions which generalize well (<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>; <a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2017_a\" href=\"#rBartlett_et+al_2017_a\">Bartlett et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\"><a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al, 2017</a></a>; <a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\"><a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\">Gunasekar et al, 2018</a></a>).<br/><br/>This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear networks and linearly separable data.",
        "This paper analyzes the implicit regularization of gradient descent and gradient flow on deep linear networks and linearly separable data.",
        "For the exponential loss, assuming the risk is minimized to 0 and the gradients converge in direction, they show that the whole network converges in direction to the maximum margin solution.",
        "Compared with <a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\">Gunasekar et al (2018</a>), this paper proves that the risk converges to 0 and the weight matrices align; the proof here proves the properties simultaneously, rather than assuming one and deriving the other.",
        "We state the main result: under Assumptions 1 and 2, gradient flow minimizes the risk, Wk and wprod all go to infinity, and the alignment phenomenon occurs.",
        "Under Assumptions 1 and 2, gradient flow iterates satisfy the following properties:",
        "Lemma 2 shows that this is true: for gradient flow, as Wk F get larger, adjacent layers get more aligned to each other, which ensures that their product has a large norm.",
        "In addition to risk convergence, these two losses enable gradient descent to find the maximum margin solution.",
        "Under Assumptions 2 and 3, for almost all data and for losses exp and log, limt\u2192\u221e v1, u = 1, where v1 is the first right singular vector of W1.",
        "One key property of gradient flow which is used in the previous proofs is that it never increases the risk, which is not necessarily true for gradient descent.",
        "For smooth losses, we can design some decaying step sizes, with which gradient descent never increases the risk, and basically the same results hold as in the gradient flow case.",
        "Under Assumption 1, 2 and 4, suppose gradient descent is run with a constant step size 1/\u03b2(R).",
        "The step size \u03b7t = min{1/\u03b2(Rt), 1}, where Rt satisfies W (t) \u2208 B(Rt \u2212 1), and if W (t + 1) \u2208 B(Rt \u2212 1), Rt+1 = Rt. Assumption 5 can be satisfied by a line search, which ensures that the gradient descent update is not too aggressive and the boundary R is increased properly.",
        "This paper rigorously proves that, for deep linear networks on linearly separable data, gradient flow and gradient descent minimize the risk to 0, align adjacent weight matrices, and align the first right singular vector of the first layer to the maximum margin solution determined by the data.",
        "A refined analysis with rates for practical step sizes would allow the algorithm to be compared to other methods which globally optimize this objective, would suggest ways to improve step sizes and initialization, and ideally even exhibit a sensitivity to the network architecture and suggest how it could be improved"
    ],
    "headline": "Compared with Gunasekar et al, this paper proves that the risk converges to 0 and the weight matrices align; the proof here proves the properties simultaneously, rather than assuming one and deriving the other",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06509"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20Foster%2C%20Dylan%20Telgarsky%2C%20Matus%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20Foster%2C%20Dylan%20Telgarsky%2C%20Matus%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Bubeck_2015_a",
            "entry": "Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231\u2013357, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20Sebastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20Sebastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "Du_et+al_2018_a",
            "entry": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00900"
        },
        {
            "id": "Gunasekar_et+al_2018_a",
            "entry": "Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00468"
        },
        {
            "id": "Ji_2018_a",
            "entry": "Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07300"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffery%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffery%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Laurent_2017_a",
            "entry": "Thomas Laurent and James von Brecht. Deep linear neural networks with arbitrary loss: All local minima are global. arXiv preprint arXiv:1712.01473, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01473"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. arXiv preprint arXiv:1602.04915, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.04915"
        },
        {
            "id": "Lu_2017_a",
            "entry": "Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08580"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345v3, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345v3"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and landscape properties. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Yi%20Liang%2C%20Yingbin%20Critical%20points%20of%20linear%20neural%20networks%3A%20Analytical%20forms%20and%20landscape%20properties%202018"
        },
        {
            "id": "Recall_2018_a",
            "entry": "Proof of Lemma 2. The first claim is true for k = L since WL is a row vector. For any 1 \u2264 k < L, recall that Arora et al. (2018); Du et al. (2018) give the following relation: (5)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20of%20Lemma%202%20The%20first%20claim%20is%20true%20for%20k%20%20L%20since%20WL%20is%20a%20row%20vector%20For%20any%201%20%20k%20%20L%20recall%20that%20Arora%20et%20al%202018%20Du%20et%20al%202018%20give%20the%20following%20relation%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20of%20Lemma%202%20The%20first%20claim%20is%20true%20for%20k%20%20L%20since%20WL%20is%20a%20row%20vector%20For%20any%201%20%20k%20%20L%20recall%20that%20Arora%20et%20al%202018%20Du%20et%20al%202018%20give%20the%20following%20relation%205"
        },
        {
            "id": "2",
            "entry": "2. Regarding the last claim, first recall that since the difference between the squares of Frobenius norms of any two layers is a constant, max1\u2264k\u2264L Wk F \u2192 \u221e implies Wk F \u2192 \u221e for any k. We further have the following. Proof of Lemma 3. Soudry et al. (2017) Lemma 12 proves that, with probability 1, there are at most d support vectors, and moreover, the i-th support vector zi has a positive dual variable \u03b1i, such that i\u2208S \u03b1izi = u. Suppose there exists some \u03be \u22a5 u, such that maxi\u2208S \u03be, zi \u2264 0. Since \u03b1i \u03be, zi = \u03be, \u03b1izi = \u03be, u = 0, i\u2208S",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Regarding%20the%20last%20claim%20first%20recall%20that%20since%20the%20difference%20between%20the%20squares%20of%20Frobenius%20norms%20of%20any%20two%20layers%20is%20a%20constant%20max1kL%20Wk%20F%20%20%20implies%20Wk%20F%20%20%20for%20any%20k%20We%20further%20have%20the%20following%20Proof%20of%20Lemma%203%20Soudry%20et%20al%202017%20Lemma%2012%20proves%20that%20with%20probability%201%20there%20are%20at%20most%20d%20support%20vectors%20and%20moreover%20the%20ith%20support%20vector%20zi%20has%20a%20positive%20dual%20variable%20%CE%B1i%20such%20that%20iS%20%CE%B1izi%20%20u%20Suppose%20there%20exists%20some%20%CE%BE%20%20u%20such%20that%20maxiS%20%CE%BE%20zi%20%200%20Since%20%CE%B1i%20%CE%BE%20zi%20%20%CE%BE%20%CE%B1izi%20%20%CE%BE%20u%20%200%20iS",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Regarding%20the%20last%20claim%20first%20recall%20that%20since%20the%20difference%20between%20the%20squares%20of%20Frobenius%20norms%20of%20any%20two%20layers%20is%20a%20constant%20max1kL%20Wk%20F%20%20%20implies%20Wk%20F%20%20%20for%20any%20k%20We%20further%20have%20the%20following%20Proof%20of%20Lemma%203%20Soudry%20et%20al%202017%20Lemma%2012%20proves%20that%20with%20probability%201%20there%20are%20at%20most%20d%20support%20vectors%20and%20moreover%20the%20ith%20support%20vector%20zi%20has%20a%20positive%20dual%20variable%20%CE%B1i%20such%20that%20iS%20%CE%B1izi%20%20u%20Suppose%20there%20exists%20some%20%CE%BE%20%20u%20such%20that%20maxiS%20%CE%BE%20zi%20%200%20Since%20%CE%B1i%20%CE%BE%20zi%20%20%CE%BE%20%CE%B1izi%20%20%CE%BE%20u%20%200%20iS"
        },
        {
            "id": "0",
            "entry": "0. On the other hand, since after t \u2265 t0, wprod, zi ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=On%20the%20other%20hand%20since%20after%20t%20%20t0%20wprod%20zi"
        }
    ]
}
