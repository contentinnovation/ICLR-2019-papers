{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR",
        "author": "SENTENCE STRUCTURE IN CONTEXTUALIZED WORD",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJzSgnRcKX"
        },
        "abstract": "Contextualized representation models such as ELMo (<a class=\"ref-link\" id=\"cPeters_et+al_2018_a\" href=\"#rPeters_et+al_2018_a\">Peters et al., 2018a</a>) and BERT (<a class=\"ref-link\" id=\"cDevlin_et+al_2018_a\" href=\"#rDevlin_et+al_2018_a\">Devlin et al., 2018</a>) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "language understanding",
            "url": "https://en.wikipedia.org/wiki/language_understanding"
        },
        {
            "term": "language modeling",
            "url": "https://en.wikipedia.org/wiki/language_modeling"
        },
        {
            "term": "semantic role labeling",
            "url": "https://en.wikipedia.org/wiki/semantic_role_labeling"
        },
        {
            "term": "BERT",
            "url": "https://en.wikipedia.org/wiki/BERT"
        },
        {
            "term": "long range",
            "url": "https://en.wikipedia.org/wiki/long_range"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "SPR": "Semantic proto-role",
        "Rel.": "Relation Classification",
        "DPR": "Definite Pronoun Resolution",
        "BWB": "Billion Word Benchmark",
        "DNC": "Diverse Natural Language Collection",
        "NLI": "natural language inference"
    },
    "highlights": [
        "Instead of using a single, fixed vector per word type, these models run a pretrained encoder network over the sentence to produce contextual embeddings of each token",
        "This is pronounced on constituent and semantic role labeling, where the model may be benefiting from better handling of morphology by character-level or subword representations",
        "We find that the improvements of the BERT models are not uniform across tasks",
        "The SemEval relation classification task is designed to require semantic reasoning, but in this case we see a large improvement from contextual encoders, with ELMo improving by 22 F1 points on the lexical baseline (50% relative error reduction) and BERT-large improving by another 4.6 points",
        "We introduce a suite of \u201cedge probing\u201d tasks designed to probe the sub-sentential structure of contextualized word embeddings",
        "We focus on four recent models for contextualized word embeddings\u2013CoVe, ELMo, OpenAI GPT, and BERT"
    ],
    "key_statements": [
        "Instead of using a single, fixed vector per word type, these models run a pretrained encoder network over the sentence to produce contextual embeddings of each token",
        "We focus on these models because their pretrained weights and code are available, since these are most likely to be used by researchers",
        "We compare to word-level baselines to separate the contribution of context from lexical priors, and experiment with augmented baselines to better understand the role of pretraining and the ability of encoders to capture long-range dependencies.\n2 EDGE PROBING",
        "We report F1 scores for ELMo, CoVe, GPT, and BERT in Table 2",
        "It is important to note that while ELMo, CoVe, and the GPT can be applied to the same problems, they differ in architecture, training objective, and both the quantity and genre of training data (\u00a7 3.2)",
        "On all tasks except for Winograd coreference, the lexical representations used by the ELMo and GPT models outperform GloVe vectors",
        "This is pronounced on constituent and semantic role labeling, where the model may be benefiting from better handling of morphology by character-level or subword representations",
        "We find that the improvements of the BERT models are not uniform across tasks",
        "The semantic proto-role labeling task (SPR1, SPR2) looks at the same type of core predicate-argument pairs but tests for higher-level semantic properties (\u00a7 2), which we find to be only weakly captured by the contextual encoder (+1-5 F1 for ELMo)",
        "The SemEval relation classification task is designed to require semantic reasoning, but in this case we see a large improvement from contextual encoders, with ELMo improving by 22 F1 points on the lexical baseline (50% relative error reduction) and BERT-large improving by another 4.6 points",
        "We find that the CNN \u00b12 model matches ELMo performance, suggesting that while the ELMo encoder propagates a large amount of information about constituents (+15.4",
        "An alternative approach, which we adopt in this paper, is to directly probe the token representations for word- and phrase-level properties. This approach has been used previously to show that the representations learned by neural machine translation systems encode token-level properties like part-of-speech, semantic tags, and morphology (<a class=\"ref-link\" id=\"cShi_et+al_2016_a\" href=\"#rShi_et+al_2016_a\">Shi et al, 2016</a>; <a class=\"ref-link\" id=\"cBelinkov_et+al_2017_a\" href=\"#rBelinkov_et+al_2017_a\">Belinkov et al, 2017a</a>;b), as well as pairwise dependency relations (<a class=\"ref-link\" id=\"cBelinkov_2018_a\" href=\"#rBelinkov_2018_a\">Belinkov, 2018</a>)",
        "We extend sub-sentence probing to a broader range of syntactic and semantic tasks, including long-range and high-level relations such as predicate-argument structure",
        "We note that some of the tasks we explore overlap with those included in the Diverse Natural Language Collection, in particular, named entities, Semantic proto-role and Winograd",
        "We introduce a suite of \u201cedge probing\u201d tasks designed to probe the sub-sentential structure of contextualized word embeddings",
        "These tasks are derived from core NLP tasks and encompass a range of syntactic and semantic phenomena",
        "We focus on four recent models for contextualized word embeddings\u2013CoVe, ELMo, OpenAI GPT, and BERT"
    ],
    "summary": [
        "Instead of using a single, fixed vector per word type, these models run a pretrained encoder network over the sentence to produce contextual embeddings of each token.",
        "The model can access only embeddings within given spans, such as a predicate-argument pair, and must predict properties, such as semantic roles, which typically require whole-sentence context.",
        "In order to probe the effect of each contextual encoder, we train a version of our probing model directly on the most closely related context-independent word representations.",
        "We observe that ELMo, CoVe, and GPT all follow a similar trend across our suite (Table 2), showing the largest gains on tasks which are considered to be largely syntactic, such as dependency and constituent labeling, and smaller gains on tasks which are considered to require more semantic reasoning, such as SPR and Winograd.",
        "The semantic proto-role labeling task (SPR1, SPR2) looks at the same type of core predicate-argument pairs but tests for higher-level semantic properties (\u00a7 2), which we find to be only weakly captured by the contextual encoder (+1-5 F1 for ELMo).",
        "The SemEval relation classification task is designed to require semantic reasoning, but in this case we see a large improvement from contextual encoders, with ELMo improving by 22 F1 points on the lexical baseline (50% relative error reduction) and BERT-large improving by another 4.6 points.",
        "The full ELMo model holds up better, with performance dropping only 7 F1 points between d = 0 tokens and d = 8, suggesting the pretrained encoder does encode useful long-distance dependencies.",
        "Recent work has consistently demonstrated the strong empirical performance of contextualized word representations, including CoVe (<a class=\"ref-link\" id=\"cMccann_et+al_2017_a\" href=\"#rMccann_et+al_2017_a\">McCann et al, 2017</a>), ULMFit (<a class=\"ref-link\" id=\"cHoward_2018_a\" href=\"#rHoward_2018_a\">Howard & Ruder, 2018</a>), ELMo (<a class=\"ref-link\" id=\"cPeters_et+al_2018_a\" href=\"#rPeters_et+al_2018_a\">Peters et al, 2018a</a>; <a class=\"ref-link\" id=\"cLee_et+al_2018_a\" href=\"#rLee_et+al_2018_a\">Lee et al, 2018</a>; <a class=\"ref-link\" id=\"cStrubell_et+al_2018_a\" href=\"#rStrubell_et+al_2018_a\">Strubell et al, 2018</a>; <a class=\"ref-link\" id=\"cKitaev_2018_a\" href=\"#rKitaev_2018_a\">Kitaev & Klein, 2018</a>).",
        "This approach has been used previously to show that the representations learned by neural machine translation systems encode token-level properties like part-of-speech, semantic tags, and morphology (<a class=\"ref-link\" id=\"cShi_et+al_2016_a\" href=\"#rShi_et+al_2016_a\">Shi et al, 2016</a>; <a class=\"ref-link\" id=\"cBelinkov_et+al_2017_a\" href=\"#rBelinkov_et+al_2017_a\">Belinkov et al, 2017a</a>;b), as well as pairwise dependency relations (<a class=\"ref-link\" id=\"cBelinkov_2018_a\" href=\"#rBelinkov_2018_a\">Belinkov, 2018</a>).",
        "We extend sub-sentence probing to a broader range of syntactic and semantic tasks, including long-range and high-level relations such as predicate-argument structure.",
        "The performance of ELMo cannot be fully explained by a model with access to local context, suggesting that the contextualized representations do encode distant linguistic information, which can help disambiguate longer-range dependency relations and higher-level syntactic structures.",
        "We focus on four recent models for contextualized word embeddings\u2013CoVe, ELMo, OpenAI GPT, and BERT"
    ],
    "headline": "Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline",
    "reference_links": [
        {
            "id": "Adi_et+al_2017_a",
            "entry": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In Proceedings of ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adi%2C%20Yossi%20Kermany%2C%20Einat%20Belinkov%2C%20Yonatan%20Lavi%2C%20Ofer%20Fine-grained%20analysis%20of%20sentence%20embeddings%20using%20auxiliary%20prediction%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adi%2C%20Yossi%20Kermany%2C%20Einat%20Belinkov%2C%20Yonatan%20Lavi%2C%20Ofer%20Fine-grained%20analysis%20of%20sentence%20embeddings%20using%20auxiliary%20prediction%20tasks%202017"
        },
        {
            "id": "Belinkov_2018_a",
            "entry": "Yonatan Belinkov. On internal language representations in deep learning: An analysis of machine translation and speech recognition. PhD thesis, Massachusetts Institute of Technology, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belinkov%2C%20Yonatan%20On%20internal%20language%20representations%20in%20deep%20learning%3A%20An%20analysis%20of%20machine%20translation%20and%20speech%20recognition%202018"
        },
        {
            "id": "Belinkov_et+al_2017_a",
            "entry": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? In Proceedings of EMNLP, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belinkov%2C%20Yonatan%20Durrani%2C%20Nadir%20Dalvi%2C%20Fahim%20Sajjad%2C%20Hassan%20What%20do%20neural%20machine%20translation%20models%20learn%20about%20morphology%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belinkov%2C%20Yonatan%20Durrani%2C%20Nadir%20Dalvi%2C%20Fahim%20Sajjad%2C%20Hassan%20What%20do%20neural%20machine%20translation%20models%20learn%20about%20morphology%3F%202017"
        },
        {
            "id": "Belinkov_et+al_2017_b",
            "entry": "Yonatan Belinkov, Llu\u0131s Marquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks. In Proceedings of IJCNLP, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belinkov%2C%20Yonatan%20Marquez%2C%20Llu%C4%B1s%20Sajjad%2C%20Hassan%20Durrani%2C%20Nadir%20Evaluating%20layers%20of%20representation%20in%20neural%20machine%20translation%20on%20part-of-speech%20and%20semantic%20tagging%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belinkov%2C%20Yonatan%20Marquez%2C%20Llu%C4%B1s%20Sajjad%2C%20Hassan%20Durrani%2C%20Nadir%20Evaluating%20layers%20of%20representation%20in%20neural%20machine%20translation%20on%20part-of-speech%20and%20semantic%20tagging%20tasks%202017"
        },
        {
            "id": "Blevins_et+al_2018_a",
            "entry": "Terra Blevins, Omer Levy, and Luke Zettlemoyer. Deep RNNs encode soft hierarchical syntax. In Proceedings of ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blevins%2C%20Terra%20Levy%2C%20Omer%20Zettlemoyer%2C%20Luke%20Deep%20RNNs%20encode%20soft%20hierarchical%20syntax%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blevins%2C%20Terra%20Levy%2C%20Omer%20Zettlemoyer%2C%20Luke%20Deep%20RNNs%20encode%20soft%20hierarchical%20syntax%202018"
        },
        {
            "id": "Bojar_et+al_2017_a",
            "entry": "Ondrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer (eds.). Proceedings of the Second Conference on Machine Translation. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ondrej%20Bojar%20Christian%20Buck%20Rajen%20Chatterjee%20Christian%20Federmann%20Yvette%20Graham%20Barry%20Haddow%20Matthias%20Huck%20Antonio%20Jimeno%20Yepes%20Philipp%20Koehn%20and%20Julia%20Kreutzer%20eds%20Proceedings%20of%20the%20Second%20Conference%20on%20Machine%20Translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ondrej%20Bojar%20Christian%20Buck%20Rajen%20Chatterjee%20Christian%20Federmann%20Yvette%20Graham%20Barry%20Haddow%20Matthias%20Huck%20Antonio%20Jimeno%20Yepes%20Philipp%20Koehn%20and%20Julia%20Kreutzer%20eds%20Proceedings%20of%20the%20Second%20Conference%20on%20Machine%20Translation%202017"
        },
        {
            "id": "Chelba_et+al_2014_a",
            "entry": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. In Proceedings of Interspeech, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chelba%2C%20Ciprian%20Mikolov%2C%20Tomas%20Schuster%2C%20Mike%20Ge%2C%20Qi%20One%20billion%20word%20benchmark%20for%20measuring%20progress%20in%20statistical%20language%20modeling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chelba%2C%20Ciprian%20Mikolov%2C%20Tomas%20Schuster%2C%20Mike%20Ge%2C%20Qi%20One%20billion%20word%20benchmark%20for%20measuring%20progress%20in%20statistical%20language%20modeling%202014"
        },
        {
            "id": "Conneau_2018_a",
            "entry": "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20SentEval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20SentEval%3A%20An%20evaluation%20toolkit%20for%20universal%20sentence%20representations%202018"
        },
        {
            "id": "Conneau_et+al_2017_a",
            "entry": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u0131c Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017"
        },
        {
            "id": "Conneau_et+al_2018_b",
            "entry": "Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u0131c Barrault, and Marco Baroni. What you can cram into a single $&#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kruszewski%2C%20German%20Lample%2C%20Guillaume%20Barrault%2C%20Lo%C4%B1c%20What%20you%20can%20cram%20into%20a%20single%20%24%26%23%2A%20vector%3A%20Probing%20sentence%20embeddings%20for%20linguistic%20properties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kruszewski%2C%20German%20Lample%2C%20Guillaume%20Barrault%2C%20Lo%C4%B1c%20What%20you%20can%20cram%20into%20a%20single%20%24%26%23%2A%20vector%3A%20Probing%20sentence%20embeddings%20for%20linguistic%20properties%202018"
        },
        {
            "id": "Dasgupta_et+al_2018_a",
            "entry": "Ishita Dasgupta, Demi Guo, Andreas Stuhlmuller, Samuel J Gershman, and Noah D Goodman. Evaluating compositionality in sentence embeddings. arXiv preprint 1802.04302, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04302"
        },
        {
            "id": "Devlin_et+al_2018_a",
            "entry": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint 1810.04805, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.04805"
        },
        {
            "id": "Ettinger_et+al_2018_a",
            "entry": "Allyson Ettinger, Ahmed Elgohary, Colin Phillips, and Philip Resnik. Assessing composition in sentence vector representations. In Proceedings of COLING, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ettinger%2C%20Allyson%20Elgohary%2C%20Ahmed%20Phillips%2C%20Colin%20Resnik%2C%20Philip%20Assessing%20composition%20in%20sentence%20vector%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ettinger%2C%20Allyson%20Elgohary%2C%20Ahmed%20Phillips%2C%20Colin%20Resnik%2C%20Philip%20Assessing%20composition%20in%20sentence%20vector%20representations%202018"
        },
        {
            "id": "Gardner_et+al_2018_a",
            "entry": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. AllenNLP: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20Matt%20Grus%2C%20Joel%20Neumann%2C%20Mark%20Tafjord%2C%20Oyvind%20AllenNLP%3A%20A%20deep%20semantic%20natural%20language%20processing%20platform%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20Matt%20Grus%2C%20Joel%20Neumann%2C%20Mark%20Tafjord%2C%20Oyvind%20AllenNLP%3A%20A%20deep%20semantic%20natural%20language%20processing%20platform%202018"
        },
        {
            "id": "Gildea_2002_a",
            "entry": "Daniel Gildea and Martha Palmer. The necessity of parsing for predicate argument recognition. In Proceedings of ACL, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gildea%2C%20Daniel%20Palmer%2C%20Martha%20The%20necessity%20of%20parsing%20for%20predicate%20argument%20recognition%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gildea%2C%20Daniel%20Palmer%2C%20Martha%20The%20necessity%20of%20parsing%20for%20predicate%20argument%20recognition%202002"
        },
        {
            "id": "Gulordava_et+al_2018_a",
            "entry": "Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. Colorless green recurrent networks dream hierarchically. In Proceedings of NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulordava%2C%20Kristina%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Linzen%2C%20Tal%20Colorless%20green%20recurrent%20networks%20dream%20hierarchically%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulordava%2C%20Kristina%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Linzen%2C%20Tal%20Colorless%20green%20recurrent%20networks%20dream%20hierarchically%202018"
        },
        {
            "id": "He_et+al_2018_a",
            "entry": "Luheng He, Kenton Lee, Omer Levy, and Luke Zettlemoyer. Jointly predicting predicates and arguments in neural semantic role labeling. In Proceedings of ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Luheng%20Lee%2C%20Kenton%20Levy%2C%20Omer%20Zettlemoyer%2C%20Luke%20Jointly%20predicting%20predicates%20and%20arguments%20in%20neural%20semantic%20role%20labeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Luheng%20Lee%2C%20Kenton%20Levy%2C%20Omer%20Zettlemoyer%2C%20Luke%20Jointly%20predicting%20predicates%20and%20arguments%20in%20neural%20semantic%20role%20labeling%202018"
        },
        {
            "id": "Hendrickx_et+al_2009_a",
            "entry": "Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendrickx%2C%20Iris%20Kim%2C%20Su%20Nam%20Kozareva%2C%20Zornitsa%20Nakov%2C%20Preslav%20SemEval-2010%20task%208%3A%20Multi-way%20classification%20of%20semantic%20relations%20between%20pairs%20of%20nominals%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendrickx%2C%20Iris%20Kim%2C%20Su%20Nam%20Kozareva%2C%20Zornitsa%20Nakov%2C%20Preslav%20SemEval-2010%20task%208%3A%20Multi-way%20classification%20of%20semantic%20relations%20between%20pairs%20of%20nominals%202009"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Howard_2018_a",
            "entry": "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Howard%2C%20Jeremy%20Ruder%2C%20Sebastian%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Howard%2C%20Jeremy%20Ruder%2C%20Sebastian%20Universal%20language%20model%20fine-tuning%20for%20text%20classification%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kiros_et+al_2015_a",
            "entry": "Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Proceedings of NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "Kitaev_2018_a",
            "entry": "Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In Proceedings of ACL, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20parsing%20with%20a%20self-attentive%20encoder%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20parsing%20with%20a%20self-attentive%20encoder%202018-07"
        },
        {
            "id": "Kuncoro_et+al_2018_a",
            "entry": "Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, and Phil Blunsom. LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better. In Proceedings of ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuncoro%2C%20Adhiguna%20Dyer%2C%20Chris%20Hale%2C%20John%20Yogatama%2C%20Dani%20LSTMs%20can%20learn%20syntax-sensitive%20dependencies%20well%2C%20but%20modeling%20structure%20makes%20them%20better%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuncoro%2C%20Adhiguna%20Dyer%2C%20Chris%20Hale%2C%20John%20Yogatama%2C%20Dani%20LSTMs%20can%20learn%20syntax-sensitive%20dependencies%20well%2C%20but%20modeling%20structure%20makes%20them%20better%202018"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference resolution. In Proceedings of EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kenton%20He%2C%20Luheng%20Lewis%2C%20Mike%20Zettlemoyer%2C%20Luke%20End-to-end%20neural%20coreference%20resolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kenton%20He%2C%20Luheng%20Lewis%2C%20Mike%20Zettlemoyer%2C%20Luke%20End-to-end%20neural%20coreference%20resolution%202017"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Kenton Lee, Luheng He, and Luke Zettlemoyer. Higher-order coreference resolution with coarseto-fine inference. In Proceedings of NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kenton%20He%2C%20Luheng%20Zettlemoyer%2C%20Luke%20Higher-order%20coreference%20resolution%20with%20coarseto-fine%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kenton%20He%2C%20Luheng%20Zettlemoyer%2C%20Luke%20Higher-order%20coreference%20resolution%20with%20coarseto-fine%20inference%202018"
        },
        {
            "id": "Levesque_et+al_2012_a",
            "entry": "Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levesque%2C%20Hector%20J.%20Davis%2C%20Ernest%20Morgenstern%2C%20Leora%20The%20winograd%20schema%20challenge%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levesque%2C%20Hector%20J.%20Davis%2C%20Ernest%20Morgenstern%2C%20Leora%20The%20winograd%20schema%20challenge%202012"
        },
        {
            "id": "Linzen_et+al_2016_a",
            "entry": "Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of LSTMs to learn syntaxsensitive dependencies. Transactions of the ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20LSTMs%20to%20learn%20syntaxsensitive%20dependencies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linzen%2C%20Tal%20Dupoux%2C%20Emmanuel%20Goldberg%2C%20Yoav%20Assessing%20the%20ability%20of%20LSTMs%20to%20learn%20syntaxsensitive%20dependencies%202016"
        },
        {
            "id": "Marvin_2018_a",
            "entry": "Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. In Proceedings of EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marvin%2C%20Rebecca%20Linzen%2C%20Tal%20Targeted%20syntactic%20evaluation%20of%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marvin%2C%20Rebecca%20Linzen%2C%20Tal%20Targeted%20syntactic%20evaluation%20of%20language%20models%202018"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Proceedings of NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In Proceedings of NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20Proceedings%20of%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20Proceedings%20of%20NIPS%202017"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Peters_et+al_2018_b",
            "entry": "Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of EMNLP, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20Neumann%2C%20Mark%20Zettlemoyer%2C%20Luke%20Yih%2C%20Wen-tau%20Dissecting%20contextual%20word%20embeddings%3A%20Architecture%20and%20representation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20Neumann%2C%20Mark%20Zettlemoyer%2C%20Luke%20Yih%2C%20Wen-tau%20Dissecting%20contextual%20word%20embeddings%3A%20Architecture%20and%20representation%202018"
        },
        {
            "id": "Poliak_et+al_2018_a",
            "entry": "Adam Poliak, Yonatan Belinkov, James Glass, and Benjamin Van Durme. On the evaluation of semantic phenomena in neural machine translation using natural language inference. In Proceedings of NAACL, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poliak%2C%20Adam%20Belinkov%2C%20Yonatan%20Glass%2C%20James%20Durme%2C%20Benjamin%20Van%20On%20the%20evaluation%20of%20semantic%20phenomena%20in%20neural%20machine%20translation%20using%20natural%20language%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poliak%2C%20Adam%20Belinkov%2C%20Yonatan%20Glass%2C%20James%20Durme%2C%20Benjamin%20Van%20On%20the%20evaluation%20of%20semantic%20phenomena%20in%20neural%20machine%20translation%20using%20natural%20language%20inference%202018"
        },
        {
            "id": "Poliak_et+al_2018_b",
            "entry": "Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poliak%2C%20Adam%20Haldar%2C%20Aparajita%20Rachel%20Rudinger%2C%20J.Edward%20Hu%20Pavlick%2C%20Ellie%20Collecting%20diverse%20natural%20language%20inference%20problems%20for%20sentence%20representation%20evaluation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poliak%2C%20Adam%20Haldar%2C%20Aparajita%20Rachel%20Rudinger%2C%20J.Edward%20Hu%20Pavlick%2C%20Ellie%20Collecting%20diverse%20natural%20language%20inference%20problems%20for%20sentence%20representation%20evaluation%202018"
        },
        {
            "id": "Punyakanok_et+al_2008_a",
            "entry": "Vasin Punyakanok, Dan Roth, and Wen-tau Yih. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257\u2013287, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Punyakanok%2C%20Vasin%20Roth%2C%20Dan%20Yih%2C%20Wen-tau%20The%20importance%20of%20syntactic%20parsing%20and%20inference%20in%20semantic%20role%20labeling%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Punyakanok%2C%20Vasin%20Roth%2C%20Dan%20Yih%2C%20Wen-tau%20The%20importance%20of%20syntactic%20parsing%20and%20inference%20in%20semantic%20role%20labeling%202008"
        },
        {
            "id": "Radford_et+al_2018_a",
            "entry": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://blog.openai.com/language-unsupervised, 2018.",
            "url": "https://blog.openai.com/language-unsupervised"
        },
        {
            "id": "Rahman_2012_a",
            "entry": "Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: The Winograd schema challenge. In Proceedings of EMNLP, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahman%2C%20Altaf%20Ng%2C%20Vincent%20Resolving%20complex%20cases%20of%20definite%20pronouns%3A%20The%20Winograd%20schema%20challenge%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahman%2C%20Altaf%20Ng%2C%20Vincent%20Resolving%20complex%20cases%20of%20definite%20pronouns%3A%20The%20Winograd%20schema%20challenge%202012"
        },
        {
            "id": "Rudinger_et+al_2018_a",
            "entry": "Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme. Neural Davidsonian semantic proto-role labeling. In Proceedings of EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudinger%2C%20Rachel%20Teichert%2C%20Adam%20Culkin%2C%20Ryan%20Zhang%2C%20Sheng%20Neural%20Davidsonian%20semantic%20proto-role%20labeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudinger%2C%20Rachel%20Teichert%2C%20Adam%20Culkin%2C%20Ryan%20Zhang%2C%20Sheng%20Neural%20Davidsonian%20semantic%20proto-role%20labeling%202018"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural MT learn source syntax? In Proceedings of EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Xing%20Padhi%2C%20Inkit%20Knight%2C%20Kevin%20Does%20string-based%20neural%20MT%20learn%20source%20syntax%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Xing%20Padhi%2C%20Inkit%20Knight%2C%20Kevin%20Does%20string-based%20neural%20MT%20learn%20source%20syntax%3F%202016"
        },
        {
            "id": "Silveira_et+al_2014_a",
            "entry": "Natalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Christopher D. Manning. A gold standard dependency corpus for English. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silveira%2C%20Natalia%20Dozat%2C%20Timothy%20de%20Marneffe%2C%20Marie-Catherine%20Bowman%2C%20Samuel%20A%20gold%20standard%20dependency%20corpus%20for%20English%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silveira%2C%20Natalia%20Dozat%2C%20Timothy%20de%20Marneffe%2C%20Marie-Catherine%20Bowman%2C%20Samuel%20A%20gold%20standard%20dependency%20corpus%20for%20English%202014"
        },
        {
            "id": "Strubell_et+al_2018_a",
            "entry": "Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. Linguisticallyinformed self-attention for semantic role labeling. In Proceedings of EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strubell%2C%20Emma%20Verga%2C%20Patrick%20Andor%2C%20Daniel%20Weiss%2C%20David%20Linguisticallyinformed%20self-attention%20for%20semantic%20role%20labeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strubell%2C%20Emma%20Verga%2C%20Patrick%20Andor%2C%20Daniel%20Weiss%2C%20David%20Linguisticallyinformed%20self-attention%20for%20semantic%20role%20labeling%202018"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Teichert_et+al_2017_a",
            "entry": "Adam Teichert, Adam Poliak, Benjamin Van Durme, and Matthew Gormley. Semantic proto-role labeling. In Proceedings of AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teichert%2C%20Adam%20Poliak%2C%20Adam%20Durme%2C%20Benjamin%20Van%20Gormley%2C%20Matthew%20Semantic%20proto-role%20labeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teichert%2C%20Adam%20Poliak%2C%20Adam%20Durme%2C%20Benjamin%20Van%20Gormley%2C%20Matthew%20Semantic%20proto-role%20labeling%202017"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Proceedings%20of%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Proceedings%20of%20NIPS%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Alex%20Singh%2C%20Amanpreet%20Michael%2C%20Julian%20Hill%2C%20Felix%20Glue%3A%20A%20multi-task%20benchmark%20and%20analysis%20platform%20for%20natural%20language%20understanding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Alex%20Singh%2C%20Amanpreet%20Michael%2C%20Julian%20Hill%2C%20Felix%20Glue%3A%20A%20multi-task%20benchmark%20and%20analysis%20platform%20for%20natural%20language%20understanding%202018"
        },
        {
            "id": "Weischedel_et+al_2013_a",
            "entry": "Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. OntoNotes release 5.0 LDC2013T19. Linguistic Data Consortium, Philadelphia, PA, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weischedel%2C%20Ralph%20Palmer%2C%20Martha%20Marcus%2C%20Mitchell%20Hovy%2C%20Eduard%20OntoNotes%20release%205.0%20LDC2013T19.%20Linguistic%20Data%20Consortium%202013"
        },
        {
            "id": "White_et+al_2017_a",
            "entry": "Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin Van Durme. Inference is everything: Recasting semantic resources into a unified evaluation framework. In Proceedings of IJCNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=White%2C%20Aaron%20Steven%20Rastogi%2C%20Pushpendre%20Duh%2C%20Kevin%20Durme%2C%20Benjamin%20Van%20Inference%20is%20everything%3A%20Recasting%20semantic%20resources%20into%20a%20unified%20evaluation%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=White%2C%20Aaron%20Steven%20Rastogi%2C%20Pushpendre%20Duh%2C%20Kevin%20Durme%2C%20Benjamin%20Van%20Inference%20is%20everything%3A%20Recasting%20semantic%20resources%20into%20a%20unified%20evaluation%20framework%202017"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint 1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Zhang_2018_a",
            "entry": "Kelly Zhang and Samuel Bowman. Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Kelly%20Bowman%2C%20Samuel%20Language%20modeling%20teaches%20you%20more%20than%20translation%20does%3A%20Lessons%20learned%20through%20auxiliary%20syntactic%20task%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kelly%20Bowman%2C%20Samuel%20Language%20modeling%20teaches%20you%20more%20than%20translation%20does%3A%20Lessons%20learned%20through%20auxiliary%20syntactic%20task%20analysis%202018"
        },
        {
            "id": "Zhu_et+al_2015_a",
            "entry": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yukun%20Kiros%2C%20Ryan%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yukun%20Kiros%2C%20Ryan%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%202015"
        },
        {
            "id": "This_2018_a",
            "entry": "This version of the paper has been updated to include probing results on the popular BERT (Devlin et al., 2018) model, which was released after our original submission. Aside from formatting and minor re-wording, the following changes have been made:",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20version%20of%20the%20paper%20has%20been%20updated%20to%20include%20probing%20results%20on%20the%20popular%20BERT%20Devlin%20et%20al%202018%20model%20which%20was%20released%20after%20our%20original%20submission%20Aside%20from%20formatting%20and%20minor%20rewording%20the%20following%20changes%20have%20been%20made"
        }
    ]
}
