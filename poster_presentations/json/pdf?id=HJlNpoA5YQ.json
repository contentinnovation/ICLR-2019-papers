{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE LAPLACIAN IN RL: LEARNING REPRESENTATIONS WITH EFFICIENT APPROXIMATIONS",
        "author": "Yifan Wu, Carnegie Mellon University yw,@cs.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJlNpoA5YQ"
        },
        "abstract": "The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goalachieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent."
    },
    "keywords": [
        {
            "term": "graph laplacian",
            "url": "https://en.wikipedia.org/wiki/Graph_Laplacian"
        },
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "hilbert space",
            "url": "https://en.wikipedia.org/wiki/hilbert_space"
        },
        {
            "term": "weighted graph",
            "url": "https://en.wikipedia.org/wiki/weighted_graph"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning"
    },
    "highlights": [
        "The performance of machine learning methods generally depends on the choice of data representation (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>)",
        "The ideal reward structure would be defined on state representations whose distances roughly correspond to the ability of the agent to reach one state from another",
        "There are many suitable such representations, in this paper, we focus on a specific approach based on the graph Laplacian, which is notable for this and several other desirable properties",
        "By working with Hilbert spaces, we provide a unified treatment of the Laplacian and our method for approximating its eigenvectors (<a class=\"ref-link\" id=\"cCayley_1858_a\" href=\"#rCayley_1858_a\">Cayley, 1858</a>) \u2013 eigenfunctions in Hilbert spaces (<a class=\"ref-link\" id=\"cRiesz_1910_a\" href=\"#rRiesz_1910_a\">Riesz, 1910</a>) \u2013 regardless of the underlying space",
        "We evaluate the methods with three different raw state representations of the gridworld: (i) one-hot vectors (\u201cindex\u201d), (x, y) coordinates (\u201cposition\u201d) and top-down pixel representation (\u201cimage\u201d)",
        "We have presented an approach to learning a Laplacian-based state representation in reinforcement learning settings"
    ],
    "key_statements": [
        "The performance of machine learning methods generally depends on the choice of data representation (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>)",
        "The ideal reward structure would be defined on state representations whose distances roughly correspond to the ability of the agent to reach one state from another",
        "There are many suitable such representations, in this paper, we focus on a specific approach based on the graph Laplacian, which is notable for this and several other desirable properties",
        "We present the objective in a fully general reinforcement learning setting and show how it may be stochastically optimized over minibatches of sampled experience",
        "We present the eigendecomposition framework in terms of general Hilbert spaces",
        "By working with Hilbert spaces, we provide a unified treatment of the Laplacian and our method for approximating its eigenvectors (<a class=\"ref-link\" id=\"cCayley_1858_a\" href=\"#rCayley_1858_a\">Cayley, 1858</a>) \u2013 eigenfunctions in Hilbert spaces (<a class=\"ref-link\" id=\"cRiesz_1910_a\" href=\"#rRiesz_1910_a\">Riesz, 1910</a>) \u2013 regardless of the underlying space",
        "We introduce a choice of \u03c1 and D for the Laplacian in the reinforcement learning setting",
        "We note that our work provides a convincing application of Laplacian representations on difficult reinforcement learning tasks, namely reward-shaping in continuous-control environments",
        "We evaluate the methods with three different raw state representations of the gridworld: (i) one-hot vectors (\u201cindex\u201d), (x, y) coordinates (\u201cposition\u201d) and top-down pixel representation (\u201cimage\u201d)",
        "A reward function needs to be defined in order to apply reinforcement learning to train an agent that can perform a goal achieving task",
        "If S = Z we propose two options: (i) The first is to learn an embedding \u03c6 : Z \u2192 Rd of the goal space and define rt = \u2212 \u03c6(h) \u2212 \u03c6 . The second options is to learn an an embedding \u03c6 : S \u2192 Rd of the state space and define rt = \u2212 \u03c6 \u2212 \u03c6(h\u22121) , where h\u22121(z) is defined as picking arbitrary state s that achieves h(s) = z",
        "We have presented an approach to learning a Laplacian-based state representation in reinforcement learning settings"
    ],
    "summary": [
        "The performance of machine learning methods generally depends on the choice of data representation (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\"><a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a></a>).",
        "We apply our representation learning procedure to reward shaping in goal-achieving tasks, and show that our approach outperforms both sparse rewards and rewards based on L2 distance in the raw feature space.",
        "We will show that the graph drawing objective is amenable to stochastic optimization, providing a general, scalable approach to approximating the eigenfunctions of the Laplacian.",
        "We elaborate on how to approximate the eigenfunctions of the Laplacian by optimizing the graph drawing objective via stochastic gradient descent on sampled states and pairs of states.",
        "The repulsive term is especially interesting and we are unaware of anything similar to it in other representation learning objectives: It may be interpreted as orthogonalizing the embeddings of two randomly sampled states while regularizing their norm away from zero by noticingfk(u) \u2212 \u03b4jk)fk(v) \u2212 \u03b4jk) = \u03c6(u)T \u03c6(v) 2 \u2212 ||\u03c6(u)||22 \u2212 ||\u03c6(v)||22 + d .",
        "Our main result is showing that the graph drawing objective may be used to stochastically optimize a representation module which approximates the Laplacian eigenfunctions.",
        "We note that our work provides a convincing application of Laplacian representations on difficult RL tasks, namely reward-shaping in continuous-control environments.",
        "A reward function needs to be defined in order to apply reinforcement learning to train an agent that can perform a goal achieving task.",
        "The shaped reward may either accelerate or hurt the learning process depending on the whether distances in the raw feature space accurately reflect the geometry of the environment dynamics.",
        "We expect that distance based reward shaping with our learned representations can speed up learning compared to sparse reward while avoiding the bias in the raw feature space.",
        "We observe that in the OneRoom environment all shaped reward functions significantly outperform the sparse reward, which indicates that in goal-achieving tasks properly shaped reward can accelerate learning of the policy, justifying our motivation of applying learned representations for reward shaping.",
        "In TwoRoom and HardMaze environments when the raw feature space cannot reflect an accurate distance, our Laplacian-based shaped reward learned using the graph drawing objective (\u201cmix\u201d) significantly outperforms all other reward settings.",
        "We have presented an approach to learning a Laplacian-based state representation in RL settings.",
        "We have further provided an application of our method to reward shaping in both discrete spaces and continuous-control settings.",
        "With our scalable and general approach, many more potential applications of Laplacian-based representations are within reach, and we encourage future work to continue investigating this promising direction."
    ],
    "headline": "We present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free reinforcement learning context",
    "reference_links": [
        {
            "id": "Alzate_2010_a",
            "entry": "Carlos Alzate and Johan AK Suykens. Multiway spectral clustering with out-of-sample extensions through weighted kernel pca. IEEE transactions on pattern analysis and machine intelligence, 32 (2):335\u2013347, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alzate%2C%20Carlos%20Suykens%2C%20Johan%20A.K.%20Multiway%20spectral%20clustering%20with%20out-of-sample%20extensions%20through%20weighted%20kernel%20pca.%20IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence%202010"
        },
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Bump_1998_a",
            "entry": "D. Bump. Automorphic Forms and Representations. Automorphic Forms and Representations. Cambridge University Press, 1998. ISBN 9780521658188. URL https://books.google.com/books?id=QQ1cr7B6XqQC.",
            "url": "https://books.google.com/books?id=QQ1cr7B6XqQC"
        },
        {
            "id": "Cardot_2018_a",
            "entry": "Herve Cardot and David Degras. Online principal component analysis in high dimension: Which algorithm to choose? International Statistical Review, 86(1):29\u201350, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cardot%2C%20Herve%20Degras%2C%20David%20Online%20principal%20component%20analysis%20in%20high%20dimension%3A%20Which%20algorithm%20to%20choose%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cardot%2C%20Herve%20Degras%2C%20David%20Online%20principal%20component%20analysis%20in%20high%20dimension%3A%20Which%20algorithm%20to%20choose%3F%202018"
        },
        {
            "id": "Cayley_1858_a",
            "entry": "Arthur Cayley. A memoir on the theory of matrices. Philosophical transactions of the Royal society of London, 148:17\u201337, 1858.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cayley%2C%20Arthur%20A%20memoir%20on%20the%20theory%20of%20matrices%201858",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cayley%2C%20Arthur%20A%20memoir%20on%20the%20theory%20of%20matrices%201858"
        },
        {
            "id": "Chung_1997_a",
            "entry": "Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Fan%20R.K.%20Graham%2C%20Fan%20Chung%20Spectral%20graph%20theory%201997"
        },
        {
            "id": "Dubey_et+al_2018_a",
            "entry": "Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L Griffiths, and Alexei A Efros. Investigating human priors for playing video games. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubey%2C%20Rachit%20Agrawal%2C%20Pulkit%20Pathak%2C%20Deepak%20Griffiths%2C%20Thomas%20L.%20and%20Alexei%20A%20Efros.%20Investigating%20human%20priors%20for%20playing%20video%20games%202018"
        },
        {
            "id": "Hilbert_1906_a",
            "entry": "David Hilbert. Grundzuge einer allgemeinen theorie der linearen integralgleichungen. vierte mitteilung. Nachrichten von der Gesellschaft der Wissenschaften zu Gottingen, MathematischPhysikalische Klasse, 1906:157\u2013228, 1906.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hilbert%2C%20David%20Grundzuge%20einer%20allgemeinen%20theorie%20der%20linearen%20integralgleichungen.%20vierte%20mitteilung.%20Nachrichten%20von%20der%20Gesellschaft%20der%20Wissenschaften%20zu%201906",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hilbert%2C%20David%20Grundzuge%20einer%20allgemeinen%20theorie%20der%20linearen%20integralgleichungen.%20vierte%20mitteilung.%20Nachrichten%20von%20der%20Gesellschaft%20der%20Wissenschaften%20zu%201906"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Machado_et+al_0000_a",
            "entry": "Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00956"
        },
        {
            "id": "Machado_et+al_2017_a",
            "entry": "Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representation. arXiv preprint arXiv:1710.11089, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11089"
        },
        {
            "id": "Mahadevan_2005_a",
            "entry": "Sridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Proceedings of the 22nd international conference on Machine learning, pp. 553\u2013560. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahadevan%2C%20Sridhar%20Proto-value%20functions%3A%20Developmental%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahadevan%2C%20Sridhar%20Proto-value%20functions%3A%20Developmental%20reinforcement%20learning%202005"
        },
        {
            "id": "Mall_et+al_2013_a",
            "entry": "Raghvendra Mall, Rocco Langone, and Johan AK Suykens. Kernel spectral clustering for big data networks. Entropy, 15(5):1567\u20131586, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mall%2C%20Raghvendra%20Langone%2C%20Rocco%20Suykens%2C%20Johan%20A.K.%20Kernel%20spectral%20clustering%20for%20big%20data%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mall%2C%20Raghvendra%20Langone%2C%20Rocco%20Suykens%2C%20Johan%20A.K.%20Kernel%20spectral%20clustering%20for%20big%20data%20networks%202013"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. arXiv preprint arXiv:1805.08296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Ng_et+al_2002_a",
            "entry": "Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in neural information processing systems, pp. 849\u2013856, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20spectral%20clustering%3A%20Analysis%20and%20an%20algorithm%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20spectral%20clustering%3A%20Analysis%20and%20an%20algorithm%202002"
        },
        {
            "id": "Oja_1982_a",
            "entry": "Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical biology, 15(3):267\u2013273, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982"
        },
        {
            "id": "Oja_1985_a",
            "entry": "Erkki Oja. On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oja%2C%20Erkki%20On%20stochastic%20approximation%20of%20the%20eigenvectors%20and%20eigenvalues%20of%20the%20expectation%20of%20a%20random%20matrix%201985"
        },
        {
            "id": "Pathak_et+al_0000_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "Pfau_et+al_2018_a",
            "entry": "David Pfau, Stig Petersen, Ashish Agarwal, David Barrett, and Kim Stachenfeld. Spectral inference networks: Unifying spectral methods with deep learning. arXiv preprint arXiv:1806.02215, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02215"
        },
        {
            "id": "Plappert_et+al_2018_a",
            "entry": "Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09464"
        },
        {
            "id": "Pong_et+al_2018_a",
            "entry": "Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Modelfree deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09081"
        },
        {
            "id": "Puterman_1990_a",
            "entry": "Martin L Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes.%20Handbooks%20in%20operations%20research%20and%20management%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes.%20Handbooks%20in%20operations%20research%20and%20management%201990"
        },
        {
            "id": "Rafols_et+al_2005_a",
            "entry": "Eddie J Rafols, Mark B Ring, Richard S Sutton, and Brian Tanner. Using predictive representations to improve generalization in reinforcement learning. In IJCAI, pp. 835\u2013840, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rafols%2C%20Eddie%20J.%20Ring%2C%20Mark%20B.%20Sutton%2C%20Richard%20S.%20Tanner%2C%20Brian%20Using%20predictive%20representations%20to%20improve%20generalization%20in%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rafols%2C%20Eddie%20J.%20Ring%2C%20Mark%20B.%20Sutton%2C%20Richard%20S.%20Tanner%2C%20Brian%20Using%20predictive%20representations%20to%20improve%20generalization%20in%20reinforcement%20learning%202005"
        },
        {
            "id": "Riesz_1910_a",
            "entry": "Friedrich Riesz. Untersuchungen uber systeme integrierbarer funktionen. Mathematische Annalen, 69(4):449\u2013497, 1910.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riesz%2C%20Friedrich%20Untersuchungen%20uber%20systeme%20integrierbarer%20funktionen%201910",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riesz%2C%20Friedrich%20Untersuchungen%20uber%20systeme%20integrierbarer%20funktionen%201910"
        },
        {
            "id": "Shaham_et+al_2018_a",
            "entry": "Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, and Yuval Kluger. Spectralnet: Spectral clustering using deep neural networks. arXiv preprint arXiv:1801.01587, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01587"
        },
        {
            "id": "Stachenfeld_et+al_2014_a",
            "entry": "Kimberly L Stachenfeld, Matthew Botvinick, and Samuel J Gershman. Design principles of the hippocampal cognitive map. In Advances in neural information processing systems, pp. 2528\u2013 2536, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stachenfeld%2C%20Kimberly%20L.%20Botvinick%2C%20Matthew%20Gershman%2C%20Samuel%20J.%20Design%20principles%20of%20the%20hippocampal%20cognitive%20map%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stachenfeld%2C%20Kimberly%20L.%20Botvinick%2C%20Matthew%20Gershman%2C%20Samuel%20J.%20Design%20principles%20of%20the%20hippocampal%20cognitive%20map%202014"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753\u2013 2762, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Ieee_2012_a",
            "entry": "Published as a conference paper at ICLR 2019 Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012. Bo Xie, Yingyu Liang, and Le Song. Scale up nonlinear component analysis with doubly stochastic gradients. In Advances in Neural Information Processing Systems, pp. 2341\u20132349, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Emanuel%20Todorov%20Tom%20Erez%20and%20Yuval%20Tassa%20Mujoco%20A%20physics%20engine%20for%20modelbased%20control%20In%20Intelligent%20Robots%20and%20Systems%20IROS%202012%20IEEERSJ%20International%20Conference%20on%20pp%205026%205033%20IEEE%202012%20Bo%20Xie%20Yingyu%20Liang%20and%20Le%20Song%20Scale%20up%20nonlinear%20component%20analysis%20with%20doubly%20stochastic%20gradients%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2023412349%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Emanuel%20Todorov%20Tom%20Erez%20and%20Yuval%20Tassa%20Mujoco%20A%20physics%20engine%20for%20modelbased%20control%20In%20Intelligent%20Robots%20and%20Systems%20IROS%202012%20IEEERSJ%20International%20Conference%20on%20pp%205026%205033%20IEEE%202012%20Bo%20Xie%20Yingyu%20Liang%20and%20Le%20Song%20Scale%20up%20nonlinear%20component%20analysis%20with%20doubly%20stochastic%20gradients%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2023412349%202015"
        },
        {
            "id": "Al_2017_a",
            "entry": "Implementation of baselines. Both of the approaches in Machado et al. (2017a) and Machado et al. (2017b) output d eigenoptions e1,..., ed \u2208 Rm where m is the dimension of a state feature space \u03c8: S \u2192 Rm, which can be either the raw representation (Machado et al., 2017a) or a representation learned by a forward prediction model (Machado et al., 2017b). Given the d eigenoptions, an embedding can be obtained by letting fi(s) = \u03c8(s)T ei. Following their theoretical results it can be seen that if \u03c8 is the one-hot representation of the tabular states and the stacked rows contains unique enumeration of all transitions/states \u03c6 = [f1,..., fd] spans the same subspace as the smallest d eigenvectors of the Laplacian.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Implementation%20of%20baselines%20Both%20of%20the%20approaches%20in%20Machado%20et%20al%202017a%20and%20Machado%20et%20al%202017b%20output%20d%20eigenoptions%20e1%20ed%20%20Rm%20where%20m%20is%20the%20dimension%20of%20a%20state%20feature%20space%20%CF%88%20S%20%20Rm%20which%20can%20be%20either%20the%20raw%20representation%20Machado%20et%20al%202017a%20or%20a%20representation%20learned%20by%20a%20forward%20prediction%20model%20Machado%20et%20al%202017b%20Given%20the%20d%20eigenoptions%20an%20embedding%20can%20be%20obtained%20by%20letting%20fis%20%20%CF%88sT%20ei%20Following%20their%20theoretical%20results%20it%20can%20be%20seen%20that%20if%20%CF%88%20is%20the%20onehot%20representation%20of%20the%20tabular%20states%20and%20the%20stacked%20rows%20contains%20unique%20enumeration%20of%20all%20transitionsstates%20%CF%86%20%20f1%20fd%20spans%20the%20same%20subspace%20as%20the%20smallest%20d%20eigenvectors%20of%20the%20Laplacian",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Implementation%20of%20baselines%20Both%20of%20the%20approaches%20in%20Machado%20et%20al%202017a%20and%20Machado%20et%20al%202017b%20output%20d%20eigenoptions%20e1%20ed%20%20Rm%20where%20m%20is%20the%20dimension%20of%20a%20state%20feature%20space%20%CF%88%20S%20%20Rm%20which%20can%20be%20either%20the%20raw%20representation%20Machado%20et%20al%202017a%20or%20a%20representation%20learned%20by%20a%20forward%20prediction%20model%20Machado%20et%20al%202017b%20Given%20the%20d%20eigenoptions%20an%20embedding%20can%20be%20obtained%20by%20letting%20fis%20%20%CF%88sT%20ei%20Following%20their%20theoretical%20results%20it%20can%20be%20seen%20that%20if%20%CF%88%20is%20the%20onehot%20representation%20of%20the%20tabular%20states%20and%20the%20stacked%20rows%20contains%20unique%20enumeration%20of%20all%20transitionsstates%20%CF%86%20%20f1%20fd%20spans%20the%20same%20subspace%20as%20the%20smallest%20d%20eigenvectors%20of%20the%20Laplacian"
        }
    ]
}
