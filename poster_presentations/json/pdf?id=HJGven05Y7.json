{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HOW TO TRAIN YOUR MAML",
        "author": "Antreas Antoniou University of Edinburgh {a.antoniou}@sms.ed.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJGven05Y7"
        },
        "abstract": "The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++."
    },
    "keywords": [
        {
            "term": "cross entropy method",
            "url": "https://en.wikipedia.org/wiki/cross_entropy_method"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "MAML": "Model Agnostic Meta-Learning",
        "MSL": "Multi-Step Loss Optimization",
        "DA": "Derivative-Order Annealing",
        "BNRS": "Batch Normalization Running Statistics",
        "BNWB": "Batch Normalization Weights and Biases"
    },
    "highlights": [
        "The human capacity to learn new concepts using only a handful of samples is immense",
        "The fact that standard deep neural networks fail in the small data regime can provide hints about some of their potential shortcomings",
        "In Model Agnostic Meta-Learning, the authors propose learning an initialization for a base-model such that after applying a very small number of gradient steps with respect to a training set on the base-model, the adapted model can achieve strong generalization performance on a validation set",
        "Model Agnostic Meta-Learning++ is evaluated in the few-shot learning setting where the system is able to set a new state of the art across all established few-shot learning tasks on both Omniglot and Mini-Imagenet, performing as well as or better than all established meta learning methods on both tasks.\n2 RELATED WORK",
        "Gradient Instability \u2192 Multi-Step Loss Optimization (MSL): Model Agnostic Meta-Learning works by minimizing the target set loss computed by the base-network after it has completed all of its inner-loop updates towards a support set task",
        "The results of the approach indicate that learning per-step learning rates, batch normalization parameters and optimizing on per-step target losses appears to be key for fast, highly automatic and strongly generalizable few-shot learning"
    ],
    "key_statements": [
        "The human capacity to learn new concepts using only a handful of samples is immense",
        "The fact that standard deep neural networks fail in the small data regime can provide hints about some of their potential shortcomings",
        "In Model Agnostic Meta-Learning, the authors propose learning an initialization for a base-model such that after applying a very small number of gradient steps with respect to a training set on the base-model, the adapted model can achieve strong generalization performance on a validation set",
        "Model Agnostic Meta-Learning++ is evaluated in the few-shot learning setting where the system is able to set a new state of the art across all established few-shot learning tasks on both Omniglot and Mini-Imagenet, performing as well as or better than all established meta learning methods on both tasks.\n2 RELATED WORK",
        "Gradient Instability \u2192 Multi-Step Loss Optimization (MSL): Model Agnostic Meta-Learning works by minimizing the target set loss computed by the base-network after it has completed all of its inner-loop updates towards a support set task",
        "Shared Batch Normalization Bias \u2192 Per-Step Batch Normalization Weights and Biases (BNWB): In the Model Agnostic Meta-Learning paper the authors trained their model to learn a single set of biases for each layer",
        "Each proposed methodology can individually outperform Model Agnostic Meta-Learning, the most notable improvements come from the learned per-step per-layer learning rates and the per-step batch normalization methodology",
        "In Table 1 we include the results of our own implementation of Model Agnostic Meta-Learning, which reproduces all results except the 20-way 1-shot Omniglot case",
        "The results of the approach indicate that learning per-step learning rates, batch normalization parameters and optimizing on per-step target losses appears to be key for fast, highly automatic and strongly generalizable few-shot learning"
    ],
    "summary": [
        "The human capacity to learn new concepts using only a handful of samples is immense.",
        "In MAML, the authors propose learning an initialization for a base-model such that after applying a very small number of gradient steps with respect to a training set on the base-model, the adapted model can achieve strong generalization performance on a validation set.",
        "These parameters are used to initialize the base-model, which is used for task-specific learning on a support set, which is evaluated on a target set.",
        "MAML++ is evaluated in the few-shot learning setting where the system is able to set a new state of the art across all established few-shot learning tasks on both Omniglot and Mini-Imagenet, performing as well as or better than all established meta learning methods on both tasks.",
        "In Model Agnostic meta-learning (MAML) (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>) the authors proposed increasing the gradient update steps on the base-model and replacing the meta-learner LSTM with Batch Stochastic Gradient Descent (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a>), which as a result speeds up the process of learning and interestingly improves generalization performance and achieves state of the art performance in both Omniglot and Mini-Imagenet.",
        "Meta-SGD showcases significantly improved generalization performance across all few-shot learning tasks, whilst only requiring a single inner loop update step.",
        "MAML learns good initialization parameters for a network, such that after a few steps of standard training on a few-shot dataset, the network will perform well on that few shot task.",
        "Gradient Instability \u2192 Multi-Step Loss Optimization (MSL): MAML works by minimizing the target set loss computed by the base-network after it has completed all of its inner-loop updates towards a support set task.",
        "Instead we propose minimizing the target set loss computed by the base-network after every step towards a support set task.",
        "Shared Batch Normalization Bias \u2192 Per-Step Batch Normalization Weights and Biases (BNWB): In the MAML paper the authors trained their model to learn a single set of biases for each layer.",
        "To fix this problem we propose learning a set of biases per-step within the inner-loop update process.",
        "Annealing the learning rate allows the model to fit the training set more effectively and as a result might produce higher generalization performance.",
        "The resulting approach, called MAML++sets a new state of the art across all few-shot tasks, across Omniglot and Mini-Imagenet.",
        "The results of the approach indicate that learning per-step learning rates, batch normalization parameters and optimizing on per-step target losses appears to be key for fast, highly automatic and strongly generalizable few-shot learning."
    ],
    "headline": "We propose various modifications to Model Agnostic Meta-Learning that not only stabilize the system, but substantially improve the generalization performance, convergence speed and computational overhead of Model Agnostic Meta-Learning, which we call Model Agnostic Meta-Learning++",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Antoniou_et+al_2017_a",
            "entry": "Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04340"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. In Advances In Neural Information Processing Systems, pp. 4331\u20134339, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05344"
        },
        {
            "id": "Caruana_1995_a",
            "entry": "Rich Caruana. Learning many related tasks at the same time with backpropagation. In Advances in neural information processing systems, pp. 657\u2013664, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Learning%20many%20related%20tasks%20at%20the%20same%20time%20with%20backpropagation%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Learning%20many%20related%20tasks%20at%20the%20same%20time%20with%20backpropagation%201995"
        },
        {
            "id": "Boer_et+al_2005_a",
            "entry": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19\u201367, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005"
        },
        {
            "id": "Edwards_2016_a",
            "entry": "Harrison Edwards and Amos Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02185"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Jamal_et+al_2018_a",
            "entry": "Muhammad Abdullah Jamal, Guo-Jun Qi, and Mubarak Shah. Task-agnostic meta-learning for few-shot learning. arXiv preprint arXiv:1805.07722, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07722"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Larsson_et+al_2016_a",
            "entry": "Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07648"
        },
        {
            "id": "Li_2016_a",
            "entry": "Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01885"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. arXiv preprint arXiv:1707.09835, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09835"
        },
        {
            "id": "Loshchilov_2016_a",
            "entry": "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.03983"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "Tsendsuren Munkhdalai and Hong Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00837"
        },
        {
            "id": "Nichol_et+al_2018_a",
            "entry": "Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. CoRR, abs/1803.02999, 2018. URL http://arxiv.org/abs/1803.02999.",
            "url": "http://arxiv.org/abs/1803.02999",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Ravi_2016_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202016"
        },
        {
            "id": "Rubinstein_1999_a",
            "entry": "Reuven Rubinstein. The cross-entropy method for combinatorial and continuous optimization. Methodology and computing in applied probability, 1(2):127\u2013190, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Reuven%20The%20cross-entropy%20method%20for%20combinatorial%20and%20continuous%20optimization%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Reuven%20The%20cross-entropy%20method%20for%20combinatorial%20and%20continuous%20optimization%201999"
        },
        {
            "id": "Santurkar_et+al_2018_a",
            "entry": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11604"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Evolutionary%20principles%20in%20self-referential%20learning.%20On%20learning%20how%20to%20learn%3A%20The%20meta-meta-...%20hook.%29%20Diploma%201987"
        },
        {
            "id": "Vilalta_2002_a",
            "entry": "Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial Intelligence Review, 18(2):77\u201395, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vilalta%2C%20Ricardo%20Drissi%2C%20Youssef%20A%20perspective%20view%20and%20survey%20of%20meta-learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vilalta%2C%20Ricardo%20Drissi%2C%20Youssef%20A%20perspective%20view%20and%20survey%20of%20meta-learning%202002"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630\u20133638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "J. X Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. ArXiv e-prints, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.X.%20Kurth-Nelson%2C%20Z.%20Tirumala%2C%20D.%20Soyer%2C%20H.%20Learning%20to%20reinforcement%20learn.%20ArXiv%20e-prints%202016-11"
        },
        {
            "id": "Zoph_2016_a",
            "entry": "Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
