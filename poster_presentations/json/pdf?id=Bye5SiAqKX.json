{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PRECONDITIONER ON MATRIX LIE GROUP FOR SGD",
        "author": "Xi-Lin Li GMEMS Technologies and Spectimbre 366 Fairview Way, Milpitas, CA 95035 lixilinx@gmail.com",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bye5SiAqKX"
        },
        "abstract": "We study two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods in a unified framework. We call the first one the Newton type due to its close relationship to the Newton method, and the second one the Fisher type as its preconditioner is closely related to the inverse of Fisher information matrix. Both preconditioners can be derived from one framework, and efficiently estimated on any matrix Lie groups designated by the user using natural or relative gradient descent minimizing certain preconditioner estimation criteria. Many existing preconditioners and methods, e.g., RMSProp, Adam, KFAC, equilibrated SGD, batch normalization, etc., are special cases of or closely related to either the Newton type or the Fisher type ones. Experimental results on relatively large scale machine learning problems are reported for performance study."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "fisher information matrix",
            "url": "https://en.wikipedia.org/wiki/fisher_information_matrix"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        },
        {
            "term": "graphics processing units",
            "url": "https://en.wikipedia.org/wiki/graphics_processing_units"
        },
        {
            "term": "matrix lie group",
            "url": "https://en.wikipedia.org/wiki/matrix_lie_group"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        }
    ],
    "abbreviations": {
        "SGD": "Stochastic gradient descent",
        "KFAC": "Kronecker-factored approximate curvature",
        "GPU": "graphics processing units",
        "LRN": "local response normalization"
    },
    "highlights": [
        "This paper investigates the use of preconditioner for accelerating gradient descent, especially in large scale machine learning problems",
        "Both preconditioners can be derived from one framework, and estimated on any matrix Lie groups designated by the user with almost the same natural or relative gradient descent methods minimizing specific preconditioner estimation criteria.\n2.1",
        "Mathematical optimization is not our focus. This example shows that the Newton type preconditioned Stochastic gradient descent works well for mathematical optimization.\n7.2",
        "We call it the Fisher type preconditioned Stochastic gradient descent as its preconditioner is closely related to the inverse of Fisher information matrix",
        "Both preconditioners can be efficiently learned using natural or relative gradient descent on any matrix Lie groups designated by the user"
    ],
    "key_statements": [
        "This paper investigates the use of preconditioner for accelerating gradient descent, especially in large scale machine learning problems",
        "For a large family of machine learning problems, natural gradient with the Fisher information metric is equivalent to a preconditioned gradient using inverse of the Fisher information matrix as the preconditioner (Amari, 1998)",
        "Both preconditioners can be derived from one framework, and estimated on any matrix Lie groups designated by the user with almost the same natural or relative gradient descent methods minimizing specific preconditioner estimation criteria.\n2.1",
        "The best step size is selected from sequence {. . . , 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, . . .}",
        "The Fisher type method does not fit into this problem, and performs poorly as expected",
        "Mathematical optimization is not our focus. This example shows that the Newton type preconditioned Stochastic gradient descent works well for mathematical optimization.\n7.2",
        "The one requiring Hessian-vector product for preconditioner estimation is suitable for general purpose optimization",
        "We call it the Fisher type preconditioned Stochastic gradient descent as its preconditioner is closely related to the inverse of Fisher information matrix",
        "Both preconditioners can be efficiently learned using natural or relative gradient descent on any matrix Lie groups designated by the user"
    ],
    "summary": [
        "This paper investigates the use of preconditioner for accelerating gradient descent, especially in large scale machine learning problems.",
        "Updating rule for the Fisher type preconditioner is the same except for replacing the Hessian-vector product with stochastic gradient.",
        "We seldom consider the Lie group consisting of dense invertible matrices for preconditioner estimation when the problem size is large.",
        "For matrix parameter \u0398, we can flatten \u0398 into a vector, and precondition its gradient using a Kronecker product preconditioner with Q having form Q = Q2 \u2297 Q1.",
        "The input feature vector x must be augmented with 1 such that the whitening operation forms a Lie group represented by upper triangular matrices with 1 being its last diagonal entry.",
        "RMSProp and Adam all use the Fisher type preconditioner living on the group of diagonal matrices with positive diagonal entries.",
        "Batch normalization can be viewed as preconditioned SGD using a specific scaling-andnormalization preconditioner with constraint Q1 = I and Q2 from the feature normalization Lie group.",
        "Both the Newton and Fisher type preconditioned SGD methods provide a more general and principled approach to find the optimal preconditioner, and apply to a broader range of applications.",
        "We use the square root Fisher type preconditioners in the following experiments since they are less picky on the damping factor, and seem to be more numerically robust on large scale problems.",
        "For the Fisher type method, we set \u03bb = 0.1, and step sizes 0.01 and 0.001 for preconditioner and parameter updates, respectively.",
        "For the Newton type method, we set step sizes 0.2 and 0.5 for preconditioner and parameter updates, respectively.",
        "The Newton type preconditioned SGD performs the best, and achieves top-1 validation accuracy about 56% when using only one crop for testing, while the momentum method may require 90 epochs to achieve similar performance.",
        "The Newton type preconditioned SGD requires Hessianvector product, which typically has complexity comparable to that of gradient evaluation.",
        "We call it the Fisher type preconditioned SGD as its preconditioner is closely related to the inverse of Fisher information matrix.",
        "Both preconditioners can be efficiently learned using natural or relative gradient descent on any matrix Lie groups designated by the user.",
        "The Fisher type preconditioned SGD has lower computational complexity per iteration, but may require more tuning efforts on selecting its step size and damping factor.",
        "The Newton type preconditioned SGD has higher computational complexity per iteration, but is more user friendly due to its use of normalized step size and built-in gradient noise damping ability.",
        "Even with very sparse representations, are shown to considerably accelerate convergence on relatively large scale problems"
    ],
    "headline": "We study two types of preconditioners and preconditioned stochastic gradient descent methods in a unified framework",
    "reference_links": [
        {
            "id": "Alex_et+al_2012_a",
            "entry": "K. Alex, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%2C%20K.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alex%2C%20K.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Amari_2004_a",
            "entry": "S. Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251\u2013276, 1998. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. J. F. Cardoso and B. H. Laheld. Equivariant adaptive source separation. IEEE Trans. Signal Process., 44(12):3017\u20133030, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Natural%20gradient%20works%20efficiently%20in%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.%20Natural%20gradient%20works%20efficiently%20in%20learning%202004"
        },
        {
            "id": "Dauphin_et+al_2015_a",
            "entry": "Y. N. Dauphin, H. Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex optimization. In NIPS, pp. 1504\u20131512. MIT Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Vries%2C%20H.%20Bengio%2C%20Y.%20Equilibrated%20adaptive%20learning%20rates%20for%20non-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Vries%2C%20H.%20Bengio%2C%20Y.%20Equilibrated%20adaptive%20learning%20rates%20for%20non-convex%20optimization%202015"
        },
        {
            "id": "John_et+al_2015_a",
            "entry": "D. John, H. Elad, and S. Yoram. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011. D. P. Kingma and J. L. Ba. Adam: a method for stochastic optimization. In ICLR. Ithaca, NY: arXiv.org, 2015. X. L. Li. Preconditioned stochastic gradient descent. IEEE Trans. Neural Networks and Learning",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%2C%20D.%20Elad%2C%20H.%20Yoram%2C%20S.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%2C%20D.%20Elad%2C%20H.%20Yoram%2C%20S.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202015"
        },
        {
            "id": "Systems_2015_a",
            "entry": "Systems, 29(5):1454\u20131466, 2018. J. Martens and R. B. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In ICML, pp. 2408\u20132417, 2015. Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/srt(k)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Systems%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Systems%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015"
        },
        {
            "id": "Doklady_1983_a",
            "entry": "Soviet Mathematics Doklady, 27:372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soviet%20Mathematics%20Doklady%2027372376%201983"
        },
        {
            "id": "Povey_et+al_2015_a",
            "entry": "D. Povey, X. Zhang, and S. Khudanpur. Parallel training of DNNs with natural gradient and parameter averaging. In ICLR. Ithaca, NY: arXiv.org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Povey%2C%20D.%20Zhang%2C%20X.%20Khudanpur%2C%20S.%20Parallel%20training%20of%20DNNs%20with%20natural%20gradient%20and%20parameter%20averaging.%20In%20ICLR%202015"
        },
        {
            "id": "Rumelhart_et+al_1986_a",
            "entry": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323:533\u2013536, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        }
    ]
}
