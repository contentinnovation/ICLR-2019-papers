{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DEEP LEARNING GENERALIZES BECAUSE THE PARAMETER-FUNCTION MAP IS BIASED TOWARDS",
        "author": "SIMPLE FUNCTIONS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rye4g3AqFm"
        },
        "abstract": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong bias in a model DNN for Boolean functions, as well as in much larger fully conected and convolutional networks trained on CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10and for architectures including convolutional and fully connected networks."
    },
    "keywords": [
        {
            "term": "boolean function",
            "url": "https://en.wikipedia.org/wiki/boolean_function"
        },
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "algorithmic information theory",
            "url": "https://en.wikipedia.org/wiki/algorithmic_information_theory"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "AIT": "algorithmic information theory",
        "ELBO": "evidence lower bound"
    },
    "highlights": [
        "Deep learning is a machine learning paradigm based on very large, expressive and composable models, which most often require large data sets to train",
        "The name comes from the main component in the models: deep neural networks (DNNs) with many layers of representation",
        "We show empirically that the parameter-funtion map of deep neural networks is extremely biased towards simple functions, and the prior over functions is expected to be extremely biased too",
        "Here we have only shown that high-probability functions have low complexity for a relatively small deep neural networks, the generality of the algorithmic information theory arguments from Dingle et al (2018) suggests that an exponential probability-complexity bias should hold for larger neural networks as well",
        "We tested the expected generalization error bounds described in the previous section in a variety of networks trained on binarized7 versions of MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al (1998</a>)), fashion-MNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al (2017</a>)), and CIFAR10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton (2009</a>))",
        "We present an argument that we think offers a first-order explanation of generalization in highly overparameterized deep neural networks"
    ],
    "key_statements": [
        "Deep learning is a machine learning paradigm based on very large, expressive and composable models, which most often require large data sets to train",
        "The name comes from the main component in the models: deep neural networks (DNNs) with many layers of representation",
        "We show empirically that the parameter-funtion map of deep neural networks is extremely biased towards simple functions, and the prior over functions is expected to be extremely biased too",
        "Using the Gaussian process approximation of the prior over functions, we compute PACBayes expected generalization error bounds for a variety of common deep neural networks architectures and datasets",
        "Here we have only shown that high-probability functions have low complexity for a relatively small deep neural networks, the generality of the algorithmic information theory arguments from Dingle et al (2018) suggests that an exponential probability-complexity bias should hold for larger neural networks as well",
        "We tested the expected generalization error bounds described in the previous section in a variety of networks trained on binarized7 versions of MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al (1998</a>)), fashion-MNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al (2017</a>)), and CIFAR10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton (2009</a>))",
        "We are interested in the probability of finding individual functions consistent with the training set, by two methods:(1 Training the neural network with variants of SGD8; in particular, advSGD and Adam (2 Bayesian inference using the Gaussian process corresponding to the neural network architecture",
        "We present an argument that we think offers a first-order explanation of generalization in highly overparameterized deep neural networks",
        "We demonstrated how to make this approach quantitative, approximating neural networks as Gaussian processes to calculate PAC-Bayesian bounds on the generalization error"
    ],
    "summary": [
        "Deep learning is a machine learning paradigm based on very large, expressive and composable models, which most often require large data sets to train.",
        "Using the Gaussian process approximation of the prior over functions, we compute PACBayes expected generalization error bounds for a variety of common DNN architectures and datasets.",
        "Here we have only shown that high-probability functions have low complexity for a relatively small DNN, the generality of the AIT arguments from Dingle et al (2018) suggests that an exponential probability-complexity bias should hold for larger neural networks as well.",
        "The main quantity in the PAC-Bayes theorem, P (U ), is precisely the probability of a given set of output labels for the set of instances in the training set, known as marginal likelihood, a connection explored in recent work (<a class=\"ref-link\" id=\"cSmith_2017_a\" href=\"#rSmith_2017_a\">Smith & Le (2017</a>); <a class=\"ref-link\" id=\"cGermain_et+al_2016_a\" href=\"#rGermain_et+al_2016_a\">Germain et al (2016</a>)).",
        "Bayesian sampling of the parameters of a neural network has been argued to produce generalization performance similar to the same network trained with SGD.",
        "We performed experiments showing direct evidence that the probability with which two variants of SGD find functions is close to the probability of obtaining the function by uniform sampling of parameters in the zero-error region.",
        "We are interested in the probability of finding individual functions consistent with the training set, by two methods:(1 Training the neural network with variants of SGD8; in particular, advSGD and Adam (2 Bayesian inference using the Gaussian process corresponding to the neural network architecture.",
        "We present an argument that we think offers a first-order explanation of generalization in highly overparameterized DNNs. First, PAC-Bayes shows how priors which are sufficiently biased towards the true distribution can result in generalization in highly expressive models, e.g. even if there are many more parameters than data points.",
        "To demonstrate the bias in the parameter-function map, we used both direct sampling for a small network and an equivalence with Gaussian processes for larger networks.",
        "We demonstrated how to make this approach quantitative, approximating neural networks as Gaussian processes to calculate PAC-Bayesian bounds on the generalization error.",
        "The probability that the training algorithm finds a particular function in the zero-error region can be approximated by the probability that the function obtains upon i.i.d. sampling of parameters.",
        "We think that the good agreement of our bounds constitutes good evidence for the approach we describe in the paper as well as for the claim that bias in the parameter-function map is the main reason for generalization.",
        "Since real-world problems tend to be far from random, using these same complexity measures, we expect the prior to be biased towards the right class of solutions for real-world datasets and problems"
    ],
    "headline": "We provide a new explanation",
    "reference_links": [
        {
            "id": "Advani_2017_a",
            "entry": "Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03667"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 254\u2013263. PMLR, 10\u201315 Jul 2018. URL http://proceedings.mlr.press/v80/arora18b.html.",
            "url": "http://proceedings.mlr.press/v80/arora18b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018-07"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6240\u20136249, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Baum_1989_a",
            "entry": "Eric B Baum and David Haussler. What size net gives valid generalization? In Advances in neural information processing systems, pp. 81\u201390, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baum%2C%20Eric%20B.%20Haussler%2C%20David%20What%20size%20net%20gives%20valid%20generalization%3F%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baum%2C%20Eric%20B.%20Haussler%2C%20David%20What%20size%20net%20gives%20valid%20generalization%3F%201989"
        },
        {
            "id": "Belkin_et+al_2018_a",
            "entry": "Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 541\u2013549, Stockholmsm\u00c3d\u2019ssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/belkin18a.html.",
            "url": "http://proceedings.mlr.press/v80/belkin18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Ma%2C%20Siyuan%20Mandal%2C%20Soumik%20To%20understand%20deep%20learning%20we%20need%20to%20understand%20kernel%20learning%202018-07"
        },
        {
            "id": "Blumer_et+al_1987_a",
            "entry": "Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor. Information processing letters, 24(6):377\u2013380, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumer%2C%20Anselm%20Ehrenfeucht%2C%20Andrzej%20Haussler%2C%20David%20Warmuth%2C%20Manfred%20K.%20Occam%E2%80%99s%20razor%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumer%2C%20Anselm%20Ehrenfeucht%2C%20Andrzej%20Haussler%2C%20David%20Warmuth%2C%20Manfred%20K.%20Occam%E2%80%99s%20razor%201987"
        },
        {
            "id": "Cho_2009_a",
            "entry": "Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342\u2013350, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "Cover_2012_a",
            "entry": "Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20information%20theory%202012"
        },
        {
            "id": "Dingle_2018_a",
            "entry": "Kamaludin Dingle, Chico Q Camargo, and Ard A Louis. Input\u2013output maps are strongly biased towards simple outputs. Nature communications, 9(1):761, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dingle%2C%20Kamaludin%20Camargo%2C%20Chico%20Q.%20and%20Ard%20A%20Louis.%20Input%E2%80%93output%20maps%20are%20strongly%20biased%20towards%20simple%20outputs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dingle%2C%20Kamaludin%20Camargo%2C%20Chico%20Q.%20and%20Ard%20A%20Louis.%20Input%E2%80%93output%20maps%20are%20strongly%20biased%20towards%20simple%20outputs%202018"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1019\u20131028. PMLR, 06\u201311 Aug 2017. URL http://proceedings.mlr.press/v70/dinh17b.html.",
            "url": "http://proceedings.mlr.press/v70/dinh17b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017-08"
        },
        {
            "id": "Draxler_et+al_2018_a",
            "entry": "Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1309\u20131318. PMLR, 10\u201315 Jul 2018. URL http://proceedings.mlr.press/v80/draxler18a.html.",
            "url": "http://proceedings.mlr.press/v80/draxler18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Draxler%2C%20Felix%20Veschgini%2C%20Kambis%20Salmhofer%2C%20Manfred%20Hamprecht%2C%20Fred%20Essentially%20no%20barriers%20in%20neural%20network%20energy%20landscape%202018-07"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017, 2017. URL http://auai.org/uai2017/proceedings/papers/173.pdf.",
            "url": "http://auai.org/uai2017/proceedings/papers/173.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017-08-11"
        },
        {
            "id": "Dziugaite_2018_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential privacy. arXiv preprint arXiv:1802.09583, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09583"
        },
        {
            "id": "Estevez-Rams_et+al_2013_a",
            "entry": "E Estevez-Rams, R Lora Serrano, B Arag\u00f3n Fern\u00e1ndez, and I Brito Reyes. On the non-randomness of maximum lempel ziv complexity sequences of finite size. Chaos: An Interdisciplinary Journal of Nonlinear Science, 23(2):023118, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Estevez-Rams%2C%20E.%20Serrano%2C%20R.Lora%20Fern%C3%A1ndez%2C%20B.Arag%C3%B3n%20Reyes%2C%20I.Brito%20On%20the%20non-randomness%20of%20maximum%20lempel%20ziv%20complexity%20sequences%20of%20finite%20size%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Estevez-Rams%2C%20E.%20Serrano%2C%20R.Lora%20Fern%C3%A1ndez%2C%20B.Arag%C3%B3n%20Reyes%2C%20I.Brito%20On%20the%20non-randomness%20of%20maximum%20lempel%20ziv%20complexity%20sequences%20of%20finite%20size%202013"
        },
        {
            "id": "Franco_2006_a",
            "entry": "Leonardo Franco. Generalization ability of boolean functions implemented in feedforward neural networks. Neurocomputing, 70(1):351\u2013361, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franco%2C%20Leonardo%20Generalization%20ability%20of%20boolean%20functions%20implemented%20in%20feedforward%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franco%2C%20Leonardo%20Generalization%20ability%20of%20boolean%20functions%20implemented%20in%20feedforward%20neural%20networks%202006"
        },
        {
            "id": "Franco_2004_a",
            "entry": "Leonardo Franco and Martin Anthony. On a generalization complexity measure for boolean functions. In Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, volume 2, pp. 973\u2013978. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franco%2C%20Leonardo%20Anthony%2C%20Martin%20On%20a%20generalization%20complexity%20measure%20for%20boolean%20functions%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franco%2C%20Leonardo%20Anthony%2C%20Martin%20On%20a%20generalization%20complexity%20measure%20for%20boolean%20functions%202004"
        },
        {
            "id": "Friedgut_1998_a",
            "entry": "Ehud Friedgut. Boolean functions with low average sensitivity depend on few coordinates. Combinatorica, 18(1):27\u201335, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedgut%2C%20Ehud%20Boolean%20functions%20with%20low%20average%20sensitivity%20depend%20on%20few%20coordinates%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedgut%2C%20Ehud%20Boolean%20functions%20with%20low%20average%20sensitivity%20depend%20on%20few%20coordinates%201998"
        },
        {
            "id": "Garriga-Alonso_et+al_2018_a",
            "entry": "Adri\u00e0 Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional networks as shallow Gaussian processes. arXiv preprint arXiv:1808.05587, aug 2018. URL https://arxiv.org/abs/1808.05587.",
            "url": "https://arxiv.org/abs/1808.05587",
            "arxiv_url": "https://arxiv.org/pdf/1808.05587"
        },
        {
            "id": "Germain_et+al_2016_a",
            "entry": "Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pp. 1884\u2013 1892, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Bach%2C%20Francis%20Lacoste%2C%20Alexandre%20Lacoste-Julien%2C%20Simon%20Pac-bayesian%20theory%20meets%20bayesian%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Bach%2C%20Francis%20Lacoste%2C%20Alexandre%20Lacoste-Julien%2C%20Simon%20Pac-bayesian%20theory%20meets%20bayesian%20inference%202016"
        },
        {
            "id": "Giryes_et+al_2016_a",
            "entry": "Raja Giryes, Guillermo Sapiro, and Alexander M Bronstein. Deep neural networks with random gaussian weights: a universal classification strategy? IEEE Trans. Signal Processing, 64(13): 3444\u20133457, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Bronstein%2C%20Alexander%20M.%20Deep%20neural%20networks%20with%20random%20gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Bronstein%2C%20Alexander%20M.%20Deep%20neural%20networks%20with%20random%20gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016"
        },
        {
            "id": "Golowich_et+al_2018_a",
            "entry": "Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 297\u2013299. PMLR, 06\u201309 Jul 2018. URL http://proceedings.mlr.press/v75/golowich18a.html.",
            "url": "http://proceedings.mlr.press/v75/golowich18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018-07"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Gpy_2012_a",
            "entry": "GPy. GPy: A gaussian process framework in python. http://github.com/SheffieldML/ GPy, since 2012.",
            "url": "http://github.com/SheffieldML/GPy"
        },
        {
            "id": "Greenbury_et+al_2016_a",
            "entry": "Sam F Greenbury, Steffen Schaper, Sebastian E Ahnert, and Ard A Louis. Genetic correlations greatly increase mutational robustness and can both reduce and enhance evolvability. PLoS computational biology, 12(3):e1004773, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greenbury%2C%20Sam%20F.%20Schaper%2C%20Steffen%20Ahnert%2C%20Sebastian%20E.%20and%20Ard%20A%20Louis.%20Genetic%20correlations%20greatly%20increase%20mutational%20robustness%20and%20can%20both%20reduce%20and%20enhance%20evolvability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greenbury%2C%20Sam%20F.%20Schaper%2C%20Steffen%20Ahnert%2C%20Sebastian%20E.%20and%20Ard%20A%20Louis.%20Genetic%20correlations%20greatly%20increase%20mutational%20robustness%20and%20can%20both%20reduce%20and%20enhance%20evolvability%202016"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1225\u20131234. PMLR, 20\u201322 Jun 2016. URL http://proceedings.mlr.press/v48/hardt16.html.",
            "url": "http://proceedings.mlr.press/v48/hardt16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016-06-20"
        },
        {
            "id": "Harvey_et+al_2017_a",
            "entry": "Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1064\u20131068. PMLR, 07\u201310 Jul 2017. URL http://proceedings.mlr.press/v65/harvey17a.html.",
            "url": "http://proceedings.mlr.press/v65/harvey17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017-07"
        },
        {
            "id": "Hinton_1993_a",
            "entry": "Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, COLT \u201993, pp. 5\u201313, New York, NY, USA, 1993. ACM. ISBN 0-89791-611-5. doi: 10.1145/168304.168306. URL http://doi.acm.org/10.1145/168304.168306.",
            "crossref": "https://dx.doi.org/10.1145/168304.168306",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/168304.168306"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997"
        },
        {
            "id": "Kawaguchi_et+al_2017_a",
            "entry": "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. CoRR, abs/1710.05468, 2017. URL http://arxiv.org/abs/1710.05468.",
            "url": "http://arxiv.org/abs/1710.05468",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016. URL http://arxiv.org/abs/1609.04836.",
            "url": "http://arxiv.org/abs/1609.04836",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Lei Ba. Adam: Amethod for stochastic optimization. In Proc. 3rd Int. Conf. Learn. Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20Amethod%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20Amethod%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krogh_1992_a",
            "entry": "Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950\u2013957, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "Krueger_et+al_2017_a",
            "entry": "David Krueger, Nicolas Ballas, Stanislaw Jastrzebski, Devansh Arpit, Maxinder S. Kanwal, Tegan Maharaj, Emmanuel Bengio, Asja Fischer, Aaron Courville, Simon Lacoste-Julien, and Yoshua Bengio. A closer look at memorization in deep networks. Proceedings of the 34th International Conference on Machine Learning (ICML\u201917), 2017. URL https://arxiv.org/abs/1706.05394.",
            "url": "https://arxiv.org/abs/1706.05394",
            "arxiv_url": "https://arxiv.org/pdf/1706.05394"
        },
        {
            "id": "Langford_2001_a",
            "entry": "John Langford and Matthias Seeger. Bounds for averaging classifiers. 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Seeger%2C%20Matthias%20Bounds%20for%20averaging%20classifiers%202001"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00165"
        },
        {
            "id": "Lempel_1976_a",
            "entry": "Abraham Lempel and Jacob Ziv. On the complexity of finite sequences. IEEE Transactions on information theory, 22(1):75\u201381, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lempel%2C%20Abraham%20Ziv%2C%20Jacob%20On%20the%20complexity%20of%20finite%20sequences%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lempel%2C%20Abraham%20Ziv%2C%20Jacob%20On%20the%20complexity%20of%20finite%20sequences%201976"
        },
        {
            "id": "Liao_2017_a",
            "entry": "Qianli Liao and Tomaso Poggio. Theory of deep learning ii: Landscape of the empirical risk in deep learning. arXiv preprint arXiv:1703.09833, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09833"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223\u20131247, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Henry%20W.%20Tegmark%2C%20Max%20Rolnick%2C%20David%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Henry%20W.%20Tegmark%2C%20Max%20Rolnick%2C%20David%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017"
        },
        {
            "id": "De_et+al_2018_a",
            "entry": "Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11271"
        },
        {
            "id": "Mcallester_1998_a",
            "entry": "David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 230\u2013234. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Some%20pac-bayesian%20theorems%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Some%20pac-bayesian%20theorems%201998"
        },
        {
            "id": "Citeseer_1999_a",
            "entry": "Citeseer, 1999a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Citeseer%201999a"
        },
        {
            "id": "Mcallester_1999_a",
            "entry": "David A. McAllester. Pac-bayesian model averaging. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pp. 164\u2013170, New York, NY, USA, 1999b. ACM. ISBN 1-58113-167-4. doi: 10.1145/307400.307435. URL http://doi.acm.org/10.1145/307400.307435.",
            "crossref": "https://dx.doi.org/10.1145/307400.307435",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/307400.307435"
        },
        {
            "id": "Ming_2014_a",
            "entry": "LI Ming and Paul MB Vit\u00e1nyi. Kolmogorov complexity and its applications. Algorithms and Complexity, 1:187, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ming%2C%20L.I.%20Vit%C3%A1nyi%2C%20Paul%20M.B.%20Kolmogorov%20complexity%20and%20its%20applications%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ming%2C%20L.I.%20Vit%C3%A1nyi%2C%20Paul%20M.B.%20Kolmogorov%20complexity%20and%20its%20applications%202014"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Montufar_et+al_2014_a",
            "entry": "Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pp. 2924\u20132932, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06959"
        },
        {
            "id": "Morgan_1990_a",
            "entry": "Nelson Morgan and Herv\u00e9 Bourlard. Generalization and parameter estimation in feedforward nets: Some experiments. In Advances in neural information processing systems, pp. 630\u2013637, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morgan%2C%20Nelson%20Bourlard%2C%20Herv%C3%A9%20Generalization%20and%20parameter%20estimation%20in%20feedforward%20nets%3A%20Some%20experiments%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morgan%2C%20Nelson%20Bourlard%2C%20Herv%C3%A9%20Generalization%20and%20parameter%20estimation%20in%20feedforward%20nets%3A%20Some%20experiments%201990"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376\u20131401, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015"
        },
        {
            "id": "Neyshabur_et+al_0000_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949\u20135958, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Neyshabur_et+al_2018_a",
            "entry": "Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12076"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian convolutional neural networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.05148"
        },
        {
            "id": "Poggio_et+al_2017_a",
            "entry": "Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503\u2013519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017"
        },
        {
            "id": "Poggio_et+al_2018_a",
            "entry": "Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: the non-overfitting puzzle. Technical report, CBMM memo 073, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20Tomaso%20Kawaguchi%2C%20Kenji%20Liao%2C%20Qianli%20Miranda%2C%20Brando%20Theory%20of%20deep%20learning%20iii%3A%20the%20non-overfitting%20puzzle%202018"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pp. 3360\u20133368, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.06434.",
            "url": "https://arxiv.org/abs/1511.06434",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Rissanen_1978_a",
            "entry": "Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465\u2013471, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rissanen%2C%20Jorma%20Modeling%20by%20shortest%20data%20description%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rissanen%2C%20Jorma%20Modeling%20by%20shortest%20data%20description%201978"
        },
        {
            "id": "Sagun_et+al_2017_a",
            "entry": "Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04454"
        },
        {
            "id": "Schmidhuber_1997_a",
            "entry": "J\u00fcrgen Schmidhuber. Discovering neural nets with low kolmogorov complexity and high generalization capability. Neural Networks, 10(5):857\u2013873, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Discovering%20neural%20nets%20with%20low%20kolmogorov%20complexity%20and%20high%20generalization%20capability%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Discovering%20neural%20nets%20with%20low%20kolmogorov%20complexity%20and%20high%20generalization%20capability%201997"
        },
        {
            "id": "Schmidhuber_2015_a",
            "entry": "J\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015"
        },
        {
            "id": "Schoenholz_et+al_2017_a",
            "entry": "Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. In Proceedings of the International Conference on Learning Representations (ICLR), 2017a. URL https://arxiv.org/abs/1611.01232.",
            "url": "https://arxiv.org/abs/1611.01232",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "Schoenholz_et+al_0000_a",
            "entry": "Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. A correspondence between random neural networks and statistical field theory. arXiv preprint arXiv:1710.06570, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06570"
        },
        {
            "id": "Shalev-Shwartz_2014_a",
            "entry": "Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "Smith_2017_a",
            "entry": "Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient descent. CoRR, abs/1710.06451, 2017. URL http://arxiv.org/abs/1710.06451.",
            "url": "http://arxiv.org/abs/1710.06451",
            "arxiv_url": "https://arxiv.org/pdf/1710.06451"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Such_et+al_2018_a",
            "entry": "Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. NIPS Deep Reinforcement Learning Workshop, 2018. URL https://arxiv.org/abs/1712.06567.",
            "url": "https://arxiv.org/abs/1712.06567",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "Sun_et+al_2016_a",
            "entry": "Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In AAAI, pp. 2066\u20132072, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shizhao%20Chen%2C%20Wei%20Wang%2C%20Liwei%20Xiaoguang%20Liu%2C%20and%20Tie-Yan%20Liu.%20On%20the%20depth%20of%20deep%20neural%20networks%3A%20A%20theoretical%20view%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shizhao%20Chen%2C%20Wei%20Wang%2C%20Liwei%20Xiaoguang%20Liu%2C%20and%20Tie-Yan%20Liu.%20On%20the%20depth%20of%20deep%20neural%20networks%3A%20A%20theoretical%20view%202016"
        },
        {
            "id": "Vapnik_2013_a",
            "entry": "Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20The%20nature%20of%20statistical%20learning%20theory%202013"
        },
        {
            "id": "Wolpert_1994_a",
            "entry": "David H Wolpert and R Waters. The relationship between pac, the statistical physics framework, the bayesian framework, and the vc framework. In In. Citeseer, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolpert%2C%20David%20H.%20Waters%2C%20R.%20The%20relationship%20between%20pac%2C%20the%20statistical%20physics%20framework%2C%20the%20bayesian%20framework%2C%20and%20the%20vc%20framework%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolpert%2C%20David%20H.%20Waters%2C%20R.%20The%20relationship%20between%20pac%2C%20the%20statistical%20physics%20framework%2C%20the%20bayesian%20framework%2C%20and%20the%20vc%20framework%201994"
        },
        {
            "id": "Wu_2017_a",
            "entry": "Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10239"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/abs/1708.07747.",
            "url": "http://arxiv.org/abs/1708.07747",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "Xu_2012_a",
            "entry": "Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391\u2013423, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012"
        },
        {
            "id": "Yao_et+al_2007_a",
            "entry": "Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of the International Conference on Learning Representations (ICLR), 2017a. URL https://arxiv.org/abs/1611.03530.",
            "url": "https://arxiv.org/abs/1611.03530",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "Zhang_et+al_2017_b",
            "entry": "Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Karthik Sridharan, Brando Miranda, Noah Golowich, and Tomaso Poggio. Musings on deep learning: Properties of sgd. 04/2017 2017b. URL https://cbmm.mit.edu/publications/musings-deep-learning-properties-sgd.formerly titled \"Theory of Deep Learning III: Generalization Properties of SGD\".",
            "url": "https://cbmm.mit.edu/publications/musings-deep-learning-properties-sgd.formerly",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Liao%2C%20Qianli%20Rakhlin%2C%20Alexander%20Sridharan%2C%20Karthik%20Musings%20on%20deep%20learning%3A%20Properties%20of%20sgd%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Yao Zhang, Andrew M Saxe, Madhu S Advani, and Alpha A Lee. Energy\u2013entropy competition and the effectiveness of stochastic gradient descent in machine learning. Molecular Physics, pp. 1\u201310, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yao%20Saxe%2C%20Andrew%20M.%20Advani%2C%20Madhu%20S.%20Lee%2C%20Alpha%20A.%20Energy%E2%80%93entropy%20competition%20and%20the%20effectiveness%20of%20stochastic%20gradient%20descent%20in%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yao%20Saxe%2C%20Andrew%20M.%20Advani%2C%20Madhu%20S.%20Lee%2C%20Alpha%20A.%20Energy%E2%80%93entropy%20competition%20and%20the%20effectiveness%20of%20stochastic%20gradient%20descent%20in%20machine%20learning%202018"
        },
        {
            "id": "In_2014_a",
            "entry": "In the experiments where we learn Boolean functions with the smaller neural network with 7 Boolean inputs and one Boolean output (results in Figure 1c), we use a variation of SGD similar to the method of adversarial training proposed by Ian Goodfellow Goodfellow et al. (2014). We chose this second method because SGD often did not find a solution with 0 training error for all the Boolean functions, even with many thousand iterations. By contrast, the adversarial method succeeded in almost all cases, at least for the relatively small neural networks which we focus on here.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20the%20experiments%20where%20we%20learn%20Boolean%20functions%20with%20the%20smaller%20neural%20network%20with%207%20Boolean%20inputs%20and%20one%20Boolean%20output%20results%20in%20Figure%201c%20we%20use%20a%20variation%20of%20SGD%20similar%20to%20the%20method%20of%20adversarial%20training%20proposed%20by%20Ian%20Goodfellow%20Goodfellow%20et%20al%202014%20We%20chose%20this%20second%20method%20because%20SGD%20often%20did%20not%20find%20a%20solution%20with%200%20training%20error%20for%20all%20the%20Boolean%20functions%20even%20with%20many%20thousand%20iterations%20By%20contrast%20the%20adversarial%20method%20succeeded%20in%20almost%20all%20cases%20at%20least%20for%20the%20relatively%20small%20neural%20networks%20which%20we%20focus%20on%20here",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20the%20experiments%20where%20we%20learn%20Boolean%20functions%20with%20the%20smaller%20neural%20network%20with%207%20Boolean%20inputs%20and%20one%20Boolean%20output%20results%20in%20Figure%201c%20we%20use%20a%20variation%20of%20SGD%20similar%20to%20the%20method%20of%20adversarial%20training%20proposed%20by%20Ian%20Goodfellow%20Goodfellow%20et%20al%202014%20We%20chose%20this%20second%20method%20because%20SGD%20often%20did%20not%20find%20a%20solution%20with%200%20training%20error%20for%20all%20the%20Boolean%20functions%20even%20with%20many%20thousand%20iterations%20By%20contrast%20the%20adversarial%20method%20succeeded%20in%20almost%20all%20cases%20at%20least%20for%20the%20relatively%20small%20neural%20networks%20which%20we%20focus%20on%20here"
        },
        {
            "id": "In_2004_a",
            "entry": "In Figure 5, we show results comparing the empirical frequency of labellings for a sample of 10 random MNIST images, when these frequencies are obtained by sampling parameters of a neural network (with a Gaussian distribution with parameters \u03c3w = \u03c3b = 1.0), versus that calculated using two methods to approximate the marginal likelihood of the Gaussian process corresponding to the neural network architecture we use. We compare the Laplacian and expectation-propagation approximations.(see Rasmussen (2004) for a description of the algorithms) The network has 2 fully connected hidden layers of 784 ReLU neurons each.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20Figure%205%20we%20show%20results%20comparing%20the%20empirical%20frequency%20of%20labellings%20for%20a%20sample%20of%2010%20random%20MNIST%20images%20when%20these%20frequencies%20are%20obtained%20by%20sampling%20parameters%20of%20a%20neural%20network%20with%20a%20Gaussian%20distribution%20with%20parameters%20%CF%83w%20%20%CF%83b%20%2010%20versus%20that%20calculated%20using%20two%20methods%20to%20approximate%20the%20marginal%20likelihood%20of%20the%20Gaussian%20process%20corresponding%20to%20the%20neural%20network%20architecture%20we%20use%20We%20compare%20the%20Laplacian%20and%20expectationpropagation%20approximationssee%20Rasmussen%202004%20for%20a%20description%20of%20the%20algorithms%20The%20network%20has%202%20fully%20connected%20hidden%20layers%20of%20784%20ReLU%20neurons%20each",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20Figure%205%20we%20show%20results%20comparing%20the%20empirical%20frequency%20of%20labellings%20for%20a%20sample%20of%2010%20random%20MNIST%20images%20when%20these%20frequencies%20are%20obtained%20by%20sampling%20parameters%20of%20a%20neural%20network%20with%20a%20Gaussian%20distribution%20with%20parameters%20%CF%83w%20%20%CF%83b%20%2010%20versus%20that%20calculated%20using%20two%20methods%20to%20approximate%20the%20marginal%20likelihood%20of%20the%20Gaussian%20process%20corresponding%20to%20the%20neural%20network%20architecture%20we%20use%20We%20compare%20the%20Laplacian%20and%20expectationpropagation%20approximationssee%20Rasmussen%202004%20for%20a%20description%20of%20the%20algorithms%20The%20network%20has%202%20fully%20connected%20hidden%20layers%20of%20784%20ReLU%20neurons%20each"
        }
    ]
}
