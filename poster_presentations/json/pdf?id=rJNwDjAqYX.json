{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LARGE-SCALE STUDY OF CURIOSITY-DRIVEN LEARNING",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJNwDjAqYX"
        },
        "abstract": "Reinforcement learning algorithms rely on carefully engineered rewards from the environment that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is difficult and not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is such intrinsic reward function which uses prediction error as a reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance as well as a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many games. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the predictionbased rewards in stochastic setups. Game-play videos and code are at https: //doubleblindsupplementary.github.io/large-curiosity/."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "extrinsic reward",
            "url": "https://en.wikipedia.org/wiki/extrinsic_reward"
        },
        {
            "term": "intrinsic motivation",
            "url": "https://en.wikipedia.org/wiki/intrinsic_motivation"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "intrinsic reward",
            "url": "https://en.wikipedia.org/wiki/intrinsic_reward"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "RL": "Reinforcement learning",
        "RF": "random features",
        "IDF": "Inverse Dynamics Features"
    },
    "highlights": [
        "Reinforcement learning (RL) has emerged as a popular method for training agents to perform complex tasks",
        "We found that an Inverse Dynamics Features-curious agent collects more game reward than a random agent in 75% of the Atari games, an random features-curious agent does better in 70%",
        "We have shown that our agents trained purely with a curiosity reward are able to learn useful behaviours: (a) Agent being able to play many Atari games without using any rewards. (b) Mario being able to cross over 11 levels without any extrinsic reward. (c) Walking-like behavior emerged in the Ant environment. (d) Juggling-like behavior in Robo-school environment (e) Rally-making behavior in Two-player Pong with curiosity-driven agent on both sides",
        "Our results suggest that, in many game environments designed by humans, the extrinsic reward is often aligned with the objective of seeking novelty",
        "Limitation of prediction error based curiosity: A more serious potential limitation is the handling of stochastic dynamics",
        "If the transitions in the environment are random, even with a perfect dynamics model, the expected reward will be the entropy of the transition, and the agent will seek out transitions with the highest entropy"
    ],
    "key_statements": [
        "Reinforcement learning (RL) has emerged as a popular method for training agents to perform complex tasks",
        "In Reinforcement learning, the agent\u2019s policy is trained by maximizing a reward function that is designed to align with the task",
        "An alternative to \u201cshaping\u201d an extrinsic reward is to supplement it with dense intrinsic rewards (<a class=\"ref-link\" id=\"cOudeyer_2009_a\" href=\"#rOudeyer_2009_a\">Oudeyer & Kaplan, 2009</a>), that is, rewards that are generated by the agent itself",
        "The idea is that these intrinsic rewards will bridge the gaps between sparse extrinsic rewards by guiding the agent to efficiently explore the environment to find the extrinsic reward",
        "We discover that random features are sufficient for good performance in environments that were used for training, the learned features appear to generalize better",
        "In combining intrinsic and extrinsic rewards, we found it useful to ensure that the scale of the intrinsic reward was consistent across state space",
        "We show the evaluation curves of mean extrinsic reward in on common Atari games in Figure 2 and all 48 Atari suite in Figure 8 in the appendix",
        "We found that an Inverse Dynamics Features-curious agent collects more game reward than a random agent in 75% of the Atari games, an random features-curious agent does better in 70%",
        "Note that the extrinsic reward is meaningless in this context since the agent is playing both sides, so instead, we show the length of the episode",
        "We investigate how well random features and Inverse Dynamics Features-based curiosity agents generalize to novel levels of Mario",
        "Intrinsic Motivation: A family of approaches to intrinsic motivation reward an agent based on prediction error (<a class=\"ref-link\" id=\"cSchmidhuber_1991_c\" href=\"#rSchmidhuber_1991_c\">Schmidhuber, 1991c</a>; <a class=\"ref-link\" id=\"cStadie_et+al_2015_a\" href=\"#rStadie_et+al_2015_a\">Stadie et al, 2015</a>; <a class=\"ref-link\" id=\"cPathak_et+al_2017_a\" href=\"#rPathak_et+al_2017_a\">Pathak et al, 2017</a>; <a class=\"ref-link\" id=\"cAchiam_2017_a\" href=\"#rAchiam_2017_a\">Achiam & Sastry, 2017</a>), prediction uncertainty (<a class=\"ref-link\" id=\"cStill_2012_a\" href=\"#rStill_2012_a\">Still & Precup, 2012</a>; <a class=\"ref-link\" id=\"cHouthooft_et+al_2016_a\" href=\"#rHouthooft_et+al_2016_a\">Houthooft et al, 2016</a>), or improvement (<a class=\"ref-link\" id=\"cSchmidhuber_1991_a\" href=\"#rSchmidhuber_1991_a\">Schmidhuber, 1991a</a>; <a class=\"ref-link\" id=\"cLopes_et+al_2012_a\" href=\"#rLopes_et+al_2012_a\">Lopes et al, 2012</a>) of a forward dynamics model of the environment that gets trained along with the agent\u2019s policy",
        "It is not yet clear in which situations count-based approaches should be preferred over dynamics-based approaches; we chose to focus on dynamics-based bonuses in this paper since we found them straightforward to scale and parallelize",
        "We have shown that our agents trained purely with a curiosity reward are able to learn useful behaviours: (a) Agent being able to play many Atari games without using any rewards. (b) Mario being able to cross over 11 levels without any extrinsic reward. (c) Walking-like behavior emerged in the Ant environment. (d) Juggling-like behavior in Robo-school environment (e) Rally-making behavior in Two-player Pong with curiosity-driven agent on both sides",
        "Our results suggest that, in many game environments designed by humans, the extrinsic reward is often aligned with the objective of seeking novelty",
        "Limitation of prediction error based curiosity: A more serious potential limitation is the handling of stochastic dynamics",
        "If the transitions in the environment are random, even with a perfect dynamics model, the expected reward will be the entropy of the transition, and the agent will seek out transitions with the highest entropy",
        "In Figure 6 we show how adding the noisy-TV affects the performance of Inverse Dynamics Features and random features"
    ],
    "summary": [
        "Reinforcement learning (RL) has emerged as a popular method for training agents to perform complex tasks.",
        "We perform a large-scale empirical study of agents driven purely by intrinsic rewards across a range of diverse simulated environments.",
        "We discover that random features are sufficient for good performance in environments that were used for training, the learned features appear to generalize better.",
        "In a dynamics-based curiosity formulation, there are two sources of non-stationarity: the forward dynamics model is evolving over time as it is trained and the features are changing as they learn.",
        "We systematically investigate the efficacy of a number of feature-learning methods, summarized briefly as follows: Pixels The simplest case is where \u03c6(x) = x and we fit our forward dynamics model in the observation space.",
        "The goal of this large-scale analysis is to investigate the following questions: (a) What happens when you run a pure curiosity-driven agent on a variety of games without any extrinsic rewards?",
        "This shows that a pure curiosity-driven agent can often learn to obtain external rewards without seeing any extrinsic rewards during training!",
        "Super Mario Bros has already been studied in the context of extrinsic reward free learning (<a class=\"ref-link\" id=\"cPathak_et+al_2017_a\" href=\"#rPathak_et+al_2017_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2017_a\" href=\"#rPathak_et+al_2017_a\">Pathak et al, 2017</a></a>) in small-scale experiments, and so we were keen to see how far curiosity alone can push the agent.",
        "These results might suggest that while random features perform well on training environments, learned features appear to generalize better to novel levels.",
        "In all our experiments so far, we have shown that our agents can learn useful skills without any extrinsic rewards, driven purely by curiosity.",
        "Intrinsic Motivation: A family of approaches to intrinsic motivation reward an agent based on prediction error (<a class=\"ref-link\" id=\"cSchmidhuber_1991_c\" href=\"#rSchmidhuber_1991_c\">Schmidhuber, 1991c</a>; <a class=\"ref-link\" id=\"cStadie_et+al_2015_a\" href=\"#rStadie_et+al_2015_a\">Stadie et al, 2015</a>; <a class=\"ref-link\" id=\"cPathak_et+al_2017_a\" href=\"#rPathak_et+al_2017_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2017_a\" href=\"#rPathak_et+al_2017_a\">Pathak et al, 2017</a></a>; <a class=\"ref-link\" id=\"cAchiam_2017_a\" href=\"#rAchiam_2017_a\">Achiam & Sastry, 2017</a>), prediction uncertainty (<a class=\"ref-link\" id=\"cStill_2012_a\" href=\"#rStill_2012_a\">Still & Precup, 2012</a>; <a class=\"ref-link\" id=\"cHouthooft_et+al_2016_a\" href=\"#rHouthooft_et+al_2016_a\">Houthooft et al, 2016</a>), or improvement (<a class=\"ref-link\" id=\"cSchmidhuber_1991_a\" href=\"#rSchmidhuber_1991_a\">Schmidhuber, 1991a</a>; <a class=\"ref-link\" id=\"cLopes_et+al_2012_a\" href=\"#rLopes_et+al_2012_a\">Lopes et al, 2012</a>) of a forward dynamics model of the environment that gets trained along with the agent\u2019s policy.",
        "Whilst we expect this pattern to hold true for dynamics-based exploration, we have some preliminary evidence showing that learned features appear to generalize better to novel levels in Mario Bros.",
        "We have shown that our agents trained purely with a curiosity reward are able to learn useful behaviours: (a) Agent being able to play many Atari games without using any rewards.",
        "Future Work: We have presented a simple and scalable approach that can learn nontrivial behaviors across a diverse range of environments without any reward function or end-of-episode signal."
    ],
    "headline": "Our results show surprisingly good performance as well as a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many games. We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular Reinforcement learning game benchmarks, but learned features appear to generalize better. We demonstrate limitations of the predictionbased rewards in stochastic setups",
    "reference_links": [
        {
            "id": "Achiam_2017_a",
            "entry": "Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv:1703.01732, 2017. 3, 9",
            "arxiv_url": "https://arxiv.org/pdf/1703.01732"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06-02"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, 2016. 1, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Richard Y Chen, John Schulman, Pieter Abbeel, and Szymon Sidor. UCB and infogain exploration via qensembles. arXiv:1706.01502, 2017. 9",
            "arxiv_url": "https://arxiv.org/pdf/1706.01502"
        },
        {
            "id": "Costikyan_2013_a",
            "entry": "Greg Costikyan. Uncertainty in games. Mit Press, 2013. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Costikyan%2C%20Greg%20Uncertainty%20in%20games%202013"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint, 2018. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eysenbach%2C%20Benjamin%20Gupta%2C%20Abhishek%20Ibarz%2C%20Julian%20Levine%2C%20Sergey%20Diversity%20is%20all%20you%20need%3A%20Learning%20skills%20without%20a%20reward%20function%202018"
        },
        {
            "id": "Fortunato_et+al_2017_a",
            "entry": "Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. arXiv:1706.10295, 2017. 9",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, John D Co-Reyes, and Sergey Levine. EX2: Exploration with exemplar models for deep reinforcement learning. NIPS, 2017. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Co-Reyes%2C%20John%20D.%20Levine%2C%20Sergey%20EX2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Co-Reyes%2C%20John%20D.%20Levine%2C%20Sergey%20EX2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Gregor_et+al_2017_a",
            "entry": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. ICLR Workshop, 2017. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20Rezende%2C%20Danilo%20Jimenez%20Wierstra%2C%20Daan%20Variational%20intrinsic%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20Rezende%2C%20Danilo%20Jimenez%20Wierstra%2C%20Daan%20Variational%20intrinsic%20control%202017"
        },
        {
            "id": "Houthooft_et+al_2016_a",
            "entry": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In NIPS, 2016. 1, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016"
        },
        {
            "id": "Hunicke_et+al_2004_a",
            "entry": "Robin Hunicke, Marc LeBlanc, and Robert Zubek. Mda: A formal approach to game design and game research. In AAAI Workshop on Challenges in Game AI, 2004. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hunicke%2C%20Robin%20LeBlanc%2C%20Marc%20Zubek%2C%20Robert%20Mda%3A%20A%20formal%20approach%20to%20game%20design%20and%20game%20research%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hunicke%2C%20Robin%20LeBlanc%2C%20Marc%20Zubek%2C%20Robert%20Mda%3A%20A%20formal%20approach%20to%20game%20design%20and%20game%20research%202004"
        },
        {
            "id": "Ioffe_0000_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 4",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Jarrett_et+al_2009_a",
            "entry": "Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pp. 2146\u20132153. IEEE, 2009. 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jarrett%2C%20Kevin%20Kavukcuoglu%2C%20Koray%20LeCun%2C%20Yann%20What%20is%20the%20best%20multi-stage%20architecture%20for%20object%20recognition%3F%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jarrett%2C%20Kevin%20Kavukcuoglu%2C%20Koray%20LeCun%2C%20Yann%20What%20is%20the%20best%20multi-stage%20architecture%20for%20object%20recognition%3F%202009"
        },
        {
            "id": "Juliani_et+al_2018_a",
            "entry": "Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A general platform for intelligent agents. arXiv:1809.02627, 2018. 2",
            "arxiv_url": "https://arxiv.org/pdf/1809.02627"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Lazzaro_2004_a",
            "entry": "Nicole Lazzaro. Why we play games: Four keys to more emotion in player experiences. In Proceedings of GDC, 2004. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazzaro%2C%20Nicole%20Why%20we%20play%20games%3A%20Four%20keys%20to%20more%20emotion%20in%20player%20experiences%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazzaro%2C%20Nicole%20Why%20we%20play%20games%3A%20Four%20keys%20to%20more%20emotion%20in%20player%20experiences%202004"
        },
        {
            "id": "Lehman_2008_a",
            "entry": "Joel Lehman and Kenneth O Stanley. Exploiting open-endedness to solve problems through the search for novelty. In ALIFE, 2008. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Exploiting%20open-endedness%20to%20solve%20problems%20through%20the%20search%20for%20novelty%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Exploiting%20open-endedness%20to%20solve%20problems%20through%20the%20search%20for%20novelty%202008"
        },
        {
            "id": "Lehman_2011_a",
            "entry": "Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary computation, 2011. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011"
        },
        {
            "id": "Lopes_et+al_2012_a",
            "entry": "Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS, 2012. 1, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopes%2C%20Manuel%20Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Oudeyer%2C%20Pierre-Yves%20Exploration%20in%20model-based%20reinforcement%20learning%20by%20empirically%20estimating%20learning%20progress%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopes%2C%20Manuel%20Lang%2C%20Tobias%20Toussaint%2C%20Marc%20Oudeyer%2C%20Pierre-Yves%20Exploration%20in%20model-based%20reinforcement%20learning%20by%20empirically%20estimating%20learning%20progress%202012"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mohamed_2015_a",
            "entry": "Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In NIPS, 2015. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015"
        },
        {
            "id": "Osband_et+al_2016_a",
            "entry": "Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In NIPS, 2016. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016"
        },
        {
            "id": "Ostrovski_et+al_2017_a",
            "entry": "Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration with neural density models. arXiv:1703.01310, 2017. 1, 9",
            "arxiv_url": "https://arxiv.org/pdf/1703.01310"
        },
        {
            "id": "Oudeyer_2018_a",
            "entry": "Pierre-Yves Oudeyer. Computational theories of curiosity-driven learning. arXiv preprint arXiv:1802.10546, 2018. 9",
            "arxiv_url": "https://arxiv.org/pdf/1802.10546"
        },
        {
            "id": "Oudeyer_2009_a",
            "entry": "Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 2009. 1, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by selfsupervised prediction. In ICML, 2017. 1, 2, 3, 4, 6, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In ICLR, 2018. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Plappert_et+al_2017_a",
            "entry": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv:1706.01905, 2017. 9",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "Poupart_et+al_2006_a",
            "entry": "Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete bayesian reinforcement learning. In ICML, 2006. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poupart%2C%20Pascal%20Vlassis%2C%20Nikos%20Hoey%2C%20Jesse%20Regan%2C%20Kevin%20An%20analytic%20solution%20to%20discrete%20bayesian%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poupart%2C%20Pascal%20Vlassis%2C%20Nikos%20Hoey%2C%20Jesse%20Regan%2C%20Kevin%20An%20analytic%20solution%20to%20discrete%20bayesian%20reinforcement%20learning%202006"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. 3",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "Ryan_2000_a",
            "entry": "Edward L. Ryan, Richard; Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary Educational Psychology, 2000. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ryan%2C%20Edward%20L.%20Richard%2C%20Deci%20Intrinsic%20and%20extrinsic%20motivations%3A%20Classic%20definitions%20and%20new%20directions%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ryan%2C%20Edward%20L.%20Richard%2C%20Deci%20Intrinsic%20and%20extrinsic%20motivations%3A%20Classic%20definitions%20and%20new%20directions%202000"
        },
        {
            "id": "Saxe_et+al_2011_a",
            "entry": "Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On random weights and unsupervised feature learning. In ICML, pp. 1089\u20131096, 2011. 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20M.%20Koh%2C%20Pang%20Wei%20Chen%2C%20Zhenghao%20Bhand%2C%20Maneesh%20Bipin%20Suresh%2C%20and%20Andrew%20Y%20Ng.%20On%20random%20weights%20and%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20M.%20Koh%2C%20Pang%20Wei%20Chen%2C%20Zhenghao%20Bhand%2C%20Maneesh%20Bipin%20Suresh%2C%20and%20Andrew%20Y%20Ng.%20On%20random%20weights%20and%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Schmidhuber_1991_a",
            "entry": "Jurgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pp. 1458\u20131463. IEEE, 1991a. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Curious%20model-building%20control%20systems%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Curious%20model-building%20control%20systems%201991"
        },
        {
            "id": "Schmidhuber_1991_b",
            "entry": "Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior, 1991b. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers.%20In%20From%20animals%20to%20animats%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers.%20In%20From%20animals%20to%20animats%201991"
        },
        {
            "id": "Schmidhuber_1991_c",
            "entry": "Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers, 1991c. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers%201991"
        },
        {
            "id": "Schmidhuber_2010_a",
            "entry": "Jurgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2010. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jurgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%202010%209",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jurgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%202010%209"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 4",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Singh_et+al_2005_a",
            "entry": "Satinder P Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated reinforcement learning. In NIPS, 2005. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Satinder%20P.%20Barto%2C%20Andrew%20G.%20Chentanez%2C%20Nuttapong%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Satinder%20P.%20Barto%2C%20Andrew%20G.%20Chentanez%2C%20Nuttapong%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "Smith_2005_a",
            "entry": "Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 2005. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Linda%20Gasser%2C%20Michael%20The%20development%20of%20embodied%20cognition%3A%20Six%20lessons%20from%20babies.%20Artificial%20life%202005"
        },
        {
            "id": "Stadie_et+al_2015_a",
            "entry": "Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. NIPS Workshop, 2015. 2, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stadie%2C%20Bradly%20C.%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Incentivizing%20exploration%20in%20reinforcement%20learning%20with%20deep%20predictive%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stadie%2C%20Bradly%20C.%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Incentivizing%20exploration%20in%20reinforcement%20learning%20with%20deep%20predictive%20models%202015"
        },
        {
            "id": "Stanley_2015_a",
            "entry": "Kenneth O Stanley and Joel Lehman. Why greatness cannot be planned: The myth of the objective. Springer, 2015. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Lehman%2C%20Joel%20Why%20greatness%20cannot%20be%20planned%3A%20The%20myth%20of%20the%20objective%202015"
        },
        {
            "id": "Still_2012_a",
            "entry": "Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 2012. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Still%2C%20Susanne%20Precup%2C%20Doina%20An%20information-theoretic%20approach%20to%20curiosity-driven%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Still%2C%20Susanne%20Precup%2C%20Doina%20An%20information-theoretic%20approach%20to%20curiosity-driven%20reinforcement%20learning%202012"
        },
        {
            "id": "Sukhbaatar_et+al_2018_a",
            "entry": "Sainbayar Sukhbaatar, Ilya Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In ICLR, 2018. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Kostrikov%2C%20Ilya%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Intrinsic%20motivation%20and%20automatic%20curricula%20via%20asymmetric%20self-play%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Kostrikov%2C%20Ilya%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Intrinsic%20motivation%20and%20automatic%20curricula%20via%20asymmetric%20self-play%202018"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press Cambridge, 1998. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "Tang_et+al_2019_a",
            "entry": "Under review as a conference paper at ICLR 2019 Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #Exploration: A study of count-based exploration for deep reinforcement learning. Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20Under%20review%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20Under%20review%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "9",
            "entry": "9 Pieter Wouters, Herre Van Oostendorp, Rudy Boonekamp, and Erik Van der Spek. The role of game discourse analysis and curiosity in creating engaging and effective serious games by implementing a back story and foreshadowing. Interacting with Computers, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wouters%2C%20Pieter%20Oostendorp%2C%20Herre%20Van%20Boonekamp%2C%20Rudy%20der%20Spek%2C%20Erik%20Van%20The%20role%20of%20game%20discourse%20analysis%20and%20curiosity%20in%20creating%20engaging%20and%20effective%20serious%20games%20by%20implementing%20a%20back%20story%20and%20foreshadowing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wouters%2C%20Pieter%20Oostendorp%2C%20Herre%20Van%20Boonekamp%2C%20Rudy%20der%20Spek%2C%20Erik%20Van%20The%20role%20of%20game%20discourse%20analysis%20and%20curiosity%20in%20creating%20engaging%20and%20effective%20serious%20games%20by%20implementing%20a%20back%20story%20and%20foreshadowing%202011"
        },
        {
            "id": "6",
            "entry": "6 Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1476\u20131483, 2015. 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20de%20Freitas%2C%20Nando%20Deep%20fried%20convnets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20de%20Freitas%2C%20Nando%20Deep%20fried%20convnets%202015"
        }
    ]
}
