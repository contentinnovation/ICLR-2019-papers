{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
        "author": "Shengyang Sun, Guodong Zhang, Jiaxin Shi, Roger Grosse, \u2020University of Toronto, \u2020Vector Institute, \u2021Tsinghua University {ssy, gdzhang, rgrosse}@cs.toronto.edu, shijx,@mails.tsinghua.edu.cn",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkxacs0qY7"
        },
        "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "stochastic process",
            "url": "https://en.wikipedia.org/wiki/stochastic_process"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "fBNN",
            "url": "https://en.wikipedia.org/wiki/fBNN"
        },
        {
            "term": "root mean square error",
            "url": "https://en.wikipedia.org/wiki/root_mean_square_error"
        },
        {
            "term": "Bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/Bayesian_optimization"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        }
    ],
    "abbreviations": {
        "BNNs": "Bayesian neural networks",
        "fBNNs": "Functional Variational Bayesian Neural Networks",
        "ELBO": "evidence lower bound",
        "BNN": "Bayesian neural network",
        "GP": "Gaussian process",
        "DGP": "Deep Gaussian Processes",
        "fELBO": "functional ELBO",
        "SSGE": "Spectral Stein Gradient Estimator",
        "BBB": "Bayes By Backprop",
        "GAN": "generative adversarial network",
        "log-ML": "log marginal likelihood",
        "NP": "Neural Processes",
        "VIP": "Variational Implicit Processes",
        "RMSE": "root mean square error",
        "NKN": "Neural Kernel Network",
        "BO": "Bayesian optimization",
        "MES": "Max-value Entropy Search"
    },
    "highlights": [
        "Bayesian neural networks (BNNs) (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\"><a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton & Van Camp, 1993</a></a>; <a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\"><a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\">Neal, 1995</a></a>) have the potential to combine the scalability, flexibility, and predictive performance of neural networks with principled Bayesian uncertainty modelling",
        "We introduce functional variational Bayesian neural networks, where a Bayesian neural networks is trained to produce a distribution of functions with small KL divergence to the true posterior over functions",
        "We prove that the KL divergence between stochastic processes can be expressed as the supremum of marginal KL divergences at finite sets of points",
        "We performed Bayesian optimization over functions sampled from Gaussian Processes corresponding to RBF, Matern12 and ArcCosine kernels, and found our Functional Variational Bayesian Neural Networks achieved comparable or better performance than RBF Random Feature",
        "In this paper we investigated variational inference between stochastic processes",
        "We proved that the KL divergence between stochastic processes equals the supremum of KL divergence for marginal distributions over all finite measurement sets"
    ],
    "key_statements": [
        "Bayesian neural networks (BNNs) (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\"><a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton & Van Camp, 1993</a></a>; <a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\"><a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\">Neal, 1995</a></a>) have the potential to combine the scalability, flexibility, and predictive performance of neural networks with principled Bayesian uncertainty modelling",
        "The practical effectiveness of Bayesian neural networks is limited by our ability to specify meaningful prior distributions and by the intractability of posterior inference",
        "We introduce functional variational Bayesian neural networks, where a Bayesian neural networks is trained to produce a distribution of functions with small KL divergence to the true posterior over functions",
        "We prove that the KL divergence between stochastic processes can be expressed as the supremum of marginal KL divergences at finite sets of points",
        "We present functional evidence lower bound training objective",
        "Functional variational inference maximizes the functional evidence lower bound, akin to the weight space evidence lower bound in Equation (1), except that the distributions are over functions rather than weights",
        "Since the KL divergence between stochastic processes is difficult to work with, we reduce it to a more familiar object: KL divergence between the marginal distributions of function values at finite sets of points, which we term measurement sets",
        "We present the whole algorithm for Functional Variational Bayesian Neural Networks in Algorithm 1",
        "We performed Bayesian optimization over functions sampled from Gaussian Processes corresponding to RBF, Matern12 and ArcCosine kernels, and found our Functional Variational Bayesian Neural Networks achieved comparable or better performance than RBF Random Feature",
        "In this paper we investigated variational inference between stochastic processes",
        "We proved that the KL divergence between stochastic processes equals the supremum of KL divergence for marginal distributions over all finite measurement sets"
    ],
    "summary": [
        "Bayesian neural networks (BNNs) (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\"><a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton & Van Camp, 1993</a></a>; <a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\"><a class=\"ref-link\" id=\"cNeal_1995_a\" href=\"#rNeal_1995_a\">Neal, 1995</a></a>) have the potential to combine the scalability, flexibility, and predictive performance of neural networks with principled Bayesian uncertainty modelling.",
        "Given a dataset D = {}in=1, a Bayesian neural network (BNN) is defined in terms of a prior p(w) on the weights, as well as the likelihood p(D|w).",
        "Since the KL divergence between stochastic processes is difficult to work with, we reduce it to a more familiar object: KL divergence between the marginal distributions of function values at finite sets of points, which we term measurement sets.",
        "Let X \u2208 X n denote a finite measurement set and PX the marginal distribution of function values at X.",
        "The measurement set which maximizes the KL term is likely to be close to the training data, since these are the points where one has the most information about the function.",
        "If the measurement set is close to the training data, nothing will encourage the network to exploit the structured prior for extrapolation.",
        "Variational Implicit Processes (VIP) (Ma et al, 2018) are, in a sense, the reverse of fBNNs: they specify BNN priors and use GPs to approximate the posterior.",
        "Our experiments had two main aims: (1) to test the ability of fBNNs to extrapolate using various structural motifs, including both implicit and explicit priors, and (2) to test if they perform competitively with other BNNs on standard benchmark tasks such as regression and contextual bandits.",
        "In all of our experiments, the variational posterior is represented as a stochastic neural network with independent Gaussian distributions over the weights, i.e. q(w) = N (w; \u03bc, diag(\u03c32)).3 We always used the ReLU activation function unless otherwise specified.",
        "Because the KL term in the fELBO is estimated using the SSGE, an implicit variational inference algorithm, the functional prior need not have a tractable marginal density.",
        "Observe that fBNNs are naturally scalable to large datasets because they access the data only through the expected log-likelihood term, which can be estimated stochastically.",
        "Measurement sets consist of 500 training samples and 5 or 50 points from the sampling distribution c, tuned by validation performance.",
        "We compared BBB, RBF Random Feature (<a class=\"ref-link\" id=\"cRahimi_2008_a\" href=\"#rRahimi_2008_a\">Rahimi & Recht, 2008</a>) and our fBNNs in the context of Max-value Entropy Search (MES) (<a class=\"ref-link\" id=\"cWang_2017_a\" href=\"#rWang_2017_a\">Wang & Jegelka, 2017</a>), which requires explicit function samples for Bayesian Optimization.",
        "We performed BO over functions sampled from Gaussian Processes corresponding to RBF, Matern12 and ArcCosine kernels, and found our fBNNs achieved comparable or better performance than RBF Random Feature.",
        "We proved that the KL divergence between stochastic processes equals the supremum of KL divergence for marginal distributions over all finite measurement sets.",
        "We demonstrated that fBNNs extrapolate well over various structures, estimate reliable uncertainties, and scale to large datasets"
    ],
    "headline": "We introduce functional variational Bayesian neural networks, which maximize an Evidence Lower BOund defined directly on stochastic processes, i.e. distributions over functions",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "Martin Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "Asuncion_2007_a",
            "entry": "Arthur Asuncion and David Newman. UCI machine learning repository, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asuncion%2C%20Arthur%20Newman%2C%20David%20UCI%20machine%20learning%20repository%202007"
        },
        {
            "id": "Bae_et+al_2018_a",
            "entry": "Juhan Bae, Guodong Zhang, and Roger Grosse. Eigenvalue corrected noisy natural gradient. arXiv preprint arXiv:1811.12565, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.12565"
        },
        {
            "id": "Baker_1997_a",
            "entry": "Christopher T. Baker. The Numerical Treatment of Integral Equations. Clarendon Press, Oxford, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Christopher%20T.%20The%20Numerical%20Treatment%20of%20Integral%20Equations%201997"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015"
        },
        {
            "id": "Cutajar_et+al_2017_a",
            "entry": "Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature expansions for deep Gaussian processes. In International Conference on Machine Learning, pp. 884\u2013893, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cutajar%2C%20Kurt%20Bonilla%2C%20Edwin%20V.%20Michiardi%2C%20Pietro%20Filippone%2C%20Maurizio%20Random%20feature%20expansions%20for%20deep%20Gaussian%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cutajar%2C%20Kurt%20Bonilla%2C%20Edwin%20V.%20Michiardi%2C%20Pietro%20Filippone%2C%20Maurizio%20Random%20feature%20expansions%20for%20deep%20Gaussian%20processes%202017"
        },
        {
            "id": "Depeweg_et+al_2017_a",
            "entry": "Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Uncertainty decomposition in Bayesian neural networks with latent variables. arXiv preprint arXiv:1706.08495, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08495"
        },
        {
            "id": "Eldredge_2016_a",
            "entry": "Nathaniel Eldredge. Analysis and probability on infinite-dimensional spaces. arXiv preprint arXiv:1607.03591, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.03591"
        },
        {
            "id": "Flam-Shepherd_et+al_2017_a",
            "entry": "Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Mapping Gaussian process priors to Bayesian neural networks. In NIPS Bayesian deep learning workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flam-Shepherd%2C%20Daniel%20Requeima%2C%20James%20Duvenaud%2C%20David%20Mapping%20Gaussian%20process%20priors%20to%20Bayesian%20neural%20networks.%20In%20NIPS%20Bayesian%20deep%20learning%20workshop%202017"
        },
        {
            "id": "Folland_2013_a",
            "entry": "Gerald B Folland. Real analysis: modern techniques and their applications. John Wiley & Sons, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Folland%2C%20Gerald%20B.%20Real%20analysis%3A%20modern%20techniques%20and%20their%20applications%202013"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "Gal_et+al_2017_a",
            "entry": "Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Advances in Neural Information Processing Systems, pp. 3581\u20133590, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarin%20Gal%20Jiri%20Hron%20and%20Alex%20Kendall%20Concrete%20dropout%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2035813590%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarin%20Gal%20Jiri%20Hron%20and%20Alex%20Kendall%20Concrete%20dropout%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2035813590%202017"
        },
        {
            "id": "Garnelo_et+al_2018_a",
            "entry": "M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. M. A. Eslami, and Y. Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01622"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Structured variational learning of Bayesian neural networks with horseshoe priors. In International Conference on Machine Learning, pp. 1744\u20131753, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20Soumya%20Yao%2C%20Jiayu%20Doshi-Velez%2C%20Finale%20Structured%20variational%20learning%20of%20Bayesian%20neural%20networks%20with%20horseshoe%20priors%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Soumya%20Yao%2C%20Jiayu%20Doshi-Velez%2C%20Finale%20Structured%20variational%20learning%20of%20Bayesian%20neural%20networks%20with%20horseshoe%20priors%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Graves_2011_a",
            "entry": "Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems, pp. 2348\u20132356, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Practical%20variational%20inference%20for%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Practical%20variational%20inference%20for%20neural%20networks%202011"
        },
        {
            "id": "Gray_2011_a",
            "entry": "Robert M. Gray. Entropy and Infomation Theory. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gray%2C%20Robert%20M.%20Entropy%20and%20Infomation%20Theory%202011"
        },
        {
            "id": "Hafner_et+al_2018_a",
            "entry": "Danijar Hafner, Dustin Tran, Alex Irpan, Timothy Lillicrap, and James Davidson. Reliable uncertainty estimates in deep neural networks using noise contrastive priors. arXiv preprint arXiv:1807.09289, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.09289"
        },
        {
            "id": "Hensman_et+al_2013_a",
            "entry": "James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In Uncertainty in Artificial Intelligence, pp. 282, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20James%20Fusi%2C%20Nicolo%20Lawrence%2C%20Neil%20D.%20Gaussian%20processes%20for%20big%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20James%20Fusi%2C%20Nicolo%20Lawrence%2C%20Neil%20D.%20Gaussian%20processes%20for%20big%20data%202013"
        },
        {
            "id": "Hensman_et+al_2015_a",
            "entry": "James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Artificial Intelligence and Statistics, pp. 351\u2013360, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20James%20Matthews%2C%20Alexander%20Ghahramani%2C%20Zoubin%20Scalable%20variational%20Gaussian%20process%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20James%20Matthews%2C%20Alexander%20Ghahramani%2C%20Zoubin%20Scalable%20variational%20Gaussian%20process%20classification%202015"
        },
        {
            "id": "Hern_2015_a",
            "entry": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In International Conference on Machine Learning, pp. 1861\u20131869, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Adams%2C%20Ryan%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Adams%2C%20Ryan%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015"
        },
        {
            "id": "Hern_et+al_2014_a",
            "entry": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pp. 918\u2013926, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Hoffman%2C%20Matthew%20W.%20Ghahramani%2C%20Zoubin%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Hoffman%2C%20Matthew%20W.%20Ghahramani%2C%20Zoubin%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014"
        },
        {
            "id": "Hinton_1993_a",
            "entry": "Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Conference on Computational Learning Theory, pp. 5\u201313, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Camp%2C%20Drew%20Van%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Camp%2C%20Drew%20Van%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%201993"
        },
        {
            "id": "Husz_2017_a",
            "entry": "Ferenc Husz\u00e1r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08235"
        },
        {
            "id": "Izmailov_et+al_2017_a",
            "entry": "Pavel Izmailov, Alexander Novikov, and Dmitry Kropotov. Scalable gaussian processes with billions of inducing inputs via tensor train decomposition. arXiv preprint arXiv:1710.07324, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07324"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2015_a",
            "entry": "Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Krauth_et+al_2016_a",
            "entry": "Karl Krauth, Edwin V Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring the capabilities and limitations of Gaussian process models. arXiv preprint arXiv:1610.05392, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05392"
        },
        {
            "id": "Lamperti_2012_a",
            "entry": "John Lamperti. Stochastic processes: a survey of the mathematical theory, volume 23. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lamperti%2C%20John%20Stochastic%20processes%3A%20a%20survey%20of%20the%20mathematical%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lamperti%2C%20John%20Stochastic%20processes%3A%20a%20survey%20of%20the%20mathematical%202012"
        },
        {
            "id": "L_et+al_2010_a",
            "entry": "Miguel L\u00e1zaro-Gredilla, Joaquin Qui\u00f1onero Candela, Carl Edward Rasmussen, and An\u00edbal R Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of Machine Learning Research, 11(Jun):1865\u20131881, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%C3%A1zaro-Gredilla%2C%20Miguel%20Candela%2C%20Joaquin%20Qui%C3%B1onero%20Rasmussen%2C%20Carl%20Edward%20Figueiras-Vidal%2C%20An%C3%ADbal%20R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%C3%A1zaro-Gredilla%2C%20Miguel%20Candela%2C%20Joaquin%20Qui%C3%B1onero%20Rasmussen%2C%20Carl%20Edward%20Figueiras-Vidal%2C%20An%C3%ADbal%20R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010"
        },
        {
            "id": "Le_et+al_2013_a",
            "entry": "Quoc Le, Tam\u00e1s Sarl\u00f3s, and Alex Smola. Fastfood \u2014 approximating kernel expansions in loglinear time. In International Conference on Machine Learning, volume 85, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood%20%E2%80%94%20approximating%20kernel%20expansions%20in%20loglinear%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood%20%E2%80%94%20approximating%20kernel%20expansions%20in%20loglinear%20time%202013"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jaehoon%20Sohl-dickstein%2C%20Jascha%20Pennington%2C%20Jeffrey%20Novak%2C%20Roman%20Deep%20neural%20networks%20as%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jaehoon%20Sohl-dickstein%2C%20Jascha%20Pennington%2C%20Jeffrey%20Novak%2C%20Roman%20Deep%20neural%20networks%20as%20Gaussian%20processes%202018"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In International Conference on Machine Learning, pp. 276\u2013284, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016"
        },
        {
            "id": "Lloyd_et+al_2014_a",
            "entry": "James Robert Lloyd, David K Duvenaud, Roger B Grosse, Joshua B Tenenbaum, and Zoubin Ghahramani. Automatic construction and natural-language description of nonparametric regression models. In AAAI, pp. 1242\u20131250, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lloyd%2C%20James%20Robert%20Duvenaud%2C%20David%20K.%20Grosse%2C%20Roger%20B.%20Tenenbaum%2C%20Joshua%20B.%20Automatic%20construction%20and%20natural-language%20description%20of%20nonparametric%20regression%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lloyd%2C%20James%20Robert%20Duvenaud%2C%20David%20K.%20Grosse%2C%20Roger%20B.%20Tenenbaum%2C%20Joshua%20B.%20Automatic%20construction%20and%20natural-language%20description%20of%20nonparametric%20regression%20models%202014"
        },
        {
            "id": "Louizos_2016_a",
            "entry": "Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix Gaussian posteriors. In International Conference on Machine Learning, pp. 1708\u20131716, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20Gaussian%20posteriors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20Gaussian%20posteriors%202016"
        },
        {
            "id": "Louizos_2017_a",
            "entry": "Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural networks. In International Conference on Machine Learning, pp. 2218\u20132227, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Multiplicative%20normalizing%20flows%20for%20variational%20Bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Multiplicative%20normalizing%20flows%20for%20variational%20Bayesian%20neural%20networks%202017"
        },
        {
            "id": "Louizos_et+al_2017_b",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pp. 3288\u20133298, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Chao_2018_a",
            "entry": "Chao Ma, Yingzhen Li, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Variational implicit processes. arXiv preprint arXiv:1806.02390, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02390"
        },
        {
            "id": "Mallasto_2017_a",
            "entry": "Anton Mallasto and Aasa Feragen. Learning from uncertain curves: The 2-wasserstein metric for Gaussian processes. In Advances in Neural Information Processing Systems, pp. 5660\u20135670, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mallasto%2C%20Anton%20Feragen%2C%20Aasa%20Learning%20from%20uncertain%20curves%3A%20The%202-wasserstein%20metric%20for%20Gaussian%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mallasto%2C%20Anton%20Feragen%2C%20Aasa%20Learning%20from%20uncertain%20curves%3A%20The%202-wasserstein%20metric%20for%20Gaussian%20processes%202017"
        },
        {
            "id": "De_et+al_2016_a",
            "entry": "Alexander G de G Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In Artificial Intelligence and Statistics, pp. 231\u2013239, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20G%20Matthews%2C%20Alexander%20G.%20Hensman%2C%20James%20Turner%2C%20Richard%20Ghahramani%2C%20Zoubin%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20G%20Matthews%2C%20Alexander%20G.%20Hensman%2C%20James%20Turner%2C%20Richard%20Ghahramani%2C%20Zoubin%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016"
        },
        {
            "id": "Neal_1995_a",
            "entry": "Radford M Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20Learning%20for%20Neural%20Networks%201995"
        },
        {
            "id": "Peterson_1987_a",
            "entry": "Carsten Peterson. A mean field theory learning algorithm for neural networks. Complex systems, 1: 995\u20131019, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peterson%2C%20Carsten%20A%20mean%20field%20theory%20learning%20algorithm%20for%20neural%20networks%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peterson%2C%20Carsten%20A%20mean%20field%20theory%20learning%20algorithm%20for%20neural%20networks%201987"
        },
        {
            "id": "Rahimi_2008_a",
            "entry": "Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, pp. 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "Rasmussen_2001_a",
            "entry": "Carl Edward Rasmussen and Zoubin Ghahramani. Occam\u2019s razor. In Advances in Neural Information Processing Systems, pp. 294\u2013300, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carl%20Edward%20Rasmussen%20and%20Zoubin%20Ghahramani%20Occams%20razor%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%20294300%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carl%20Edward%20Rasmussen%20and%20Zoubin%20Ghahramani%20Occams%20razor%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%20294300%202001"
        },
        {
            "id": "Rasmussen_2006_a",
            "entry": "Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "Riquelme_et+al_2018_a",
            "entry": "Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An empirical comparison of Bayesian deep networks for thompson sampling. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riquelme%2C%20Carlos%20Tucker%2C%20George%20Snoek%2C%20Jasper%20Deep%20Bayesian%20bandits%20showdown%3A%20An%20empirical%20comparison%20of%20Bayesian%20deep%20networks%20for%20thompson%20sampling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riquelme%2C%20Carlos%20Tucker%2C%20George%20Snoek%2C%20Jasper%20Deep%20Bayesian%20bandits%20showdown%3A%20An%20empirical%20comparison%20of%20Bayesian%20deep%20networks%20for%20thompson%20sampling%202018"
        },
        {
            "id": "Roeder_et+al_2017_a",
            "entry": "Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In Advances in Neural Information Processing Systems, pp. 6925\u20136934, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roeder%2C%20Geoffrey%20Wu%2C%20Yuhuai%20Duvenaud%2C%20David%20K.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roeder%2C%20Geoffrey%20Wu%2C%20Yuhuai%20Duvenaud%2C%20David%20K.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017"
        },
        {
            "id": "Russo_2016_a",
            "entry": "Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442\u20132471, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20An%20information-theoretic%20analysis%20of%20thompson%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20An%20information-theoretic%20analysis%20of%20thompson%20sampling%202016"
        },
        {
            "id": "Salimbeni_2017_a",
            "entry": "Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In Advances in Neural Information Processing Systems, pp. 4588\u20134599, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimbeni%2C%20Hugh%20Deisenroth%2C%20Marc%20Doubly%20stochastic%20variational%20inference%20for%20deep%20Gaussian%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimbeni%2C%20Hugh%20Deisenroth%2C%20Marc%20Doubly%20stochastic%20variational%20inference%20for%20deep%20Gaussian%20processes%202017"
        },
        {
            "id": "Shah_et+al_2014_a",
            "entry": "Amar Shah, Andrew Wilson, and Zoubin Ghahramani. Student-t processes as alternatives to Gaussian processes. In Artificial Intelligence and Statistics, pp. 877\u2013885, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shah%2C%20Amar%20Wilson%2C%20Andrew%20Ghahramani%2C%20Zoubin%20Student-t%20processes%20as%20alternatives%20to%20Gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shah%2C%20Amar%20Wilson%2C%20Andrew%20Ghahramani%2C%20Zoubin%20Student-t%20processes%20as%20alternatives%20to%20Gaussian%20processes%202014"
        },
        {
            "id": "Shi_et+al_2018_a",
            "entry": "Jiaxin Shi, Shengyang Sun, and Jun Zhu. Kernel implicit variational inference. In International Conference on Learning Representations, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20Kernel%20implicit%20variational%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20Kernel%20implicit%20variational%20inference%202018"
        },
        {
            "id": "Shi_et+al_2018_b",
            "entry": "Jiaxin Shi, Shengyang Sun, and Jun Zhu. A spectral approach to gradient estimation for implicit distributions. International Conference on Machine Learning, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20spectral%20approach%20to%20gradient%20estimation%20for%20implicit%20distributions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20spectral%20approach%20to%20gradient%20estimation%20for%20implicit%20distributions%202018"
        },
        {
            "id": "Snelson_2006_a",
            "entry": "Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, pp. 1257\u20131264, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snelson%2C%20Edward%20Ghahramani%2C%20Zoubin%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snelson%2C%20Edward%20Ghahramani%2C%20Zoubin%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006"
        },
        {
            "id": "S_et+al_2016_a",
            "entry": "Casper Kaae S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz\u00e1r. Amortised map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04490"
        },
        {
            "id": "Sun_et+al_2017_a",
            "entry": "Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in Bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283\u20131292, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20Bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20Bayesian%20neural%20networks%202017"
        },
        {
            "id": "Sun_et+al_2018_a",
            "entry": "Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger Grosse. Differentiable compositional kernel learning for Gaussian processes. In International Conference on Machine Learning, pp. 4828\u20134837, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shengyang%20Zhang%2C%20Guodong%20Wang%2C%20Chaoqi%20Zeng%2C%20Wenyuan%20Differentiable%20compositional%20kernel%20learning%20for%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shengyang%20Zhang%2C%20Guodong%20Wang%2C%20Chaoqi%20Zeng%2C%20Wenyuan%20Differentiable%20compositional%20kernel%20learning%20for%20Gaussian%20processes%202018"
        },
        {
            "id": "Thompson_1933_a",
            "entry": "William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "Titsias_2009_a",
            "entry": "Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Artificial Intelligence and Statistics, pp. 567\u2013574, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20Michalis%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20Michalis%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009"
        },
        {
            "id": "Wang_2017_a",
            "entry": "Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. In International Conference on Machine Learning, pp. 3627\u20133635, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zi%20Jegelka%2C%20Stefanie%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zi%20Jegelka%2C%20Stefanie%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017"
        },
        {
            "id": "Wen_et+al_2001_a",
            "entry": "Published as a conference paper at ICLR 2019 Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudoindependent weight perturbations on mini-batches. arXiv preprint arXiv:1803.04386, 2018. Christopher KI Williams and Matthias Seeger. Using the Nystr\u00f6m method to speed up kernel machines. In Advances in Neural Information Processing Systems, pp. 682\u2013688, 2001. Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). In International Conference on Machine Learning, pp. 1775\u20131784, 2015. Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as variational inference. In International Conference on Machine Learning, pp. 5852\u20135861, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.04386"
        },
        {
            "id": "We_2011_a",
            "entry": "We begin with some basic terminology and classical results. See Gray (2011) and Folland (2013) for more details.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20begin%20with%20some%20basic%20terminology%20and%20classical%20results%20See%20Gray%202011%20and%20Folland%202013%20for%20more%20details",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20begin%20with%20some%20basic%20terminology%20and%20classical%20results%20See%20Gray%202011%20and%20Folland%202013%20for%20more%20details"
        },
        {
            "id": "Theorem_2003_a",
            "entry": "Theorem 4 (Kolmogorov extension theorem (\u00d8ksendal, 2003)). Let T be an arbitrary index set. (\u03a9, F ) is a standard measurable space, whose cylindrical measurable space on T is (\u03a9T, F T ). Suppose that for each finite subset I \u2282 T, we have a probability measure \u03bcI on \u03a9I, and these measures satisfy the following compatibility relationship: for each subset J \u2282 I, we have \u03bcJ = \u03bcI \u25e6 \u03c0I\u2212\u21921 J.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theorem%204%20Kolmogorov%20extension%20theorem%20%C3%98ksendal%202003%20Let%20T%20be%20an%20arbitrary%20index%20set%20%CE%A9%20F%20%20is%20a%20standard%20measurable%20space%20whose%20cylindrical%20measurable%20space%20on%20T%20is%20%CE%A9T%20F%20T%20%20Suppose%20that%20for%20each%20finite%20subset%20I%20%20T%20we%20have%20a%20probability%20measure%20%CE%BCI%20on%20%CE%A9I%20and%20these%20measures%20satisfy%20the%20following%20compatibility%20relationship%20for%20each%20subset%20J%20%20I%20we%20have%20%CE%BCJ%20%20%CE%BCI%20%20%CF%80I1%20J"
        },
        {
            "id": "In_2017_a",
            "entry": "In the context of Gaussian processes, \u03bc is a Gaussian measure on a separable Banach space, and the \u03bcI are marginal Gaussian measures at finite sets of input positions (Mallasto & Feragen, 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20the%20context%20of%20Gaussian%20processes%20%CE%BC%20is%20a%20Gaussian%20measure%20on%20a%20separable%20Banach%20space%20and%20the%20%CE%BCI%20are%20marginal%20Gaussian%20measures%20at%20finite%20sets%20of%20input%20positions%20Mallasto%20%20Feragen%202017"
        },
        {
            "id": "2",
            "entry": "2. Correspondence between partitions implies correspondence between KL divergences. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Correspondence%20between%20partitions%20implies%20correspondence%20between%20KL%20divergences"
        }
    ]
}
