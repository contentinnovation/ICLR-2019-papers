{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "STABLE OPPONENT SHAPING IN DIFFERENTIABLE GAMES",
        "author": "Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktaschel, Shimon Whiteson, 1University of Oxford 2DeepMind 3University College London",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyGjjsC5tQ"
        },
        "abstract": "A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel \u2013 from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others\u2019 updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner\u2019s Dilemma. Although experimentally successful, we show that LOLA agents can exhibit \u2018arrogant\u2019 behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Equilibrium",
            "url": "https://en.wikipedia.org/wiki/Equilibrium"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        }
    ],
    "abbreviations": {
        "LOLA": "Learning with Opponent-Learning Awareness",
        "SOS": "Stable Opponent Shaping",
        "GANs": "generative adversarial nets",
        "RL": "reinforcement learning",
        "CO": "Consensus Optimisation",
        "SGA": "Symplectic Gradient Adjustment",
        "IPD": "Iterated Prisoner\u2019s Dilemma",
        "GD": "gradient descent",
        "NL": "naive learning",
        "SFPs": "stable fixed points"
    },
    "highlights": [
        "Learning with Opponent-Learning Awareness has no guarantees of converging or even preserving fixed points of the game",
        "Our central theoretical contribution is that LookAhead and Stable Opponent Shaping converge locally to stable fixed points and avoid strict saddles in all differentiable games",
        "While gradient descent on single losses has been studied extensively, algorithms dealing with interacting goals are proliferating, with little grasp of the underlying dynamics",
        "The analysis behind Consensus Optimisation and Symplectic Gradient Adjustment has been helpful in this respect, though lacking either in generality or convergence guarantees",
        "The first contribution of this paper is to provide a unified framework and fill this theoretical gap with robust convergence results for LookAhead in all differentiable games",
        "Capturing stable fixed points as the correct solution concept was essential for these techniques to apply"
    ],
    "key_statements": [
        "Learning with Opponent-Learning Awareness has no guarantees of converging or even preserving fixed points of the game",
        "While machine learning has traditionally focused on optimising single objectives, generative adversarial nets (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>) have showcased the potential of architectures dealing with multiple interacting goals",
        "We prove that LookAhead locally converges and avoids strict saddles in all differentiable games, filling a theoretical gap in multi-agent learning",
        "We show that Stable Opponent Shaping plays tit-for-tat in the Iterated Prisoner\u2019s Dilemma on par with Learning with Opponent-Learning Awareness, while all other methods mostly defect",
        "Adapted from <a class=\"ref-link\" id=\"cBalduzzi_et+al_2018_a\" href=\"#rBalduzzi_et+al_2018_a\">Balduzzi et al (2018</a>), the following definition insists only on differentiability for gradient-based methods to apply. This concept is strictly more general than stochastic games, whose parameters are usually restricted to action-state transition probabilities or functional approximations thereof",
        "In Appendix A we provide intuition for why stable fixed points are both closer to local minima in the context of multi-loss optimisation, and more tractable for convergence proofs",
        "We show in Appendix C that Learning with Opponent-Learning Awareness can only converge to sub-optimal scenarios with worse losses for both agents, for any \u03b1",
        "We propose Stable Opponent Shaping (SOS), an algorithm preserving both advantages at once",
        "Our central theoretical contribution is that LookAhead and Stable Opponent Shaping converge locally to stable fixed points and avoid strict saddles in all differentiable games",
        "Statements made about Stable Opponent Shaping crucially hold for any hyperparameters a, b \u2208 (0, 1)",
        "Using the second criterion for p, we prove local convergence of Stable Opponent Shaping in all differentiable games despite the presence of a shaping term",
        "Using the first criterion for p, we prove that Stable Opponent Shaping only converges to fixed points",
        "We evaluate the performance of Stable Opponent Shaping in three differentiable games",
        "Iterated Prisoner\u2019s Dilemma: Results are given in Figure 2",
        "50 runs are shown for visibility",
        "Losses at each step are displayed in part (B), averaged across 300 episodes with shaded deviations",
        "Consensus Optimisation is aimed toward two-player zero-sum generative adversarial nets optimisation, while Stable Opponent Shaping is widely applicable with strong theoretical guarantees in all differentiable games",
        "Theoretical results in machine learning have significantly helped understand the causes of success and failure in applications, from optimisation to architecture",
        "While gradient descent on single losses has been studied extensively, algorithms dealing with interacting goals are proliferating, with little grasp of the underlying dynamics",
        "The analysis behind Consensus Optimisation and Symplectic Gradient Adjustment has been helpful in this respect, though lacking either in generality or convergence guarantees",
        "The first contribution of this paper is to provide a unified framework and fill this theoretical gap with robust convergence results for LookAhead in all differentiable games",
        "Capturing stable fixed points as the correct solution concept was essential for these techniques to apply",
        "We showed that opponent shaping is both a powerful approach leading to experimental success and cooperative behaviour \u2013 while at the same time preventing Learning with Opponent-Learning Awareness from preserving fixed points in general"
    ],
    "summary": [
        "LOLA has no guarantees of converging or even preserving fixed points of the game.",
        "We begin by constructing the first explicit tandem game where LOLA agents adopt \u2018arrogant\u2019 behaviour and converge to non-fixed points.",
        "We prove that LookAhead locally converges and avoids strict saddles in all differentiable games, filling a theoretical gap in multi-agent learning.",
        "Using an intuitive and theoretically grounded criterion for this interpolation parameter, SOS inherits both strong convergence guarantees from LA and opponent shaping from LOLA.",
        "By explicitly differentiating through \u2206\u03b82 in the rightmost term, LOLA agents actively shape opponent learning.",
        "LOLA fails to preserve fixed points \u03b8of the game since (I \u2212 \u03b1Ho)\u03be(\u03b8) \u2212 \u03b1\u03c7(\u03b8) = \u2212\u03b1\u03c7(\u03b8) = 0 in general.",
        "Our central theoretical contribution is that LookAhead and SOS converge locally to SFP and avoid strict saddles in all differentiable games.",
        "Since the learning gradients involve second-order Hessian terms, our results assume thrice continuously differentiable losses.",
        "This reduces convergence of a gradient adjustment g to positive stability of \u2207g at stable fixed points.",
        "LookAhead converges locally to stable fixed points for \u03b1 > 0 sufficiently small.",
        "Using the second criterion for p, we prove local convergence of SOS in all differentiable games despite the presence of a shaping term.",
        "SOS converges locally to stable fixed points for \u03b1 > 0 sufficiently small.",
        "Our results for LookAhead and the correct criterion for p-LOLA lead to some of the strongest theoretical guarantees in multi-agent learning.",
        "Note that trained parameters and losses for SOS are almost identical to those for LOLA, displaying equal capacity in opponent shaping while inheriting convergence guarantees and outperforming LOLA in the experiment.",
        "Note that SOS even gets away from the LOLA fixed points if initialised there, converging to improved losses using the alignment criterion with LookAhead.",
        "CO is aimed toward two-player zero-sum GAN optimisation, while SOS is widely applicable with strong theoretical guarantees in all differentiable games.",
        "The first contribution of this paper is to provide a unified framework and fill this theoretical gap with robust convergence results for LookAhead in all differentiable games.",
        "We showed that opponent shaping is both a powerful approach leading to experimental success and cooperative behaviour \u2013 while at the same time preventing LOLA from preserving fixed points in general.",
        "This results in convergence guarantees stronger than all previous algorithms, but in practical superiority over LOLA in the tandem game.",
        "Capturing stable fixed points as the correct solution concept was essential for these techniques to apply"
    ],
    "headline": "We show that Learning with Opponent-Learning Awareness agents can exhibit \u2018arrogant\u2019 behaviour directly at odds with convergence",
    "reference_links": [
        {
            "id": "Balduzzi_et+al_2018_a",
            "entry": "D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel. The Mechanics of n-Player Differentiable Games. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balduzzi%2C%20D.%20Racaniere%2C%20S.%20Martens%2C%20J.%20Foerster%2C%20J.%20The%20Mechanics%20of%20n-Player%20Differentiable%20Games%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20D.%20Racaniere%2C%20S.%20Martens%2C%20J.%20Foerster%2C%20J.%20The%20Mechanics%20of%20n-Player%20Differentiable%20Games%202018"
        },
        {
            "id": "Bowling_2001_a",
            "entry": "M. Bowling and M. Veloso. Rational and convergent learning in stochastic games. In Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2, pp. 1021\u20131026. Morgan Kaufmann Publishers Inc., 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20M.%20Veloso%2C%20M.%20Rational%20and%20convergent%20learning%20in%20stochastic%20games%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20M.%20Veloso%2C%20M.%20Rational%20and%20convergent%20learning%20in%20stochastic%20games%202001"
        },
        {
            "id": "Busoniu_et+al_2008_a",
            "entry": "L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, March 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busoniu%2C%20L.%20Babuska%2C%20R.%20Schutter%2C%20B.De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Busoniu%2C%20L.%20Babuska%2C%20R.%20Schutter%2C%20B.De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008-03"
        },
        {
            "id": "Conitzer_2007_a",
            "entry": "V. Conitzer and T. Sandholm. AWESOME: A General Multiagent Learning Algorithm that Converges in Self-Play and Learns a Best Response Against Stationary Opponents. Machine Learning, 67(1):23\u201343, May 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conitzer%2C%20V.%20Sandholm%2C%20T.%20AWESOME%3A%20A%20General%20Multiagent%20Learning%20Algorithm%20that%20Converges%20in%20Self-Play%20and%20Learns%20a%20Best%20Response%20Against%20Stationary%20Opponents%202007-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conitzer%2C%20V.%20Sandholm%2C%20T.%20AWESOME%3A%20A%20General%20Multiagent%20Learning%20Algorithm%20that%20Converges%20in%20Self-Play%20and%20Learns%20a%20Best%20Response%20Against%20Stationary%20Opponents%202007-05"
        },
        {
            "id": "Daskalakis_et+al_2018_a",
            "entry": "C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with Optimism. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20C.%20Ilyas%2C%20A.%20Syrgkanis%2C%20V.%20Zeng%2C%20H.%20Training%20GANs%20with%20Optimism%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20C.%20Ilyas%2C%20A.%20Syrgkanis%2C%20V.%20Zeng%2C%20H.%20Training%20GANs%20with%20Optimism%202018"
        },
        {
            "id": "Facchinei_2007_a",
            "entry": "Francisco Facchinei and Christian Kanzow. Generalized Nash equilibrium problems. 4OR, 5(3), Sep 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Facchinei%2C%20Francisco%20Kanzow%2C%20Christian%20Generalized%20Nash%20equilibrium%20problems%202007-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Facchinei%2C%20Francisco%20Kanzow%2C%20Christian%20Generalized%20Nash%20equilibrium%20problems%202007-09"
        },
        {
            "id": "Foerster_et+al_2018_a",
            "entry": "J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with Opponent-Learning Awareness. AAMAS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20J.N.%20Chen%2C%20R.Y.%20Al-Shedivat%2C%20M.%20Whiteson%2C%20S.%20Learning%20with%20Opponent-Learning%20Awareness%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20J.N.%20Chen%2C%20R.Y.%20Al-Shedivat%2C%20M.%20Whiteson%2C%20S.%20Learning%20with%20Opponent-Learning%20Awareness%202018"
        },
        {
            "id": "Foerster_et+al_2018_b",
            "entry": "J. N. Foerster, G. Farquhar, M. Al-Shedivat, T. Rocktaschel, E. P. Xing, and S. Whiteson. DiCE: The Infinitely Differentiable Monte-Carlo Estimator. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20N%20Foerster%20G%20Farquhar%20M%20AlShedivat%20T%20Rocktaschel%20E%20P%20Xing%20and%20S%20Whiteson%20DiCE%20The%20Infinitely%20Differentiable%20MonteCarlo%20Estimator%20ICML%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20N%20Foerster%20G%20Farquhar%20M%20AlShedivat%20T%20Rocktaschel%20E%20P%20Xing%20and%20S%20Whiteson%20DiCE%20The%20Infinitely%20Differentiable%20MonteCarlo%20Estimator%20ICML%202018"
        },
        {
            "id": "Gemp_2018_a",
            "entry": "I. Gemp and S. Mahadevan. Global Convergence to the Equilibrium of GANs using Variational Inequalities. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gemp%2C%20I.%20Mahadevan%2C%20S.%20Global%20Convergence%20to%20the%20Equilibrium%20of%20GANs%20using%20Variational%20Inequalities.%20ArXiv%20e-prints%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20Adversarial%20Networks%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20Adversarial%20Networks%20NIPS%202014"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20Trained%20by%20a%20Two%20Time-Scale%20Update%20Rule%20Converge%20to%20a%20Local%20Nash%20Equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20Trained%20by%20a%20Two%20Time-Scale%20Update%20Rule%20Converge%20to%20a%20Local%20Nash%20Equilibrium%202017"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and K. Kavukcuoglu. Decoupled Neural Interfaces using Synthetic Gradients. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Czarnecki%2C%20W.M.%20Osindero%2C%20S.%20Vinyals%2C%20O.%20Decoupled%20Neural%20Interfaces%20using%20Synthetic%20Gradients%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Czarnecki%2C%20W.M.%20Osindero%2C%20S.%20Vinyals%2C%20O.%20Decoupled%20Neural%20Interfaces%20using%20Synthetic%20Gradients%202017"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Only Converges to Minimizers. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.D.%20Simchowitz%2C%20M.%20Jordan%2C%20M.I.%20Recht%2C%20B.%20Gradient%20Descent%20Only%20Converges%20to%20Minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20J.D.%20Simchowitz%2C%20M.%20Jordan%2C%20M.I.%20Recht%2C%20B.%20Gradient%20Descent%20Only%20Converges%20to%20Minimizers%202016"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order Methods Almost Always Avoid Saddle Points. ArXiv e-prints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.D.%20Panageas%2C%20I.%20Piliouras%2C%20G.%20Simchowitz%2C%20M.%20First-order%20Methods%20Almost%20Always%20Avoid%20Saddle%20Points.%20ArXiv%20e-prints%202017"
        },
        {
            "id": "Mertikopoulos_2018_a",
            "entry": "Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, Mar 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mertikopoulos%2C%20Panayotis%20Zhou%2C%20Zhengyuan%20Learning%20in%20games%20with%20continuous%20action%20sets%20and%20unknown%20payoff%20functions%202018-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mertikopoulos%2C%20Panayotis%20Zhou%2C%20Zhengyuan%20Learning%20in%20games%20with%20continuous%20action%20sets%20and%20unknown%20payoff%20functions%202018-03"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "L. Mescheder, S. Nowozin, and A. Geiger. The Numerics of GANs. NIPS, 2017. V. Nagarajan and J. Kolter. Gradient descent GAN optimization is locally stable. NIPS, 2017. J. Ortega and W. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. Society for Industrial and Applied Mathematics, 2000. I. Panageas and G. Piliouras. Gradient Descent Only Converges to Minimizers: Non-Isolated Critical",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20Numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20Numerics%20of%20GANs%202017"
        },
        {
            "id": "Points_2017_a",
            "entry": "Points and Invariant Regions. In ITCS 2017, volume 67 of Leibniz International Proceedings in Informatics, pp. 2:1\u20132:12, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Points%20and%20Invariant%20Regions%20In%20ITCS%202017%20volume%2067%20of%20Leibniz%20International%20Proceedings%20in%20Informatics%20pp%2021212%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Points%20and%20Invariant%20Regions%20In%20ITCS%202017%20volume%2067%20of%20Leibniz%20International%20Proceedings%20in%20Informatics%20pp%2021212%202017"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven Exploration by Self-supervised Prediction. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction%202017"
        },
        {
            "id": "Racaniere_et+al_2017_a",
            "entry": "S. Racaniere, T. Weber, D. P. Reichert, L. Buesing, A. Guez, D. Jimenez Rezende, A. Puigdomenech Badia, O. Vinyals, N. Heess, Y. Li, R. Pascanu, P. Battaglia, D. Hassabis, D. Silver, and D. Wierstra. Imagination-Augmented Agents for Deep Reinforcement Learning. NIPS, 2017. J.B. Rosen. Existence and Uniqueness of Equilibrium Points for Concave N-Person Games. Econometrica, 33, Jul 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racaniere%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.P.%20Buesing%2C%20L.%20Imagination-Augmented%20Agents%20for%20Deep%20Reinforcement%20Learning%202017-07-33",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racaniere%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.P.%20Buesing%2C%20L.%20Imagination-Augmented%20Agents%20for%20Deep%20Reinforcement%20Learning%202017-07-33"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. FeUdal Networks for Hierarchical Reinforcement Learning. ICML, 2017. G. Wayne and L. F. Abbott. Hierarchical control using networks trained with higher-level forward models. Neural Computation, 26(10):2163\u20132193, 2014. C. Zhang and V. Lesser. Multi-Agent Learning with Policy Prediction. AAAI Conference on Artificial Intelligence, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20Networks%20for%20Hierarchical%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20A.S.%20Osindero%2C%20S.%20Schaul%2C%20T.%20Heess%2C%20N.%20FeUdal%20Networks%20for%20Hierarchical%20Reinforcement%20Learning%202017"
        }
    ]
}
