{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
        "author": "Aditya Grover, Eric Wang, Aaron Zweig & Stefano Ermon Computer Science Department Stanford University {adityag,ejwang,azweig,ermon}@cs.stanford.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1eSS3CcKX"
        },
        "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm."
    },
    "keywords": [
        {
            "term": "k-nearest neighbors",
            "url": "https://en.wikipedia.org/wiki/k-nearest_neighbors"
        },
        {
            "term": "neural net",
            "url": "https://en.wikipedia.org/wiki/neural_net"
        },
        {
            "term": "principal components",
            "url": "https://en.wikipedia.org/wiki/principal_components"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "information retrieval",
            "url": "https://en.wikipedia.org/wiki/information_retrieval"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "kNN": "k-nearest neighbors",
        "SCG": "stochastic computation graphs",
        "NN": "neural net",
        "MSE": "mean squared error",
        "PCA": "principal components"
    },
    "highlights": [
        "Learning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification (<a class=\"ref-link\" id=\"cBerrada_et+al_2018_a\" href=\"#rBerrada_et+al_2018_a\">Berrada et al, 2018</a>), ranking documents for information retrieval (Liu et al, 2009), and multi-object target tracking in computer vision (<a class=\"ref-link\" id=\"cBar-Shalom_1995_a\" href=\"#rBar-Shalom_1995_a\">Bar-Shalom & Li, 1995</a>)",
        "We propose an algorithm that learns a basis representation for the k-nearest neighbors classifier in an end-to-end procedure",
        "The stochastic NeuralSort approach is the consistent best performer on mean squared error, while the deterministic NeuralSort is slightly better on the R2 metric.\n6.3",
        "The performance is much closer to the accuracy of a standard CNN",
        "We proposed NeuralSort, a continuous relaxation of the sorting operator to the set of unimodal row-stochastic matrices"
    ],
    "key_statements": [
        "Learning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification (<a class=\"ref-link\" id=\"cBerrada_et+al_2018_a\" href=\"#rBerrada_et+al_2018_a\">Berrada et al, 2018</a>), ranking documents for information retrieval (Liu et al, 2009), and multi-object target tracking in computer vision (<a class=\"ref-link\" id=\"cBar-Shalom_1995_a\" href=\"#rBar-Shalom_1995_a\">Bar-Shalom & Li, 1995</a>)",
        "Deep neural networks can instead be used to learn representations, but these representations cannot be optimized end-to-end for a downstream sorting-based objective, since the sorting operator is not differentiable with respect to its input",
        "We propose an algorithm that learns a basis representation for the k-nearest neighbors classifier in an end-to-end procedure",
        "In Section 3, we propose to extend stochastic computation graphs with nodes corresponding to a relaxation of the deterministic sort operator",
        "Our work considers a generalized setting where sorting based operators can be inserted anywhere in computation graphs to extend traditional pipelines e.g., k-nearest neighbors",
        "The stochastic NeuralSort approach is the consistent best performer on mean squared error, while the deterministic NeuralSort is slightly better on the R2 metric.\n6.3",
        "The performance is much closer to the accuracy of a standard CNN",
        "We proposed NeuralSort, a continuous relaxation of the sorting operator to the set of unimodal row-stochastic matrices"
    ],
    "summary": [
        "Learning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification (<a class=\"ref-link\" id=\"cBerrada_et+al_2018_a\" href=\"#rBerrada_et+al_2018_a\"><a class=\"ref-link\" id=\"cBerrada_et+al_2018_a\" href=\"#rBerrada_et+al_2018_a\">Berrada et al, 2018</a></a>), ranking documents for information retrieval (Liu et al, 2009), and multi-object target tracking in computer vision (<a class=\"ref-link\" id=\"cBar-Shalom_1995_a\" href=\"#rBar-Shalom_1995_a\"><a class=\"ref-link\" id=\"cBar-Shalom_1995_a\" href=\"#rBar-Shalom_1995_a\">Bar-Shalom & Li, 1995</a></a>).",
        "By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.",
        "In Section 3, we propose to extend stochastic computation graphs with nodes corresponding to a relaxation of the deterministic sort operator.",
        "In Section 4, we further use this relaxation to extend computation graphs to include stochastic nodes corresponding to distributions over permutations.",
        "Our relaxation to the sort operator is based on a standard identity for evaluating the sum of the k largest elements in any input vector.",
        "Let Psort(s) denote the continuous relaxation to the permutation matrix Psort(s) for an arbitrary input vector s and temperature \u03c4 defined in Eq 5.",
        "Unimodality allows for efficient projection of the relaxed permutation matrix Psort(s) to the hard matrix Psort(s) via a row-wise arg max, e.g., for straight-through gradients.",
        "Where \u03b8 and s denote sets of parameters, Pz is the permutation matrix corresponding to the permutation z, q(\u00b7) is a parameterized distribution over the elements of the symmetric group Zn, and f (\u00b7) is an arbitrary function of interest assumed to be differentiable in \u03b8 and z.",
        "Proposition 5 provides a method for sampling from PL distributions with parameters s by adding Gumbel perturbations to the logscores and applying the sort operator to the perturbed log-scores.",
        "In order to obtain a differentiable surrogate, we approximate the objective based on the NeuralSort relaxation to the sort operator: Eg f (Psort; \u03b8) \u2248 Eg f (Psort; \u03b8) := L(\u03b8, s).",
        "Our work considers a generalized setting where sorting based operators can be inserted anywhere in computation graphs to extend traditional pipelines e.g., kNN.",
        "The PL distribution permits efficient sampling, exact and tractable density estimation, making it an attractive choice for several applications, e.g., variational inference over latent permutations.",
        "The Sinkhorn and GumbelSinkhorn baselines, as discussed in Section 5, use the Sinkhorn operator to map the stacked CNN representations for the n objects into a doubly-stochastic matrix.",
        "We proposed NeuralSort, a continuous relaxation of the sorting operator to the set of unimodal row-stochastic matrices.",
        "We derived a reparameterized gradient estimator for the Plackett-Luce distribution for efficient stochastic optimization over permutations.",
        "On three illustrative tasks including a fully differentiable k-nearest neighbors, our proposed relaxations outperform prior work in end-to-end learning of semantic orderings of high-dimensional objects."
    ],
    "headline": "We propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for largescale machine learning. In Operating Systems Design and Implementation, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20a%20system%20for%20largescale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20a%20system%20for%20largescale%20machine%20learning%202016"
        },
        {
            "id": "Adams_2011_a",
            "entry": "Ryan Prescott Adams and Richard S Zemel. Ranking via Sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1106.1925"
        },
        {
            "id": "Balog_et+al_2017_a",
            "entry": "Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, and Adrian Weller. Lost relatives of the Gumbel trick. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balog%2C%20Matej%20Tripuraneni%2C%20Nilesh%20Ghahramani%2C%20Zoubin%20Weller%2C%20Adrian%20Lost%20relatives%20of%20the%20Gumbel%20trick%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balog%2C%20Matej%20Tripuraneni%2C%20Nilesh%20Ghahramani%2C%20Zoubin%20Weller%2C%20Adrian%20Lost%20relatives%20of%20the%20Gumbel%20trick%202017"
        },
        {
            "id": "Bar-Shalom_1995_a",
            "entry": "Yaakov Bar-Shalom and Xiao-Rong Li. Multitarget-multisensor tracking: principles and techniques. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bar-Shalom%2C%20Yaakov%20Li%2C%20Xiao-Rong%20Multitarget-multisensor%20tracking%3A%20principles%20and%20techniques%201995"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Berrada_et+al_2018_a",
            "entry": "Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Smooth loss functions for deep top-k classification. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berrada%2C%20Leonard%20Zisserman%2C%20Andrew%20Kumar%2C%20M.Pawan%20Smooth%20loss%20functions%20for%20deep%20top-k%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berrada%2C%20Leonard%20Zisserman%2C%20Andrew%20Kumar%2C%20M.Pawan%20Smooth%20loss%20functions%20for%20deep%20top-k%20classification%202018"
        },
        {
            "id": "Burges_et+al_2005_a",
            "entry": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. In International Conference on Machine learning, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burges%2C%20Chris%20Shaked%2C%20Tal%20Renshaw%2C%20Erin%20Lazier%2C%20Ari%20Learning%20to%20rank%20using%20gradient%20descent%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burges%2C%20Chris%20Shaked%2C%20Tal%20Renshaw%2C%20Erin%20Lazier%2C%20Ari%20Learning%20to%20rank%20using%20gradient%20descent%202005"
        },
        {
            "id": "Chierichetti_et+al_2018_a",
            "entry": "Flavio Chierichetti, Ravi Kumar, and Andrew Tomkins. Discrete choice, permutations, and reconstruction. In Symposium on Discrete Algorithms, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chierichetti%2C%20Flavio%20Kumar%2C%20Ravi%20Tomkins%2C%20Andrew%20Discrete%20choice%2C%20permutations%2C%20and%20reconstruction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chierichetti%2C%20Flavio%20Kumar%2C%20Ravi%20Tomkins%2C%20Andrew%20Discrete%20choice%2C%20permutations%2C%20and%20reconstruction%202018"
        },
        {
            "id": "Fiori_et+al_2013_a",
            "entry": "Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, and Guillermo Sapiro. Robust multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiori%2C%20Marcelo%20Sprechmann%2C%20Pablo%20Vogelstein%2C%20Joshua%20Muse%2C%20Pablo%20Robust%20multimodal%20graph%20matching%3A%20Sparse%20coding%20meets%20graph%20matching%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fiori%2C%20Marcelo%20Sprechmann%2C%20Pablo%20Vogelstein%2C%20Joshua%20Muse%2C%20Pablo%20Robust%20multimodal%20graph%20matching%3A%20Sparse%20coding%20meets%20graph%20matching%202013"
        },
        {
            "id": "Fogel_et+al_2013_a",
            "entry": "Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexandre d\u2019Aspremont. Convex relaxations for permutation problems. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20d%E2%80%99Aspremont%2C%20Alexandre%20Convex%20relaxations%20for%20permutation%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20d%E2%80%99Aspremont%2C%20Alexandre%20Convex%20relaxations%20for%20permutation%20problems%202013"
        },
        {
            "id": "Fu_2006_a",
            "entry": "Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13:575\u2013616, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Michael%20C.%20Gradient%20estimation.%20Handbooks%20in%20operations%20research%20and%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Michael%20C.%20Gradient%20estimation.%20Handbooks%20in%20operations%20research%20and%202006"
        },
        {
            "id": "Gao_2017_a",
            "entry": "Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00805"
        },
        {
            "id": "Glasserman_2013_a",
            "entry": "Paul Glasserman. Monte Carlo methods in financial engineering, volume 53. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glasserman%2C%20Paul%20Monte%20Carlo%20methods%20in%20financial%20engineering%2C%20volume%2053%202013"
        },
        {
            "id": "Glynn_1990_a",
            "entry": "Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75\u201384, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glynn%2C%20Peter%20W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glynn%2C%20Peter%20W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990"
        },
        {
            "id": "Goyal_et+al_2018_a",
            "entry": "Kartik Goyal, Graham Neubig, Chris Dyer, and Taylor Berg-Kirkpatrick. A continuous relaxation of beam search for end-to-end training of neural sequence models. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Kartik%20Neubig%2C%20Graham%20Dyer%2C%20Chris%20Berg-Kirkpatrick%2C%20Taylor%20A%20continuous%20relaxation%20of%20beam%20search%20for%20end-to-end%20training%20of%20neural%20sequence%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Kartik%20Neubig%2C%20Graham%20Dyer%2C%20Chris%20Berg-Kirkpatrick%2C%20Taylor%20A%20continuous%20relaxation%20of%20beam%20search%20for%20end-to-end%20training%20of%20neural%20sequence%20models%202018"
        },
        {
            "id": "Jang_et+al_2017_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017"
        },
        {
            "id": "Kim_et+al_2016_a",
            "entry": "Carolyn Kim, Ashish Sabharwal, and Stefano Ermon. Exact sampling with integer linear programs and random perturbations. In AAAI Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Carolyn%20Sabharwal%2C%20Ashish%20Ermon%2C%20Stefano%20Exact%20sampling%20with%20integer%20linear%20programs%20and%20random%20perturbations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Carolyn%20Sabharwal%2C%20Ashish%20Ermon%2C%20Stefano%20Exact%20sampling%20with%20integer%20linear%20programs%20and%20random%20perturbations%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Lim_2014_a",
            "entry": "Cong Han Lim and Stephen J Wright. Sorting network relaxations for vector permutation problems. arXiv preprint arXiv:1407.6609, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.6609"
        },
        {
            "id": "Linderman_et+al_2018_a",
            "entry": "Scott W Linderman, Gonzalo E Mena, Hal Cooper, Liam Paninski, and John P Cunningham. Reparameterizing the birkhoff polytope for variational permutation inference. In International Conference on Artificial Intelligence and Statistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linderman%2C%20Scott%20W.%20Mena%2C%20Gonzalo%20E.%20Cooper%2C%20Hal%20Paninski%2C%20Liam%20Reparameterizing%20the%20birkhoff%20polytope%20for%20variational%20permutation%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linderman%2C%20Scott%20W.%20Mena%2C%20Gonzalo%20E.%20Cooper%2C%20Hal%20Paninski%2C%20Liam%20Reparameterizing%20the%20birkhoff%20polytope%20for%20variational%20permutation%20inference%202018"
        },
        {
            "id": "Liu_2009_a",
            "entry": "Tie-Yan Liu et al. Learning to rank for information retrieval. Foundations and Trends R in Information Retrieval, 3(3):225\u2013331, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Tie-Yan%20Learning%20to%20rank%20for%20information%20retrieval%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Tie-Yan%20Learning%20to%20rank%20for%20information%20retrieval%202009"
        }
    ]
}
