{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "STOCHASTIC GRADIENT/MIRROR DESCENT: MINIMAX OPTIMALITY AND IMPLICIT REGULARIZATION",
        "author": "Navid Azizan California Institute of Technology Pasadena, CA 91125 azizan@caltech.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJf9ZhC9FX"
        },
        "abstract": "Stochastic descent methods (of the gradient and mirror varieties) have become increasingly popular in optimization. In fact, it is now widely recognized that the success of deep learning is not only due to the special deep architecture of the models, but also due to the behavior of the stochastic descent methods used, which play a key role in reaching \u201cgood\u201d solutions that generalize well to unseen data. In an attempt to shed some light on why this is the case, we revisit some minimax properties of stochastic gradient descent (SGD) for the square loss of linear models\u2014originally developed in the 1990\u2019s\u2014and extend them to general stochastic mirror descent (SMD) algorithms for general loss functions and nonlinear models. In particular, we show that there is a fundamental identity which holds for SMD (and SGD) under very general conditions, and which implies the minimax optimality of SMD (and SGD) for sufficiently small step size, and for a general class of loss functions and general nonlinear models. We further show that this identity can be used to naturally establish other properties of SMD (and SGD), namely convergence and implicit regularization for over-parameterized linear models (in what is now being called the \u201cinterpolating regime\u201d), some of which have been shown in certain cases in prior literature. We also argue how this identity can be used in the so-called \u201chighly over-parameterized\u201d nonlinear setting (where the number of parameters far exceeds the number of data points) to provide insights into why SMD (and SGD) may have similar convergence and implicit regularization properties for deep learning."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "SMD": "Stochastic Mirror Descent"
    },
    "highlights": [
        "Deep learning has proven to be extremely successful in a wide variety of tasks (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">LeCun et al, 2015</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>)",
        "We present an alternative explanation of the behavior of stochastic gradient descent, and more generally, the stochastic mirror descent (SMD) family of algorithms, which includes stochastic gradient descent as a special case",
        "We show that for general nonlinear models and general loss functions, when the step size is sufficiently small, Stochastic Mirror Descent is the optimal solution of a certain minimax filtering problem",
        "We show that many properties recently proven in the learning/optimization literature, such as the implicit regularization of Stochastic Mirror Descent in the over-parameterized linear case\u2014when convergence happens\u2014(Gunasekar et al, 2018a), naturally follow from this theory",
        "We should remark that all the results stated throughout the paper extend to the case of time-varying step size \u03b7i, with minimal modification",
        "Our main result will be the same as in Theorem 6, with the only difference that the small-step-size condition in this case is the convexity of \u03c8(w) \u2212 \u03b7iLi(w) for all i, and the Stochastic Mirror Descent with time-varying step size will be the optimal solution to the following minimax problem min"
    ],
    "key_statements": [
        "Deep learning has proven to be extremely successful in a wide variety of tasks (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">LeCun et al, 2015</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>)",
        "We present an alternative explanation of the behavior of stochastic gradient descent, and more generally, the stochastic mirror descent (SMD) family of algorithms, which includes stochastic gradient descent as a special case",
        "We show that for general nonlinear models and general loss functions, when the step size is sufficiently small, Stochastic Mirror Descent is the optimal solution of a certain minimax filtering problem",
        "We show that many properties recently proven in the learning/optimization literature, such as the implicit regularization of Stochastic Mirror Descent in the over-parameterized linear case\u2014when convergence happens\u2014(Gunasekar et al, 2018a), naturally follow from this theory",
        "We should remark that all the results stated throughout the paper extend to the case of time-varying step size \u03b7i, with minimal modification",
        "Our main result will be the same as in Theorem 6, with the only difference that the small-step-size condition in this case is the convexity of \u03c8(w) \u2212 \u03b7iLi(w) for all i, and the Stochastic Mirror Descent with time-varying step size will be the optimal solution to the following minimax problem min",
        "The convergence and implicit regularization results can be proven under the same conditions (See Appendix D for more details on the time-varying case)"
    ],
    "summary": [
        "Deep learning has proven to be extremely successful in a wide variety of tasks (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\"><a class=\"ref-link\" id=\"cLecun_et+al_2015_a\" href=\"#rLecun_et+al_2015_a\">LeCun et al, 2015</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\"><a class=\"ref-link\" id=\"cWu_et+al_2016_a\" href=\"#rWu_et+al_2016_a\">Wu et al, 2016</a></a>).",
        "We show that for general nonlinear models and general loss functions, when the step size is sufficiently small, SMD is the optimal solution of a certain minimax filtering problem.",
        "We provide a characterization of the behavior of general SMD, on general loss functions and general nonlinear models, in terms of a fundamental identity and minimax optimality.",
        "In this setting, we can show that SMD is minimax optimal in a manner that generalizes Theorem 3 of Section 3, in the following 3 ways: 1) General potential \u03c8(\u00b7), 2) General model f (\u00b7, \u00b7), and 3) General loss function l(\u00b7).",
        "For sufficiently small step size, i.e., for any \u03b7 > 0 for which \u03c8(w) \u2212 \u03b7Li(w) is convex for all i, and for any number of steps T \u2265 1, the SMD iterates {wi} given by Eq (15), w.r.t. any strictly convex potential \u03c8(\u00b7), is the optimal solution to the following minimization problem min",
        "For the case of square loss and a linear model, the result reduces to the following form.",
        "Any initialization w0, any sufficiently small step \u2265 1, the SMD iterates {wi} given by Eq (15), size, w.r.t. any \u03b1-strongly convex potential \u03c8(\u00b7), is the optimal solution to min max",
        "2. our result generalizes the result of (<a class=\"ref-link\" id=\"cKivinen_et+al_2006_a\" href=\"#rKivinen_et+al_2006_a\">Kivinen et al, 2006</a>), which is the special case for the p-norm algorithms, again, with square loss and a linear model.",
        "For any differentiable loss l(\u00b7), any initialization w0, and any step size \u03b7, consider the SMD iterates given in Eq (15) with respect to any strictly convex potential \u03c8(\u00b7).",
        "Our proofs of convergence and implicit regularization for SGD and SMD in the linear case relied on two facts: (i) DLi (w, wi\u22121) was non-negative, and DLi (w, wi\u22121) was independent of w.",
        "It is suggestive of the fact that SGD and SMD, when performed on highly-overparameterized nonlinear models, as occurs in deep learning, may exhibit implicit regularization.",
        "Where Ei = D\u03c8 \u2212 \u03b7iDLi + \u03b7iLi. As a consequence, our main result will be the same as in Theorem 6, with the only difference that the small-step-size condition in this case is the convexity of \u03c8(w) \u2212 \u03b7iLi(w) for all i, and the SMD with time-varying step size will be the optimal solution to the following minimax problem min"
    ],
    "headline": "We show that there is a fundamental identity which holds for Stochastic Mirror Descent under very general conditions, and which implies the minimax optimality of Stochastic Mirror Descent for sufficiently small step size, and for a general class of loss functions and general nonlinear models",
    "reference_links": [
        {
            "id": "Achille_2017_a",
            "entry": "Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01350"
        },
        {
            "id": "Azizan_2019_a",
            "entry": "Navid Azizan and Babak Hassibi. A characterization of stochastic mirror descent algorithms and their convergence properties. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizan%2C%20Navid%20Hassibi%2C%20Babak%20A%20characterization%20of%20stochastic%20mirror%20descent%20algorithms%20and%20their%20convergence%20properties%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azizan%2C%20Navid%20Hassibi%2C%20Babak%20A%20characterization%20of%20stochastic%20mirror%20descent%20algorithms%20and%20their%20convergence%20properties%202019"
        },
        {
            "id": "Basar_2008_a",
            "entry": "Tamer Basar and Pierre Bernhard. H-infinity optimal control and related minimax design problems: a dynamic game approach. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Basar%2C%20Tamer%20Bernhard%2C%20Pierre%20H-infinity%20optimal%20control%20and%20related%20minimax%20design%20problems%3A%20a%20dynamic%20game%20approach%202008"
        },
        {
            "id": "Beck_2003_a",
            "entry": "Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003"
        },
        {
            "id": "Cesa-Bianchi_et+al_2012_a",
            "entry": "Nicolo Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and feels no regret). In Advances in Neural Information Processing Systems, pp. 980\u2013988, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolo%20CesaBianchi%20Pierre%20Gaillard%20Gabor%20Lugosi%20and%20Gilles%20Stoltz%20Mirror%20descent%20meets%20fixed%20share%20and%20feels%20no%20regret%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%20980988%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolo%20CesaBianchi%20Pierre%20Gaillard%20Gabor%20Lugosi%20and%20Gilles%20Stoltz%20Mirror%20descent%20meets%20fixed%20share%20and%20feels%20no%20regret%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%20980988%202012"
        },
        {
            "id": "Chaudhari_2018_a",
            "entry": "Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhari%2C%20Pratik%20Soatto%2C%20Stefano%20Stochastic%20gradient%20descent%20performs%20variational%20inference%2C%20converges%20to%20limit%20cycles%20for%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhari%2C%20Pratik%20Soatto%2C%20Stefano%20Stochastic%20gradient%20descent%20performs%20variational%20inference%2C%20converges%20to%20limit%20cycles%20for%20deep%20networks%202018"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Francis_1987_a",
            "entry": "Bruce A Francis. A course in H-infinity control theory. Berlin; New York: Springer-Verlag, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Francis%2C%20Bruce%20A.%20A%20course%20in%20H-infinity%20control%20theory%201987"
        },
        {
            "id": "Gentile_2003_a",
            "entry": "Claudio Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265\u2013299, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gentile%2C%20Claudio%20The%20robustness%20of%20the%20p-norm%20algorithms%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gentile%2C%20Claudio%20The%20robustness%20of%20the%20p-norm%20algorithms%202003"
        },
        {
            "id": "Grove_et+al_2001_a",
            "entry": "Adam J Grove, Nick Littlestone, and Dale Schuurmans. General convergence results for linear discriminant updates. Machine Learning, 43(3):173\u2013210, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grove%2C%20Adam%20J.%20Littlestone%2C%20Nick%20Schuurmans%2C%20Dale%20General%20convergence%20results%20for%20linear%20discriminant%20updates%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grove%2C%20Adam%20J.%20Littlestone%2C%20Nick%20Schuurmans%2C%20Dale%20General%20convergence%20results%20for%20linear%20discriminant%20updates%202001"
        },
        {
            "id": "Gunasekar_et+al_2017_a",
            "entry": "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6152\u20136160, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "Gunasekar_et+al_0000_a",
            "entry": "Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08246"
        },
        {
            "id": "Gunasekar_et+al_0000_b",
            "entry": "Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00468"
        },
        {
            "id": "Hassibi_1995_a",
            "entry": "Babak Hassibi and Thomas Kailath. Hoo optimal training algorithms and their relation to backpropagation. In Advances in Neural Information Processing Systems 7, pp. 191\u2013198. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Kailath%2C%20Thomas%20Hoo%20optimal%20training%20algorithms%20and%20their%20relation%20to%20backpropagation%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Kailath%2C%20Thomas%20Hoo%20optimal%20training%20algorithms%20and%20their%20relation%20to%20backpropagation%201995"
        },
        {
            "id": "Hassibi_et+al_1994_a",
            "entry": "Babak Hassibi, Ali H. Sayed, and Thomas Kailath. Hoo optimality criteria for LMS and backpropagation. In Advances in Neural Information Processing Systems 6, pp. 351\u2013358. 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Hoo%20optimality%20criteria%20for%20LMS%20and%20backpropagation%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Hoo%20optimality%20criteria%20for%20LMS%20and%20backpropagation%201994"
        },
        {
            "id": "Hassibi_et+al_1996_a",
            "entry": "Babak Hassibi, Ali H Sayed, and Thomas Kailath. Hoo optimality of the LMS algorithm. IEEE Transactions on Signal Processing, 44(2):267\u2013280, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Hoo%20optimality%20of%20the%20LMS%20algorithm%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Hoo%20optimality%20of%20the%20LMS%20algorithm%201996"
        },
        {
            "id": "Hassibi_et+al_1999_a",
            "entry": "Babak Hassibi, Ali H Sayed, and Thomas Kailath. Indefinite-Quadratic Estimation and Control: A Unified Approach to H2 and H-infinity Theories, volume 16. SIAM, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Indefinite-Quadratic%20Estimation%20and%20Control%3A%20A%20Unified%20Approach%20to%20H2%20and%20H-infinity%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Sayed%2C%20Ali%20H.%20Kailath%2C%20Thomas%20Indefinite-Quadratic%20Estimation%20and%20Control%3A%20A%20Unified%20Approach%20to%20H2%20and%20H-infinity%201999"
        },
        {
            "id": "Hazan_2016_a",
            "entry": "Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157\u2013325, 2016. ISSN 2167-3888.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kivinen_et+al_2006_a",
            "entry": "Jyrki Kivinen, Manfred K Warmuth, and Babak Hassibi. The p-norm generalization of the LMS algorithm for adaptive filtering. IEEE Transactions on Signal Processing, 54(5):1782\u20131793, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kivinen%2C%20Jyrki%20Warmuth%2C%20Manfred%20K.%20Hassibi%2C%20Babak%20The%20p-norm%20generalization%20of%20the%20LMS%20algorithm%20for%20adaptive%20filtering%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kivinen%2C%20Jyrki%20Warmuth%2C%20Manfred%20K.%20Hassibi%2C%20Babak%20The%20p-norm%20generalization%20of%20the%20LMS%20algorithm%20for%20adaptive%20filtering%202006"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pp. 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "Ma_et+al_2017_a",
            "entry": "Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10467"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 3325\u20133334. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Siyuan%20Bassily%2C%20Raef%20Belkin%2C%20Mikhail%20The%20power%20of%20interpolation%3A%20Understanding%20the%20effectiveness%20of%20SGD%20in%20modern%20over-parametrized%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Siyuan%20Bassily%2C%20Raef%20Belkin%2C%20Mikhail%20The%20power%20of%20interpolation%3A%20Understanding%20the%20effectiveness%20of%20SGD%20in%20modern%20over-parametrized%20learning%202018"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Nemirovskii_et+al_1983_a",
            "entry": "Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovskii%2C%20Arkadii%20Yudin%2C%20David%20Borisovich%20Dawson%2C%20Edgar%20Ronald%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03071"
        },
        {
            "id": "Shalev-Shwartz_2012_a",
            "entry": "Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2012. ISSN 1935-8237.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Online%20learning%20and%20online%20convex%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Online%20learning%20and%20online%20convex%20optimization%202012"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Simon_2006_a",
            "entry": "Dan Simon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley & Sons, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%2C%20Dan%20Optimal%20state%20estimation%3A%20Kalman%2C%20H%20infinity%2C%20and%20nonlinear%20approaches%202006"
        },
        {
            "id": "Soltanolkotabi_et+al_2017_a",
            "entry": "Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04926"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u2013 31, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Wilson_et+al_2017_a",
            "entry": "Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151\u20134161, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2041514161%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2041514161%202017"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. In Advances in Neural Information Processing Systems, pp. 7043\u20137052, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Zhengyuan%20Mertikopoulos%2C%20Panayotis%20Bambos%2C%20Nicholas%20Boyd%2C%20Stephen%20Stochastic%20mirror%20descent%20in%20variationally%20coherent%20optimization%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Zhengyuan%20Mertikopoulos%2C%20Panayotis%20Bambos%2C%20Nicholas%20Boyd%2C%20Stephen%20Stochastic%20mirror%20descent%20in%20variationally%20coherent%20optimization%20problems%202017"
        }
    ]
}
