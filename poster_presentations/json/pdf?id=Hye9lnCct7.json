{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING ACTIONABLE REPRESENTATIONS WITH GOAL-CONDITIONED POLICIES",
        "author": "Dibya Ghosh, Abhishek Gupta, & Sergey Levine \u2217 Department of Electrical Engineering and Computer Science University of California, Berkeley Berkeley, CA 94703, USA",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hye9lnCct7"
        },
        "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all the underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making \u2013 that are \u201cactionable.\u201d These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, eliminating the need for explicit reconstruction. We show how these learned representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "center of mass",
            "url": "https://en.wikipedia.org/wiki/center_of_mass"
        },
        {
            "term": "decision making",
            "url": "https://en.wikipedia.org/wiki/decision_making"
        },
        {
            "term": "multi-dimensional scaling",
            "url": "https://en.wikipedia.org/wiki/multi-dimensional_scaling"
        },
        {
            "term": "representation learning",
            "url": "https://en.wikipedia.org/wiki/representation_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "ARC": "actionable representations for control",
        "CoM": "center of mass",
        "MDS": "multi-dimensional scaling",
        "GCP": "goal-conditioned policy"
    },
    "highlights": [
        "Representation learning refers to a transformation of an observation, such as a camera image or state observation, into a form that is easier to manipulate to deduce a desired output or perform a downstream task, such as prediction or control",
        "How can we learn a representation that is aware of the dynamical structure of the environment? We propose that a basic understanding of the world can be obtained from a goal-conditioned policy, a policy that can knows how to reach arbitrary goal states from a given state",
        "We compare actionable representations for control to other representation learning methods used in previous works for control: variational autoencoders (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a>) (VAE), variational autoencoders trained for feature slowness (<a class=\"ref-link\" id=\"cJonschkowski_2015_a\" href=\"#rJonschkowski_2015_a\">Jonschkowski & Brock, 2015</a>), features extracted from a predictive model (<a class=\"ref-link\" id=\"cOh_et+al_2015_a\" href=\"#rOh_et+al_2015_a\">Oh et al, 2015</a>), features extracted from inverse models (<a class=\"ref-link\" id=\"cAgrawal_et+al_2016_a\" href=\"#rAgrawal_et+al_2016_a\">Agrawal et al, 2016</a>; Burda et al, 2018a), and a na\u0131ve baseline that uses the full state space as the representation",
        "We introduce actionable representations for control (ARC), which capture representations of state important for decision making",
        "The learned state representations are implicitly aware of the dynamics, and capture meaningful distances in representation space",
        "actionable representations for control are useful for tasks such as learning policies, HRL and exploration"
    ],
    "key_statements": [
        "Representation learning refers to a transformation of an observation, such as a camera image or state observation, into a form that is easier to manipulate to deduce a desired output or perform a downstream task, such as prediction or control",
        "Much of the prior work on representation learning in reinforcement learning has focused on generative approaches",
        "We instead aim to learn functionally salient representations: representations that are not necessarily complete in capturing all factors of variation in the observation space, but rather aim to capture factors of variation that are relevant for decision making \u2013 that are actionable",
        "How can we learn a representation that is aware of the dynamical structure of the environment? We propose that a basic understanding of the world can be obtained from a goal-conditioned policy, a policy that can knows how to reach arbitrary goal states from a given state",
        "We propose actionable representations for control (ARC), representations in which Euclidean distances between states correspond to expected differences between actions taken to reach them",
        "We observe that actionable representations for control exhibits better generalization, and can provide effective reward shaping for goals that are very difficult to reach with the goal-conditioned policy.\n4.3",
        "We compare actionable representations for control to other representation learning methods used in previous works for control: variational autoencoders (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a>) (VAE), variational autoencoders trained for feature slowness (<a class=\"ref-link\" id=\"cJonschkowski_2015_a\" href=\"#rJonschkowski_2015_a\">Jonschkowski & Brock, 2015</a>), features extracted from a predictive model (<a class=\"ref-link\" id=\"cOh_et+al_2015_a\" href=\"#rOh_et+al_2015_a\">Oh et al, 2015</a>), features extracted from inverse models (<a class=\"ref-link\" id=\"cAgrawal_et+al_2016_a\" href=\"#rAgrawal_et+al_2016_a\">Agrawal et al, 2016</a>; Burda et al, 2018a), and a na\u0131ve baseline that uses the full state space as the representation",
        "We evaluate the two schemes for hierarchical reasoning with actionable representations for control detailed tasks in Section 4.3: commanding directly in representation space or through a k-means clustering of the representation space",
        "actionable representations for control likely works better than commanding goals in spaces learned by other representation learning algorithms, because the learned actionable representations for control space is more structured for high-level control, which makes search and clustering simpler",
        "We introduce actionable representations for control (ARC), which capture representations of state important for decision making",
        "The learned state representations are implicitly aware of the dynamics, and capture meaningful distances in representation space",
        "actionable representations for control are useful for tasks such as learning policies, HRL and exploration",
        "While actionable representations for control are learned by first training a goal-conditioned policy, learning this policy using offpolicy data is a promising direction for future work"
    ],
    "summary": [
        "Representation learning refers to a transformation of an observation, such as a camera image or state observation, into a form that is easier to manipulate to deduce a desired output or perform a downstream task, such as prediction or control.",
        "As we demonstrate in our experiments, representations extracted from goal-conditioned policies can be used to better learn more challenging tasks than simple goal reaching, which cannot be contextualized by goal states.",
        "We observe that ARC exhibits better generalization, and can provide effective reward shaping for goals that are very difficult to reach with the goal-conditioned policy.",
        "One approach to solving such tasks learns a high-level controller \u03c0meta(g|s) via RL that produces desired goal states for a goal-conditioned policy to reach sequentially (<a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\"><a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a></a>).",
        "We compare ARC to other representation learning methods used in previous works for control: variational autoencoders (<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma & Welling, 2013</a>) (VAE), variational autoencoders trained for feature slowness (<a class=\"ref-link\" id=\"cJonschkowski_2015_a\" href=\"#rJonschkowski_2015_a\">Jonschkowski & Brock, 2015</a>), features extracted from a predictive model (<a class=\"ref-link\" id=\"cOh_et+al_2015_a\" href=\"#rOh_et+al_2015_a\">Oh et al, 2015</a>), features extracted from inverse models (<a class=\"ref-link\" id=\"cAgrawal_et+al_2016_a\" href=\"#rAgrawal_et+al_2016_a\">Agrawal et al, 2016</a>; Burda et al, 2018a), and a na\u0131ve baseline that uses the full state space as the representation.",
        "As desribed in Section 4.2, distances in ARC space can be used for reward shaping to solve tasks that present a large exploration challenge with sparse reward functions.",
        "The task is to reach arbitrary goals in S , but with only a sparse goal completion reward, so exploration is challenging.We shape the reward with a term corresponding to distance between the representation of the current and desired state: r(s, g) = rsparse \u2212 \u03c6(s) \u2212 \u03c6(g) 2.",
        "As shown in Fig 7, ARC demonstrates faster learning speed and better asymptotic performance over all compared methods, when all are initialized from the goal conditioned policy trained on the small region.",
        "We consider using the ARC representation as a feature space for learning policies for tasks that cannot be expressed with a goal-reaching objective.",
        "We train a high-level controller \u03c0h with TRPO which outputs as actions either a direct point in the latent space zh or a cluster index ch, from which a goal gh is decoded and passed to the goal-conditioned policy to follow for 50 timesteps.",
        "ARC likely works better than commanding goals in spaces learned by other representation learning algorithms, because the learned ARC space is more structured for high-level control, which makes search and clustering simpler.",
        "As compared to learning from scratch via TRPO and standard HRL methods such as option critic (<a class=\"ref-link\" id=\"cKlissarov_et+al_2017_a\" href=\"#rKlissarov_et+al_2017_a\">Klissarov et al, 2017</a>) and an on-policy adaptation of HIRO (<a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\"><a class=\"ref-link\" id=\"cNachum_et+al_2018_a\" href=\"#rNachum_et+al_2018_a\">Nachum et al, 2018</a></a>), commanding in representation space enables more effective search and high-level control."
    ],
    "headline": "We show how these learned representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks",
    "reference_links": [
        {
            "id": "Agrawal_et+al_2016_a",
            "entry": "Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. CoRR, abs/1606.07419, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07419"
        },
        {
            "id": "Assael_et+al_2015_a",
            "entry": "John-Alexander M. Assael, Niklas Wahlstrom, Thomas B. Schon, and Marc Peter Deisenroth. Dataefficient learning of feedback policies from image pixels using deep dynamical models. CoRR, abs/1510.02173, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.02173"
        },
        {
            "id": "Barreto_et+al_2016_a",
            "entry": "Andre Barreto, Remi Munos, Tom Schaul, and David Silver. Successor features for transfer in reinforcement learning. CoRR, abs/1606.05312, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05312"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R. Devon Hjelm, and Aaron C. Courville. MINE: mutual information neural estimation. CoRR, abs/1801.04062, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04062"
        },
        {
            "id": "Bengio_et+al_2017_a",
            "entry": "Emmanuel Bengio, Valentin Thomas, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features. CoRR, abs/1703.07718, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07718"
        },
        {
            "id": "Borg_2005_a",
            "entry": "I. Borg and P.J.F. Groenen. Modern Multidimensional Scaling: Theory and Applications. Springer, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borg%2C%20I.%20Groenen%2C%20P.J.F.%20Modern%20Multidimensional%20Scaling%3A%20Theory%20and%20Applications%202005"
        },
        {
            "id": "Burda_et+al_0000_a",
            "entry": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. Large-scale study of curiosity-driven learning. In arXiv:1808.04355, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04355"
        },
        {
            "id": "Burda_et+al_0000_b",
            "entry": "Yuri Burda, Harrison Edwards, Deepak Pathak, Amos J. Storkey, Trevor Darrell, and Alexei A. Efros. Large-scale study of curiosity-driven learning. CoRR, abs/1808.04355, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04355"
        },
        {
            "id": "Chopra_et+al_2005_a",
            "entry": "Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), 20-26 June 2005, San Diego, CA, USA, pp. 539\u2013546, 2005. doi: 10.1109/CVPR.2005.202.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2005.202",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2005.202"
        },
        {
            "id": "Curran_et+al_2015_a",
            "entry": "William Curran, Tim Brys, Matthew E. Taylor, and William D. Smart. Using PCA to efficiently represent state spaces. CoRR, abs/1505.00322, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00322"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Mart\u0131n Arjovsky, Olivier Mastropietro, and Aaron C. Courville. Adversarially learned inference. CoRR, abs/1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "Finn_et+al_2015_a",
            "entry": "Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Learning visual feature spaces for robotic manipulation with deep spatial autoencoders. CoRR, abs/1509.06113, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.06113"
        },
        {
            "id": "Ghadirzadeh_et+al_2017_a",
            "entry": "Ali Ghadirzadeh, Atsuto Maki, Danica Kragic, and Marten Bjorkman. Deep predictive policy training using reinforcement learning. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017, pp. 2351\u2013 2358, 2017. doi: 10.1109/IROS.2017.8206046.",
            "crossref": "https://dx.doi.org/10.1109/IROS.2017.8206046",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IROS.2017.8206046"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%2C%20volume%201%202016"
        },
        {
            "id": "Goroshin_et+al_2015_a",
            "entry": "Ross Goroshin, Michael Mathieu, and Yann LeCun. Learning to linearize under uncertainty. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1234\u20131242, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goroshin%2C%20Ross%20Mathieu%2C%20Michael%20LeCun%2C%20Yann%20Learning%20to%20linearize%20under%20uncertainty%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goroshin%2C%20Ross%20Mathieu%2C%20Michael%20LeCun%2C%20Yann%20Learning%20to%20linearize%20under%20uncertainty%202015-12-07"
        },
        {
            "id": "Haarnoja_et+al_2017_a",
            "entry": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. CoRR, abs/1702.08165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08165"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Arka Pal, Andrei A. Rusu, Lo\u0131c Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: improving zero-shot transfer in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1480\u20131490, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Pal%2C%20Arka%20Rusu%2C%20Andrei%20A.%20Matthey%2C%20Lo%C4%B1c%20DARLA%3A%20improving%20zero-shot%20transfer%20in%20reinforcement%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Pal%2C%20Arka%20Rusu%2C%20Andrei%20A.%20Matthey%2C%20Lo%C4%B1c%20DARLA%3A%20improving%20zero-shot%20transfer%20in%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Houthooft_et+al_2016_a",
            "entry": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. CoRR, abs/1605.09674, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09674"
        },
        {
            "id": "Jonschkowski_2015_a",
            "entry": "Rico Jonschkowski and Oliver Brock. Learning state representations with robotic priors. Auton. Robots, 39(3):407\u2013428, 2015. doi: 10.1007/s10514-015-9459-7.",
            "crossref": "https://dx.doi.org/10.1007/s10514-015-9459-7",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10514-015-9459-7"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Klissarov_et+al_2017_a",
            "entry": "Martin Klissarov, Pierre-Luc Bacon, Jean Harb, and Doina Precup. Learnings options end-to-end for continuous action tasks. CoRR, abs/1712.00004, 2017. URL http://arxiv.org/abs/1712.00004.",
            "url": "http://arxiv.org/abs/1712.00004",
            "arxiv_url": "https://arxiv.org/pdf/1712.00004"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pp. 1106\u20131114, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012-12-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012-12-03"
        },
        {
            "id": "Kurutach_et+al_2018_a",
            "entry": "Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J. Russell, and Pieter Abbeel. Learning plannable representations with causal infogan. CoRR, abs/1807.09341, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.09341"
        },
        {
            "id": "Laversanne-Finot_et+al_2018_a",
            "entry": "Adrien Laversanne-Finot, Alexandre Pere, and Pierre-Yves Oudeyer. Curiosity driven exploration of learned disentangled goal spaces. In 2nd Annual Conference on Robot Learning, CoRL 2018, Zurich, Switzerland, 29-31 October 2018, Proceedings, pp. 487\u2013504, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laversanne-Finot%2C%20Adrien%20Pere%2C%20Alexandre%20Oudeyer%2C%20Pierre-Yves%20Curiosity%20driven%20exploration%20of%20learned%20disentangled%20goal%20spaces%202018-10-29",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laversanne-Finot%2C%20Adrien%20Pere%2C%20Alexandre%20Oudeyer%2C%20Pierre-Yves%20Curiosity%20driven%20exploration%20of%20learned%20disentangled%20goal%20spaces%202018-10-29"
        },
        {
            "id": "Lesort_et+al_2018_a",
            "entry": "Timothee Lesort, Natalia D\u0131az Rodr\u0131guez, Jean-Francois Goudou, and David Filliat. State representation learning for control: An overview. CoRR, abs/1802.04181, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04181"
        },
        {
            "id": "Levine_et+al_2015_a",
            "entry": "Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00702"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. CoRR, abs/1805.08296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Nagabandi_et+al_2017_a",
            "entry": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. CoRR, abs/1708.02596, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02596"
        },
        {
            "id": "Nair_et+al_2018_a",
            "entry": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. CoRR, abs/1807.04742, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04742"
        },
        {
            "id": "Oh_et+al_2015_a",
            "entry": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh. Actionconditional video prediction using deep networks in atari games. CoRR, abs/1507.08750, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.08750"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops, Honolulu, HI, USA, July 21-26, 2017, pp. 488\u2013489, 2017. doi: 10.1109/CVPRW.2017.70.",
            "crossref": "https://dx.doi.org/10.1109/CVPRW.2017.70",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPRW.2017.70"
        },
        {
            "id": "Rasmus_et+al_2015_a",
            "entry": "Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semisupervised learning with ladder network. CoRR, abs/1507.02672, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.02672"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20I.%20Trust%20region%20policy%20optimization%202015-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20I.%20Trust%20region%20policy%20optimization%202015-07-06"
        },
        {
            "id": "Sermanet_et+al_2018_a",
            "entry": "Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp. 1134\u20131141, 2018. doi: 10.1109/ICRA.2018.8462891.",
            "crossref": "https://dx.doi.org/10.1109/ICRA.2018.8462891",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICRA.2018.8462891"
        },
        {
            "id": "Srinivas_et+al_2018_a",
            "entry": "Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00645"
        },
        {
            "id": "Todorov_2006_a",
            "entry": "Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pp. 1369\u20131376, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Linearly-solvable%20markov%20decision%20problems%202006-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Linearly-solvable%20markov%20decision%20problems%202006-12"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Watter_et+al_2015_a",
            "entry": "Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2746\u20132754, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Tobias%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20A.%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Tobias%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20A.%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015-12-07"
        },
        {
            "id": "Weinberger_2009_a",
            "entry": "Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207\u2013244, 2009. doi: 10. 1145/1577069.1577078.",
            "crossref": "https://dx.doi.org/10.1145/1577069.1577078",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1577069.1577078"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning. CoRR, abs/1804.10689, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1804.10689"
        },
        {
            "id": "Zhang_et+al_0000_b",
            "entry": "Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey Levine. SOLAR: deep structured latent representations for model-based reinforcement learning. CoRR, abs/1808.09105, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1808.09105"
        },
        {
            "id": "250",
            "entry": "250 200 2000 0.1.02",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=200%202000%200102",
            "oa_query": "https://api.scholarcy.com/oa_version?query=200%202000%200102"
        }
    ]
}
