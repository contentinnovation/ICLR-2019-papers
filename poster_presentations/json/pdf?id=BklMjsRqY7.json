{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "ACCUMULATION BIT-WIDTH SCALING FOR ULTRALOW PRECISION TRAINING OF DEEP NETWORKS",
        "author": "Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag, Kailash Gopalakrishnan, \u2020 Dept. of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign {sakr,shanbhag}@illinois.edu \u2021 IBM T.J. Watson Research Center {nwang,cchen,choij,ankuragr,kailash}@us.ibm.com",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BklMjsRqY7"
        },
        "abstract": "Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems."
    },
    "keywords": [
        {
            "term": "partial sum",
            "url": "https://en.wikipedia.org/wiki/partial_sum"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "numerical precision",
            "url": "https://en.wikipedia.org/wiki/numerical_precision"
        },
        {
            "term": "floating-point units",
            "url": "https://en.wikipedia.org/wiki/Floating-Point_Unit"
        }
    ],
    "abbreviations": {
        "FPU": "floating-point units",
        "VRR": "variance retention ratio",
        "GEMM": "generalized matrix multiplication",
        "IH": "IFM Height",
        "KH": "Kernel Height",
        "OH": "OFM Height",
        "PH": "Product Height",
        "IFM": "Input Feature Maps",
        "OFM": "Output Feature Maps",
        "KW": "Kernel Width",
        "IW": "IFM Width",
        "OW": "OFM Width",
        "PW": "Product Width",
        "FWD": "forward propagation",
        "BWD": "backward propagation",
        "GRAD": "gradient computation",
        "NZR": "non-zero ratio",
        "PP": "precision perturbation",
        "CCC": "Cognitive Computing Cluster"
    },
    "highlights": [
        "Deep learning techniques have been remarkably successful in a wide spectrum of applications through the use of very large and deep models trained using massive datasets",
        "The reason being reduced precision accumulation can result in severe training instability and accuracy degradation, as illustrated in Figure 1 (a) for ResNet 18 (ImageNet) model training. This is especially unfortunate, since the hardware complexity in reduced precision floating-point numbers (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>; <a class=\"ref-link\" id=\"cMicikevicius_et+al_2017_a\" href=\"#rMicikevicius_et+al_2017_a\">Micikevicius et al, 2017</a>) is dominated by the accumulator bit-width. To illustrate this dominance we developed a model underpinned by the hardware synthesis of low-precision floating-point units (FPU), that translates precision into area complexity of the floating-point units",
        "We predict the mantissa precisions required by the three generalized matrix multiplication functions for training the following networks: ResNet 32 on the CIFAR-10 dataset, ResNet 18 and AlexNet on the ImageNet dataset",
        "We have presented an analytical method to predict the precision required for partial sum accumulation in the three generalized matrix multiplication functions in deep learning training",
        "Training via backpropagation in time could make the gradient computation accumulation very large depending on the number of past time-steps used",
        "Our analysis is of great relevance to training precision optimization. This analysis is a useful tool for hardware designers implementing reduced-precision floating-point units, who in the past have resorted to computationally prohibitive brute-force emulations"
    ],
    "key_statements": [
        "Deep learning techniques have been remarkably successful in a wide spectrum of applications through the use of very large and deep models trained using massive datasets",
        "The reason being reduced precision accumulation can result in severe training instability and accuracy degradation, as illustrated in Figure 1 (a) for ResNet 18 (ImageNet) model training. This is especially unfortunate, since the hardware complexity in reduced precision floating-point numbers (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>; <a class=\"ref-link\" id=\"cMicikevicius_et+al_2017_a\" href=\"#rMicikevicius_et+al_2017_a\">Micikevicius et al, 2017</a>) is dominated by the accumulator bit-width. To illustrate this dominance we developed a model underpinned by the hardware synthesis of low-precision floating-point units (FPU), that translates precision into area complexity of the floating-point units",
        "We introduce the variance retention ratio (VRR) of a reduced precision accumulation in the context of the three deep learning dot products",
        "Floating-point operations: One of the most pervasive arithmetic functions used in deep learning is the dot product between two vectors which is the building block of the generalized matrix multiplication (GEMM)",
        "The variance retention ratio of an length n = n1 \u00d7 n2 accumulation with chunking, where n1 is the chunk size and n2 is the number of chunks, using mp and macc mantissa bits for the input products and partial sum terms, respectively, is given by: V RRchunking = V RR \u00d7 V RR) , n2) (3)\n4.3",
        "For a given accumulation setup, one may compute the variance retention ratio and observe how close it is from the ideal value of 1 in order to judge the suitability of the mantissa precision assignment",
        "We predict the mantissa precisions required by the three generalized matrix multiplication functions for training the following networks: ResNet 32 on the CIFAR-10 dataset, ResNet 18 and AlexNet on the ImageNet dataset",
        "Since our work focuses on accumulation precision, in our experiments, the baseline denotes accumulation in full precision",
        "We have presented an analytical method to predict the precision required for partial sum accumulation in the three generalized matrix multiplication functions in deep learning training",
        "Training via backpropagation in time could make the gradient computation accumulation very large depending on the number of past time-steps used",
        "Our analysis is of great relevance to training precision optimization. This analysis is a useful tool for hardware designers implementing reduced-precision floating-point units, who in the past have resorted to computationally prohibitive brute-force emulations"
    ],
    "summary": [
        "Deep learning techniques have been remarkably successful in a wide spectrum of applications through the use of very large and deep models trained using massive datasets.",
        "This is especially unfortunate, since the hardware complexity in reduced precision floating-point numbers (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>; <a class=\"ref-link\" id=\"cMicikevicius_et+al_2017_a\" href=\"#rMicikevicius_et+al_2017_a\">Micikevicius et al, 2017</a>) is dominated by the accumulator bit-width.",
        "We introduce the variance retention ratio (VRR) of a reduced precision accumulation in the context of the three deep learning dot products.",
        "One intuition concerning accumulation with reduced precision is that, due to swamping, some product terms may vanish from the summation, resulting in a lower variance than expected: V ar(s) = nV ar(p), where n < n.",
        "It is important to note that our upcoming analysis differs, in style, from many works on reduced precision deep learning where it is common to model quantization effects as additive noise causing increased variance (<a class=\"ref-link\" id=\"cMckinstry_et+al_2018_a\" href=\"#rMckinstry_et+al_2018_a\">McKinstry et al, 2018</a>).",
        "The variance retention ratio of a length n accumulation using macc mantissa bits, and when only considering full swamping, is given by: V RRfull swamping =",
        "The variance retention ratio of a length n accumulation using mp and macc mantissa bits for the input products and partial sum terms, respectively, is given by: V RR =",
        "The variance retention ratio of an length n = n1 \u00d7 n2 accumulation with chunking, where n1 is the chunk size and n2 is the number of chunks, using mp and macc mantissa bits for the input products and partial sum terms, respectively, is given by: V RRchunking = V RR \u00d7 V RR) , n2) (3)",
        "When considering the VRR with chunking, we may use knowledge of sparsity to obtain the effective intra-accumulation length as N ZR \u00d7 n1.",
        "For a given accumulation setup, one may compute the VRR and observe how close it is from the ideal value of 1 in order to judge the suitability of the mantissa precision assignment.",
        "We predict the mantissa precisions required by the three GEMM functions for training the following networks: ResNet 32 on the CIFAR-10 dataset, ResNet 18 and AlexNet on the ImageNet dataset.",
        "In order to better visualize how the accumulation precision affect convergence, we plot in Figure 6 (d) the accuracy degradation as a function of precision perturbation for each of our three networks with both normal and chunk-based accumulations.",
        "We have presented an analytical method to predict the precision required for partial sum accumulation in the three GEMM functions in deep learning training.",
        "We believe this work addresses a critical missing link on the path to truly low-precision floating-point hardware for DNN training"
    ],
    "headline": "We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training",
    "reference_links": [
        {
            "id": "Castaldo_et+al_2008_a",
            "entry": "Castaldo, A. M., Whaley, R. C., and Chronopoulos, A. T. (2008). Reducing floating point error in dot product using the superblock family of algorithms. SIAM journal on scientific computing, 31(2):1156\u20131174.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castaldo%2C%20A.M.%20Whaley%2C%20R.C.%20Chronopoulos%2C%20A.T.%20Reducing%20floating%20point%20error%20in%20dot%20product%20using%20the%20superblock%20family%20of%20algorithms%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castaldo%2C%20A.M.%20Whaley%2C%20R.C.%20Chronopoulos%2C%20A.T.%20Reducing%20floating%20point%20error%20in%20dot%20product%20using%20the%20superblock%20family%20of%20algorithms%202008"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Chen, C.-Y., Choi, J., Gopalakrishnan, K., Srinivasan, V., and Venkataramani, S. (2018). Exploiting approximate computing for deep learning acceleration. In Design, Automation, Test in Europe Conference Exhibition (DATE).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20C.-Y.%20Choi%2C%20J.%20Gopalakrishnan%2C%20K.%20Srinivasan%2C%20V.%20Exploiting%20approximate%20computing%20for%20deep%20learning%20acceleration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20C.-Y.%20Choi%2C%20J.%20Gopalakrishnan%2C%20K.%20Srinivasan%2C%20V.%20Exploiting%20approximate%20computing%20for%20deep%20learning%20acceleration%202018"
        },
        {
            "id": "Gastaldi_2017_a",
            "entry": "Gastaldi, X. (2017). Shake-shake regularization. arXiv preprint arXiv:1705.07485.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07485"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. (2017). Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "Gupta_et+al_2015_a",
            "entry": "Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning with limited numerical precision. In International Conference on Machine Learning, pages 1737\u20131746.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20S.%20Agrawal%2C%20A.%20Gopalakrishnan%2C%20K.%20Narayanan%2C%20P.%20Deep%20learning%20with%20limited%20numerical%20precision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20S.%20Agrawal%2C%20A.%20Gopalakrishnan%2C%20K.%20Narayanan%2C%20P.%20Deep%20learning%20with%20limited%20numerical%20precision%202015"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Han, S., Mao, H., and Dally, W. J. (2015a). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "Han_et+al_2015_b",
            "entry": "Han, S., Pool, J., Tran, J., and Dally, W. (2015b). Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pages 1135\u20131143.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Pool%2C%20J.%20Tran%2C%20J.%20Dally%2C%20W.%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Pool%2C%20J.%20Tran%2C%20J.%20Dally%2C%20W.%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "Higham_1993_a",
            "entry": "Higham, N. J. (1993). The accuracy of floating point summation. SIAM Journal on Scientific Computing, 14(4):783\u2013799.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higham%2C%20N.J.%20The%20accuracy%20of%20floating%20point%20summation%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higham%2C%20N.J.%20The%20accuracy%20of%20floating%20point%20summation%201993"
        },
        {
            "id": "Hubara_et+al_2016_a",
            "entry": "Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. (2016). Binarized neural networks. In Advances in neural information processing systems, pages 4107\u20134115.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubara%2C%20I.%20Courbariaux%2C%20M.%20Soudry%2C%20D.%20El-Yaniv%2C%20R.%20Binarized%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubara%2C%20I.%20Courbariaux%2C%20M.%20Soudry%2C%20D.%20El-Yaniv%2C%20R.%20Binarized%20neural%20networks%202016"
        },
        {
            "id": "Koster_et+al_2017_a",
            "entry": "Koster, U., Webb, T., Wang, X., Nassar, M., Bansal, A. K., Constable, W., Elibol, O., Gray, S., Hall, S., Hornof, L., et al. (2017). Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Advances in Neural Information Processing Systems, pages 1742\u20131752.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koster%2C%20U.%20Webb%2C%20T.%20Wang%2C%20X.%20Nassar%2C%20M.%20Flexpoint%3A%20An%20adaptive%20numerical%20format%20for%20efficient%20training%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koster%2C%20U.%20Webb%2C%20T.%20Wang%2C%20X.%20Nassar%2C%20M.%20Flexpoint%3A%20An%20adaptive%20numerical%20format%20for%20efficient%20training%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Lin_et+al_2016_a",
            "entry": "Lin, D., Talathi, S., and Annapureddy, S. (2016). Fixed point quantization of deep convolutional networks. In International Conference on Machine Learning, pages 2849\u20132858.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20D.%20Talathi%2C%20S.%20Annapureddy%2C%20S.%20Fixed%20point%20quantization%20of%20deep%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20D.%20Talathi%2C%20S.%20Annapureddy%2C%20S.%20Fixed%20point%20quantization%20of%20deep%20convolutional%20networks%202016"
        },
        {
            "id": "Mckinstry_et+al_2018_a",
            "entry": "McKinstry, J. L., Esser, S. K., Appuswamy, R., Bablani, D., Arthur, J. V., Yildiz, I. B., and Modha, D. S. (2018). Discovering low-precision networks close to full-precision networks for efficient embedded inference. arXiv preprint arXiv:1809.04191.",
            "arxiv_url": "https://arxiv.org/pdf/1809.04191"
        },
        {
            "id": "Micikevicius_et+al_2017_a",
            "entry": "Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaev, O., Venkatesh, G., et al. (2017). Mixed precision training. arXiv preprint arXiv:1710.03740.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03740"
        },
        {
            "id": "Robertazzi_1988_a",
            "entry": "Robertazzi, T. G. and Schwartz, S. C. (1988). Best \u201cordering\u201d for floating-point addition. ACM Transactions on Mathematical Software (TOMS), 14(1):101\u2013110.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robertazzi%2C%20T.G.%20Schwartz%2C%20S.C.%20Best%20%E2%80%9Cordering%E2%80%9D%20for%20floating-point%20addition%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robertazzi%2C%20T.G.%20Schwartz%2C%20S.C.%20Best%20%E2%80%9Cordering%E2%80%9D%20for%20floating-point%20addition%201988"
        },
        {
            "id": "Sakr_et+al_2017_a",
            "entry": "Sakr, C., Kim, Y., and Shanbhag, N. (2017). Analytical guarantees on numerical precision of deep neural networks. In International Conference on Machine Learning, pages 3007\u20133016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakr%2C%20C.%20Kim%2C%20Y.%20Shanbhag%2C%20N.%20Analytical%20guarantees%20on%20numerical%20precision%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakr%2C%20C.%20Kim%2C%20Y.%20Shanbhag%2C%20N.%20Analytical%20guarantees%20on%20numerical%20precision%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K. (2018). Training deep neural networks with 8-bit floating point numbers. In Advances in neural information processing systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20N.%20Choi%2C%20J.%20Brand%2C%20D.%20Chen%2C%20C.-Y.%20Training%20deep%20neural%20networks%20with%208-bit%20floating%20point%20numbers.%20In%20Advances%20in%20neural%20information%20processing%20systems%202018"
        },
        {
            "id": "Widrow_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Widrow, B. and Kollar, I. (2008). Quantization noise. Cambridge University Press, 2:5. Wu, S., Li, G., Chen, F., and Shi, L. (2018). Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04680"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. (2016). Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Zhu, C., Han, S., Mao, H., and Dally, W. J. (2016). Trained ternary quantization. CoRR, abs/1612.01064.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        }
    ]
}
