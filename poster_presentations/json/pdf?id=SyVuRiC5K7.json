{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING",
        "author": "Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, & Yi Yang,\u2020 1CAI, University of Technology Sydney, 2University of Oxford 3AITRICS, 4KAIST, 5Baidu Research csyanbin@gmail.com, juho.lee@stats.ox.ac.uk, {mike_seop, shkim}@aitrics.com, {eunhoy, sjhwang,}@kaist.ac.kr, Yi.Yang@uts.edu.au",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyVuRiC5K7"
        },
        "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner. We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results."
    },
    "keywords": [
        {
            "term": "Transductive Support Vector Machines",
            "url": "https://en.wikipedia.org/wiki/Transductive_Support_Vector_Machine"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "transductive inference",
            "url": "https://en.wikipedia.org/wiki/transductive_inference"
        }
    ],
    "abbreviations": {
        "TPN": "Transductive Propagation Network",
        "TSVMs": "Transductive Support Vector Machines",
        "LNP": "Linear Neighborhood Propagation"
    },
    "highlights": [
        "Recent breakthroughs in deep learning (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan and Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>) highly rely on the availability of large amounts of labeled data",
        "We introduce Transductive Propagation Network (TPN) illustrated in Figure 2, which consists of four components: feature embedding with a convolutional neural network; graph construction that produces example-wise parameters to exploit the manifold structure; label propagation that spreads labels from the support set S to the query set Q; a loss generation step that computes a crossentropy loss between propagated labels and the ground-truths on Q to jointly train all parameters in the framework.\n3.2.1",
        "The proposed Transductive Propagation Network achieves the state-of-the-art results and surpasses all the others with a large margin even when the model is trained with regular shots",
        "Namely Transductive Propagation Network (TPN), utilizes the entire test set for transductive inference",
        "Our approach is composed of four steps: feature embedding, graph construction, label propagation, and loss computation",
        "Graph construction is a key step that produces examplewise parameters to exploit the manifold structure in each episode"
    ],
    "key_statements": [
        "Recent breakthroughs in deep learning (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan and Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>) highly rely on the availability of large amounts of labeled data",
        "With the meta-learning by episodic training, we can learn the label propagation network as the query examples sampled from the training set can be used to simulate the real test set for transductive inference",
        "We propose Transductive Propagation Network (TPN) to deal with the low-data problem",
        "The experimental results show that our Transductive Propagation Network outperforms the state-of-the-art methods on both datasets",
        "We introduce the proposed algorithm that utilizes the manifold structure of the given few-shot classification task to improve the performance.\n3.1",
        "We introduce Transductive Propagation Network (TPN) illustrated in Figure 2, which consists of four components: feature embedding with a convolutional neural network; graph construction that produces example-wise parameters to exploit the manifold structure; label propagation that spreads labels from the support set S to the query set Q; a loss generation step that computes a crossentropy loss between propagated labels and the ground-truths on Q to jointly train all parameters in the framework.\n3.2.1",
        "Example-wise length-scale parameter To obtain a proper neighborhood graph in meta-learning, we propose a graph construction module built on the union set of support set and query set: S \u222a Q",
        "We evaluate and compare our Transductive Propagation Network with state-of-the-art approaches on two datasets, i.e., miniImageNet (<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi and Larochelle, 2017</a>) and tieredImageNet (<a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\">Ren et al, 2018</a>)",
        "Label propagation without learning to propagate outperforms other baseline methods in most cases, which verifies the necessity of transduction",
        "The proposed Transductive Propagation Network achieves the state-of-the-art results and surpasses all the others with a large margin even when the model is trained with regular shots",
        "Transductive Propagation Network is originally designed to perform transductive inference, we show that it can be successfully adapted to semi-supervised learning tasks with little modification",
        "We proposed the transductive setting for few-shot learning",
        "Namely Transductive Propagation Network (TPN), utilizes the entire test set for transductive inference",
        "Our approach is composed of four steps: feature embedding, graph construction, label propagation, and loss computation",
        "Graph construction is a key step that produces examplewise parameters to exploit the manifold structure in each episode"
    ],
    "summary": [
        "Recent breakthroughs in deep learning (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\"><a class=\"ref-link\" id=\"cSimonyan_2015_a\" href=\"#rSimonyan_2015_a\">Simonyan and Zisserman, 2015</a></a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>) highly rely on the availability of large amounts of labeled data.",
        "In previous work (<a class=\"ref-link\" id=\"cJoachims_1999_a\" href=\"#rJoachims_1999_a\">Joachims, 1999</a>; <a class=\"ref-link\" id=\"cZhou_et+al_2004_a\" href=\"#rZhou_et+al_2004_a\">Zhou et al, 2004</a>; Vapnik, 1999), transductive inference has shown to outperform inductive methods which predict test examples one by one, especially in small training sets.",
        "With the meta-learning by episodic training, we can learn the label propagation network as the query examples sampled from the training set can be used to simulate the real test set for transductive inference.",
        "With the propagated scores and ground truth labels of the query set, we compute the cross-entropy loss with respect to the feature embedding and graph construction parameters.",
        "In transductive inference, we propose to learn to propagate labels between data instances for unseen classes via episodic meta-learning.",
        "We evaluate our approach on two benchmark datasets for few-shot learning, namely miniImageNet and tieredImageNet. The experimental results show that our Transductive Propagation Network outperforms the state-of-the-art methods on both datasets.",
        "Our algorithm assumes a transductive setting, in which we utilize the union of support set and query set to exploit the manifold structure of novel class space by using episodic-wise parameters.",
        "We introduce Transductive Propagation Network (TPN) illustrated in Figure 2, which consists of four components: feature embedding with a convolutional neural network; graph construction that produces example-wise parameters to exploit the manifold structure; label propagation that spreads labels from the support set S to the query set Q; a loss generation step that computes a crossentropy loss between propagated labels and the ground-truths on Q to jointly train all parameters in the framework.",
        "Example-wise length-scale parameter To obtain a proper neighborhood graph in meta-learning, we propose a graph construction module built on the union set of support set and query set: S \u222a Q.",
        "Note that the scale parameter is determined example-wisely and learned in an episodic training procedure, which adapts well to different tasks and makes it suitable for few-shot learning.",
        "The objective of this step is to compute the classification loss between the predictions of the union of support and query set via label propagation and the ground-truths.",
        "This confirms that our model effectively finds the episodic-wise manifold structure of test examples through learning to construct the graph for label propagation.",
        "In order to compare with semi-supervised methods, Figure 4: 5-way performance with various training/test shots.",
        "All parameters are learned end-to-end using cross-entropy loss with respect to the ground truth labels and the prediction scores in the query set.",
        "We are going to explore the episodic-wise distance metric rather than only using example-wise parameters for the Euclidean distance"
    ],
    "headline": "We propose Transductive Propagation Network, a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem",
    "reference_links": [
        {
            "id": "Chung_1997_a",
            "entry": "Fan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Fan%20R.K.%20Graham%2C%20Fan%20Chung%20Spectral%20graph%20theory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Fan%20R.K.%20Graham%2C%20Fan%20Chung%20Spectral%20graph%20theory%201997"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Fu_et+al_2015_a",
            "entry": "Yanwei Fu, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view zero-shot learning. IEEE transactions on pattern analysis and machine intelligence, 37(11):2332\u20132345, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Yanwei%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Transductive%20multi-view%20zero-shot%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Yanwei%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Transductive%20multi-view%20zero-shot%20learning%202015"
        },
        {
            "id": "Fujiwara_2014_a",
            "entry": "Yasuhiro Fujiwara and Go Irie. Efficient label propagation. In International Conference on Machine Learning, pages 784\u2013792, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujiwara%2C%20Yasuhiro%20Irie%2C%20Go%20Efficient%20label%20propagation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fujiwara%2C%20Yasuhiro%20Irie%2C%20Go%20Efficient%20label%20propagation%202014"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jia_et+al_2014_a",
            "entry": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM International Conference on Multimedia, pages 675\u2013678. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014"
        },
        {
            "id": "Joachims_1999_a",
            "entry": "Thorsten Joachims. Transductive inference for text classification using support vector machines. In International Conference on Machine Learning, volume 99, pages 200\u2013209, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20Thorsten%20Transductive%20inference%20for%20text%20classification%20using%20support%20vector%20machines%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20Thorsten%20Transductive%20inference%20for%20text%20classification%20using%20support%20vector%20machines%201999"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), volume 5, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lake_et+al_2011_a",
            "entry": "Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Conference of the Cognitive Science Society, volume 33, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20Salakhutdinov%2C%20Ruslan%20Gross%2C%20Jason%20Tenenbaum%2C%20Joshua%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011"
        },
        {
            "id": "Lee_2018_a",
            "entry": "Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, pages 2933\u20132942, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yoonho%20Choi%2C%20Seungjin%20Gradient-based%20meta-learning%20with%20learned%20layerwise%20metric%20and%20subspace%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yoonho%20Choi%2C%20Seungjin%20Gradient-based%20meta-learning%20with%20learned%20layerwise%20metric%20and%20subspace%202018"
        },
        {
            "id": "Liang_2018_a",
            "entry": "De-Ming Liang and Yu-Feng Li. Lightweight label propagation for large-scale network data. In IJCAI, pages 3421\u20133427, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20De-Ming%20Li%2C%20Yu-Feng%20Lightweight%20label%20propagation%20for%20large-scale%20network%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20De-Ming%20Li%2C%20Yu-Feng%20Lightweight%20label%20propagation%20for%20large-scale%20network%20data%202018"
        },
        {
            "id": "Matthias_et+al_2017_a",
            "entry": "Bauer\u00efijN Matthias, Rojas-Carulla Mateo, Jakub Bart\u0142omiej Swiatkowski, Bernhard Sch\u00f6lkopf, and Richard E Turner. Discriminative k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00326"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20meta-learner%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20meta-learner%202018"
        },
        {
            "id": "Munkhdalai_et+al_2018_a",
            "entry": "Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with conditionally shifted neurons. In International Conference on Machine Learning, pages 3661\u20133670, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munkhdalai%2C%20Tsendsuren%20Yuan%2C%20Xingdi%20Mehri%2C%20Soroush%20Trischler%2C%20Adam%20Rapid%20adaptation%20with%20conditionally%20shifted%20neurons%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munkhdalai%2C%20Tsendsuren%20Yuan%2C%20Xingdi%20Mehri%2C%20Soroush%20Trischler%2C%20Adam%20Rapid%20adaptation%20with%20conditionally%20shifted%20neurons%202018"
        },
        {
            "id": "Nichol_et+al_2018_a",
            "entry": "Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Oreshkin_et+al_2018_a",
            "entry": "Boris N Oreshkin, Alexandre Lacoste, and Pau Rodriguez. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oreshkin%2C%20Boris%20N.%20Lacoste%2C%20Alexandre%20Rodriguez%2C%20Pau%20Tadam%3A%20Task%20dependent%20adaptive%20metric%20for%20improved%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oreshkin%2C%20Boris%20N.%20Lacoste%2C%20Alexandre%20Rodriguez%2C%20Pau%20Tadam%3A%20Task%20dependent%20adaptive%20metric%20for%20improved%20few-shot%20learning%202018"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Ren_et+al_2018_a",
            "entry": "Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Mengye%20Triantafillou%2C%20Eleni%20Ravi%2C%20Sachin%20Snell%2C%20Jake%20Meta-learning%20for%20semi-supervised%20few-shot%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Mengye%20Triantafillou%2C%20Eleni%20Ravi%2C%20Sachin%20Snell%2C%20Jake%20Meta-learning%20for%20semi-supervised%20few-shot%20classification%202018"
        },
        {
            "id": "Rohrbach_et+al_2013_a",
            "entry": "Marcus Rohrbach, Sandra Ebert, and Bernt Schiele. Transfer learning in a transductive setting. In Advances in Neural Information Processing Systems, pages 46\u201354, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohrbach%2C%20Marcus%20Ebert%2C%20Sandra%20Schiele%2C%20Bernt%20Transfer%20learning%20in%20a%20transductive%20setting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohrbach%2C%20Marcus%20Ebert%2C%20Sandra%20Schiele%2C%20Bernt%20Transfer%20learning%20in%20a%20transductive%20setting%202013"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Evolutionary%20principles%20in%20self-referential%20learning%2C%20or%20on%20learning%20how%20to%20learn%3A%20the%20meta-meta-%201987"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4080\u20134090, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Sugiyama_2007_a",
            "entry": "Masashi Sugiyama. Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis. Journal of Machine Learning Research, 8:1027\u20131061, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20Masashi%20Dimensionality%20reduction%20of%20multimodal%20labeled%20data%20by%20local%20fisher%20discriminant%20analysis%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sugiyama%2C%20Masashi%20Dimensionality%20reduction%20of%20multimodal%20labeled%20data%20by%20local%20fisher%20discriminant%20analysis%202007"
        },
        {
            "id": "Sung_et+al_2018_a",
            "entry": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018"
        },
        {
            "id": "Thrun_2012_a",
            "entry": "Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012. Vladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988\u2013999, 1999. Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630\u20133638, 2016. Fei Wang and Changshui Zhang. Label propagation through linear neighborhoods. In International Conference on Machine Learning, pages 985\u2013992. ACM, 2006. Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data. In Computer Vision and Pattern Recognition, 2018. Zhongwen Xu, Linchao Zhu, and Yi Yang. Few-shot object recognition from machine-labeled web images. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20Pratt%2C%20Lorien%20Learning%20to%20learn%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thrun%2C%20Sebastian%20Pratt%2C%20Lorien%20Learning%20to%20learn%202012"
        },
        {
            "id": "Yang_2017_a",
            "entry": "Computer Vision and Pattern Recognition, 2017. Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, pages 40\u201348, 2016. Lihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural Information",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20William%20Cohen%20Salakhudinov%2C%20Ruslan%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20William%20Cohen%20Salakhudinov%2C%20Ruslan%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202017"
        },
        {
            "id": "Zhou_et+al_2004_a",
            "entry": "Processing Systems, 2004. Denny Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch\u00f6lkopf. Learning with local and global consistency. In Advances in Neural Information Processing Systems, pages 321\u2013328, 2004. Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Tech.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Olivier%20Bousquet%20Lal%2C%20Thomas%20N.%20Weston%2C%20Jason%20Sch%C3%B6lkopf%2C%20Bernhard%20Learning%20with%20local%20and%20global%20consistency%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Olivier%20Bousquet%20Lal%2C%20Thomas%20N.%20Weston%2C%20Jason%20Sch%C3%B6lkopf%2C%20Bernhard%20Learning%20with%20local%20and%20global%20consistency%202004"
        },
        {
            "id": "Rep.,_2002_a",
            "entry": "Rep., Technical Report CMU-CALD-02\u2013107, Carnegie Mellon University, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rep%20Technical%20Report%20CMUCALD02107%20Carnegie%20Mellon%20University%202002"
        }
    ]
}
