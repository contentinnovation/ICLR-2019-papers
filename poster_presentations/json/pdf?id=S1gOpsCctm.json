{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING FINITE STATE REPRESENTATIONS OF RECURRENT POLICY NETWORKS",
        "author": "Anurag Koul & Alan Fern School of EECS Oregon State University Corvallis, Oregon, USA {koula,alan.fern}@oregonstate.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1gOpsCctm"
        },
        "abstract": "Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior. We present results of this approach on synthetic environments and six Atari games. The resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy. We also show that these finite policy representations lead to improved interpretability."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "automata",
            "url": "https://en.wikipedia.org/wiki/automata"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "finite state machines",
            "url": "https://en.wikipedia.org/wiki/finite_state_machines"
        },
        {
            "term": "Moore Machine",
            "url": "https://en.wikipedia.org/wiki/Moore_Machine"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "finite state",
            "url": "https://en.wikipedia.org/wiki/finite_state"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks",
        "RL": "reinforcement learning",
        "IL": "imitation learning",
        "QBN": "Quantized Bottleneck Network",
        "MMN": "Moore Machine Network",
        "FSMs": "finite state machines",
        "MM": "Moore Machine",
        "QBNs": "quantized bottleneck networks",
        "MCEs": "Mode Counter Environments"
    },
    "highlights": [
        "Deep reinforcement learning (RL) and imitation learning (IL) have demonstrated impressive performance across a wide range of applications",
        "To accomplish this we introduce the idea of Quantized Bottleneck Network (QBN) insertion",
        "\u2217Work done at Oregon State University policy represented as a Moore Machine Network (MMN) with quantized memory and observations that is nearly equivalent to the original recurrent neural networks",
        "Our work extends that approach to learn Moore Machine Network and more importantly introduces the method of Quantized Bottleneck Network insertion as a way of learning via guidance from a continuous recurrent neural networks.This transforms any pre-trained recurrent policy into a finite representation",
        "We have found that learning Moore Machine Network from scratch can be quite difficult for non-trivial problems, even when an recurrent neural networks can be learned with relative ease",
        "Motivated by the goal of better understanding memory use in recurrent neural networks polices, we introduced an approach for extracting finite state Moore Machines from those policies"
    ],
    "key_statements": [
        "Deep reinforcement learning (RL) and imitation learning (IL) have demonstrated impressive performance across a wide range of applications",
        "Our main contribution is to introduce an approach for transforming an recurrent neural networks policy with continuous memory and continuous observations to a finite-state representation known as a Moore Machine",
        "To accomplish this we introduce the idea of Quantized Bottleneck Network (QBN) insertion",
        "\u2217Work done at Oregon State University policy represented as a Moore Machine Network (MMN) with quantized memory and observations that is nearly equivalent to the original recurrent neural networks",
        "While training quantized networks is often considered to be quite challenging, we show that a simple approach works well in the case of Quantized Bottleneck Network",
        "We present experiments in synthetic domains designed to exercise different types of memory use as well as benchmark grammar learning problems",
        "We show that in most cases it is possible to extract near-equivalent Moore Machine Network and that the Moore Machine Network can be surprisingly small",
        "The approach is based on directly learning recurrent networks with finite memory, which are qualitatively similar to the memory representation of our Moore Machine Network",
        "Our work extends that approach to learn Moore Machine Network and more importantly introduces the method of Quantized Bottleneck Network insertion as a way of learning via guidance from a continuous recurrent neural networks.This transforms any pre-trained recurrent policy into a finite representation",
        "We have found that learning Moore Machine Network from scratch can be quite difficult for non-trivial problems, even when an recurrent neural networks can be learned with relative ease",
        "While there are a variety of ways to deal with this issue, we have found that the straight-through estimator, as suggested and used in prior work (<a class=\"ref-link\" id=\"cHinton_2012_a\" href=\"#rHinton_2012_a\">Hinton, 2012</a>; <a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a>; <a class=\"ref-link\" id=\"cCourbariaux_et+al_2016_a\" href=\"#rCourbariaux_et+al_2016_a\">Courbariaux et al, 2016</a>) is quite effective",
        "We used the same general architecture for the Quantized Bottleneck Network bf as used for the Mode Counter Environments experiments, but adjusted the encoder input and decoder output sizes to match the dimension of the continuous observation features ft",
        "Each training episode was generated by executing the learned recurrent neural networks for a random number of steps and executing an -greedy version of the recurrent neural networks policy. This is intended to increase the diversity of the training data and we found that it helps to more robustly learn the Quantized Bottleneck Network",
        "Motivated by the goal of better understanding memory use in recurrent neural networks polices, we introduced an approach for extracting finite state Moore Machines from those policies",
        "In most cases, the learned Moore Machine Network maintain similar performance to the original recurrent neural networks policies"
    ],
    "summary": [
        "Deep reinforcement learning (RL) and imitation learning (IL) have demonstrated impressive performance across a wide range of applications.",
        "Our main contribution is to introduce an approach for transforming an RNN policy with continuous memory and continuous observations to a finite-state representation known as a Moore Machine.",
        "\u2217Work done at Oregon State University policy represented as a Moore Machine Network (MMN) with quantized memory and observations that is nearly equivalent to the original RNN.",
        "The extracted MMNs give insights into the memory usage that are not obvious based on just observing the RNN policy in action.",
        "The approach is based on directly learning recurrent networks with finite memory, which are qualitatively similar to the memory representation of our MMNs. That work, focused on learning from scratch rather than aiming to describe the behavior of a continuous RNN.",
        "Our work extends that approach to learn MMNs and more importantly introduces the method of QBN insertion as a way of learning via guidance from a continuous RNN.This transforms any pre-trained recurrent policy into a finite representation.",
        "Given the current observation ot and current state ht, an RNN performs the following operations: 1) Extract a set of observation features ft from ot, for example, using a CNN when observations are images, 2) Outputting an action at = \u03c0 according to policy \u03c0, which is often a linear softmax function of ht, 3) transition to a new state ht+1 = \u03b4 where \u03b4 is the transition function, which is often implemented via different types of gating networks such as LSTMs or GRUs. The continuous and high dimensional nature of ht and ft can make interpreting the role of memory difficult.",
        "Given a trained RNN, our key idea is to first learn quantized bottleneck networks (QBNs) for embedding the continuous observation features and hidden state into a k-level quantized representation.",
        "We used the same general architecture for the QBN bf as used for the MCE experiments, but adjusted the encoder input and decoder output sizes to match the dimension of the continuous observation features ft.",
        "Motivated by the goal of better understanding memory use in RNN polices, we introduced an approach for extracting finite state Moore Machines from those policies.",
        "The key idea, bottleneck insertion, is to train Quantized Bottleneck Networks to produce binary encodings of continuous RNN memory and input features, and insert those bottlenecks into the RNN.",
        "Near equivalent Moore Machine Network (MMN) which has quantized memory and observation features."
    ],
    "headline": "We introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features",
    "reference_links": [
        {
            "id": "Arras_et+al_2017_a",
            "entry": "Leila Arras, Gregoire Montavon, Klaus-Robert Muller, and Wojciech Samek. Explaining recurrent neural network predictions in sentiment analysis. arXiv preprint arXiv:1706.07206, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.07206"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Adelmo_et+al_2003_a",
            "entry": "Adelmo Luis Cechin, D Regina, P Simon, and K Stertz. State automata extraction from recurrent neural nets using k-means and fuzzy clustering. In Chilean Computer Science Society, 2003. SCCC 2003. Proceedings. 23rd International Conference of the, pp. 73\u201378. IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adelmo%20Luis%20Cechin%2C%20D.Regina%20Simon%2C%20P.%20Stertz%2C%20K.%20State%20automata%20extraction%20from%20recurrent%20neural%20nets%20using%20k-means%20and%20fuzzy%20clustering%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adelmo%20Luis%20Cechin%2C%20D.Regina%20Simon%2C%20P.%20Stertz%2C%20K.%20State%20automata%20extraction%20from%20recurrent%20neural%20nets%20using%20k-means%20and%20fuzzy%20clustering%202003"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1259"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Courbariaux_et+al_2016_a",
            "entry": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "Greydanus_et+al_2017_a",
            "entry": "Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. arXiv preprint arXiv:1711.00138, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00138"
        },
        {
            "id": "Hinton_2012_a",
            "entry": "Geoffrey Hinton. Neural networks for machine learning. video lectures, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Neural%20networks%20for%20machine%20learning%202012"
        },
        {
            "id": "Hinton_2006_a",
            "entry": "Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jacobsson_2005_a",
            "entry": "Henrik Jacobsson. Rule extraction from recurrent neural networks: Ataxonomy and review. Neural Computation, 17(6):1223\u20131263, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobsson%2C%20Henrik%20Rule%20extraction%20from%20recurrent%20neural%20networks%3A%20Ataxonomy%20and%20review%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobsson%2C%20Henrik%20Rule%20extraction%20from%20recurrent%20neural%20networks%3A%20Ataxonomy%20and%20review%202005"
        },
        {
            "id": "Karpathy_et+al_2015_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "Krizhevsky_2010_a",
            "entry": "Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 40(7), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoff%20Convolutional%20deep%20belief%20networks%20on%20cifar-10%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoff%20Convolutional%20deep%20belief%20networks%20on%20cifar-10%202010"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tom Mikolov, Martin Karafiat, Luk Burget, and Sanjeev Khudanpur. Recurrent neural network based language model. INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, pp. 1045\u20131048, 2010. URL http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf.",
            "url": "http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tom%20Karafiat%2C%20Martin%20Burget%2C%20Luk%20Khudanpur%2C%20Sanjeev%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518, 2015. doi: 10.1038/nature14236. URL https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Murdoch_2017_a",
            "entry": "W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. arXiv preprint arXiv:1702.02540, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02540"
        },
        {
            "id": "Omlin_1996_a",
            "entry": "Christian W Omlin and C Lee Giles. Extraction of rules from discrete-time recurrent neural networks. Neural networks, 9(1):41\u201352, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omlin%2C%20Christian%20W.%20Giles%2C%20C.Lee%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omlin%2C%20Christian%20W.%20Giles%2C%20C.Lee%20Extraction%20of%20rules%20from%20discrete-time%20recurrent%20neural%20networks%201996"
        },
        {
            "id": "Paull_1959_a",
            "entry": "Marvin C Paull and Stephen H Unger. Minimizing the number of states in incompletely specified sequential switching functions. IRE Transactions on Electronic Computers, (3):356\u2013367, 1959.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paull%2C%20Marvin%20C.%20Unger%2C%20Stephen%20H.%20Minimizing%20the%20number%20of%20states%20in%20incompletely%20specified%20sequential%20switching%20functions%201959",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paull%2C%20Marvin%20C.%20Unger%2C%20Stephen%20H.%20Minimizing%20the%20number%20of%20states%20in%20incompletely%20specified%20sequential%20switching%20functions%201959"
        },
        {
            "id": "Pitis_2017_a",
            "entry": "Silviu Pitis. Beyond binary: Ternary and one-hot neurons. https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html, 2017.",
            "url": "https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature Publishing Group, 550, 2017. doi: 10.1038/nature24270. URL https://www.nature.com/nature/journal/v550/n7676/pdf/nature24270.pdf.",
            "crossref": "https://dx.doi.org/10.1038/nature24270"
        },
        {
            "id": "Strobelt_et+al_2016_a",
            "entry": "Hendrik Strobelt, Sebastian Gehrmann, Bernd Huber, Hanspeter Pfister, and Alexander M Rush. Visual analysis of hidden state dynamics in recurrent neural networks. arXiv preprint arXiv:1606.07461, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07461"
        },
        {
            "id": "Tomita_1982_a",
            "entry": "Masaru Tomita. Dynamic construction of finite-state automata from examples using hill-climbing. In Proceedings of the Fourth Annual Conference of the Cognitive Science Society, pp. 105\u2013108, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomita%2C%20Masaru%20Dynamic%20construction%20of%20finite-state%20automata%20from%20examples%20using%20hill-climbing%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomita%2C%20Masaru%20Dynamic%20construction%20of%20finite-state%20automata%20from%20examples%20using%20hill-climbing%201982"
        },
        {
            "id": "Watrous_1992_a",
            "entry": "Raymond L Watrous and Gary M Kuhn. Induction of finite-state automata using second-order recurrent networks. In Advances in neural information processing systems, pp. 309\u2013317, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watrous%2C%20Raymond%20L.%20Kuhn%2C%20Gary%20M.%20Induction%20of%20finite-state%20automata%20using%20second-order%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watrous%2C%20Raymond%20L.%20Kuhn%2C%20Gary%20M.%20Induction%20of%20finite-state%20automata%20using%20second-order%20recurrent%20networks%201992"
        },
        {
            "id": "Weiss_et+al_2017_a",
            "entry": "Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. arXiv preprint arXiv:1711.09576, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09576"
        },
        {
            "id": "Zahavy_et+al_2016_a",
            "entry": "Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In International Conference on Machine Learning, pp. 1899\u20131908, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zahavy%2C%20Tom%20Ben-Zrihem%2C%20Nir%20Mannor%2C%20Shie%20Graying%20the%20black%20box%3A%20Understanding%20dqns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zahavy%2C%20Tom%20Ben-Zrihem%2C%20Nir%20Mannor%2C%20Shie%20Graying%20the%20black%20box%3A%20Understanding%20dqns%202016"
        },
        {
            "id": "Zeng_et+al_1993_a",
            "entry": "Zheng Zeng, Rodney M Goodman, and Padhraic Smyth. Learning finite state machines with selfclustering recurrent networks. Neural Computation, 5(6):976\u2013990, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeng%2C%20Zheng%20Goodman%2C%20Rodney%20M.%20Smyth%2C%20Padhraic%20Learning%20finite%20state%20machines%20with%20selfclustering%20recurrent%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeng%2C%20Zheng%20Goodman%2C%20Rodney%20M.%20Smyth%2C%20Padhraic%20Learning%20finite%20state%20machines%20with%20selfclustering%20recurrent%20networks%201993"
        }
    ]
}
