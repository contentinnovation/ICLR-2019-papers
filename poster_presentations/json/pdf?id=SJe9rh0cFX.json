{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE UNIVERSAL APPROXIMABILITY AND COMPLEXITY BOUNDS OF QUANTIZED RELU NEURAL NETWORKS",
        "author": "Yukun Ding, Jinglan Liu, Jinjun Xiong, Yiyu Shi, 1 University of Notre Dame 2 IBM Thomas J. Watson Research Center {yding,jliu,yshi,}@nd.edu, jinjun@us.ibm.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJe9rh0cFX"
        },
        "abstract": "Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and functiondependent structures. Our results reveal that, to attain an approximation error bound of , the number of weights needed by a quantized network is no more than O log5(1/ ) times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the first in-depth study on the complexity bounds of quantized neural networks."
    },
    "keywords": [
        {
            "term": "language processing",
            "url": "https://en.wikipedia.org/wiki/language_processing"
        },
        {
            "term": "approximation error",
            "url": "https://en.wikipedia.org/wiki/approximation_error"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "LDR": "low displacement rank",
        "CSR": "compressed sparse row"
    },
    "highlights": [
        "Various deep neural networks deliver state-of-the-art performance on many tasks such as object recognition and natural language processing using new learning strategies and architectures (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\"><a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a></a>; He et al, 2016; <a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\">Kumar et al, 2016</a></a>; <a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\"><a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a>)",
        "We focus on ReLU networks, which is among the most widely used in deep neural networks (<a class=\"ref-link\" id=\"cXu_et+al_2015_a\" href=\"#rXu_et+al_2015_a\">Xu et al, 2015</a>)",
        "We further show that our upper bounds have good tightness by comparing them with the lower bound of unquantized ReLU networks established in the literature",
        "Comparison with the Upper Bound: The quality of an upper bound lies on its tightness",
        "Compared with the most recent work on unquantized ReLU networks (<a class=\"ref-link\" id=\"cYarotsky_2017_a\" href=\"#rYarotsky_2017_a\">Yarotsky, 2017</a>), where the upper bound d on the number of weights to attain an approximation error is given by O log(1/ ) (1/ ) n , our result for a quantized ReLU network is given by O",
        "By comparing the expressions of two bounds while treating \u03bb as a constant, we can show that, to attain the same approximation error bound , the number of weights needed by a quantized ReLU network is no more than O) times that needed by an unquantized ReLU network"
    ],
    "key_statements": [
        "Various deep neural networks deliver state-of-the-art performance on many tasks such as object recognition and natural language processing using new learning strategies and architectures (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\"><a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a></a>; He et al, 2016; <a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\">Kumar et al, 2016</a></a>; <a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\"><a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a>)",
        "The memory consumption of neural networks can be reduced by either directly reducing the number of weights or decreasing the number of bits needed to represent and store each weight, which can be employed on top of each other (<a class=\"ref-link\" id=\"cChoi_et+al_2016_a\" href=\"#rChoi_et+al_2016_a\">Choi et al, 2016</a>)",
        "We focus on ReLU networks, which is among the most widely used in deep neural networks (<a class=\"ref-link\" id=\"cXu_et+al_2015_a\" href=\"#rXu_et+al_2015_a\">Xu et al, 2015</a>)",
        "We prove that even the most extremely quantized ReLU networks using two distinct weight values are capable of representing a wide class of functions with arbitrary accuracy",
        "Given the number of distinct weights and the desired approximation error bound, we provide upper bounds on the number of weights and the memory size",
        "We further show that our upper bounds have good tightness by comparing them with the lower bound of unquantized ReLU networks established in the literature",
        "We show that, to attain the same approximation error bound , the number of weights needed by a quantized network is no more than O log5(1/ ) times that of an unquantized network",
        "We demonstrate how a theoretical complexity bound can be used to estimate an optimal bit-width, which in turn enables the best cost-effectiveness for a given task",
        "We prove the universal approximability and the upper bounds with function-independent structure in Section 4 and extend it to function-dependent structure in Section 5",
        "Comparison with the Upper Bound: The quality of an upper bound lies on its tightness",
        "Compared with the most recent work on unquantized ReLU networks (<a class=\"ref-link\" id=\"cYarotsky_2017_a\" href=\"#rYarotsky_2017_a\">Yarotsky, 2017</a>), where the upper bound d on the number of weights to attain an approximation error is given by O log(1/ ) (1/ ) n , our result for a quantized ReLU network is given by O",
        "By comparing the expressions of two bounds while treating \u03bb as a constant, we can show that, to attain the same approximation error bound , the number of weights needed by a quantized ReLU network is no more than O) times that needed by an unquantized ReLU network"
    ],
    "summary": [
        "Various deep neural networks deliver state-of-the-art performance on many tasks such as object recognition and natural language processing using new learning strategies and architectures (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\"><a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a></a>; He et al, 2016; <a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKumar_et+al_2016_a\" href=\"#rKumar_et+al_2016_a\">Kumar et al, 2016</a></a>; <a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\"><a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a></a>; <a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a>).",
        "Given the number of distinct weight values \u03bb and a target function f , we construct a network that can approximate f with an arbitrarily small error bound to prove the universal approximability.",
        "We can approximate a connection with any weight in an unquantized network by a quantized sub-network that only uses a finite number of given weight values.",
        "We show that, to attain the same approximation error bound , the number of weights needed by a quantized network is no more than O log5(1/ ) times that of an unquantized network.",
        "The complexity of such approximation depends on the distribution of those continuous-value weights, which may vary depending on the training data and network structure and a closed-form expression for the upper bounds is not possible.",
        "For any f \u2208 Fd,n , given \u03bb distinct weights, there is a ReLU network with fixed structure that can approximate f with any error \u2208 (0, 1), such that (i) the depth is",
        "For linear quantization, which strictly determines the available weight values once \u03bb is given, we can use the same proof for nonlinear quantization except for a different subnetwork for weight approximation with width t and depth t log \u03bb",
        "For any f \u2208 F1,1 , given \u03bb distinct weights, there is a ReLU network with function-dependent structure that can approximate f with any error \u2208 (0, 1), such that (i) the depth is O \u03bb) \u03bb\u22121 + log (1/ ) ; the number of weights is",
        "The memory bound expression as derived in this paper helps us to determine whether there is an optimal \u03bb that would lead to the lowest bound and most compact network for a given target function.",
        "Compared with the most recent work on unquantized ReLU networks (<a class=\"ref-link\" id=\"cYarotsky_2017_a\" href=\"#rYarotsky_2017_a\">Yarotsky, 2017</a>), where the upper bound d on the number of weights to attain an approximation error is given by O log(1/ ) (1/ ) n , our result for a quantized ReLU network is given by O",
        "By comparing the expressions of two bounds while treating \u03bb as a constant, we can show that, to attain the same approximation error bound , the number of weights needed by a quantized ReLU network is no more than O) times that needed by an unquantized ReLU network."
    ],
    "headline": "We study the representation power of quantized neural networks",
    "reference_links": [
        {
            "id": "Blott_et+al_2017_a",
            "entry": "Michaela Blott, Thomas B Preu\u00dfer, Nicholas Fraser, Giulio Gambardella, Kenneth O\u2019Brien, Yaman Umuroglu, and Miriam Leeser. Scaling neural network performance through customized hardware architectures on reconfigurable logic. In Computer Design (ICCD), 2017 IEEE International Conference on, pp. 419\u2013422. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blott%2C%20Michaela%20Preu%C3%9Fer%2C%20Thomas%20B.%20Fraser%2C%20Nicholas%20Gambardella%2C%20Giulio%20Scaling%20neural%20network%20performance%20through%20customized%20hardware%20architectures%20on%20reconfigurable%20logic%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blott%2C%20Michaela%20Preu%C3%9Fer%2C%20Thomas%20B.%20Fraser%2C%20Nicholas%20Gambardella%2C%20Giulio%20Scaling%20neural%20network%20performance%20through%20customized%20hardware%20architectures%20on%20reconfigurable%20logic%202017"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. In Advances in Neural Information Processing Systems, pp. 742\u2013751, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Guobin%20Choi%2C%20Wongun%20Yu%2C%20Xiang%20Han%2C%20Tony%20Learning%20efficient%20object%20detection%20models%20with%20knowledge%20distillation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Guobin%20Choi%2C%20Wongun%20Yu%2C%20Xiang%20Han%2C%20Tony%20Learning%20efficient%20object%20detection%20models%20with%20knowledge%20distillation%202017"
        },
        {
            "id": "Cheng_et+al_2018_a",
            "entry": "Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, and Han-qing Lu. Recent advances in efficient computation of deep convolutional neural networks. Frontiers of Information Technology & Electronic Engineering, 19(1):64\u201377, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Jian%20Wang%2C%20Pei-song%20Li%2C%20Gang%20Hu%2C%20Qing-hao%20Recent%20advances%20in%20efficient%20computation%20of%20deep%20convolutional%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Jian%20Wang%2C%20Pei-song%20Li%2C%20Gang%20Hu%2C%20Qing-hao%20Recent%20advances%20in%20efficient%20computation%20of%20deep%20convolutional%20neural%20networks%202018"
        },
        {
            "id": "Choi_et+al_2016_a",
            "entry": "Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization. arXiv preprint arXiv:1612.01543, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01543"
        },
        {
            "id": "Courbariaux_et+al_2015_a",
            "entry": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pp. 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "Denton_et+al_2014_a",
            "entry": "Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems, pp. 1269\u20131277, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20L.%20Zaremba%2C%20Wojciech%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Exploiting%20linear%20structure%20within%20convolutional%20networks%20for%20efficient%20evaluation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20L.%20Zaremba%2C%20Wojciech%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Exploiting%20linear%20structure%20within%20convolutional%20networks%20for%20efficient%20evaluation%202014"
        },
        {
            "id": "Faraone_et+al_2018_a",
            "entry": "Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip HW Leong. Syq: Learning symmetric quantization for efficient deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4300\u20134309, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faraone%2C%20Julian%20Fraser%2C%20Nicholas%20Blott%2C%20Michaela%20Leong%2C%20Philip%20H.W.%20Syq%3A%20Learning%20symmetric%20quantization%20for%20efficient%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faraone%2C%20Julian%20Fraser%2C%20Nicholas%20Blott%2C%20Michaela%20Leong%2C%20Philip%20H.W.%20Syq%3A%20Learning%20symmetric%20quantization%20for%20efficient%20deep%20neural%20networks%202018"
        },
        {
            "id": "Gysel_et+al_2016_a",
            "entry": "Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approximation of convolutional neural networks. arXiv preprint arXiv:1604.03168, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.03168"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "Han_et+al_2015_b",
            "entry": "Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135\u20131143, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "Hanin_2017_a",
            "entry": "Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. arXiv preprint arXiv:1708.02691, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02691"
        },
        {
            "id": "He_et+al_2018_a",
            "entry": "Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 784\u2013800, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Yihui%20Lin%2C%20Ji%20Liu%2C%20Zhijian%20Wang%2C%20Hanrui%20Amc%3A%20Automl%20for%20model%20compression%20and%20acceleration%20on%20mobile%20devices%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Yihui%20Lin%2C%20Ji%20Liu%2C%20Zhijian%20Wang%2C%20Hanrui%20Amc%3A%20Automl%20for%20model%20compression%20and%20acceleration%20on%20mobile%20devices%202018"
        },
        {
            "id": "Hubara_et+al_2016_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07061"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Kumar_et+al_2016_a",
            "entry": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378\u20131387, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Leng_et+al_2017_a",
            "entry": "Cong Leng, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with admm. arXiv preprint arXiv:1707.09870, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09870"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.04711"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pp. 5811\u20135821, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hao%20De%2C%20Soham%20Xu%2C%20Zheng%20Studer%2C%20Christoph%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hao%20De%2C%20Soham%20Xu%2C%20Zheng%20Studer%2C%20Christoph%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017"
        },
        {
            "id": "Liang_2016_a",
            "entry": "Shiyu Liang and R Srikant. Why deep neural networks for function approximation? arXiv preprint arXiv:1610.04161, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04161"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806\u2013814, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Hrushikesh_1992_a",
            "entry": "Hrushikesh N Mhaskar and Charles A Micchelli. Approximation by superposition of sigmoidal and radial basis functions. Advances in Applied mathematics, 13(3):350\u2013373, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hrushikesh%20N%20Mhaskar%20and%20Charles%20A%20Micchelli%20Approximation%20by%20superposition%20of%20sigmoidal%20and%20radial%20basis%20functions%20Advances%20in%20Applied%20mathematics%20133350373%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hrushikesh%20N%20Mhaskar%20and%20Charles%20A%20Micchelli%20Approximation%20by%20superposition%20of%20sigmoidal%20and%20radial%20basis%20functions%20Advances%20in%20Applied%20mathematics%20133350373%201992"
        },
        {
            "id": "Sandler_et+al_2018_a",
            "entry": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04381"
        },
        {
            "id": "Shaham_et+al_2016_a",
            "entry": "Uri Shaham, Alexander Cloninger, and Ronald R Coifman. Provable approximation properties for deep neural networks. Applied and Computational Harmonic Analysis, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaham%2C%20Uri%20Cloninger%2C%20Alexander%20Coifman%2C%20Ronald%20R.%20Provable%20approximation%20properties%20for%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaham%2C%20Uri%20Cloninger%2C%20Alexander%20Coifman%2C%20Ronald%20R.%20Provable%20approximation%20properties%20for%20deep%20neural%20networks%202016"
        },
        {
            "id": "Shayar_et+al_2017_a",
            "entry": "Oran Shayar, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick. arXiv preprint arXiv:1710.07739, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07739"
        },
        {
            "id": "Sonoda_2017_a",
            "entry": "Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233\u2013268, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sonoda%2C%20Sho%20Murata%2C%20Noboru%20Neural%20network%20with%20unbounded%20activation%20functions%20is%20universal%20approximator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sonoda%2C%20Sho%20Murata%2C%20Noboru%20Neural%20network%20with%20unbounded%20activation%20functions%20is%20universal%20approximator%202017"
        },
        {
            "id": "Su_et+al_2018_a",
            "entry": "Jiang Su, Nicholas J Fraser, Giulio Gambardella, Michaela Blott, Gianluca Durelli, David B Thomas, Philip HW Leong, and Peter YK Cheung. Accuracy to throughput trade-offs for reduced precision neural networks on reconfigurable logic. In Applied Reconfigurable Computing. Architectures, Tools, and Applications: 14th International Symposium, ARC 2018, Santorini, Greece, May 2-4, 2018, Proceedings, volume 10824, pp. 29.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20Jiang%20Fraser%2C%20Nicholas%20J.%20Gambardella%2C%20Giulio%20Blott%2C%20Michaela%20Accuracy%20to%20throughput%20trade-offs%20for%20reduced%20precision%20neural%20networks%20on%20reconfigurable%20logic%202018-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20Jiang%20Fraser%2C%20Nicholas%20J.%20Gambardella%2C%20Giulio%20Blott%2C%20Michaela%20Accuracy%20to%20throughput%20trade-offs%20for%20reduced%20precision%20neural%20networks%20on%20reconfigurable%20logic%202018-05"
        },
        {
            "id": "Sze_et+al_2017_a",
            "entry": "Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. Efficient processing of deep neural networks: A tutorial and survey. arXiv preprint arXiv:1703.09039, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09039"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, and Deming Chen. Design flow of accelerating hybrid extremely low bit-width neural network in embedded fpga. arXiv preprint arXiv:1808.04311, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04311"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074\u20132082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00853"
        },
        {
            "id": "Yarotsky_2017_a",
            "entry": "Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20ReLU%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20ReLU%20networks%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Xiaofan Zhang, Xinheng Liu, Anand Ramachandran, Chuanhao Zhuge, Shibin Tang, Peng Ouyang, Zuofu Cheng, Kyle Rupnow, and Deming Chen. High-performance video content recognition with long-term recurrent convolutional network for fpga. In Field Programmable Logic and Applications (FPL), 2017 27th International Conference on, pp. 1\u20134. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Xiaofan%20Liu%2C%20Xinheng%20Ramachandran%2C%20Anand%20Zhuge%2C%20Chuanhao%20High-performance%20video%20content%20recognition%20with%20long-term%20recurrent%20convolutional%20network%20for%20fpga%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Xiaofan%20Liu%2C%20Xinheng%20Ramachandran%2C%20Anand%20Zhuge%2C%20Chuanhao%20High-performance%20video%20content%20recognition%20with%20long-term%20recurrent%20convolutional%20network%20for%20fpga%202017"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, and Bo Yuan. Theoretical properties for neural networks with weight matrices of low displacement rank. arXiv preprint arXiv:1703.00144, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00144"
        },
        {
            "id": "Zhou_et+al_0000_a",
            "entry": "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03044"
        },
        {
            "id": "Zhou_et+al_0000_b",
            "entry": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quantization for deep neural network. arXiv preprint arXiv:1712.01048, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01048"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        },
        {
            "id": "Zhuang_et+al_2017_a",
            "entry": "Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective lowbitwidth convolutional neural networks. arXiv preprint arXiv:1711.00205, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00205"
        }
    ]
}
