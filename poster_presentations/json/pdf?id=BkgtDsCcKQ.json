{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "FUNCTION SPACE PARTICLE OPTIMIZATION FOR BAYESIAN NEURAL NETWORKS",
        "author": "Ziyu Wang, Tongzheng Ren, Jun Zhu, Bo Zhang Department of Computer Science & Technology, Institute for Artificial Intelligence, State Key Lab for Intell. Tech. & Sys., BNRist Center, THBI Lab, Tsinghua University {wzy,rtz19970824}@gmail.com, {dcszj,dcszb}@tsinghua.edu.cn",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkgtDsCcKQ"
        },
        "abstract": "While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and overparameterized nature. Recently, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as they have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning."
    },
    "keywords": [
        {
            "term": "repulsive force",
            "url": "https://en.wikipedia.org/wiki/repulsive_force"
        },
        {
            "term": "maximum likelihood estimation",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimation"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "gradient flow",
            "url": "https://en.wikipedia.org/wiki/gradient_flow"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "conditional distribution",
            "url": "https://en.wikipedia.org/wiki/conditional_distribution"
        },
        {
            "term": "kernel density estimation",
            "url": "https://en.wikipedia.org/wiki/kernel_density_estimation"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        }
    ],
    "abbreviations": {
        "BNNs": "Bayesian Neural Networks",
        "VI": "Variational inference",
        "POVI": "Particle-Optimization based Variational Inference",
        "SVGD": "Stein variational gradient descent",
        "MAP": "maximum a posteriori",
        "GD": "gradient descent",
        "MLE": "maximum likelihood estimation",
        "RF": "repulsive force",
        "GF": "gradient flow",
        "KDE": "kernel density estimation",
        "GP": "Gaussian process",
        "VIP": "variational implicit process",
        "I-FGSM": "iterative fast gradient sign method"
    },
    "highlights": [
        "Bayesian nerual networks (BNNs) provide a principled approach to reasoning about the epistemic uncertainty\u2014uncertainty in model prediction due to the lack of knowledge",
        "Let f (\u00b7; \u03b8) : X \u2192 repulsive force denote a mapping function parameterized by a neural network, where F will be clear according to the task",
        "Our method is built on the insight that when we model with Bayesian Neural Networks, there exists a map from the network weights \u03b8 to a corresponding regression function f , \u03b8 \u2192 f (\u00b7; \u03b8), and the prior on \u03b8 implicitly defines a prior measure on the space of f , denoted as p(f )",
        "We only present results implemented with Stein variational gradient descent for brevity; results using other Particle-Optimization based Variational Inference methods are similar, and can be found in Appendix A.1 and A.2.2",
        "We found that such pathology exists in all weight-space Particle-Optimization based Variational Inference methods, and amplifies as model complexity increases; eventually, all weight-space methods yield degenerated posteriors concentrating on a single function",
        "We present a flexible approximate inference method for Bayesian regression models, building upon particle-optimization based variational inference procedures"
    ],
    "key_statements": [
        "Bayesian nerual networks (BNNs) provide a principled approach to reasoning about the epistemic uncertainty\u2014uncertainty in model prediction due to the lack of knowledge",
        "Recent work has demonstrated the potential of Bayesian Neural Networks in safety-critical applications like medicine and autonomous driving, deep reinforcement learning, and defense against adversarial samples",
        "These limitations have motivated the recent development of implicit Variational inference methods (<a class=\"ref-link\" id=\"cLi_2018_a\" href=\"#rLi_2018_a\">Li & Turner, 2018</a>; <a class=\"ref-link\" id=\"cShi_et+al_2018_b\" href=\"#rShi_et+al_2018_b\">Shi et al, 2018b</a>), which allow the use of flexible approximate distributions without a tractable density",
        "Particle-Optimization based Variational Inference methods iteratively update a set of particles, so that the corresponding empirical probability measure approximates the target posterior well",
        "One may hope to alleviate such a problem by switching to other Particle-Optimization based Variational Inference methods that could be more suitable for Bayesian Neural Networks inference; this is not the case: Bayesian Neural Networks is over-parameterized, and there exist a large number of local modes in the weight-space posterior that are distant from each other, yet corresponding to the same regression function",
        "To address the above issue, we propose to perform Particle-Optimization based Variational Inference directly for the posterior of regression functions, i.e. the function-space posterior, instead for the weight-space posterior",
        "In Section 3 we present our algorithm for function-space Particle-Optimization based Variational Inference",
        "Let f (\u00b7; \u03b8) : X \u2192 repulsive force denote a mapping function parameterized by a neural network, where F will be clear according to the task",
        "To address the above degeneracy issue of existing Particle-Optimization based Variational Inference methods, we present a new perspective as well as a simple yet efficient algorithm to perform posterior inference in the space of regression functions, rather than in the space of weights",
        "Our method is built on the insight that when we model with Bayesian Neural Networks, there exists a map from the network weights \u03b8 to a corresponding regression function f , \u03b8 \u2192 f (\u00b7; \u03b8), and the prior on \u03b8 implicitly defines a prior measure on the space of f , denoted as p(f )",
        "We refer to this algorithm as the exact version of function-space Particle-Optimization based Variational Inference, even though the posterior is still approximated by particles",
        "Algorithm 1 Function Space Particle-Optimization based Variational Inference for Bayesian Neural Network\n1: Input: (Possibly approximated) function-space prior p(f (x)) for any finite x; training set (X, Y); a continuous distribution \u03bd supported on X; a choice of v from Table 1; batch size B, B ; and a set of initial particles {\u03b80i }ni=1.\n2: Output: A set of particles {\u03b8i}in=1, such that f (\u00b7; \u03b8i) approximates the target distribution. 3: for iteration do 4: Sample a mini-batch xb, yb from the training set, and x1...B\u2212B i.\u223ci.d. \u03bd",
        "We present comparisons to variational implicit process in Appendix A.2.3",
        "We only present results implemented with Stein variational gradient descent for brevity; results using other Particle-Optimization based Variational Inference methods are similar, and can be found in Appendix A.1 and A.2.2",
        "To evaluate the approximation quality of our method qualitatively, and to demonstrate the curseof-dimensionality problem encountered by weight space Particle-Optimization based Variational Inference methods, we first experiment on a simulated dataset",
        "We found that such pathology exists in all weight-space Particle-Optimization based Variational Inference methods, and amplifies as model complexity increases; eventually, all weight-space methods yield degenerated posteriors concentrating on a single function",
        "PREDICTIVE PERFORMANCE Following previous work on Bayesian neural networks (e.g. Hernandez-Lobato & Adams, 2015), we evaluate the predictive performance of our method on two sets of real-world datasets: a number of UCI datasets for real-valued regression, and the MNIST dataset for classification.\n5.2.1",
        "We present a flexible approximate inference method for Bayesian regression models, building upon particle-optimization based variational inference procedures"
    ],
    "summary": [
        "Bayesian nerual networks (BNNs) provide a principled approach to reasoning about the epistemic uncertainty\u2014uncertainty in model prediction due to the lack of knowledge.",
        "POVI methods iteratively update a set of particles, so that the corresponding empirical probability measure approximates the target posterior well.",
        "One may hope to alleviate such a problem by switching to other POVI methods that could be more suitable for BNN inference; this is not the case: BNN is over-parameterized, and there exist a large number of local modes in the weight-space posterior that are distant from each other, yet corresponding to the same regression function.",
        "POVI methods (<a class=\"ref-link\" id=\"cLiu_2016_a\" href=\"#rLiu_2016_a\">Liu & Wang, 2016</a>; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a>) view the approximate inference task as minimizing some energy functionals over probability measures, which obtain their minimum at the true posterior.",
        "To address the above degeneracy issue of existing POVI methods, we present a new perspective as well as a simple yet efficient algorithm to perform posterior inference in the space of regression functions, rather than in the space of weights.",
        "Our method is built on the insight that when we model with BNNs, there exists a map from the network weights \u03b8 to a corresponding regression function f , \u03b8 \u2192 f (\u00b7; \u03b8), and the prior on \u03b8 implicitly defines a prior measure on the space of f , denoted as p(f ).",
        "We refer to this algorithm as the exact version of function-space POVI, even though the posterior is still approximated by particles.",
        "We address this issue by approximating function-space particles in a weight space, and presenting a mini-batch version of the update rule.",
        "As discussed in Section 2, commonly used weight-space kernels cannot characterize the distance between model predictions in over-parameterized models like BNNs. In contrary, the function-space RF in our method directly accounts for the difference between model predictions, and is far more efficient.",
        "1: Input: (Possibly approximated) function-space prior p(f (x)) for any finite x; training set (X, Y); a continuous distribution \u03bd supported on X; a choice of v from Table 1; batch size B, B ; and a set of initial particles {\u03b80i }ni=1.",
        "To evaluate the approximation quality of our method qualitatively, and to demonstrate the curseof-dimensionality problem encountered by weight space POVI methods, we first experiment on a simulated dataset.",
        "We use 50 particles for weight space SVGD and our method, and use Hamiltonian same model, i.e., they correspond to the same likelihood function.",
        "We present a flexible approximate inference method for Bayesian regression models, building upon particle-optimization based variational inference procedures."
    ],
    "headline": "We propose to solve this issue by performing particle optimization directly in the space of regression functions",
    "reference_links": [
        {
            "id": "Amari_1997_a",
            "entry": "Shun-ichi Amari. Neural learning in structured parameter spaces-natural riemannian gradient. In Advances in neural information processing systems, pp. 127\u2013133, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-ichi%20Neural%20learning%20in%20structured%20parameter%20spaces-natural%20riemannian%20gradient%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-ichi%20Neural%20learning%20in%20structured%20parameter%20spaces-natural%20riemannian%20gradient%201997"
        },
        {
            "id": "Amari_2016_a",
            "entry": "Shun-ichi Amari. Information geometry and its applications. Springer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-ichi%20Information%20geometry%20and%20its%20applications%202016"
        },
        {
            "id": "Ambrosio_et+al_2008_a",
            "entry": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ambrosio%2C%20Luigi%20Gigli%2C%20Nicola%20Savare%2C%20Giuseppe%20Gradient%20flows%3A%20in%20metric%20spaces%20and%20in%20the%20space%20of%20probability%20measures%202008"
        },
        {
            "id": "Azizpour_et+al_2018_a",
            "entry": "Hossein Azizpour, Mattias Teye, and Kevin Smith. Bayesian uncertainty estimation for batch normalized deep networks. In International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizpour%2C%20Hossein%20Teye%2C%20Mattias%20Smith%2C%20Kevin%20Bayesian%20uncertainty%20estimation%20for%20batch%20normalized%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azizpour%2C%20Hossein%20Teye%2C%20Mattias%20Smith%2C%20Kevin%20Bayesian%20uncertainty%20estimation%20for%20batch%20normalized%20deep%20networks%202018"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pp. 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015"
        },
        {
            "id": "Bonnabel_2013_a",
            "entry": "Silvere Bonnabel et al. Stochastic gradient descent on riemannian manifolds. IEEE Trans. Automat. Contr., 58(9):2217\u20132229, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonnabel%2C%20Silvere%20Stochastic%20gradient%20descent%20on%20riemannian%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonnabel%2C%20Silvere%20Stochastic%20gradient%20descent%20on%20riemannian%20manifolds%202013"
        },
        {
            "id": "Chapelle_2011_a",
            "entry": "Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pp. 2249\u20132257, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particleoptimization framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11659"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In International Conference on Machine Learning, pp. 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014"
        },
        {
            "id": "Cutajar_et+al_2016_a",
            "entry": "Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature expansions for deep gaussian processes. arXiv preprint arXiv:1610.04386, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04386"
        },
        {
            "id": "De_et+al_2018_a",
            "entry": "Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= H1-nGgWC-.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20G.%20Matthews%2C%20Alexander%20G.%20Hron%2C%20Jiri%20Rowland%2C%20Mark%20Turner%2C%20Richard%20E.%20Gaussian%20process%20behaviour%20in%20wide%20deep%20neural%20networks%202018"
        },
        {
            "id": "Dong_et+al_2018_a",
            "entry": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9185\u20139193, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Yinpeng%20Liao%2C%20Fangzhou%20Pang%2C%20Tianyu%20Su%2C%20Hang%20Boosting%20adversarial%20attacks%20with%20momentum%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Yinpeng%20Liao%2C%20Fangzhou%20Pang%2C%20Tianyu%20Su%2C%20Hang%20Boosting%20adversarial%20attacks%20with%20momentum%202018"
        },
        {
            "id": "Feng_et+al_2018_a",
            "entry": "Di Feng, Lars Rosenbaum, and Klaus Dietmayer. Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection. arXiv preprint arXiv:1804.05132, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05132"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "Ganin_2015_a",
            "entry": "Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1180\u20131189, Lille, France, 07\u201309 Jul 2015. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Lempitsky%2C%20Victor%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Lempitsky%2C%20Victor%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015-07"
        },
        {
            "id": "Garriga-Alonso_et+al_2018_a",
            "entry": "Adria Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional networks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05587"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Structured variational learning of Bayesian neural networks with horseshoe priors. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1744\u20131753, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/ghosh18a.html.",
            "url": "http://proceedings.mlr.press/v80/ghosh18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Soumya%20Yao%2C%20Jiayu%20Doshi-Velez%2C%20Finale%20Structured%20variational%20learning%20of%20Bayesian%20neural%20networks%20with%20horseshoe%20priors%202018-07"
        },
        {
            "id": "Girolami_2011_a",
            "entry": "Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2): 123\u2013214, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girolami%2C%20Mark%20Calderhead%2C%20Ben%20Riemann%20manifold%20langevin%20and%20hamiltonian%20monte%20carlo%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girolami%2C%20Mark%20Calderhead%2C%20Ben%20Riemann%20manifold%20langevin%20and%20hamiltonian%20monte%20carlo%20methods%202011"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Hernandez-Lobato_0000_a",
            "entry": "Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861\u2013 1869, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Lobato%2C%20Jose%20Miguel%20Adams%2C%20Ryan%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20bayesian%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Lobato%2C%20Jose%20Miguel%20Adams%2C%20Ryan%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20bayesian%20neural%20networks"
        },
        {
            "id": "Hron_et+al_2018_a",
            "entry": "Jiri Hron, Alex Matthews, and Zoubin Ghahramani. Variational Bayesian dropout: pitfalls and fixes. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2019\u20132028, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/hron18a.html.",
            "url": "http://proceedings.mlr.press/v80/hron18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hron%2C%20Jiri%20Matthews%2C%20Alex%20Ghahramani%2C%20Zoubin%20Variational%20Bayesian%20dropout%3A%20pitfalls%20and%20fixes%202018-07"
        },
        {
            "id": "Khan_et+al_2018_a",
            "entry": "Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint arXiv:1806.04854, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04854"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= B1EA-M-0Z.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jaehoon%20Sohl-dickstein%2C%20Jascha%20Pennington%2C%20Jeffrey%20Novak%2C%20Roman%20Deep%20neural%20networks%20as%20gaussian%20processes%202018"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Chunyuan Li, Changyou Chen, David E Carlson, and Lawrence Carin. Preconditioned stochastic gradient langevin dynamics for deep neural networks. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carlson%2C%20David%20E.%20Carin%2C%20Lawrence%20Preconditioned%20stochastic%20gradient%20langevin%20dynamics%20for%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carlson%2C%20David%20E.%20Carin%2C%20Lawrence%20Preconditioned%20stochastic%20gradient%20langevin%20dynamics%20for%20deep%20neural%20networks%202016"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. arXiv preprint arXiv:1703.02914, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02914"
        },
        {
            "id": "Li_2018_a",
            "entry": "Yingzhen Li and Richard E. Turner. Gradient estimators for implicit models. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SJi9WOeRb.",
            "url": "https://openreview.net/forum?id=SJi9WOeRb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yingzhen%20Turner%2C%20Richard%20E.%20Gradient%20estimators%20for%20implicit%20models%202018"
        },
        {
            "id": "Liao_et+al_2017_a",
            "entry": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against adversarial attacks using high-level representation guided denoiser. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liao%2C%20Fangzhou%20Liang%2C%20Ming%20Dong%2C%20Yinpeng%20Pang%2C%20Tianyu%20Defense%20against%20adversarial%20attacks%20using%20high-level%20representation%20guided%20denoiser%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liao%2C%20Fangzhou%20Liang%2C%20Ming%20Dong%2C%20Yinpeng%20Pang%2C%20Tianyu%20Defense%20against%20adversarial%20attacks%20using%20high-level%20representation%20guided%20denoiser%202017"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, Jun Zhu, and Lawrence Carin. Accelerated first-order methods on the wasserstein space for bayesian inference. arXiv preprint arXiv:1807.01750, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01750"
        },
        {
            "id": "Liu_2017_a",
            "entry": "Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information Processing Systems 30, pp. 3115\u20133123. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017"
        },
        {
            "id": "Liu_2016_a",
            "entry": "Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems 29, pp. 2378\u20132386. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20bayesian%20inference%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20bayesian%20inference%20algorithm%202016"
        },
        {
            "id": "Liu_et+al_2017_b",
            "entry": "Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. arXiv preprint arXiv:1704.02399, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02399"
        },
        {
            "id": "Louizos_2016_a",
            "entry": "Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix gaussian posteriors. In International Conference on Machine Learning, pp. 1708\u20131716, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%202016"
        },
        {
            "id": "Louizos_2017_a",
            "entry": "Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural networks. In International Conference on Machine Learning, pp. 2218\u20132227, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Multiplicative%20normalizing%20flows%20for%20variational%20bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Multiplicative%20normalizing%20flows%20for%20variational%20bayesian%20neural%20networks%202017"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Chao Ma, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Variational implicit processes. arXiv preprint arXiv:1806.02390, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02390"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Novak_et+al_2019_a",
            "entry": "Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1g30j0qF7.",
            "url": "https://openreview.net/forum?id=B1g30j0qF7",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20Roman%20Xiao%2C%20Lechao%20Bahri%2C%20Yasaman%20Lee%2C%20Jaehoon%20Bayesian%20deep%20convolutional%20networks%20with%20many%20channels%20are%20gaussian%20processes%202019"
        },
        {
            "id": "Ombach_2012_a",
            "entry": "Jerzy Ombach and Dawid Tar\u0142owski. Nonautonomous stochastic search in global optimization. Journal of Nonlinear Science, 22(2):169\u2013185, Apr 2012. ISSN 1432-1467. doi: 10.1007/ s00332-011-9112-3. URL https://doi.org/10.1007/s00332-011-9112-3.",
            "crossref": "https://dx.doi.org/10.1007/s00332-011-9112-3",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s00332-011-9112-3"
        },
        {
            "id": "Opitz_1999_a",
            "entry": "David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of artificial intelligence research, 11:169\u2013198, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opitz%2C%20David%20Maclin%2C%20Richard%20Popular%20ensemble%20methods%3A%20An%20empirical%20study%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opitz%2C%20David%20Maclin%2C%20Richard%20Popular%20ensemble%20methods%3A%20An%20empirical%20study%201999"
        },
        {
            "id": "Pang_et+al_2018_a",
            "entry": "Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 4584\u20134594. Curran Associates, Inc., 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20Tianyu%20Du%2C%20Chao%20Dong%2C%20Yinpeng%20Zhu%2C%20Jun%20Towards%20robust%20detection%20of%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20Tianyu%20Du%2C%20Chao%20Dong%2C%20Yinpeng%20Zhu%2C%20Jun%20Towards%20robust%20detection%20of%20adversarial%20examples%202018"
        },
        {
            "id": "Pang_et+al_2018_b",
            "entry": "Tianyu Pang, Chao Du, and Jun Zhu. Max-Mahalanobis linear discriminant analysis networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4016\u20134025, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018b. PMLR. URL http://proceedings.mlr.press/v80/pang18a/pang18a.pdf.",
            "url": "http://proceedings.mlr.press/v80/pang18a/pang18a.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20Tianyu%20Du%2C%20Chao%20Zhu%2C%20Jun%20Max-Mahalanobis%20linear%20discriminant%20analysis%20networks%202018-07"
        },
        {
            "id": "Rawat_et+al_2017_a",
            "entry": "A. Rawat, M. Wistuba, and M.-I. Nicolae. Adversarial Phenomenon in the Eyes of Bayesian Deep Learning. ArXiv e-prints, November 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rawat%2C%20A.%20Wistuba%2C%20M.%20Nicolae%2C%20M.-I.%20Adversarial%20Phenomenon%20in%20the%20Eyes%20of%20Bayesian%20Deep%20Learning.%20ArXiv%20e-prints%202017-11"
        },
        {
            "id": "Riquelme_et+al_2018_a",
            "entry": "Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= SyYe6k-CW.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riquelme%2C%20Carlos%20Tucker%2C%20George%20Snoek%2C%20Jasper%20Deep%20bayesian%20bandits%20showdown%3A%20An%20empirical%20comparison%20of%20bayesian%20deep%20networks%20for%20thompson%20sampling%202018"
        },
        {
            "id": "Schlimmer_1981_a",
            "entry": "Jeff Schlimmer. Mushroom records drawn from the audubon society field guide to north american mushrooms. GH Lincoff (Pres), New York, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlimmer%2C%20Jeff%20Mushroom%20records%20drawn%20from%20the%20audubon%20society%20field%20guide%20to%20north%20american%20mushrooms.%20GH%20Lincoff%20%28Pres%29%201981"
        },
        {
            "id": "Shi_et+al_2017_a",
            "entry": "Jiaxin Shi, Jianfei. Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong Gu, and Yuhao Zhou. ZhuSuan: A library for Bayesian deep learning. arXiv preprint arXiv:1709.05870, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.05870"
        },
        {
            "id": "Shi_et+al_2018_a",
            "entry": "Jiaxin Shi, Shengyang Sun, and Jun Zhu. A spectral approach to gradient estimation for implicit distributions. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, pp. 4644\u20134653, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20spectral%20approach%20to%20gradient%20estimation%20for%20implicit%20distributions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20A%20spectral%20approach%20to%20gradient%20estimation%20for%20implicit%20distributions%202018"
        },
        {
            "id": "Shi_et+al_2018_b",
            "entry": "Jiaxin Shi, Shengyang Sun, and Jun Zhu. Kernel implicit variational inference. In International Conference on Learning Representations, 2018b. URL https://openreview.net/forum?id=r1l4eQW0Z.",
            "url": "https://openreview.net/forum?id=r1l4eQW0Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Jiaxin%20Sun%2C%20Shengyang%20Zhu%2C%20Jun%20Kernel%20implicit%20variational%20inference%202018"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detection. arXiv preprint arXiv:1803.08533, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08533"
        },
        {
            "id": "Sun_et+al_2017_a",
            "entry": "Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in bayesian neural networks. In Artificial Intelligence and Statistics, pp. 1283\u20131292, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20bayesian%20neural%20networks%202017"
        },
        {
            "id": "Thompson_1933_a",
            "entry": "William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "Trippe_2018_a",
            "entry": "Brian Trippe and Richard Turner. Overpruning in variational bayesian neural networks. arXiv preprint arXiv:1801.06230, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.06230"
        },
        {
            "id": "Tripuraneni_et+al_2018_a",
            "entry": "Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I. Jordan. Averaging stochastic gradient descent on Riemannian manifolds. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 650\u2013687. PMLR, 06\u201309 Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tripuraneni%2C%20Nilesh%20Flammarion%2C%20Nicolas%20Bach%2C%20Francis%20Jordan%2C%20Michael%20I.%20Averaging%20stochastic%20gradient%20descent%20on%20Riemannian%20manifolds%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tripuraneni%2C%20Nilesh%20Flammarion%2C%20Nicolas%20Bach%2C%20Francis%20Jordan%2C%20Michael%20I.%20Averaging%20stochastic%20gradient%20descent%20on%20Riemannian%20manifolds%202018-07"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical models. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5219\u20135227, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/wang18l.html.",
            "url": "http://proceedings.mlr.press/v80/wang18l.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Dilin%20Zeng%2C%20Zhe%20Liu%2C%20Qiang%20Stein%20variational%20message%20passing%20for%20continuous%20graphical%20models%202018-07"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as variational inference. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5852\u20135861, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018a. PMLR. URL http://proceedings.mlr.press/v80/zhang18l.html.",
            "url": "http://proceedings.mlr.press/v80/zhang18l.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Guodong%20Sun%2C%20Shengyang%20Duvenaud%2C%20David%20Grosse%2C%20Roger%20Noisy%20natural%20gradient%20as%20variational%20inference%202018-07"
        },
        {
            "id": "Zhang_et+al_2018_b",
            "entry": "Ruiyi Zhang, Chunyuan Li, Changyou Chen, and Lawrence Carin. Learning structural weight uncertainty for sequential decision-making. In International Conference on Artificial Intelligence and Statistics, pp. 1137\u20131146, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Ruiyi%20Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structural%20weight%20uncertainty%20for%20sequential%20decision-making%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Ruiyi%20Li%2C%20Chunyuan%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structural%20weight%20uncertainty%20for%20sequential%20decision-making%202018"
        },
        {
            "id": "Zhuo_et+al_2018_a",
            "entry": "Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing stein variational gradient descent. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, pp. 6018\u20136027, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhuo%2C%20Jingwei%20Liu%2C%20Chang%20Shi%2C%20Jiaxin%20Zhu%2C%20Jun%20Message%20passing%20stein%20variational%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhuo%2C%20Jingwei%20Liu%2C%20Chang%20Shi%2C%20Jiaxin%20Zhu%2C%20Jun%20Message%20passing%20stein%20variational%20gradient%20descent%202018"
        }
    ]
}
