{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "RELGAN: RELATIONAL GENERATIVE ADVERSARIAL NETWORKS FOR TEXT GENERATION",
        "author": "Weili Nie, Rice University wn,@rice.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJedV3R5tm"
        },
        "abstract": "Generative adversarial networks (GANs) have achieved great success at generating realistic images. However, the text generation still remains a challenging task for modern GAN architectures. In this work, we propose RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates. Our experiments show that RelGAN outperforms current state-of-the-art models in terms of sample quality and diversity, and we also reveal via ablation studies that each component of RelGAN contributes critically to its performance improvements. Moreover, a key advantage of our method, that distinguishes it from other GANs, is the ability to control the trade-off between sample quality and diversity via the use of a single adjustable parameter. Finally, RelGAN is the first architecture that makes GANs with Gumbel-Softmax relaxation succeed in generating realistic text."
    },
    "keywords": [
        {
            "term": "text generation",
            "url": "https://en.wikipedia.org/wiki/text_generation"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "maximum likelihood estimation",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimation"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "long distance",
            "url": "https://en.wikipedia.org/wiki/long_distance"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial networks",
        "RL": "reinforcement learning",
        "MLE": "maximum likelihood estimation",
        "MLP": "multi-layer perceptron",
        "RSGAN": "Relativistic standard GAN"
    },
    "highlights": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) were originally designed to generate continuous data and have achieved a lot of success at generating continuous samples, such as images",
        "We propose a new Generative adversarial networks architecture \u2013 Relational Generative adversarial networks (RelGAN), whose design is motivated by the issues identified above",
        "The RelGAN architecture mainly consists of three parts: 1) a relational memory (<a class=\"ref-link\" id=\"cSantoro_et+al_2018_a\" href=\"#rSantoro_et+al_2018_a\">Santoro et al, 2018</a>) based generator, which promises more expressive power and better ability of modeling longer-range dependencies in text; 2) Gumbel-Softmax relaxation (<a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a>; Maddison et al, 2016) for training Generative adversarial networks on discrete data, which simplifies our model, enabling us to stay within a classical Generative adversarial networks framework without intensive reinforcement learning heuristics; 3) multiple embedded representations in the discriminator, enabling a more diverse and informative signal for the generator updates",
        "We proposed a new Generative adversarial networks architecture called RelGAN for text generation, that outperforms most current models in terms of sample quality and diversity on both synthetic and real data",
        "By applying Gumbel-Softmax relaxation to deal with the non-differentiable issue, our architecture is simple to implement without employing intensive reinforcement learning heuristics",
        "Since we have demonstrated that Generative adversarial networks with Gumbel-Softmax relaxation is very promising for text generation, we would like to explore further in this direction"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) were originally designed to generate continuous data and have achieved a lot of success at generating continuous samples, such as images",
        "We propose a new Generative adversarial networks architecture \u2013 Relational Generative adversarial networks (RelGAN), whose design is motivated by the issues identified above",
        "The RelGAN architecture mainly consists of three parts: 1) a relational memory (<a class=\"ref-link\" id=\"cSantoro_et+al_2018_a\" href=\"#rSantoro_et+al_2018_a\">Santoro et al, 2018</a>) based generator, which promises more expressive power and better ability of modeling longer-range dependencies in text; 2) Gumbel-Softmax relaxation (<a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a>; Maddison et al, 2016) for training Generative adversarial networks on discrete data, which simplifies our model, enabling us to stay within a classical Generative adversarial networks framework without intensive reinforcement learning heuristics; 3) multiple embedded representations in the discriminator, enabling a more diverse and informative signal for the generator updates",
        "We experimentally demonstrate that RelGAN outperforms most current models in terms of sample quality and diversity",
        "We show via ablation studies that each part of RelGAN plays an important role in its performance improvements",
        "We evaluate the trade-off between sample quality and diversity as a function of the maximum inverse temperature \u03b2max and the results are shown in Figure 3",
        "The human score results are provided in Table 4, where we can see that RelAGN generates better human-looking samples than other Generative adversarial networks and the maximum likelihood estimation baseline model.\n3.4",
        "To show the impact of relational memory in RelGAN, we propose to replace relational memory by LSTM-32 and LSTM-512 as the generator architecture, respectively, and see how the performance differs",
        "To show the impact of Gumbel-Softmax relaxation in RelGAN, we can instead apply the vanilla REINFORCE method to deal with the non-differentiable issue of RelGAN on text generation",
        "We proposed a new Generative adversarial networks architecture called RelGAN for text generation, that outperforms most current models in terms of sample quality and diversity on both synthetic and real data",
        "By applying Gumbel-Softmax relaxation to deal with the non-differentiable issue, our architecture is simple to implement without employing intensive reinforcement learning heuristics",
        "Since we have demonstrated that Generative adversarial networks with Gumbel-Softmax relaxation is very promising for text generation, we would like to explore further in this direction"
    ],
    "summary": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) were originally designed to generate continuous data and have achieved a lot of success at generating continuous samples, such as images.",
        "The RelGAN architecture mainly consists of three parts: 1) a relational memory (<a class=\"ref-link\" id=\"cSantoro_et+al_2018_a\" href=\"#rSantoro_et+al_2018_a\">Santoro et al, 2018</a>) based generator, which promises more expressive power and better ability of modeling longer-range dependencies in text; 2) Gumbel-Softmax relaxation (<a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a>; Maddison et al, 2016) for training GANs on discrete data, which simplifies our model, enabling us to stay within a classical GAN framework without intensive RL heuristics; 3) multiple embedded representations in the discriminator, enabling a more diverse and informative signal for the generator updates.",
        "Since NLLoracle cannot be evaluated without an oracle, we instead apply the commonly-used BLEU scores (<a class=\"ref-link\" id=\"cPapineni_et+al_2002_a\" href=\"#rPapineni_et+al_2002_a\">Papineni et al, 2002</a>) to measure the sample quality and compare with the MLE baseline, along with other start-of-the-art GANs, including SeqGAN (<a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a>), RankGAN (<a class=\"ref-link\" id=\"cLin_et+al_2017_a\" href=\"#rLin_et+al_2017_a\">Lin et al, 2017</a>) and LeakGAN (<a class=\"ref-link\" id=\"cGuo_et+al_2017_a\" href=\"#rGuo_et+al_2017_a\">Guo et al, 2017</a>).",
        "We evaluate the trade-off between sample quality and diversity as a function of the maximum inverse temperature \u03b2max and the results are shown in Figure 3.",
        "We can see that RelGAN is significantly and consistently better than other models in terms of all the BLEU scores, which means its ability of generating high-quality sentences of COCO Image Captions.",
        "We can see that RelGAN consistently outperforms previous models in terms of all the BLEU scores, demonstrating its ability of generating high-quality sentences on EMNLP2017 WMT News.",
        "The human score results are provided in Table 4, where we can see that RelAGN generates better human-looking samples than other GANs and the MLE baseline model.",
        "To show the impact of Gumbel-Softmax relaxation in RelGAN, we can instead apply the vanilla REINFORCE method to deal with the non-differentiable issue of RelGAN on text generation.",
        "More <a class=\"ref-link\" id=\"cKusner_2016_a\" href=\"#rKusner_2016_a\">Kusner & Hernandez-Lobato (2016</a>) provides some initial experiments of training GANs with Gumbel-Softmax relaxation on a synthetic task, but scaling them to work on real text dataset remains a challenging open problem.",
        "We proposed a new GAN architecture called RelGAN for text generation, that outperforms most current models in terms of sample quality and diversity on both synthetic and real data.",
        "In RelGAN, we used the relational memory based generator to improve its ability of modeling long distance dependencies and applied multiple embedded representations in discriminator such that it can provide more diverse and informative guiding signal for generator.",
        "Extending RelGAN to a conditional model for many text generation related applications is another interesting direction"
    ],
    "headline": "We propose RelGAN, a new Generative adversarial networks architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training Generative adversarial networks on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal for the generator updates",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "Arjovsky_et+al_2017_b",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Che_et+al_2017_a",
            "entry": "Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07983"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Liqun Chen, Shuyang Dai, Chenyang Tao, Dinghan Shen, Zhe Gan, Haichao Zhang, Yizhe Zhang, and Lawrence Carin. Adversarial text generation via feature-mover\u2019s distance. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Liqun%20Dai%2C%20Shuyang%20Tao%2C%20Chenyang%20Shen%2C%20Dinghan%20Adversarial%20text%20generation%20via%20feature-mover%E2%80%99s%20distance%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Liqun%20Dai%2C%20Shuyang%20Tao%2C%20Chenyang%20Shen%2C%20Dinghan%20Adversarial%20text%20generation%20via%20feature-mover%E2%80%99s%20distance%202018"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00325"
        },
        {
            "id": "Durugkar_et+al_2016_a",
            "entry": "Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv preprint arXiv:1611.01673, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01673"
        },
        {
            "id": "Fedus_et+al_2018_a",
            "entry": "William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via filling in the. arXiv preprint arXiv:1801.07736, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07736"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gu_et+al_2017_a",
            "entry": "Jiatao Gu, Daniel Jiwoong Im, and Victor OK Li. Neural machine translation with gumbel-greedy decoding. arXiv preprint arXiv:1706.07518, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.07518"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767\u20135777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. arXiv preprint arXiv:1709.08624, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08624"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Jolicoeur-Martineau_2018_a",
            "entry": "Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.00734"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.5882"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kusner_2016_a",
            "entry": "Matt J Kusner and Jose Miguel Hernandez-Lobato. Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04051"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Kevin%20Li%2C%20Dianqi%20He%2C%20Xiaodong%20Zhang%2C%20Zhengyou%20Adversarial%20ranking%20for%20language%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Kevin%20Li%2C%20Dianqi%20He%2C%20Xiaodong%20Zhang%2C%20Zhengyou%20Adversarial%20ranking%20for%20language%20generation%202017"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Sidi Lu, Lantao Yu, Weinan Zhang, and Yong Yu. Cot: Cooperative training for generative modeling. arXiv preprint arXiv:1804.03782, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03782"
        },
        {
            "id": "Maddison_2016_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Papineni_et+al_2002_a",
            "entry": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "Parmar_et+al_2018_a",
            "entry": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz Kaiser, Noam Shazeer, and Alexander Ku. Image transformer. arXiv preprint arXiv:1802.05751, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05751"
        },
        {
            "id": "Santoro_et+al_2018_a",
            "entry": "Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural networks. arXiv preprint arXiv:1806.01822, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01822"
        },
        {
            "id": "Semeniuta_et+al_2018_a",
            "entry": "Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04936"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "Theis_et+al_2015_a",
            "entry": "Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01844"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Lantao%20Zhang%2C%20Weinan%20Wang%2C%20Jun%20Yu%2C%20Yong%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient.%20In%20AAAI%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhang_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial feature matching for text generation. In International Conference on Machine Learning, 2017. Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush, and Yann LeCun. Adversarially regularized autoencoders. In International Conference on Machine Learning, 2018. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. arXiv preprint arXiv:1802.01886, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01886"
        },
        {
            "id": "Unless_2014_a",
            "entry": "Unless stated otherwise, for the CNN-based discriminator architecture, we use filter windows of sizes {3,4,5} and 300 feature maps each. For relational memory, we set memory size to be 256, memory slots to be 1, number of heads to be 2. The batch size is set to be 64. For embedding dimensions, we set the embedding dimension of the input token for generator to be 32 and that for discriminator to be 1 with the number of embedded representations S = 64. We use Adam (Kingma & Ba, 2014) with \u03b21 = 0.9 and \u03b22 = 0.999 and gradient clipping is applied if the norm of gradients exceeds 5. We first pre-train the generator via MLE with learning rate of 1e-2 for 150 epochs and then start adversarial training with learning rate of 1e-4 for both discriminator and generator. For adversarial training, we set the maximum number of iterations N = 5000 and we perform 5 gradient descent steps on the discriminator for every step on the generator.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Unless%20stated%20otherwise%20for%20the%20CNNbased%20discriminator%20architecture%20we%20use%20filter%20windows%20of%20sizes%20345%20and%20300%20feature%20maps%20each%20For%20relational%20memory%20we%20set%20memory%20size%20to%20be%20256%20memory%20slots%20to%20be%201%20number%20of%20heads%20to%20be%202%20The%20batch%20size%20is%20set%20to%20be%2064%20For%20embedding%20dimensions%20we%20set%20the%20embedding%20dimension%20of%20the%20input%20token%20for%20generator%20to%20be%2032%20and%20that%20for%20discriminator%20to%20be%201%20with%20the%20number%20of%20embedded%20representations%20S%20%2064%20We%20use%20Adam%20Kingma%20%20Ba%202014%20with%20%CE%B21%20%2009%20and%20%CE%B22%20%200999%20and%20gradient%20clipping%20is%20applied%20if%20the%20norm%20of%20gradients%20exceeds%205%20We%20first%20pretrain%20the%20generator%20via%20MLE%20with%20learning%20rate%20of%201e2%20for%20150%20epochs%20and%20then%20start%20adversarial%20training%20with%20learning%20rate%20of%201e4%20for%20both%20discriminator%20and%20generator%20For%20adversarial%20training%20we%20set%20the%20maximum%20number%20of%20iterations%20N%20%205000%20and%20we%20perform%205%20gradient%20descent%20steps%20on%20the%20discriminator%20for%20every%20step%20on%20the%20generator"
        }
    ]
}
