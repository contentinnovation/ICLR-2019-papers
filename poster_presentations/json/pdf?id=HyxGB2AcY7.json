{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "CONTINGENCY-AWARE EXPLORATION IN REINFORCEMENT LEARNING",
        "author": "Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh,\u2020 Neal Wu, Mohammad Norouzi, Honglak Lee",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyxGB2AcY7"
        },
        "abstract": "This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.1 For example, we report a state-of-the-art score of >11,000 points on MONTEZUMA\u2019S REVENGE without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "high level",
            "url": "https://en.wikipedia.org/wiki/high_level"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "dynamic model",
            "url": "https://en.wikipedia.org/wiki/dynamic_model"
        }
    ],
    "abbreviations": {
        "ALE": "Arcade Learning Environment",
        "ADM": "attentive dynamics model",
        "RL": "reinforcement learning",
        "CTS": "Context-Tree Switching",
        "LSH": "localitysensitive hashing",
        "MLP": "multi-layer perceptron",
        "PPO": "Policy Optimization",
        "ARI": "Adjusted Rand Index"
    },
    "highlights": [
        "The success of reinforcement learning (RL) algorithms in complex environments hinges on the way they balance exploration and exploitation",
        "The key idea that we focus on in this work is the notion of contingency awareness (Watson, 1966; <a class=\"ref-link\" id=\"cBellemare_et+al_2012_a\" href=\"#rBellemare_et+al_2012_a\">Bellemare et al, 2012</a>) \u2014 the agent\u2019s understanding of the environmental dynamics and recognizing that some aspects of the dynamics are under the agent\u2019s control",
        "To discover the region of the observation that is controllable by the agent, we develop an instance of attentive dynamics model (ADM) based on inverse dynamics finv",
        "We proposed a method of providing contingency-awareness through an attentive dynamics model (ADM)",
        "The agent is able to estimate its position in the space and benefits from a compact and informative representation of the world. This idea combined with a variant of countbased exploration achieves strong results in various sparse-reward Atari games",
        "Though in this work we focus mostly on 2D environments in the form of sparse-reward Atari games, we view our presented high-level concept and approach as a stepping stone towards more universal algorithms capable of similar abilities in various reinforcement learning environments"
    ],
    "key_statements": [
        "The success of reinforcement learning (RL) algorithms in complex environments hinges on the way they balance exploration and exploitation",
        "We investigate whether we can learn a complementary, more intuitive, and interpretable high-level abstraction that can be very effective in exploration by using the ideas of contingency awareness and controllable dynamics",
        "The key idea that we focus on in this work is the notion of contingency awareness (Watson, 1966; <a class=\"ref-link\" id=\"cBellemare_et+al_2012_a\" href=\"#rBellemare_et+al_2012_a\">Bellemare et al, 2012</a>) \u2014 the agent\u2019s understanding of the environmental dynamics and recognizing that some aspects of the dynamics are under the agent\u2019s control",
        "We investigate the concept of contingency awareness based on self-localization, i.e., the awareness of where the agent is located in the abstract state space",
        "We report very strong results on sparse-reward Atari games, including the state-of-the-art performance on the notoriously difficult MONTEZUMA\u2019S REVENGE, when combining our proposed exploration strategy with Policy Optimization (Schulman et al, 2017), without using expert demonstrations, explicit high-level information (e.g., RAM states), or resetting the environment to an arbitrary state",
        "We develop a novel instance of attentive dynamics model using contingency and controllable dynamics to provide robust localization abilities across the most challenging Atari environments",
        "To discover the region of the observation that is controllable by the agent, we develop an instance of attentive dynamics model (ADM) based on inverse dynamics finv",
        "EXPERIMENTS WITH A2C We evaluate the proposed exploration strategy on several difficult exploration Atari 2600 games from the Arcade Learning Environment (ALE) (<a class=\"ref-link\" id=\"cBellemare_et+al_2013_a\" href=\"#rBellemare_et+al_2013_a\">Bellemare et al, 2013</a>)",
        "Our proposed technique, which augments A2C with count-based exploration with the location information learned by the attentive dynamics model, denoted A2C+CoEX (CoEX stands for \u201cContingency-aware Exploration\u201d), significantly outperforms the A2C baseline on six out of the 8 games",
        "We argue that using Q-learning as the base learner with DQN-PixelCNN makes the direct comparison with A2C+CoEX not completely adequate",
        "Given an observation in Atari games, we compare the discrete representation based on the embedding from random projection to the ground-truth room number extracted from RAM",
        "This paper investigates whether discovering controllable dynamics via an attentive dynamics model (ADM) can help exploration in challenging sparse-reward environments",
        "We proposed a method of providing contingency-awareness through an attentive dynamics model (ADM)",
        "The agent is able to estimate its position in the space and benefits from a compact and informative representation of the world. This idea combined with a variant of countbased exploration achieves strong results in various sparse-reward Atari games",
        "We report state-of-the-art results of >11,000 points on the infamously challenging MONTEZUMA\u2019S REVENGE without using expert demonstrations or supervision",
        "Though in this work we focus mostly on 2D environments in the form of sparse-reward Atari games, we view our presented high-level concept and approach as a stepping stone towards more universal algorithms capable of similar abilities in various reinforcement learning environments"
    ],
    "summary": [
        "The success of reinforcement learning (RL) algorithms in complex environments hinges on the way they balance exploration and exploitation.",
        "Count-based exploration with neural density estimation (<a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\"><a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\">Bellemare et al, 2016</a></a>; Tang et al, 2017; <a class=\"ref-link\" id=\"cOstrovski_et+al_2017_a\" href=\"#rOstrovski_et+al_2017_a\">Ostrovski et al, 2017</a>) presents one of the state-of-the-art techniques on the most challenging Atari games with sparse rewards.",
        "Inspired by previous work (<a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\"><a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\">Bellemare et al, 2016</a></a>; Tang et al, 2017), we add an exploration bonus of r+ to the environment reward, where r+(s) = 1/ #(\u03c8(s)) and #(\u03c8(s)) denotes the visitation count of the mapped state \u03c8(s), which consists of the contingent region (x, y).",
        "In the experiments below we investigate the following key questions: Does the contingency awareness in terms of self-localization provide a useful state abstraction for exploration?",
        "Our proposed technique, which augments A2C with count-based exploration with the location information learned by the attentive dynamics model, denoted A2C+CoEX (CoEX stands for \u201cContingency-aware Exploration\u201d), significantly outperforms the A2C baseline on six out of the 8 games.",
        "We evaluate A2C+CoEX+RAM\u2217, our contingency-aware exploration method together with the ground-truth location information obtained from game\u2019s RAM.",
        "T\u22121 t =0 rext of the controllable entity in the current observation discovered by ADM (Section 3.1), a context representation c \u2208 Z that denotes the high level visual context, and a cumulative environment reward R \u2208 Z that represents the exploration behavioral state.",
        "As shown in Table 1, provided the representation (x, y, c, R) is perfect, A2C+CoEX+RAM achieves a significant improvement over A2C by encouraging the agent to visit novel locations, and could nearly solve these hard exploration games as training goes on.",
        "A2C+CoEX using representations learned with our proposed attentive dynamics model and observation embedding clustering outperforms the A2C baseline.",
        "On FREEWAY, FROSTBITE, HERO, MONTEZUMA\u2019S REVENGE, QBERT and SEAQUEST, the performance is comparable with A2C+CoEX+RAM, demonstrating the usefulness of the contigency-awareness information discovered by ADM.",
        "4.5 ANALYSIS OF OBSERVATION EMBEDDING CLUSTERING To make the agent aware of a change in high-level visual context in some games such as MONTEZUMA\u2019S REVENGE, VENTURE, HERO, and PRIVATEEYE, we obtain a representation of the high-level context and use it for exploration.",
        "Given an observation in Atari games, we compare the discrete representation based on the embedding from random projection to the ground-truth room number extracted from RAM.",
        "This paper investigates whether discovering controllable dynamics via an attentive dynamics model (ADM) can help exploration in challenging sparse-reward environments.",
        "Though in this work we focus mostly on 2D environments in the form of sparse-reward Atari games, we view our presented high-level concept and approach as a stepping stone towards more universal algorithms capable of similar abilities in various RL environments"
    ],
    "headline": "This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning",
    "reference_links": [
        {
            "id": "Achiam_2017_a",
            "entry": "Joshua Achiam and Shankar Sastry. Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning. arXiv preprint arXiv:1703.01732, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01732"
        },
        {
            "id": "Agrawal_et+al_2016_a",
            "entry": "Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to Poke by Poking: Experiential Learning of Intuitive Physics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20Poke%20by%20Poking%3A%20Experiential%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20Poke%20by%20Poking%3A%20Experiential%20Learning%202016"
        },
        {
            "id": "Baeyens_et+al_1990_a",
            "entry": "Frank Baeyens, Paul Eelen, and Omer van den Bergh. Contingency awareness in evaluative conditioning: A case for unaware affective-evaluative learning. Cognition and emotion, 4(1):3\u201318, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baeyens%2C%20Frank%20Eelen%2C%20Paul%20van%20den%20Bergh%2C%20Omer%20Contingency%20awareness%20in%20evaluative%20conditioning%3A%20A%20case%20for%20unaware%20affective-evaluative%20learning%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baeyens%2C%20Frank%20Eelen%2C%20Paul%20van%20den%20Bergh%2C%20Omer%20Contingency%20awareness%20in%20evaluative%20conditioning%3A%20A%20case%20for%20unaware%20affective-evaluative%20learning%201990"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015"
        },
        {
            "id": "Banino_et+al_2018_a",
            "entry": "Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705):429\u2013433, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banino%2C%20Andrea%20Barry%2C%20Caswell%20Uria%2C%20Benigno%20Blundell%2C%20Charles%20Vector-based%20navigation%20using%20grid-like%20representations%20in%20artificial%20agents%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banino%2C%20Andrea%20Barry%2C%20Caswell%20Uria%2C%20Benigno%20Blundell%2C%20Charles%20Vector-based%20navigation%20using%20grid-like%20representations%20in%20artificial%20agents%202018"
        },
        {
            "id": "Bellemare_et+al_2012_a",
            "entry": "Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating Contingency Awareness Using Atari 2600 Games. In AAAI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Veness%2C%20Joel%20Bowling%2C%20Michael%20Investigating%20Contingency%20Awareness%20Using%20Atari%202600%20Games%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Veness%2C%20Joel%20Bowling%2C%20Michael%20Investigating%20Contingency%20Awareness%20Using%20Atari%202600%20Games%202012"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research 47, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20Evaluation%20Platform%20for%20General%20Agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20Evaluation%20Platform%20for%20General%20Agents%202013"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marc%20G%20Bellemare%20Sriram%20Srinivasan%20Georg%20Ostrovski%20Tom%20Schaul%20David%20Saxton%20and%20Remi%20Munos%20Unifying%20CountBased%20Exploration%20and%20Intrinsic%20Motivation%20In%20NIPS%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marc%20G%20Bellemare%20Sriram%20Srinivasan%20Georg%20Ostrovski%20Tom%20Schaul%20David%20Saxton%20and%20Remi%20Munos%20Unifying%20CountBased%20Exploration%20and%20Intrinsic%20Motivation%20In%20NIPS%202016"
        },
        {
            "id": "Bengio_et+al_2017_a",
            "entry": "Emmanuel Bengio, Valentin Thomas, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently Controllable Features. arXiv preprint arXiv:1703.07718, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07718"
        },
        {
            "id": "Burda_et+al_2018_a",
            "entry": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04355"
        },
        {
            "id": "Burda_et+al_2019_a",
            "entry": "Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In ICLR, 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym.",
            "url": "https://openreview.net/forum?id=H1lJJnR5Ym",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Edwards%2C%20Harrison%20Storkey%2C%20Amos%20Klimov%2C%20Oleg%20Exploration%20by%20random%20network%20distillation%202019"
        },
        {
            "id": "Charikar_2002_a",
            "entry": "Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pp. 380\u2013388. ACM, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charikar%2C%20Moses%20S.%20Similarity%20estimation%20techniques%20from%20rounding%20algorithms%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charikar%2C%20Moses%20S.%20Similarity%20estimation%20techniques%20from%20rounding%20algorithms%202002"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. OpenAI Baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Dilokthanakul_et+al_2017_a",
            "entry": "Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, and Murray Shanahan. Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning. arXiv preprint arXiv:1705.06769, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06769"
        },
        {
            "id": "Durrant-Whyte_2006_a",
            "entry": "Hugh Durrant-Whyte and Tim Bailey. Simultaneous Localization and Mapping: Part I. IEEE robotics & automation magazine, 13(2):99\u2013110, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durrant-Whyte%2C%20Hugh%20Bailey%2C%20Tim%20Simultaneous%20Localization%20and%20Mapping%3A%20Part%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durrant-Whyte%2C%20Hugh%20Bailey%2C%20Tim%20Simultaneous%20Localization%20and%20Mapping%3A%20Part%202006"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is All You Need: Learning Skills without a Reward Function. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eysenbach%2C%20Benjamin%20Gupta%2C%20Abhishek%20Ibarz%2C%20Julian%20Levine%2C%20Sergey%20Diversity%20is%20All%20You%20Need%3A%20Learning%20Skills%20without%20a%20Reward%20Function%202018"
        },
        {
            "id": "Houthooft_et+al_2016_a",
            "entry": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: Variational Information Maximizing Exploration. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rein%20Houthooft%20Xi%20Chen%20Yan%20Duan%20John%20Schulman%20Filip%20De%20Turck%20and%20Pieter%20Abbeel%20VIME%20Variational%20Information%20Maximizing%20Exploration%20In%20NIPS%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rein%20Houthooft%20Xi%20Chen%20Yan%20Duan%20John%20Schulman%20Filip%20De%20Turck%20and%20Pieter%20Abbeel%20VIME%20Variational%20Information%20Maximizing%20Exploration%20In%20NIPS%202016"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20Learning%20with%20Unsupervised%20Auxiliary%20Tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20Learning%20with%20Unsupervised%20Auxiliary%20Tasks%202017"
        },
        {
            "id": "Kulis_2012_a",
            "entry": "Brian Kulis and Michael I. Jordan. Revisiting k-means: New Algorithms via Bayesian Nonparametrics. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulis%2C%20Brian%20Jordan%2C%20Michael%20I.%20Revisiting%20k-means%3A%20New%20Algorithms%20via%20Bayesian%20Nonparametrics%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulis%2C%20Brian%20Jordan%2C%20Michael%20I.%20Revisiting%20k-means%3A%20New%20Algorithms%20via%20Bayesian%20Nonparametrics%202012"
        },
        {
            "id": "Machado_et+al_2017_a",
            "entry": "Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523\u2013562, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Talvitie%2C%20Erik%20Veness%2C%20Joel%20Revisiting%20the%20arcade%20learning%20environment%3A%20Evaluation%20protocols%20and%20open%20problems%20for%20general%20agents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Machado%2C%20Marlos%20C.%20Bellemare%2C%20Marc%20G.%20Talvitie%2C%20Erik%20Veness%2C%20Joel%20Revisiting%20the%20arcade%20learning%20environment%3A%20Evaluation%20protocols%20and%20open%20problems%20for%20general%20agents%202017"
        },
        {
            "id": "Martin_et+al_2017_a",
            "entry": "Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-Based Exploration in Feature Space for Reinforcement Learning. In IJCAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%2C%20Jarryd%20Sasikumar%2C%20Suraj%20Narayanan%20Everitt%2C%20Tom%20Hutter%2C%20Marcus%20Count-Based%20Exploration%20in%20Feature%20Space%20for%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%2C%20Jarryd%20Sasikumar%2C%20Suraj%20Narayanan%20Everitt%2C%20Tom%20Hutter%2C%20Marcus%20Count-Based%20Exploration%20in%20Feature%20Space%20for%20Reinforcement%20Learning%202017"
        },
        {
            "id": "Martins_2016_a",
            "entry": "Andr\u00e9 F T Martins and Ram\u00f3n Fernandez Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martins%2C%20Andr%C3%A9%20F.T.%20Astudillo%2C%20Ram%C3%B3n%20Fernandez%20From%20Softmax%20to%20Sparsemax%3A%20A%20Sparse%20Model%20of%20Attention%20and%20Multi-Label%20Classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martins%2C%20Andr%C3%A9%20F.T.%20Astudillo%2C%20Ram%C3%B3n%20Fernandez%20From%20Softmax%20to%20Sparsemax%3A%20A%20Sparse%20Model%20of%20Attention%20and%20Multi-Label%20Classification%202016"
        },
        {
            "id": "Mirowski_et+al_2017_a",
            "entry": "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Hadsell. Learning to Navigate in Complex Environments. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Piotr%20Mirowski%20Razvan%20Pascanu%20Fabio%20Viola%20Hubert%20Soyer%20Andrew%20J%20Ballard%20Andrea%20Banino%20Misha%20Denil%20Ross%20Goroshin%20Laurent%20Sifre%20Koray%20Kavukcuoglu%20Dharshan%20Kumaran%20and%20Raia%20Hadsell%20Learning%20to%20Navigate%20in%20Complex%20Environments%20In%20ICLR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Piotr%20Mirowski%20Razvan%20Pascanu%20Fabio%20Viola%20Hubert%20Soyer%20Andrew%20J%20Ballard%20Andrea%20Banino%20Misha%20Denil%20Ross%20Goroshin%20Laurent%20Sifre%20Koray%20Kavukcuoglu%20Dharshan%20Kumaran%20and%20Raia%20Hadsell%20Learning%20to%20Navigate%20in%20Complex%20Environments%20In%20ICLR%202017"
        },
        {
            "id": "Mirowski_et+al_2018_a",
            "entry": "Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, and Raia Hadsell. Learning to Navigate in Cities Without a Map. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirowski%2C%20Piotr%20Grimes%2C%20Matthew%20Koichi%20Malinowski%2C%20Mateusz%20Hermann%2C%20Karl%20Moritz%20Learning%20to%20Navigate%20in%20Cities%20Without%20a%20Map%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirowski%2C%20Piotr%20Grimes%2C%20Matthew%20Koichi%20Malinowski%2C%20Mateusz%20Hermann%2C%20Karl%20Moritz%20Learning%20to%20Navigate%20in%20Cities%20Without%20a%20Map%202018"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016"
        },
        {
            "id": "Moser_et+al_2015_a",
            "entry": "May-Britt Moser, David Rowland, and Edvard I Moser. Place Cells, Grid Cells, and Memory. Cold Spring Harbor perspectives in medicine, 5, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moser%2C%20May-Britt%20Rowland%2C%20David%20Moser%2C%20Edvard%20I.%20Place%20Cells%2C%20Grid%20Cells%2C%20and%20Memory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moser%2C%20May-Britt%20Rowland%2C%20David%20Moser%2C%20Edvard%20I.%20Place%20Cells%2C%20Grid%20Cells%2C%20and%20Memory%202015"
        },
        {
            "id": "Oh_et+al_2015_a",
            "entry": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder P Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Action-Conditional%20Video%20Prediction%20using%20Deep%20Networks%20in%20Atari%20Games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Action-Conditional%20Video%20Prediction%20using%20Deep%20Networks%20in%20Atari%20Games%202015"
        },
        {
            "id": "Osband_et+al_2016_a",
            "entry": "Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep Exploration via Bootstrapped DQN. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20Exploration%20via%20Bootstrapped%20DQN%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20Exploration%20via%20Bootstrapped%20DQN%202016"
        },
        {
            "id": "Ostrovski_et+al_2017_a",
            "entry": "Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-Based Exploration with Neural Density Models. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Georg%20Ostrovski%20Marc%20G%20Bellemare%20Aaron%20van%20den%20Oord%20and%20Remi%20Munos%20CountBased%20Exploration%20with%20Neural%20Density%20Models%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Georg%20Ostrovski%20Marc%20G%20Bellemare%20Aaron%20van%20den%20Oord%20and%20Remi%20Munos%20CountBased%20Exploration%20with%20Neural%20Density%20Models%20In%20ICML%202017"
        },
        {
            "id": "Oudeyer_2009_a",
            "entry": "Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational approaches. Frontiers in Neurorobotics, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20A%20typology%20of%20computational%20approaches%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20A%20typology%20of%20computational%20approaches%202009"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven Exploration by Self-supervised Prediction. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction%202017"
        },
        {
            "id": "Plappert_et+al_2018_a",
            "entry": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter Space Noise for Exploration. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Plappert%2C%20Matthias%20Houthooft%2C%20Rein%20Dhariwal%2C%20Prafulla%20Sidor%2C%20Szymon%20Parameter%20Space%20Noise%20for%20Exploration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Plappert%2C%20Matthias%20Houthooft%2C%20Rein%20Dhariwal%2C%20Prafulla%20Sidor%2C%20Szymon%20Parameter%20Space%20Noise%20for%20Exploration%202018"
        },
        {
            "id": "Rand_1971_a",
            "entry": "William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846\u2013850, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rand%2C%20William%20M.%20Objective%20criteria%20for%20the%20evaluation%20of%20clustering%20methods%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rand%2C%20William%20M.%20Objective%20criteria%20for%20the%20evaluation%20of%20clustering%20methods%201971"
        },
        {
            "id": "Sawada_2018_a",
            "entry": "Yoshihide Sawada. Disentangling Controllable and Uncontrollable Factors of Variation by Interacting with the World. arXiv preprint arXiv:1804.06955, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06955"
        },
        {
            "id": "Schmidhuber_1991_a",
            "entry": "J\u00fcrgen Schmidhuber. Adaptive confidence and adaptive curiosity. 1991. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own Reward: Self-Supervision for Reinforcement Learning. arXiv preprint arXiv:1612.07307, 2017. Satinder Singh, Nuttapong Chentanez, and Andrew G. Barto. Intrinsically Motivated Reinforcement",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Learning_2008_a",
            "entry": "Learning. In NIPS, 2004. Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8), 2008. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. In NIPS, 2017. John S Watson. The development and generalization of \"contingency awareness\" in early infancy: Some hypotheses. Merrill-Palmer Quarterly of Behavior and Development, 12(2):123\u2013135, 1966. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML, 2015. Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On Learning Intrinsic Rewards for Policy Gradient Methods. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Learning.%20In%20NIPS%2C%202004.%20Alexander%20L.%20Strehl%20Littman%2C%20Michael%20L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Learning.%20In%20NIPS%2C%202004.%20Alexander%20L.%20Strehl%20Littman%2C%20Michael%20L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008"
        }
    ]
}
