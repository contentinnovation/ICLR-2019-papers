{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SNAS: STOCHASTIC NEURAL ARCHITECTURE SEARCH",
        "author": "Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin SenseTime {xiesirui, zhenghehui, liuchunxiao}@sensetime.com linliang@ieee.org",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rylqooRqK7"
        },
        "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical endto-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of backpropagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes fewer epochs to find a cell architecture with state-of-theart accuracy than non-differentiable evolution-based and reinforcement-learningbased NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets."
    },
    "keywords": [
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "directed acyclic graph",
            "url": "https://en.wikipedia.org/wiki/directed_acyclic_graph"
        }
    ],
    "abbreviations": {
        "SNAS": "Stochastic Neural Architecture Search",
        "NAS": "Neural Architecture Search",
        "DAG": "directed acyclic graph",
        "PPO": "policy optimization",
        "GAE": "generalized advantage estimator"
    },
    "highlights": [
        "The trend to seek for state-of-the-art neural network architecture automatically has been growing since <a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\"><a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\">Zoph & Le (2016</a></a>), given the enormous effort needed in scientific research",
        "We propose a novel, efficient and highly automated framework, Stochastic Neural Architecture Search (SNAS), that trains neural operation parameters and architecture distribution parameters in same round of back propagation, while maintaining the completeness and differentiability of the Neural Architecture Search pipeline",
        "Results Table 3 presents the results of the evaluation on ImageNet and shows that the cell found by Stochastic Neural Architecture Search on CIFAR-10 can be successfully transferred to ImageNet",
        "We presented Stochastic Neural Architecture Search, a novel and economical end-to-end neural architecture search framework",
        "The key contribution of Stochastic Neural Architecture Search is that by making use of gradient information from generic differentiable loss without sacrificing the completeness of Neural Architecture Search pipeline, stochastic architecture search could be more efficient. This improvement is proved by comparing the credit assigned by the search gradient with reinforcement-learning-based Neural Architecture Search",
        "Experiments showed that Stochastic Neural Architecture Search searches well on CIFAR-10, whose result could be transferred to ImageNet as well"
    ],
    "key_statements": [
        "The trend to seek for state-of-the-art neural network architecture automatically has been growing since <a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\"><a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\">Zoph & Le (2016</a></a>), given the enormous effort needed in scientific research",
        "As Neural Architecture Search is modeled as a Markov Decision Process, credits are assigned to structural decisions with temporal-difference (TD) learning (Sutton et al, 1998), whose efficiency and interpretability suffer from delayed rewards (<a class=\"ref-link\" id=\"cArjona-Medina_et+al_2018_a\" href=\"#rArjona-Medina_et+al_2018_a\">Arjona-Medina et al, 2018</a>)",
        "We propose a novel, efficient and highly automated framework, Stochastic Neural Architecture Search (SNAS), that trains neural operation parameters and architecture distribution parameters in same round of back propagation, while maintaining the completeness and differentiability of the Neural Architecture Search pipeline",
        "We prove that Stochastic Neural Architecture Search optimizes the same objective as reinforcement-learning-based Neural Architecture Search, except the training loss is used as reward",
        "Phenomena shown in Figure 4 and Table 1 verify our claim that searching process in Stochastic Neural Architecture Search is less biased from the objective, i.e. Equation (4), and could possibly save computation resources for parameter retraining when extended to Neural Architecture Search on large datasets",
        "Searching Results Three levels of resource constraint, mild, moderate and aggressive are examined in Stochastic Neural Architecture Search",
        "As shown in Figure 5, more edges are dropped, leaving only two, which leads to the drop of some nodes, including the input node ck\u22121, and two intermediate nodes x2 and x3. Note that this child graph is a structure that ENAS and DARTS are not able to discover 4",
        "The test error of Stochastic Neural Architecture Search is on par with the state-of-the-art RL-based and evolution-based Neural Architecture Search while using three orders of magnitude less computation resources",
        "With a more aggressive resource constraint, Stochastic Neural Architecture Search can sparsify the architecture even further to distinguish from ENAS and DARTS with only a slight drop in performance, which is still on par with 1st-order DARTS",
        "Results Table 3 presents the results of the evaluation on ImageNet and shows that the cell found by Stochastic Neural Architecture Search on CIFAR-10 can be successfully transferred to ImageNet",
        "Getting rid of these auxiliary mechanisms, ENAS (<a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a>) is the state-of-the-art Neural Architecture Search framework, proposing parameter sharing among all possible child graphs, which is followed by Stochastic Neural Architecture Search",
        "In Section 2 we introduced Stochastic Neural Architecture Search\u2019s relation with ENAS in details",
        "We presented Stochastic Neural Architecture Search, a novel and economical end-to-end neural architecture search framework",
        "The key contribution of Stochastic Neural Architecture Search is that by making use of gradient information from generic differentiable loss without sacrificing the completeness of Neural Architecture Search pipeline, stochastic architecture search could be more efficient. This improvement is proved by comparing the credit assigned by the search gradient with reinforcement-learning-based Neural Architecture Search",
        "Experiments showed that Stochastic Neural Architecture Search searches well on CIFAR-10, whose result could be transferred to ImageNet as well",
        "As a more efficient and less-biased framework, Stochastic Neural Architecture Search will serve as a possible candidate for full-fledged Neural Architecture Search on large datasets in the future"
    ],
    "summary": [
        "The trend to seek for state-of-the-art neural network architecture automatically has been growing since <a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\"><a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\">Zoph & Le (2016</a></a>), given the enormous effort needed in scientific research.",
        "In Section 2.3, the search gradient of SNAS is connected to the policy gradient in reinforcement-learning-based NAS (<a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\">Zoph & Le, 2016</a>; <a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\"><a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a></a>), interpreting SNAS\u2019s credit assignment with contribution analysis.",
        "This differentiates SNAS from attention-based NAS like DARTS, which avoids the sampling process by taking analytical expectation at each edge over all operations.",
        "Searching Process The normal and reduction cells learned on CIFAR-10 using SNAS with mild resource constraint are shown in Figure 2.",
        "Phenomena shown in Figure 4 and Table 1 verify our claim that searching process in SNAS is less biased from the objective, i.e. Equation (4), and could possibly save computation resources for parameter retraining when extended to NAS on large datasets.",
        "Motivation In the searching stage, we follow the economical setup of DARTS to use only one single GPU, which constrains the parameter size of the child network.",
        "This network is trained from scratch as in DARTS and ENAS to report the performance of the cells learned by SNAS on CIFAR-10.",
        "With slightly longer wall-clock-time, SNAS outperforms 1st-order DARTS and ENAS by discovering convolutional cells with both a smaller error rate and fewer parameters.",
        "To show whether the cells learned on by SNAS CIFAR-10 can be generalized to larger datasets, we apply the same cells evaluated in Section 3.2 to the classification task on ImageNet. Dataset The mobile setting is adopted where the size of the input images is 224 \u00d7 224 and the number of multiply-add operations in the model is restricted to be less than 600M.",
        "Results Table 3 presents the results of the evaluation on ImageNet and shows that the cell found by SNAS on CIFAR-10 can be successfully transferred to ImageNet. Notably, SNAS is able to achieve competitive test error with the state-of-the-art RL-based NAS using three orders of magnitude less computation resources.",
        "Getting rid of these auxiliary mechanisms, ENAS (<a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\"><a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a></a>) is the state-of-the-art NAS framework, proposing parameter sharing among all possible child graphs, which is followed by SNAS.",
        "The most important motivation of SNAS is to leverage the gradient information in generic differentiable loss to update architecture distribution, which is shared by DARTS (<a class=\"ref-link\" id=\"cLiu_et+al_2019_a\" href=\"#rLiu_et+al_2019_a\">Liu et al, 2019</a>).",
        "The key contribution of SNAS is that by making use of gradient information from generic differentiable loss without sacrificing the completeness of NAS pipeline, stochastic architecture search could be more efficient.",
        "As a more efficient and less-biased framework, SNAS will serve as a possible candidate for full-fledged NAS on large datasets in the future"
    ],
    "headline": "We propose Stochastic Neural Architecture Search, an economical endto-end solution to Neural Architecture Search that trains neural operation parameters and architecture distribution parameters in same round of backpropagation, while maintaining the completeness and differentiability of the Neural Architecture Search pipeline",
    "reference_links": [
        {
            "id": "Arjona-Medina_et+al_2018_a",
            "entry": "Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. arXiv preprint arXiv:1806.07857, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07857"
        },
        {
            "id": "Baker_et+al_2017_a",
            "entry": "Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. arXiv preprint arXiv:1705.10823, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10823"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05344"
        },
        {
            "id": "Deng_et+al_2017_a",
            "entry": "Boyang Deng, Junjie Yan, and Dahua Lin. Peephole: Predicting network performance before training. arXiv preprint arXiv:1712.03351, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03351"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Gordon_et+al_2018_a",
            "entry": "Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast & simple resource-constrained structure learning of deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Ariel%20Eban%2C%20Elad%20Nachum%2C%20Ofir%20Chen%2C%20Bo%20Morphnet%3A%20Fast%20%26%20simple%20resource-constrained%20structure%20learning%20of%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Ariel%20Eban%2C%20Elad%20Nachum%2C%20Ofir%20Chen%2C%20Bo%20Morphnet%3A%20Fast%20%26%20simple%20resource-constrained%20structure%20learning%20of%20deep%20networks%202018"
        },
        {
            "id": "Gu_et+al_2015_a",
            "entry": "Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05176"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Kschischang_et+al_2001_a",
            "entry": "Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on information theory, 47(2):498\u2013519, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kschischang%2C%20Frank%20R.%20Frey%2C%20Brendan%20J.%20Loeliger%2C%20H.-A.%20Factor%20graphs%20and%20the%20sum-product%20algorithm%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kschischang%2C%20Frank%20R.%20Frey%2C%20Brendan%20J.%20Loeliger%2C%20H.-A.%20Factor%20graphs%20and%20the%20sum-product%20algorithm%202001"
        },
        {
            "id": "Liu_et+al_0000_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "Liu_et+al_0000_b",
            "entry": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00436"
        },
        {
            "id": "Liu_et+al_2019_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=S1eYHoC5FX.",
            "url": "https://openreview.net/forum?id=S1eYHoC5FX",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Yang%2C%20Yiming%20DARTS%3A%20Differentiable%20architecture%20search%202019"
        },
        {
            "id": "Louizos_et+al_2017_a",
            "entry": "Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l 0 regularization. arXiv preprint arXiv:1712.01312, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01312"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. arXiv preprint arXiv:1807.11164, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.11164"
        },
        {
            "id": "Maddison_2016_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Montavon_et+al_0000_a",
            "entry": "Gregoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Muller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65:211\u2013222, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montavon%2C%20Gregoire%20Lapuschkin%2C%20Sebastian%20Binder%2C%20Alexander%20Samek%2C%20Wojciech%20Explaining%20nonlinear%20classification%20decisions%20with%20deep%20taylor%20decomposition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montavon%2C%20Gregoire%20Lapuschkin%2C%20Sebastian%20Binder%2C%20Alexander%20Samek%2C%20Wojciech%20Explaining%20nonlinear%20classification%20decisions%20with%20deep%20taylor%20decomposition"
        },
        {
            "id": "Montavon_et+al_2017_a",
            "entry": "Gregoire Montavon, Wojciech Samek, and Klaus-Robert Muller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montavon%2C%20Gregoire%20Samek%2C%20Wojciech%20Muller%2C%20Klaus-Robert%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montavon%2C%20Gregoire%20Samek%2C%20Wojciech%20Muller%2C%20Klaus-Robert%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017"
        },
        {
            "id": "Pham_et+al_2018_a",
            "entry": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pham%2C%20Hieu%20Guan%2C%20Melody%20Y.%20Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Efficient%20neural%20architecture%20search%20via%20parameter%20sharing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pham%2C%20Hieu%20Guan%2C%20Melody%20Y.%20Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Efficient%20neural%20architecture%20search%20via%20parameter%20sharing%202018"
        },
        {
            "id": "Precup_2000_a",
            "entry": "Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Precup%2C%20Doina%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Precup%2C%20Doina%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial Intelligence and Statistics, pp. 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Real_et+al_2018_a",
            "entry": "Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "Schmidhuber_1990_a",
            "entry": "J Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. Institut fur Informatik, Technische Universitat Munchen. Technical Report FKI-126, 90, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Making%20the%20world%20differentiable%3A%20On%20using%20fully%20recurrent%20self-supervised%20neural%20networks%20for%20dynamic%20reinforcement%20learning%20and%20planning%20in%20non-stationary%20environments%201990"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Stanley_2002_a",
            "entry": "Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99\u2013127, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Tucker_et+al_2018_a",
            "entry": "George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E Turner, Zoubin Ghahramani, and Sergey Levine. The mirage of action-dependent baselines in reinforcement learning. arXiv preprint arXiv:1802.10031, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10031"
        },
        {
            "id": "Wierstra_et+al_2008_a",
            "entry": "Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on, pp. 3381\u20133387. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daan%20Wierstra%20Tom%20Schaul%20Jan%20Peters%20and%20Juergen%20Schmidhuber%20Natural%20evolution%20strategies%20In%20Evolutionary%20Computation%202008%20CEC%202008IEEE%20World%20Congress%20on%20Computational%20Intelligence%20IEEE%20Congress%20on%20pp%2033813387%20IEEE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daan%20Wierstra%20Tom%20Schaul%20Jan%20Peters%20and%20Juergen%20Schmidhuber%20Natural%20evolution%20strategies%20In%20Evolutionary%20Computation%202008%20CEC%202008IEEE%20World%20Congress%20on%20Computational%20Intelligence%20IEEE%20Congress%20on%20pp%2033813387%20IEEE%202008"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. arXiv preprint arXiv:1805.09801, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09801"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "X Zhang, X Zhou, M Lin, and J Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arxiv 2017. arXiv preprint arXiv:1707.01083.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        },
        {
            "id": "Zoph_2016_a",
            "entry": "Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        },
        {
            "id": "Zoph_et+al_2017_a",
            "entry": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2(6), 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07012"
        }
    ]
}
