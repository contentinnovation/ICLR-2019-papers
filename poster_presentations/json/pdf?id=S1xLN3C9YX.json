{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNABLE EMBEDDING SPACE FOR EFFICIENT NEURAL ARCHITECTURE COMPRESSION",
        "author": "Shengcao Cao, School of EECS Peking University Beijing, 100871, China caoshengcao@pku.edu.cn",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1xLN3C9YX"
        },
        "abstract": "We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al., 2018</a>). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al., 2018</a>). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training."
    },
    "keywords": [
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "random search",
            "url": "https://en.wikipedia.org/wiki/random_search"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "Bayesian Optimization",
            "url": "https://en.wikipedia.org/wiki/Bayesian_Optimization"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        }
    ],
    "abbreviations": {
        "BO": "Bayesian Optimization",
        "NAS": "Neural Architecture Search",
        "GP": "Gaussian process",
        "RS": "random search"
    },
    "highlights": [
        "It is common practice to make use of well-known deep network architectures (e.g., VGG (<a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), GoogleNet (<a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>), ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>)) and to adapt them to a new task without optimizing the architecture for that task",
        "We focus on the task of compressed architecture search \u2013 the automatic discovery of compressed network architectures based on a given large network",
        "We demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training",
        "We address the task of searching for a compressed network architecture by using Bayesian Optimization",
        "We demonstrate that the learned embedding space can be transferred to new settings for architecture search without any training",
        "Possible future directions include extending our method to the general Neural Architecture Search problem to search for desired architectures from the scratch and combining our proposed embedding space with <a class=\"ref-link\" id=\"cHernandez-Lobato_et+al_2016_a\" href=\"#rHernandez-Lobato_et+al_2016_a\">Hernandez-Lobato et al (2016</a>) to identify the Pareto set of the architectures that are both small and accurate"
    ],
    "key_statements": [
        "It is common practice to make use of well-known deep network architectures (e.g., VGG (<a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), GoogleNet (<a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>), ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>)) and to adapt them to a new task without optimizing the architecture for that task",
        "We focus on the task of compressed architecture search \u2013 the automatic discovery of compressed network architectures based on a given large network",
        "To enable the careful selection of architectures for evaluation, we propose a method to incrementally learn an embedding space over the domain of network architectures",
        "We address the task of compressed architecture search by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation",
        "We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a>)",
        "We demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training",
        "Contributions: (1) We propose a novel method to incrementally learn an embedding space over the domain of network architectures",
        "Based on the learnable embedding space, we present a framework of searching for compressed network architectures with Bayesian Optimization",
        "Different from them, we aim to develop an algorithm that can automatically search for an efficient network architecture with minimal human efforts involved in the architecture design",
        "Our work is closely related to N2N (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a>), which searches for a compressed architecture based on a given teacher network using reinforcement learning",
        "We focus on searching for a compressed network architecture based on a given teacher network and our goal is to find a network architecture which contains as few parameters as possible but still can obtain a similar performance to the teacher network",
        "To enable the careful selection of architectures for evaluation, we propose a method to incrementally learn an embedding space over the domain of network architecture that can be used to generate a priority ordering of architectures for evaluation",
        "We develop the search algorithm based on Bayesian Optimization with a kernel function defined over our proposed embedding space",
        "We propose to learn an embedding space for the neural architecture domain and define the kernel function based on the learned embedding space",
        "Motivated by N2N (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a>), we propose a search space for maximizing the acquisition function, which is constrained by the teacher network, and provides a practical method to explore the search space",
        "Layer shrinkage refers to shrinking the size of layers, in particular, the number of filters in convolutional layers, as we focus on convolutional networks in this work",
        "N2N (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a>), which uses reinforcement learning to search for compressed network architectures, does not support forming skip connections in their action space",
        "We implement the search algorithm with the proposed learnable kernel function but notice that the highest function value among evaluated architectures stops increasing after a few steps. We conjecture this is due to that the learned kernel is overfitted to the training samples since we only evaluate hundreds of architectures in the whole search process",
        "We provide results on four architectures as the teacher network: VGG19 (<a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a>), ResNet-18, ResNet-34 (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>) and ShuffleNet (<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a>)",
        "We show the value of f (x) as an indication of how well each architecture x meets our requirement in terms of both the accuracy and the compression ratio",
        "We compare the compressed architectures found by our algorithm to the state-of-the-art manually-designed compact network architecture ShuffleNet",
        "We vary the number of channels and the number of groups in ShuffleNet and compare the compressed architectures found by our proposed method against these different configurations of ShuffleNet",
        "In Table 2, VGG-19, ResNet-18, ResNet-34 and ShuffleNet refer to the compressed architectures found by our algorithm based on the corresponding teacher network and do not refer to the original architecture indicated by the name",
        "The teacher ShuffleNet used in the experiments is \u2018ShuffleNet 1\u00d7(g = 2)\u2019 as mentioned above. \u20180.5\u00d7(g = 1)\u2019 and so on in Table 2 refer to different configurations of ShuffleNet and we show the accuracy of these original ShuffleNet in the table",
        "The maximizer of the acquisition function is a compressed architecture for the target setting. We evaluate this architecture in the target setting and compare it with the architecture found by applying algorithms directly to the target setting",
        "We first run our search algorithm in setting (a) and transfer the learned kernel to setting (b), (c) and (d) respectively to see how much the learned kernel can transfer to a larger teacher network in the same architecture family, a different architecture family or a harder dataset",
        "In all the three transfer scenarios, the learned kernel in the source setting (a) can help find reasonably good architectures in the target setting without training the kernel in the target setting, whose performance is better than the architecture found by applying N2N directly to the target setting",
        "These results proves that the learned architecture embedding space or the learned kernel is able to generalize to new settings for architecture search without any additional training",
        "We address the task of searching for a compressed network architecture by using Bayesian Optimization",
        "We demonstrate that the learned embedding space can be transferred to new settings for architecture search without any training",
        "Possible future directions include extending our method to the general Neural Architecture Search problem to search for desired architectures from the scratch and combining our proposed embedding space with <a class=\"ref-link\" id=\"cHernandez-Lobato_et+al_2016_a\" href=\"#rHernandez-Lobato_et+al_2016_a\">Hernandez-Lobato et al (2016</a>) to identify the Pareto set of the architectures that are both small and accurate"
    ],
    "summary": [
        "It is common practice to make use of well-known deep network architectures (e.g., VGG (<a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), GoogleNet (<a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a></a>), ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>)) and to adapt them to a new task without optimizing the architecture for that task.",
        "To enable the careful selection of architectures for evaluation, we propose a method to incrementally learn an embedding space over the domain of network architectures.",
        "We address the task of compressed architecture search by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation.",
        "Our work is closely related to N2N (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\"><a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a></a>), which searches for a compressed architecture based on a given teacher network using reinforcement learning.",
        "We develop the search algorithm based on BO with a kernel function defined over our proposed embedding space.",
        "We propose to map a diverse range of discrete architectures to a continuous embedding space through the use of recurrent neural networks and define the kernel function based on the learned embedding space.",
        "N2N (<a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\"><a class=\"ref-link\" id=\"cAshok_et+al_2018_a\" href=\"#rAshok_et+al_2018_a\">Ashok et al, 2018</a></a>), which uses reinforcement learning to search for compressed network architectures, does not support forming skip connections in their action space.",
        "In each step of the search process, we train multiple kernel functions on uniformly sampled subsets of D, the set of all the available evaluated architectures.",
        "As shown in Table 1, our search algorithm successfully compresses \u2018ShuffleNet 1 \u00d7 (g = 2)\u2019 by 10.43\u00d7 and 4.74\u00d7 on CIFAR-10 and CIFAR-100 respectively and the compressed architectures can still achieve similar accuracy to the original teacher network.",
        "We compare the compressed architectures found by our algorithm to the state-of-the-art manually-designed compact network architecture ShuffleNet. We vary the number of channels and the number of groups in ShuffleNet and compare the compressed architectures found by our proposed method against these different configurations of ShuffleNet. We conduct experiments on CIFAR-100 and the results are summarized in Table 2.",
        "In Table 2, VGG-19, ResNet-18, ResNet-34 and ShuffleNet refer to the compressed architectures found by our algorithm based on the corresponding teacher network and do not refer to the original architecture indicated by the name.",
        "These results proves that the learned architecture embedding space or the learned kernel is able to generalize to new settings for architecture search without any additional training.",
        "Possible future directions include extending our method to the general NAS problem to search for desired architectures from the scratch and combining our proposed embedding space with <a class=\"ref-link\" id=\"cHernandez-Lobato_et+al_2016_a\" href=\"#rHernandez-Lobato_et+al_2016_a\"><a class=\"ref-link\" id=\"cHernandez-Lobato_et+al_2016_a\" href=\"#rHernandez-Lobato_et+al_2016_a\">Hernandez-Lobato et al (2016</a></a>) to identify the Pareto set of the architectures that are both small and accurate."
    ],
    "headline": "We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search",
    "reference_links": [
        {
            "id": "Al-Shedivat_et+al_2017_a",
            "entry": "Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, and Eric P Xing. Learning scalable deep kernels with recurrent structure. The Journal of Machine Learning Research, 18(1): 2850\u20132886, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Shedivat%2C%20Maruan%20Wilson%2C%20Andrew%20Gordon%20Saatchi%2C%20Yunus%20Hu%2C%20Zhiting%20and%20Eric%20P%20Xing.%20Learning%20scalable%20deep%20kernels%20with%20recurrent%20structure%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Shedivat%2C%20Maruan%20Wilson%2C%20Andrew%20Gordon%20Saatchi%2C%20Yunus%20Hu%2C%20Zhiting%20and%20Eric%20P%20Xing.%20Learning%20scalable%20deep%20kernels%20with%20recurrent%20structure%202017"
        },
        {
            "id": "Ashok_et+al_2018_a",
            "entry": "Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and Kris M. Kitani. N2n learning: Network to network compression via policy gradient reinforcement learning. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= B1hcZZ-AW.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashok%2C%20Anubhav%20Rhinehart%2C%20Nicholas%20Beainy%2C%20Fares%20Kitani%2C%20Kris%20M.%20N2n%20learning%3A%20Network%20to%20network%20compression%20via%20policy%20gradient%20reinforcement%20learning%202018"
        },
        {
            "id": "Bergstra_et+al_2013_a",
            "entry": "James Bergstra, Daniel Yamins, and David Daniel Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20Yamins%2C%20Daniel%20Cox%2C%20David%20Daniel%20Making%20a%20science%20of%20model%20search%3A%20Hyperparameter%20optimization%20in%20hundreds%20of%20dimensions%20for%20vision%20architectures%202013"
        },
        {
            "id": "Bergstra_et+al_2011_a",
            "entry": "James S Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pp. 2546\u20132554, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20S.%20Bardenet%2C%20Remi%20Bengio%2C%20Yoshua%20Kegl%2C%20Balazs%20Algorithms%20for%20hyper-parameter%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20S.%20Bardenet%2C%20Remi%20Bengio%2C%20Yoshua%20Kegl%2C%20Balazs%20Algorithms%20for%20hyper-parameter%20optimization%202011"
        },
        {
            "id": "Dong_et+al_2018_a",
            "entry": "Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. arXiv preprint arXiv:1806.08198, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08198"
        },
        {
            "id": "Elsken_et+al_0000_a",
            "entry": "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architecture search via lamarckian evolution. arXiv preprint arXiv:1804.09081, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09081"
        },
        {
            "id": "Elsken_et+al_0000_b",
            "entry": "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. arXiv preprint arXiv:1808.05377, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05377"
        },
        {
            "id": "Gomez-Bombarelli_et+al_2018_a",
            "entry": "Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjam\u0131n Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez-Bombarelli%2C%20Rafael%20Wei%2C%20Jennifer%20N.%20Duvenaud%2C%20David%20Hernandez-Lobato%2C%20Jose%20Miguel%20Automatic%20chemical%20design%20using%20a%20data-driven%20continuous%20representation%20of%20molecules%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez-Bombarelli%2C%20Rafael%20Wei%2C%20Jennifer%20N.%20Duvenaud%2C%20David%20Hernandez-Lobato%2C%20Jose%20Miguel%20Automatic%20chemical%20design%20using%20a%20data-driven%20continuous%20representation%20of%20molecules%202018"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hernandez-Lobato_et+al_2016_a",
            "entry": "Daniel Hernandez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy search for multi-objective bayesian optimization. In International Conference on Machine Learning, pp. 1492\u20131501, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Lobato%2C%20Daniel%20Hernandez-Lobato%2C%20Jose%20Shah%2C%20Amar%20Adams%2C%20Ryan%20Predictive%20entropy%20search%20for%20multi-objective%20bayesian%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Lobato%2C%20Daniel%20Hernandez-Lobato%2C%20Jose%20Shah%2C%20Amar%20Adams%2C%20Ryan%20Predictive%20entropy%20search%20for%20multi-objective%20bayesian%20optimization%202016"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Hsu_et+al_2018_a",
            "entry": "Chi-Hung Hsu, Shu-Huan Chang, Da-Cheng Juan, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and ShihChieh Chang. Monas: Multi-objective neural architecture search using reinforcement learning. arXiv preprint arXiv:1806.10332, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10332"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017-07"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. group, 3(12):11, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Shichen%20der%20Maaten%2C%20Laurens%20Van%20Weinberger%2C%20Kilian%20Q.%20Condensenet%3A%20An%20efficient%20densenet%20using%20learned%20group%20convolutions%202018"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Jenatton_et+al_2017_a",
            "entry": "Rodolphe Jenatton, Cedric Archambeau, Javier Gonzalez, and Matthias Seeger. Bayesian optimization with tree-structured dependencies. In International Conference on Machine Learning, pp. 1655\u20131664, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jenatton%2C%20Rodolphe%20Archambeau%2C%20Cedric%20Gonzalez%2C%20Javier%20Seeger%2C%20Matthias%20Bayesian%20optimization%20with%20tree-structured%20dependencies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jenatton%2C%20Rodolphe%20Archambeau%2C%20Cedric%20Gonzalez%2C%20Javier%20Seeger%2C%20Matthias%20Bayesian%20optimization%20with%20tree-structured%20dependencies%202017"
        },
        {
            "id": "Kandasamy_et+al_2018_a",
            "entry": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07191"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Liu_et+al_0000_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "Liu_et+al_0000_b",
            "entry": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00436"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09055"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Xiaoyu Lu, Javier Gonzalez, Zhenwen Dai, and Neil Lawrence. Structured variationally autoencoded optimization. In International Conference on Machine Learning, pp. 3273\u20133281, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Xiaoyu%20Gonzalez%2C%20Javier%20Dai%2C%20Zhenwen%20Lawrence%2C%20Neil%20Structured%20variationally%20autoencoded%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Xiaoyu%20Gonzalez%2C%20Javier%20Dai%2C%20Zhenwen%20Lawrence%2C%20Neil%20Structured%20variationally%20autoencoded%20optimization%202018"
        },
        {
            "id": "Luo_et+al_2018_a",
            "entry": "Renqian Luo, Fei Tian, Tao Qin, and Tie-Yan Liu. Neural architecture optimization. arXiv preprint arXiv:1808.07233, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07233"
        },
        {
            "id": "Mendoza_et+al_2016_a",
            "entry": "Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. In Workshop on Automatic Machine Learning, pp. 58\u201365, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mendoza%2C%20Hector%20Klein%2C%20Aaron%20Feurer%2C%20Matthias%20Springenberg%2C%20Jost%20Tobias%20Towards%20automatically-tuned%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mendoza%2C%20Hector%20Klein%2C%20Aaron%20Feurer%2C%20Matthias%20Springenberg%2C%20Jost%20Tobias%20Towards%20automatically-tuned%20neural%20networks%202016"
        },
        {
            "id": "Mockus_1991_a",
            "entry": "JB Mockus and LJ Mockus. Bayesian approach to global optimization and application to multiobjective and constrained problems. Journal of Optimization Theory and Applications, 70(1): 157\u2013172, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mockus%2C%20J.B.%20Mockus%2C%20L.J.%20Bayesian%20approach%20to%20global%20optimization%20and%20application%20to%20multiobjective%20and%20constrained%20problems%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mockus%2C%20J.B.%20Mockus%2C%20L.J.%20Bayesian%20approach%20to%20global%20optimization%20and%20application%20to%20multiobjective%20and%20constrained%20problems%201991"
        },
        {
            "id": "Pham_et+al_2018_a",
            "entry": "Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03268"
        },
        {
            "id": "Real_et+al_2018_a",
            "entry": "Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "Sandler_et+al_2018_a",
            "entry": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Snoek_et+al_2012_a",
            "entry": "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "Swersky_et+al_1409_a",
            "entry": "Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, and Michael A Osborne. Raiders of the lost architecture: Kernels for bayesian optimization in conditional parameter spaces. arXiv preprint arXiv:1409.4011, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.4011"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Tan_et+al_2018_a",
            "entry": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platformaware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.11626"
        },
        {
            "id": "Williams_2006_a",
            "entry": "Christopher K Williams and Carl Edward Rasmussen. Gaussian processes for machine learning. the MIT Press, 2(3):4, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.%20Rasmussen%2C%20Carl%20Edward%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "Wilson_et+al_2016_a",
            "entry": "Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic variational deep kernel learning. In Advances in Neural Information Processing Systems, pp. 2586\u20132594, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20G.%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20R.%20and%20Eric%20P%20Xing.%20Stochastic%20variational%20deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20G.%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20R.%20and%20Eric%20P%20Xing.%20Stochastic%20variational%20deep%20kernel%20learning%202016"
        },
        {
            "id": "Wilson_0000_a",
            "entry": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artificial Intelligence and Statistics, pp. 370\u2013378, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning"
        },
        {
            "id": "Zela_et+al_2018_a",
            "entry": "Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search. arXiv preprint arXiv:1807.06906, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06906"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Xiangyu%20Zhou%2C%20Xinyu%20Lin%2C%20Mengxiao%20Sun%2C%20Jian%20Shufflenet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Xiangyu%20Zhou%2C%20Xinyu%20Lin%2C%20Mengxiao%20Sun%2C%20Jian%20Shufflenet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%202018-06"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Yanqi Zhou, Siavash Ebrahimi, Sercan O Ar\u0131k, Haonan Yu, Hairong Liu, and Greg Diamos. Resource-efficient neural architect. arXiv preprint arXiv:1806.07912, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07912"
        },
        {
            "id": "Zoph_2016_a",
            "entry": "Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        },
        {
            "id": "Zoph_et+al_2017_a",
            "entry": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2(6), 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07012"
        }
    ]
}
