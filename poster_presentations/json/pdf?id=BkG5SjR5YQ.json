{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "POST SELECTION INFERENCE WITH INCOMPLETE MAXIMUM MEAN DISCREPANCY ESTIMATOR",
        "author": "Makoto Yamada,\u2217 Denny Wu, Yao-Hung Hubert Tsai, Hirofumi Ohta, Ichiro Takeuchi, Ruslan Salakhutdinov, Kenji Fukumizu, Kyoto University, RIKEN AIP, JST PRESTO, Institute of Statistical Mathematics, University of Toronto, Vector Institute, Carnegie Mellon University, University of Tokyo, Nagoya Institute of Technology, myamada@i.kyoto-u.ac.jp,dennywu@cs.toronto.edu,yaohungt@cs.cmu.edu ohtahirofumi@gmail.com, takeuchi.ichiro@nitech.ac.jp rsalakhu@cs.cmu.edu, fukumizu@ism.ac.jp",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkG5SjR5YQ"
        },
        "abstract": "Measuring divergence between two distributions is essential in machine learning and statistics and has various applications including binary classification, change point detection, and two-sample test. Furthermore, in the era of big data, designing divergence measure that is interpretable and can handle high-dimensional and complex data becomes extremely important. In this paper, we propose a post selection inference (PSI) framework for divergence measure, which can select a set of statistically significant features that discriminate two distributions. Specifically, we employ an additive variant of maximum mean discrepancy (MMD) for features and introduce a general hypothesis test for PSI. A novel MMD estimator using the incomplete U-statistics, which has an asymptotically normal distribution (under mild assumptions) and gives high detection power in PSI, is also proposed and analyzed theoretically. Through synthetic and real-world feature selection experiments, we show that the proposed framework can successfully detect statistically significant features. Last, we propose a sample selection framework for analyzing different members in the Generative Adversarial Networks (GANs) family."
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "false positive rate",
            "url": "https://en.wikipedia.org/wiki/false_positive_rate"
        },
        {
            "term": "binary classification",
            "url": "https://en.wikipedia.org/wiki/binary_classification"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "cumulative distribution function",
            "url": "https://en.wikipedia.org/wiki/cumulative_distribution_function"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "metrics",
            "url": "https://en.wikipedia.org/wiki/metrics"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "kernel density estimation",
            "url": "https://en.wikipedia.org/wiki/kernel_density_estimation"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "PSI": "post-selection inference",
        "MMD": "maximum mean discrepancy",
        "GANs": "generative adversarial networks",
        "RKHS": "reproducing kernel Hilbert space",
        "HSIC": "Hilbert-Schmidt Independence Criterion",
        "FID": "Frechet inception distance",
        "KDE": "kernel density estimation",
        "AIS": "annealed importance sampling",
        "CDF": "cumulative distribution function",
        "FPR": "false positive rate"
    },
    "highlights": [
        "Computing the divergence between two probability distributions is fundamental to machine learning and has many important applications such as binary classification (<a class=\"ref-link\" id=\"cFriedman_et+al_2001_a\" href=\"#rFriedman_et+al_2001_a\">Friedman et al, 2001</a>), change point detection (<a class=\"ref-link\" id=\"cYamada_et+al_2013_a\" href=\"#rYamada_et+al_2013_a\">Yamada et al, 2013a</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2013_a\" href=\"#rLiu_et+al_2013_a\">Liu et al, 2013</a>), two-sample test (<a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\">Gretton et al, 2012</a>; <a class=\"ref-link\" id=\"cYamada_et+al_2013_b\" href=\"#rYamada_et+al_2013_b\">Yamada et al, 2013b</a>), and generative models such as generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>; <a class=\"ref-link\" id=\"cLi_et+al_2015_b\" href=\"#rLi_et+al_2015_b\">Li et al, 2015b</a>; <a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>), to name a few",
        "We propose mmdInf, a post selection inference (PSI) algorithm for distribution comparison, which finds a set of statistically significant features that discriminate between two distributions. mmdInf enjoys several compelling properties",
        "We propose a new empirical estimate of maximum mean discrepancy based on the incomplete U-statistics (<a class=\"ref-link\" id=\"cBlom_1976_a\" href=\"#rBlom_1976_a\">Blom, 1976</a>; <a class=\"ref-link\" id=\"cJanson_1984_a\" href=\"#rJanson_1984_a\">Janson, 1984</a>; <a class=\"ref-link\" id=\"cLee_1990_a\" href=\"#rLee_1990_a\">Lee, 1990</a>) and show that it asymptotically follows the normal distribution, and has much greater detection power in post-selection inference",
        "We proposed a novel statistical testing framework mmdInf, which can find a set of statistically significant features that can discriminate two distributions",
        "We proposed a method for sample selection based on mmdInf and applied it in the evaluation of generative models"
    ],
    "key_statements": [
        "Computing the divergence between two probability distributions is fundamental to machine learning and has many important applications such as binary classification (<a class=\"ref-link\" id=\"cFriedman_et+al_2001_a\" href=\"#rFriedman_et+al_2001_a\">Friedman et al, 2001</a>), change point detection (<a class=\"ref-link\" id=\"cYamada_et+al_2013_a\" href=\"#rYamada_et+al_2013_a\">Yamada et al, 2013a</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2013_a\" href=\"#rLiu_et+al_2013_a\">Liu et al, 2013</a>), two-sample test (<a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\">Gretton et al, 2012</a>; <a class=\"ref-link\" id=\"cYamada_et+al_2013_b\" href=\"#rYamada_et+al_2013_b\">Yamada et al, 2013b</a>), and generative models such as generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>; <a class=\"ref-link\" id=\"cLi_et+al_2015_b\" href=\"#rLi_et+al_2015_b\">Li et al, 2015b</a>; <a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a>), to name a few",
        "For reproducing kernel Hilbert space associated with characteristic kernels (<a class=\"ref-link\" id=\"cSriperumbudur_et+al_2011_a\" href=\"#rSriperumbudur_et+al_2011_a\">Sriperumbudur et al, 2011</a>), the difference in mean embeddings defines a proper metric, and maximum mean discrepancy can capture potentially non-linear difference between distributions.\n22",
        "We propose mmdInf, a post selection inference (PSI) algorithm for distribution comparison, which finds a set of statistically significant features that discriminate between two distributions. mmdInf enjoys several compelling properties",
        "We propose a new empirical estimate of maximum mean discrepancy based on the incomplete U-statistics (<a class=\"ref-link\" id=\"cBlom_1976_a\" href=\"#rBlom_1976_a\">Blom, 1976</a>; <a class=\"ref-link\" id=\"cJanson_1984_a\" href=\"#rJanson_1984_a\">Janson, 1984</a>; <a class=\"ref-link\" id=\"cLee_1990_a\" href=\"#rLee_1990_a\">Lee, 1990</a>) and show that it asymptotically follows the normal distribution, and has much greater detection power in post-selection inference",
        "The contributions of our paper are summarized as follows: (1) We propose a non-parametric post-selection inference algorithm mmdInf for distribution comparison",
        "(2) We propose the incomplete maximum mean discrepancy estimator and investigate its theoretical properties. (3) We propose a sample selection framework based on mmdInf that can be used for analyzing generative models.\n2 RELATED WORK",
        "We propose an alternative kernel based inference algorithm for distribution comparison called mmdInf, which can be used for both feature selection and binary classification",
        "We proposed a novel statistical testing framework mmdInf, which can find a set of statistically significant features that can discriminate two distributions",
        "Through synthetic and realworld experiments, we demonstrated that mmdInf can successfully find important features and/or datasets",
        "We proposed a method for sample selection based on mmdInf and applied it in the evaluation of generative models"
    ],
    "summary": [
        "Computing the divergence between two probability distributions is fundamental to machine learning and has many important applications such as binary classification (<a class=\"ref-link\" id=\"cFriedman_et+al_2001_a\" href=\"#rFriedman_et+al_2001_a\"><a class=\"ref-link\" id=\"cFriedman_et+al_2001_a\" href=\"#rFriedman_et+al_2001_a\">Friedman et al, 2001</a></a>), change point detection (<a class=\"ref-link\" id=\"cYamada_et+al_2013_a\" href=\"#rYamada_et+al_2013_a\"><a class=\"ref-link\" id=\"cYamada_et+al_2013_a\" href=\"#rYamada_et+al_2013_a\">Yamada et al, 2013a</a></a>; <a class=\"ref-link\" id=\"cLiu_et+al_2013_a\" href=\"#rLiu_et+al_2013_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2013_a\" href=\"#rLiu_et+al_2013_a\">Liu et al, 2013</a></a>), two-sample test (<a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\"><a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\"><a class=\"ref-link\" id=\"cGretton_et+al_2012_a\" href=\"#rGretton_et+al_2012_a\">Gretton et al, 2012</a></a></a>; <a class=\"ref-link\" id=\"cYamada_et+al_2013_b\" href=\"#rYamada_et+al_2013_b\"><a class=\"ref-link\" id=\"cYamada_et+al_2013_b\" href=\"#rYamada_et+al_2013_b\">Yamada et al, 2013b</a></a>), and generative models such as generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a>; <a class=\"ref-link\" id=\"cLi_et+al_2015_b\" href=\"#rLi_et+al_2015_b\"><a class=\"ref-link\" id=\"cLi_et+al_2015_b\" href=\"#rLi_et+al_2015_b\">Li et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\"><a class=\"ref-link\" id=\"cNowozin_et+al_2016_a\" href=\"#rNowozin_et+al_2016_a\">Nowozin et al, 2016</a></a>), to name a few.",
        "We propose mmdInf, a post selection inference (PSI) algorithm for distribution comparison, which finds a set of statistically significant features that discriminate between two distributions.",
        "The proposed framework is general and can be applied to not only feature selection but other distribution comparison problems, such as dataset comparison.",
        "We propose a new empirical estimate of MMD based on the incomplete U-statistics (<a class=\"ref-link\" id=\"cBlom_1976_a\" href=\"#rBlom_1976_a\">Blom, 1976</a>; <a class=\"ref-link\" id=\"cJanson_1984_a\" href=\"#rJanson_1984_a\">Janson, 1984</a>; <a class=\"ref-link\" id=\"cLee_1990_a\" href=\"#rLee_1990_a\">Lee, 1990</a>) and show that it asymptotically follows the normal distribution, and has much greater detection power in PSI.",
        "We propose a framework to analyze different members in the GAN (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) family based on mmdInf. We elucidate the theoretical properties of the incomplete U-statistics estimate of MMD and show that mmdInf can successfully detect significant features through feature selection experiments.",
        "(3) We propose a sample selection framework based on mmdInf that can be used for analyzing generative models.",
        "We propose an alternative kernel based inference algorithm for distribution comparison called mmdInf, which can be used for both feature selection and binary classification.",
        "Incomplete U-statistics estimator: The described problems of the block-estimator motivated us to design a new MMD estimator that is normally distributed and has smaller variance.",
        "In this paper we apply mmdInf to compare the performance of GANs. We first select the model whose generated samples has the smallest MMD score with the real data and perform the hypothesis test.",
        "Incomplete U-statistics estimator of MMD is asymptotically normally distributed as",
        "The incomplete Ustatistics estimator of MMD is asymptotically normally distributed as",
        "The empirical distribution of the incomplete estimator is normal for small sampling parameter r, and becomes similar to its complete counterpart if r is large; this is supported by Theorem 2 (\u03b3 = \u221e).",
        "We compared mmdInf with a naive testing baseline, which first selects features using MMD and estimates corresponding p-values with the same data of feature selection without adjustment for the selection event.",
        "The number of features d is fixed to 50, and for each feature, data is randomly generated following a Gaussian distribution with set mean and variance.",
        "We proposed a novel statistical testing framework mmdInf, which can find a set of statistically significant features that can discriminate two distributions.",
        "We proposed a method for sample selection based on mmdInf and applied it in the evaluation of generative models."
    ],
    "headline": "We propose a post selection inference framework for divergence measure, which can select a set of statistically significant features that discriminate two distributions",
    "reference_links": [
        {
            "id": "Ali_1966_a",
            "entry": "Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B (Methodological), pp. 131\u2013142, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ali%2C%20Syed%20Mumtaz%20Silvey%2C%20Samuel%20D.%20A%20general%20class%20of%20coefficients%20of%20divergence%20of%20one%20distribution%20from%20another%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ali%2C%20Syed%20Mumtaz%20Silvey%2C%20Samuel%20D.%20A%20general%20class%20of%20coefficients%20of%20divergence%20of%20one%20distribution%20from%20another%201966"
        },
        {
            "id": "Anderson_2001_a",
            "entry": "Marti J Anderson. A new method for non-parametric multivariate analysis of variance. Austral ecology, 26(1):32\u201346, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Marti%20J.%20A%20new%20method%20for%20non-parametric%20multivariate%20analysis%20of%20variance%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Marti%20J.%20A%20new%20method%20for%20non-parametric%20multivariate%20analysis%20of%20variance%202001"
        },
        {
            "id": "Bellemare_et+al_2017_a",
            "entry": "Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10743"
        },
        {
            "id": "Berthelot_et+al_2017_a",
            "entry": "David Berthelot, Tom Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "Billingsley_2008_a",
            "entry": "Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Billingsley%2C%20Patrick%20Probability%20and%20measure%202008"
        },
        {
            "id": "Blom_1976_a",
            "entry": "Gunnar Blom. Some properties of incomplete u-statistics. Biometrika, 63(3):573\u2013580, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blom%2C%20Gunnar%20Some%20properties%20of%20incomplete%20u-statistics%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blom%2C%20Gunnar%20Some%20properties%20of%20incomplete%20u-statistics%201976"
        },
        {
            "id": "Che_et+al_2016_a",
            "entry": "Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.02136"
        },
        {
            "id": "Cover_2012_a",
            "entry": "Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20information%20theory%202012"
        },
        {
            "id": "Fan_et+al_2013_a",
            "entry": "Jianqing Fan, Yuan Liao, and Martina Mincheva. Large covariance estimation by thresholding principal orthogonal complements. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(4):603\u2013680, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Jianqing%20Liao%2C%20Yuan%20Mincheva%2C%20Martina%20Large%20covariance%20estimation%20by%20thresholding%20principal%20orthogonal%20complements%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Jianqing%20Liao%2C%20Yuan%20Mincheva%2C%20Martina%20Large%20covariance%20estimation%20by%20thresholding%20principal%20orthogonal%20complements%202013"
        },
        {
            "id": "Friedman_et+al_2001_a",
            "entry": "Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20The%20elements%20of%20statistical%20learning%2C%20volume%201.%20Springer%20series%20in%20statistics%20New%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20The%20elements%20of%20statistical%20learning%2C%20volume%201.%20Springer%20series%20in%20statistics%20New%202001"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "Gretton_et+al_2005_a",
            "entry": "Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In ALT, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Bousquet%2C%20Olivier%20Smola%2C%20Alex%20Scholkopf%2C%20Bernhard%20Measuring%20statistical%20dependence%20with%20Hilbert-Schmidt%20norms%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Bousquet%2C%20Olivier%20Smola%2C%20Alex%20Scholkopf%2C%20Bernhard%20Measuring%20statistical%20dependence%20with%20Hilbert-Schmidt%20norms%202005"
        },
        {
            "id": "Gretton_et+al_2007_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A kernel method for the two-sample-problem. In NIPS, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Scholkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Scholkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. JMLR, 13(Mar):723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Gao Huang, Yang Yuan, Qiantong Xu, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberge. An empirical study on evaluation metrics of generative adversarial networks. arXiv preprint arXiv:1610.06545, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06545"
        },
        {
            "id": "Janson_1984_a",
            "entry": "Svante Janson. The asymptotic distributions of incomplete u-statistics. Probability Theory and Related Fields, 66(4):495\u2013505, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janson%2C%20Svante%20The%20asymptotic%20distributions%20of%20incomplete%20u-statistics%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janson%2C%20Svante%20The%20asymptotic%20distributions%20of%20incomplete%20u-statistics%201984"
        },
        {
            "id": "Jitkrittum_et+al_2016_a",
            "entry": "Wittawat Jitkrittum, Zoltan Szabo, Kacper P Chwialkowski, and Arthur Gretton. Interpretable distribution features with maximum testing power. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jitkrittum%2C%20Wittawat%20Szabo%2C%20Zoltan%20Chwialkowski%2C%20Kacper%20P.%20Gretton%2C%20Arthur%20Interpretable%20distribution%20features%20with%20maximum%20testing%20power%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jitkrittum%2C%20Wittawat%20Szabo%2C%20Zoltan%20Chwialkowski%2C%20Kacper%20P.%20Gretton%2C%20Arthur%20Interpretable%20distribution%20features%20with%20maximum%20testing%20power%202016"
        },
        {
            "id": "Kodali_et+al_2017_a",
            "entry": "Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of GANs. arXiv preprint arXiv:1705.07215, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07215"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D Lee, Dennis L Sun, Yuekai Sun, Jonathan E Taylor, et al. Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3):907\u2013927, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Sun%2C%20Dennis%20L.%20Sun%2C%20Yuekai%20Taylor%2C%20Jonathan%20E.%20Exact%20post-selection%20inference%2C%20with%20application%20to%20the%20lasso%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Sun%2C%20Dennis%20L.%20Sun%2C%20Yuekai%20Taylor%2C%20Jonathan%20E.%20Exact%20post-selection%20inference%2C%20with%20application%20to%20the%20lasso%202016"
        },
        {
            "id": "Lee_1990_a",
            "entry": "Justin Lee. U-statistics: Theory and practice. 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Justin%20U-statistics%3A%20Theory%20and%20practice%201990"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Shuang Li, Yao Xie, Hanjun Dai, and Le Song. M-statistic for kernel change-point detection. In NIPS, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Shuang%20Xie%2C%20Yao%20Dai%2C%20Hanjun%20Song%2C%20Le%20M-statistic%20for%20kernel%20change-point%20detection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Shuang%20Xie%2C%20Yao%20Dai%2C%20Hanjun%20Song%2C%20Le%20M-statistic%20for%20kernel%20change-point%20detection%202015"
        },
        {
            "id": "Li_et+al_2015_b",
            "entry": "Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In NIPS, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "Liu_et+al_2013_a",
            "entry": "Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in timeseries data by relative density-ratio estimation. Neural Networks, 43:72\u201383, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Song%20Yamada%2C%20Makoto%20Collier%2C%20Nigel%20Sugiyama%2C%20Masashi%20Change-point%20detection%20in%20timeseries%20data%20by%20relative%20density-ratio%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Song%20Yamada%2C%20Makoto%20Collier%2C%20Nigel%20Sugiyama%2C%20Masashi%20Change-point%20detection%20in%20timeseries%20data%20by%20relative%20density-ratio%20estimation%202013"
        },
        {
            "id": "Lopez-Paz_2016_a",
            "entry": "David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06545"
        },
        {
            "id": "Miyato_et+al_2017_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICML Implicit Models Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Mueller_2015_a",
            "entry": "Jonas W Mueller and Tommi Jaakkola. Principal differences analysis: Interpretable characterization of differences between distributions. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mueller%2C%20Jonas%20W.%20Jaakkola%2C%20Tommi%20Principal%20differences%20analysis%3A%20Interpretable%20characterization%20of%20differences%20between%20distributions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mueller%2C%20Jonas%20W.%20Jaakkola%2C%20Tommi%20Principal%20differences%20analysis%3A%20Interpretable%20characterization%20of%20differences%20between%20distributions%202015"
        },
        {
            "id": "Muller_1997_a",
            "entry": "Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Poczos_2011_a",
            "entry": "Barnabas Poczos and Jeff Schneider. On the estimation of alpha-divergences. In AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poczos%2C%20Barnabas%20Schneider%2C%20Jeff%20On%20the%20estimation%20of%20alpha-divergences%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poczos%2C%20Barnabas%20Schneider%2C%20Jeff%20On%20the%20estimation%20of%20alpha-divergences%202011"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Renyi_1961_a",
            "entry": "Alfred Renyi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Renyi%2C%20Alfred%20On%20measures%20of%20entropy%20and%20information%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Renyi%2C%20Alfred%20On%20measures%20of%20entropy%20and%20information%201961"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Shorack_2000_a",
            "entry": "Galen R Shorack and GR Shorack. Probability for statisticians. Number 04; QA273, S4. Springer, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shorack%2C%20Galen%20R.%20Shorack%2C%20G.R.%20Probability%20for%20statisticians%202000"
        },
        {
            "id": "Sriperumbudur_et+al_2011_a",
            "entry": "Bharath. K. Sriperumbudur, Kenji. Fukumizu, and Gert. RG. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. Journal of Machine Learning Research, 12(Jul): 2389\u20132410, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20Bharath%20K.%20Fukumizu%2C%20Kenji%20Lanckriet%2C%20Gert%20R.G.%20Universality%2C%20characteristic%20kernels%20and%20RKHS%20embedding%20of%20measures%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriperumbudur%2C%20Bharath%20K.%20Fukumizu%2C%20Kenji%20Lanckriet%2C%20Gert%20R.G.%20Universality%2C%20characteristic%20kernels%20and%20RKHS%20embedding%20of%20measures%202011"
        },
        {
            "id": "Sutherland_et+al_2016_a",
            "entry": "Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. arXiv preprint arXiv:1611.04488, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04488"
        },
        {
            "id": "Tibshirani_1996_a",
            "entry": "Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "Villani_2008_a",
            "entry": "Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20Cedric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "Warde-Farley_2016_a",
            "entry": "David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising feature matching. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Warde-Farley%2C%20David%20Bengio%2C%20Yoshua%20Improving%20generative%20adversarial%20networks%20with%20denoising%20feature%20matching%202016"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. arXiv preprint arXiv:1611.04273, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04273"
        },
        {
            "id": "Yamada_et+al_2013_a",
            "entry": "Makoto Yamada, Akisato Kimura, Futoshi Naya, and Hiroshi Sawada. Change-point detection with feature selection in high-dimensional time-series data. In IJCAI, 2013a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamada%2C%20Makoto%20Kimura%2C%20Akisato%20Naya%2C%20Futoshi%20Sawada%2C%20Hiroshi%20Change-point%20detection%20with%20feature%20selection%20in%20high-dimensional%20time-series%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamada%2C%20Makoto%20Kimura%2C%20Akisato%20Naya%2C%20Futoshi%20Sawada%2C%20Hiroshi%20Change-point%20detection%20with%20feature%20selection%20in%20high-dimensional%20time-series%20data%202013"
        },
        {
            "id": "Yamada_et+al_2013_b",
            "entry": "Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Relative density-ratio estimation for robust distribution comparison. Neural computation, 25(5): 1324\u20131370, 2013b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamada%2C%20Makoto%20Suzuki%2C%20Taiji%20Kanamori%2C%20Takafumi%20Hachiya%2C%20Hirotaka%20Relative%20density-ratio%20estimation%20for%20robust%20distribution%20comparison%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamada%2C%20Makoto%20Suzuki%2C%20Taiji%20Kanamori%2C%20Takafumi%20Hachiya%2C%20Hirotaka%20Relative%20density-ratio%20estimation%20for%20robust%20distribution%20comparison%202013"
        },
        {
            "id": "Yamada_et+al_2018_a",
            "entry": "Makoto Yamada, Yuta Umezu, Kenji Fukumizu, and Ichiro Takeuchi. Post selection inference with kernels. In AISTATS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamada%2C%20Makoto%20Umezu%2C%20Yuta%20Fukumizu%2C%20Kenji%20Takeuchi%2C%20Ichiro%20Post%20selection%20inference%20with%20kernels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamada%2C%20Makoto%20Umezu%2C%20Yuta%20Fukumizu%2C%20Kenji%20Takeuchi%2C%20Ichiro%20Post%20selection%20inference%20with%20kernels%202018"
        },
        {
            "id": "Zaremba_et+al_2013_a",
            "entry": "Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko. B-test: A non-parametric, low variance kernel two-sample test. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaremba%2C%20Wojciech%20Gretton%2C%20Arthur%20Blaschko%2C%20Matthew%20B-test%3A%20A%20non-parametric%2C%20low%20variance%20kernel%20two-sample%20test%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaremba%2C%20Wojciech%20Gretton%2C%20Arthur%20Blaschko%2C%20Matthew%20B-test%3A%20A%20non-parametric%2C%20low%20variance%20kernel%20two-sample%20test%202013"
        }
    ]
}
