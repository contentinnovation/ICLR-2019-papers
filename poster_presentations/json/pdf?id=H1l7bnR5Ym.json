{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PROBGAN: TOWARDS PROBABILISTIC GAN WITH THEORETICAL GUARANTEES",
        "author": "Hao He, Hao Wang, Guang-He Lee, Yonglong Tian Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology {haohe,hwang,guanghe,yonglong}@mit.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1l7bnR5Ym"
        },
        "abstract": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs."
    },
    "keywords": [
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        }
    ],
    "abbreviations": {
        "GAN": "Generative Adversarial Networks",
        "GMA": "Gaussian Mixture Approximation",
        "PSA": "Partial Summation Approximation",
        "IS": "Inception Score",
        "FID": "Frechet Inception Distance"
    },
    "highlights": [
        "Generative Adversarial Networks (GAN) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is notoriously hard to train and suffers from mode collapse",
        "It shows that modelling the distribution of generator helps alleviate mode collapse and motivates the interpretability of the learned generators. This probabilistic framework is built upon Bayesian models for generator and discriminator, whose maximum likelihood estimation can be realized as a metaphor of typical Generative Adversarial Networks objectives",
        "Because Inception score and Frechet Inception Distance may not be consistent with each other, it is possible that the Inception score improves but the Frechet Inception Distance gets worse when we evaluate the model at a different checkpoint",
        "Figure 3 displays the samples randomly generated by the baselines and our ProbGAN",
        "We propose ProbGAN, a novel probabilistic modelling framework for Generative Adversarial Networks",
        "From the perspective of Bayesian Modelling, it contributes a novel likelihood function establishing a connection to existing Generative Adversarial Networks models and a novel prior stabilizing the inference process"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GAN) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is notoriously hard to train and suffers from mode collapse",
        "It shows that modelling the distribution of generator helps alleviate mode collapse and motivates the interpretability of the learned generators. This probabilistic framework is built upon Bayesian models for generator and discriminator, whose maximum likelihood estimation can be realized as a metaphor of typical Generative Adversarial Networks objectives",
        "We prove the previous Bayesian method (<a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci & Wilson, 2017</a>) for any minimax Generative Adversarial Networks objective induces incompatibility of its defined conditional distributions",
        "We propose two special Monte Carlo inference algorithms for our probabilistic model which efficiently approximate the gradient of a non-differentiable criterion",
        "Bayesian Generative Adversarial Networks proposed by <a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci & Wilson (2017</a>) adopts a different approach which models generator and discriminator distributions by defining the conditional posteriors (Eqn 8)",
        "A detailed discussion of the motivation of our modelling and the comparison with Bayesian Generative Adversarial Networks is included in Section 4.\n3.1",
        "ProbGAN updates generator/discriminator distributions based on their distributions in previous time step and the target data distribution as shown in Eqn 2",
        "In Section 4.2, we show the Bayesian Generative Adversarial Networks suffers from bad convergence due to its fixed and weakly informative prior",
        "We have introduced our ProbGAN model",
        "We demonstrate the superior convergence property of our model on a categorical distribution, where analytic posterior computation of various choices of likelihood and prior are feasible; we compute the exact equilibrium of Generative Adversarial Networks models under all the four combinations of the priors and likelihoods(ProbGAN\u2019s choices v.s",
        "We further show BGAN\u2019s choice of likelihood and prior leads to theoretical issues",
        "We evaluate our model with two inference algorithms proposed in Section 3.3",
        "Probabilistic methods including our algorithms and BGAN always achieve a hit ratio of 1, which means every data point generated from these models is very close to one mode of the target distribution",
        "Optimization based methods, both Generative Adversarial Networks and MGAN, consistently have a significantly larger hit error, and sometimes may even generate data samples that do not belong to any mode",
        "Data generated by Generative Adversarial Networks or MGAN tend to be under dispersed and hardly cover the whole square region of the true mode, while data generated by probabilistic methods align much better with the ground truth distribution",
        "For the probabilistic model such as Bayesian Generative Adversarial Networks, we observe a significant enhancement of generated image qualities compare to the original result (Figure 7 in <a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci & Wilson (2017</a>))",
        "Because Inception score and Frechet Inception Distance may not be consistent with each other, it is possible that the Inception score improves but the Frechet Inception Distance gets worse when we evaluate the model at a different checkpoint",
        "Probabilistic methods achieve better scores than optimization based methods, which indicates that injecting stochasticity into Generative Adversarial Networks training helps generate more multi-modal images",
        "Figure 3 displays the samples randomly generated by the baselines and our ProbGAN",
        "In Figure 3 and Figure 10, all the three baselines noticeably suffer from mode collapse",
        "Almost in every training trial, one or two generators of the baseline models degenerate during the training",
        "<a class=\"ref-link\" id=\"cHoang_et+al_2018_a\" href=\"#rHoang_et+al_2018_a\">Hoang et al (2018</a>) already notice that mode collapse in one of the generators could happen after a long training procedure",
        "Visual results for the entire ablation study on CIFAR-10 are included in Section F of the appendix",
        "We propose ProbGAN, a novel probabilistic modelling framework for Generative Adversarial Networks",
        "From the perspective of Bayesian Modelling, it contributes a novel likelihood function establishing a connection to existing Generative Adversarial Networks models and a novel prior stabilizing the inference process"
    ],
    "summary": [
        "Generative Adversarial Networks (GAN) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is notoriously hard to train and suffers from mode collapse.",
        "This probabilistic framework is built upon Bayesian models for generator and discriminator, whose maximum likelihood estimation can be realized as a metaphor of typical GAN objectives.",
        "Bayesian GAN proposed by <a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\"><a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci & Wilson (2017</a></a>) adopts a different approach which models generator and discriminator distributions by defining the conditional posteriors (Eqn 8).",
        "We elaborate ProbGAN, our probabilistic modelling for GAN, and introduce its Bayesian interpretation by developing constituent prior and likelihood formulations.",
        "Like Bayesian GAN, ProbGAN learns distributions of the generator and the discriminator.",
        "ProbGAN updates generator/discriminator distributions based on their distributions in previous time step and the target data distribution as shown in Eqn 2.",
        "Unlike Bayesian GAN using normal distributions for both generator and discriminator, ProbGAN has less standard priors.",
        "The data of the i-th mode is generated by the following process, 2We use original GAN for synthetic dataset and DCGAN for image generation task",
        "Probabilistic methods including our algorithms and BGAN always achieve a hit ratio of 1, which means every data point generated from these models is very close to one mode of the target distribution.",
        "Optimization based methods, both GAN and MGAN, consistently have a significantly larger hit error, and sometimes may even generate data samples that do not belong to any mode.",
        "The data distribution generated by the optimization-based methods fits the target uniform distribution much worse than its probabilistic counterparts, which is quantitatively reflected by the cover error showed in the right side of Table 2 and visually demonstrated by the projected hit sets in Figure 2.",
        "Data generated by GAN or MGAN tend to be under dispersed and hardly cover the whole square region of the true mode, while data generated by probabilistic methods align much better with the ground truth distribution.",
        "Inception Score computes exp(Ex[KL(p(y|x) p(y))]) where p(y|x) standards the predicted label distribution by a pre-trained Inception model (<a class=\"ref-link\" id=\"cSzegedy_et+al_2015_a\" href=\"#rSzegedy_et+al_2015_a\">Szegedy et al, 2015</a>) and p(y) is the average of p(y|x) over all images in the dataset.",
        "For the probabilistic model such as Bayesian GAN, we observe a significant enhancement of generated image qualities compare to the original result (Figure 7 in <a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\"><a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci & Wilson (2017</a></a>)).",
        "Probabilistic methods achieve better scores than optimization based methods, which indicates that injecting stochasticity into GAN training helps generate more multi-modal images.",
        "From the perspective of Bayesian Modelling, it contributes a novel likelihood function establishing a connection to existing GAN models and a novel prior stabilizing the inference process."
    ],
    "headline": "We propose a novel probabilistic framework for Generative Adversarial Networks, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Arnold_1989_a",
            "entry": "Barry C Arnold and S James Press. Compatible conditional distributions. Journal of the American Statistical Association, 84(405):152\u2013156, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arnold%2C%20Barry%20C.%20Press%2C%20S.James%20Compatible%20conditional%20distributions%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arnold%2C%20Barry%20C.%20Press%2C%20S.James%20Compatible%20conditional%20distributions%201989"
        },
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "Barratt_2018_a",
            "entry": "Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In International Conference on Machine Learning, pp. 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Ghosh_et+al_2017_a",
            "entry": "Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip HS Torr, and Puneet K Dokania. Multiagent diverse generative adversarial networks. arXiv preprint arXiv:1704.02906, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02906"
        },
        {
            "id": "Goodfellow_2016_a",
            "entry": "Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626\u20136637, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Hoang_et+al_2018_a",
            "entry": "Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Mgan: Training generative adversarial nets with multiple generators, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoang%2C%20Quan%20Nguyen%2C%20Tu%20Dinh%20Le%2C%20Trung%20Phung%2C%20Dinh%20Mgan%3A%20Training%20generative%20adversarial%20nets%20with%20multiple%20generators%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_et+al_2010_a",
            "entry": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). 2010. URL http://www.cs.toronto.edu/\u0303kriz/cifar.html.",
            "url": "http://www.cs.toronto.edu/\u0303kriz/cifar.html"
        },
        {
            "id": "Maas_et+al_2013_a",
            "entry": "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, pp. 3, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Saatci_2017_a",
            "entry": "Yunus Saatci and Andrew G Wilson. Bayesian gan. In Advances in neural information processing systems, pp. 3622\u20133631, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20gan%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%2036223631%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20gan%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%2036223631%202017"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scholkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing Systems, pp. 5430\u20135439, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tolstikhin%2C%20Ilya%20O.%20Gelly%2C%20Sylvain%20Bousquet%2C%20Olivier%20Simon-Gabriel%2C%20Carl-Johann%20Adagan%3A%20Boosting%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tolstikhin%2C%20Ilya%20O.%20Gelly%2C%20Sylvain%20Bousquet%2C%20Olivier%20Simon-Gabriel%2C%20Carl-Johann%20Adagan%3A%20Boosting%20generative%20models%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Hao Wang and Dit-Yan Yeung. Towards Bayesian deep learning: A framework and some existing methods. IEEE Transactions on Knowledge and Data Engineering, 27(5):1343\u20131355, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Hao%20Yeung%2C%20Dit-Yan%20Towards%20Bayesian%20deep%20learning%3A%20A%20framework%20and%20some%20existing%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Hao%20Yeung%2C%20Dit-Yan%20Towards%20Bayesian%20deep%20learning%3A%20A%20framework%20and%20some%20existing%20methods%202016"
        },
        {
            "id": "Wang_et+al_0000_a",
            "entry": "Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Natural-parameter networks: A class of probabilistic neural networks. In Advances in Neural Information Processing Systems, pp. 118\u2013126, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Hao%20Shi%2C%20Xingjian%20Yeung%2C%20Dit-Yan%20Natural-parameter%20networks%3A%20A%20class%20of%20probabilistic%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Hao%20Shi%2C%20Xingjian%20Yeung%2C%20Dit-Yan%20Natural-parameter%20networks%3A%20A%20class%20of%20probabilistic%20neural%20networks"
        },
        {
            "id": "Wang_et+al_0000_b",
            "entry": "Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial networks. arXiv preprint arXiv:1612.00991, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00991"
        },
        {
            "id": "Ye_et+al_2018_a",
            "entry": "Nanyang Ye and Zhanxing Zhu. Bayesian adversarial learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 6892\u20136901. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf.",
            "url": "http://papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nanyang%20Ye%20and%20Zhanxing%20Zhu%20Bayesian%20adversarial%20learning%20In%20S%20Bengio%20H%20Wallach%20H%20Larochelle%20K%20Grauman%20N%20CesaBianchi%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2031%20pp%2068926901%20Curran%20Associates%20Inc%202018%20URL%20httppapersnipsccpaper7921bayesianadversariallearningpdf"
        }
    ]
}
