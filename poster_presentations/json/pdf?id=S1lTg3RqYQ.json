{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EXEMPLAR GUIDED UNSUPERVISED IMAGE-TOIMAGE TRANSLATION WITH SEMANTIC CONSISTENCY",
        "author": "Liqian Ma, Xu Jia,\u2217 Stamatios Georgoulis, Tinne Tuytelaars, Luc Van Gool, 1KU-Leuven/PSI, TRACE (Toyota Res in Europe) 2KU-Leuven/PSI 3ETH Zurich 4Huawei Noah\u2019s Ark Lab {liqian.ma, xu.jia, tinne.tuytelaars, luc.vangool}@esat.kuleuven.be {georgous, vangool}@vision.ee.ethz.ch",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lTg3RqYQ"
        },
        "abstract": "Image-to-image translation has recently received significant attention due to advances in deep learning. Most works focus on learning either a one-to-one mapping in an unsupervised way or a many-to-many mapping in a supervised way. However, a more practical setting is many-to-many mapping in an unsupervised way, which is harder due to the lack of supervision and the complex inner- and cross-domain variations. To alleviate these issues, we propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. We assume that an image comprises of a content component which is shared across domains, and a style component specific to each domain. Under the guidance of an exemplar from the target domain we apply Adaptive Instance Normalization to the shared content component, which allows us to transfer the style information of the target domain to the source domain. To avoid semantic inconsistencies during translation that naturally appear due to the large inner- and cross-domain variations, we introduce the concept of feature masks that provide coarse semantic guidance without requiring the use of any semantic labels. Experimental results on various datasets show that EGSC-IT does not only translate the source image to diverse instances in the target domain, but also preserves the semantic consistency during the process."
    },
    "keywords": [
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "black white",
            "url": "https://en.wikipedia.org/wiki/Black_White"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        }
    ],
    "abbreviations": {
        "ESB": "Exemplar-specific branch B",
        "AdaIN": "adaptive instance normalization",
        "DNN": "deep neural network",
        "WCT": "whitening and coloring transform",
        "mIoU": "mean Intersection over Union"
    },
    "highlights": [
        "Image-to-image (I2I) translation refers to the task of mapping an image from a source domain to a target domain, e.g. semantic maps to real images, gray-scale to color images, low-resolution to high-resolution images, and so on",
        "We apply adaptive instance normalization (AdaIN) (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a>) to the shared content component of the source domain image using the adaptive instance normalization parameters computed from the target domain exemplar",
        "To address this problem we propose to add a target domain exemplar as guidance during image translation through adaptive instance normalization (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a>)",
        "Our goal is to learn a many-to-many mapping between two domains in an unsupervised way, which is guided by the style of an exemplar while retaining the semantic consistency at the same time",
        "We introduced the EGSC-IT framework to learn a multimodal mapping across domains in an unsupervised way",
        "Under the guidance of an exemplar from the target domain, we showed how to combine adaptive instance normalization with feature masks in order to transfer the style of the exemplar to the source image, while retaining semantic consistency at the same time"
    ],
    "key_statements": [
        "Image-to-image (I2I) translation refers to the task of mapping an image from a source domain to a target domain, e.g. semantic maps to real images, gray-scale to color images, low-resolution to high-resolution images, and so on",
        "The recent advances in deep learning have greatly improved the quality of I2I translation methods for a number of applications, including super-resolution (<a class=\"ref-link\" id=\"cDong_et+al_2014_a\" href=\"#rDong_et+al_2014_a\">Dong et al, 2014</a>), colorization (<a class=\"ref-link\" id=\"cZhang_et+al_2016_a\" href=\"#rZhang_et+al_2016_a\">Zhang et al, 2016</a>), inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al, 2016</a>), attribute transfer (<a class=\"ref-link\" id=\"cLee_et+al_2018_a\" href=\"#rLee_et+al_2018_a\">Lee et al, 2018</a>), style transfer (<a class=\"ref-link\" id=\"cGatys_et+al_2016_a\" href=\"#rGatys_et+al_2016_a\">Gatys et al, 2016</a>), and domain adaptation (<a class=\"ref-link\" id=\"cHoffman_et+al_2018_a\" href=\"#rHoffman_et+al_2018_a\">Hoffman et al, 2018</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>)",
        "For many tasks it is not easy, or even possible, to obtain such paired data that show how an image in the source domain should be translated to an image in the target domain, e.g. in cross-city street view translation or male-female face translation",
        "We propose to condition the image translation process on an arbitrary image in the target domain, i.e. an exemplar",
        "We apply adaptive instance normalization (AdaIN) (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a>) to the shared content component of the source domain image using the adaptive instance normalization parameters computed from the target domain exemplar",
        "To maintain the semantic consistency during image translation without using any semantic labels we propose to compute feature masks",
        "To address this problem we propose to add a target domain exemplar as guidance during image translation through adaptive instance normalization (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a>)",
        "Our goal is to learn a many-to-many mapping between two domains in an unsupervised way, which is guided by the style of an exemplar while retaining the semantic consistency at the same time",
        "Inspired by <a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie (2017</a>), who showed that adaptive instance normalization\u2019s affine parameters have a big influence on the output image\u2019s style, we propose to apply adaptive instance normalization to the shared content component before the decoding stage",
        "For diversity, we want to check whether a method would suffer from the mode collapse issue and translate the images to the dominant mode, i.e., while for generalization, we want to check whether the model can be applied to new styles in the target domain that never appear in the training set, e.g. translate number 6 from black foreground and white background to blue foreground and red background",
        "We introduced the EGSC-IT framework to learn a multimodal mapping across domains in an unsupervised way",
        "Under the guidance of an exemplar from the target domain, we showed how to combine adaptive instance normalization with feature masks in order to transfer the style of the exemplar to the source image, while retaining semantic consistency at the same time"
    ],
    "summary": [
        "Image-to-image (I2I) translation refers to the task of mapping an image from a source domain to a target domain, e.g. semantic maps to real images, gray-scale to color images, low-resolution to high-resolution images, and so on.",
        "We apply adaptive instance normalization (AdaIN) (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\"><a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a></a>) to the shared content component of the source domain image using the AdaIN parameters computed from the target domain exemplar.",
        "To address this problem we propose to add a target domain exemplar as guidance during image translation through AdaIN (<a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\"><a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Belongie, 2017</a></a>).",
        "Concurrent to our work, MUNIT (<a class=\"ref-link\" id=\"cHuang_et+al_2018_a\" href=\"#rHuang_et+al_2018_a\">Huang et al, 2018</a>), proposed to use AdaIN to transfer style information from the target domain to the source domain.",
        "Unlike MUNIT, before applying AdaIN to the shared content component we compute feature masks to decouple different semantic categories and preserve the semantic consistency during the translation process.",
        "By applying feature masks to the feature maps of the shared content component, each channel can specialize and model the style difference only for a single semantic category, which is crucial when handling domains with complex scenes.",
        "Our method using the combination of AdaIN and feature masks under the guidance of perceptual loss is, to the best of our knowledge, the first to achieve multimodal I2I translations in the unsupervised setting with high semantic consistency, without requiring any ground-truth semantic labels.",
        "For the translation flow xA \u2192 xAB, the shared content component cA and feature masks are computed from xA, while AdaIN\u2019s affine parameters \u03b3B and \u03b2B are computed from the target domain exemplar xB.",
        "For diversity, we want to check whether a method would suffer from the mode collapse issue and translate the images to the dominant mode, i.e., while for generalization, we want to check whether the model can be applied to new styles in the target domain that never appear in the training set, e.g. translate number 6 from black foreground and white background to blue foreground and red background.",
        "The mean Intersection over Union scores in Tab. 3 show that training with our translated synthetic images can improve the segmentation results, which indicates that our method can reasonably translate the source GTA5 image to the target domain style with semantic consistency and reduce the domain difference successfully.",
        "Under the guidance of an exemplar from the target domain, we showed how to combine AdaIN with feature masks in order to transfer the style of the exemplar to the source image, while retaining semantic consistency at the same time.",
        "Numerous quantitative and qualitative results demonstrate the effectiveness of our method in this particular setting"
    ],
    "headline": "We propose the Exemplar Guided & Semantically Consistent Image-to-image Translation network which conditions the translation process on an exemplar image in the target domain",
    "reference_links": [
        {
            "id": "Almahairi_et+al_2018_a",
            "entry": "Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Almahairi%2C%20Amjad%20Rajeswar%2C%20Sai%20Sordoni%2C%20Alessandro%20Bachman%2C%20Philip%20Augmented%20cyclegan%3A%20Learning%20many-to-many%20mappings%20from%20unpaired%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Almahairi%2C%20Amjad%20Rajeswar%2C%20Sai%20Sordoni%2C%20Alessandro%20Bachman%2C%20Philip%20Augmented%20cyclegan%3A%20Learning%20many-to-many%20mappings%20from%20unpaired%20data%202018"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20Deeplab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20crfs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20Deeplab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20crfs%202018"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "Dong_et+al_2014_a",
            "entry": "Chao Dong, Chen C. Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Chao%20Loy%2C%20Chen%20C.%20He%2C%20Kaiming%20Tang%2C%20Xiaoou%20Learning%20a%20deep%20convolutional%20network%20for%20image%20super-resolution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Chao%20Loy%2C%20Chen%20C.%20He%2C%20Kaiming%20Tang%2C%20Xiaoou%20Learning%20a%20deep%20convolutional%20network%20for%20image%20super-resolution%202014"
        },
        {
            "id": "Gatys_et+al_2016_a",
            "entry": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Gatys_et+al_2017_a",
            "entry": "Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling perceptual factors in neural style transfer. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leon%20A%20Gatys%20Alexander%20S%20Ecker%20Matthias%20Bethge%20Aaron%20Hertzmann%20and%20Eli%20Shechtman%20Controlling%20perceptual%20factors%20in%20neural%20style%20transfer%20In%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leon%20A%20Gatys%20Alexander%20S%20Ecker%20Matthias%20Bethge%20Aaron%20Hertzmann%20and%20Eli%20Shechtman%20Controlling%20perceptual%20factors%20in%20neural%20style%20transfer%20In%20CVPR%202017"
        },
        {
            "id": "Gonzalez-Garcia_et+al_2018_a",
            "entry": "Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain disentanglement. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonzalez-Garcia%2C%20Abel%20van%20de%20Weijer%2C%20Joost%20Bengio%2C%20Yoshua%20Image-to-image%20translation%20for%20cross-domain%20disentanglement%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonzalez-Garcia%2C%20Abel%20van%20de%20Weijer%2C%20Joost%20Bengio%2C%20Yoshua%20Image-to-image%20translation%20for%20cross-domain%20disentanglement%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20J%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20C%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20J%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20C%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "Hoffman_et+al_2018_a",
            "entry": "Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Judy%20Tzeng%2C%20Eric%20Park%2C%20Taesung%20Zhu%2C%20Jun-Yan%20Cycada%3A%20Cycle-consistent%20adversarial%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Judy%20Tzeng%2C%20Eric%20Park%2C%20Taesung%20Zhu%2C%20Jun-Yan%20Cycada%3A%20Cycle-consistent%20adversarial%20domain%20adaptation%202018"
        },
        {
            "id": "Huang_2017_a",
            "entry": "Xun Huang and Serge J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Belongie%2C%20Serge%20J.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Belongie%2C%20Serge%20J.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Liu%2C%20Ming-Yu%20Belongie%2C%20Serge%20Kautz%2C%20Jan%20Multimodal%20unsupervised%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Liu%2C%20Ming-Yu%20Belongie%2C%20Serge%20Kautz%2C%20Jan%20Multimodal%20unsupervised%20image-to-image%20translation%202018"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and superresolution. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20superresolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20superresolution%202016"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Taeksoo%20Cha%2C%20Moonsu%20Kim%2C%20Hyunsoo%20Lee%2C%20Jung%20Kwon%20Learning%20to%20discover%20cross-domain%20relations%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Taeksoo%20Cha%2C%20Moonsu%20Kim%2C%20Hyunsoo%20Lee%2C%20Jung%20Kwon%20Learning%20to%20discover%20cross-domain%20relations%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Larsen_et+al_2016_a",
            "entry": "Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Hsin-Ying%20Tseng%2C%20Hung-Yu%20Huang%2C%20Jia-Bin%20Singh%2C%20Maneesh%20Diverse%20image-to-image%20translation%20via%20disentangled%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Hsin-Ying%20Tseng%2C%20Hung-Yu%20Huang%2C%20Jia-Bin%20Singh%2C%20Maneesh%20Diverse%20image-to-image%20translation%20via%20disentangled%20representations%202018"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yijun%20Fang%2C%20Chen%20Yang%2C%20Jimei%20Wang%2C%20Zhaowen%20Universal%20style%20transfer%20via%20feature%20transforms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yijun%20Fang%2C%20Chen%20Yang%2C%20Jimei%20Wang%2C%20Zhaowen%20Universal%20style%20transfer%20via%20feature%20transforms%202017"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz. A closed-form solution to photorealistic image stylization. arXiv preprint arXiv:1802.06474, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06474"
        },
        {
            "id": "Lin_et+al_2018_a",
            "entry": "Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Conditional image-to-image translation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Jianxin%20Xia%2C%20Yingce%20Qin%2C%20Tao%20Chen%2C%20Zhibo%20Conditional%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Jianxin%20Xia%2C%20Yingce%20Qin%2C%20Tao%20Chen%2C%20Zhibo%20Conditional%20image-to-image%20translation%202018"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ming-Yu%20Breuel%2C%20Thomas%20Kautz%2C%20Jan%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015"
        },
        {
            "id": "Luan_et+al_2017_a",
            "entry": "Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luan%2C%20Fujun%20Paris%2C%20Sylvain%20Shechtman%2C%20Eli%20Bala%2C%20Kavita%20Deep%20photo%20style%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luan%2C%20Fujun%20Paris%2C%20Sylvain%20Shechtman%2C%20Eli%20Bala%2C%20Kavita%20Deep%20photo%20style%20transfer%202017"
        },
        {
            "id": "Van_2008_a",
            "entry": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Kr\u00e4henb\u00fchl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Richter_et+al_2016_a",
            "entry": "Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richter%2C%20Stephan%20R.%20Vineet%2C%20Vibhav%20Roth%2C%20Stefan%20Koltun%2C%20Vladlen%20Playing%20for%20data%3A%20Ground%20truth%20from%20computer%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richter%2C%20Stephan%20R.%20Vineet%2C%20Vibhav%20Roth%2C%20Stefan%20Koltun%2C%20Vladlen%20Playing%20for%20data%3A%20Ground%20truth%20from%20computer%20games%202016"
        },
        {
            "id": "Royer_et+al_2017_a",
            "entry": "Am\u00e9lie Royer, Konstantinos Bousmalis, Stephan Gouws, Fred Bertsch, Inbar Moressi, Forrester Cole, and Kevin Murphy. Xgan: Unsupervised image-to-image translation for many-to-many mappings. arXiv preprint arXiv:1711.05139, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05139"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202018"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-to-end learning of driving models from large-scale video datasets. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huazhe%20Gao%2C%20Yang%20Yu%2C%20Fisher%20Darrell%2C%20Trevor%20End-to-end%20learning%20of%20driving%20models%20from%20large-scale%20video%20datasets%202017"
        },
        {
            "id": "Zili_2017_a",
            "entry": "Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-toimage translation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zili%20Yi%2C%20Hao%20%28Richard%29%20Zhang%2C%20Ping%20Tan%20Gong%2C%20Minglun%20Dualgan%3A%20Unsupervised%20dual%20learning%20for%20image-toimage%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zili%20Yi%2C%20Hao%20%28Richard%29%20Zhang%2C%20Ping%20Tan%20Gong%2C%20Minglun%20Dualgan%3A%20Unsupervised%20dual%20learning%20for%20image-toimage%20translation%202017"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Richard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Richard%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Colorful%20image%20colorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Richard%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Colorful%20image%20colorization%202016"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "Zhu_et+al_2017_b",
            "entry": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In NIPS, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Zhang%2C%20Richard%20Pathak%2C%20Deepak%20Darrell%2C%20Trevor%20Alexei%20A%20Efros%2C%20Oliver%20Wang%2C%20and%20Eli%20Shechtman.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Zhang%2C%20Richard%20Pathak%2C%20Deepak%20Darrell%2C%20Trevor%20Alexei%20A%20Efros%2C%20Oliver%20Wang%2C%20and%20Eli%20Shechtman.%20Toward%20multimodal%20image-to-image%20translation%202017"
        },
        {
            "id": "Letter-digit_2017_a",
            "entry": "Letter-digit translation. To further evaluate the generalization ability of our method, we use EMNIST (Cohen et al., 2017) for three translation tasks: 1) black-white letter \u2194 colored digit; 2) black-white digit \u2194 colored letter; 3) black-white letter \u2194 colored letter. EMNIST dataset, a set of handwritten character digits, has the same image format and dataset structure with MNIST dataset. We apply the same process to EMNIST letter images as that for MNIST single-digit images in the main paper. As shown in 10, our model trained with only single-digit data can successfully generalize to the letter data.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Letterdigit%20translation%20To%20further%20evaluate%20the%20generalization%20ability%20of%20our%20method%20we%20use%20EMNIST%20Cohen%20et%20al%202017%20for%20three%20translation%20tasks%201%20blackwhite%20letter%20%20colored%20digit%202%20blackwhite%20digit%20%20colored%20letter%203%20blackwhite%20letter%20%20colored%20letter%20EMNIST%20dataset%20a%20set%20of%20handwritten%20character%20digits%20has%20the%20same%20image%20format%20and%20dataset%20structure%20with%20MNIST%20dataset%20We%20apply%20the%20same%20process%20to%20EMNIST%20letter%20images%20as%20that%20for%20MNIST%20singledigit%20images%20in%20the%20main%20paper%20As%20shown%20in%2010%20our%20model%20trained%20with%20only%20singledigit%20data%20can%20successfully%20generalize%20to%20the%20letter%20data"
        }
    ]
}
