{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AN EMPIRICAL STUDY OF BINARY NEURAL NETWORKS\u2019 OPTIMISATION",
        "author": "Milad Alizadeh, Javier Fernandez-Marques, Nicholas D. Lane & Yarin Gal Department of Computer Science University of Oxford",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJfUCoR5KX"
        },
        "abstract": "Neural networks with deterministic binary weights using the Straight-ThroughEstimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy between the evaluated function in the forward path, and the weight updates in the backpropagation, updates which do not correspond to gradients of the forward path. Efficient convergence and accuracy of binary models often rely on careful finetuning and various ad-hoc techniques. In this work, we empirically identify and study the effectiveness of the various ad-hoc techniques commonly used in the literature, providing best-practices for efficient training of binary models. We show that adapting learning rates using second-moment methods is crucial for the successful use of the STE, and that other optimisers can easily get stuck in local minima. We also find that many of the commonly employed tricks are only effective towards the end of the training, with these methods making early stages of the training considerably slower. Our analysis disambiguates necessary from unnecessary ad-hoc techniques for the training of binary neural networks, paving the way for future development of solid theoretical foundations for these. Our newly-found insights further lead to new procedures which make training of existing binary neural networks notably faster."
    },
    "keywords": [
        {
            "term": "empirical study",
            "url": "https://en.wikipedia.org/wiki/empirical_study"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "Batch normalisation",
            "url": "https://en.wikipedia.org/wiki/Batch_normalisation"
        }
    ],
    "abbreviations": {
        "DNNs": "Deep Neural Networks",
        "BNNs": "Binary Neural Networks",
        "ReLUs": "rectified linear units",
        "BN": "Batch normalisation"
    },
    "highlights": [
        "There is great interest in expanding usage of Deep Neural Networks (DNNs) from running remotely in the cloud to performing local on-device inference on resource-constrained devices (<a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\"><a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\">Sze et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\"><a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\">Lane & Warden, 2018</a></a>)",
        "We identify the essential techniques required for successful optimisation of binary models and show that end-to-end training of binary networks crucially relies on the optimiser taking advantage of second-moment gradient estimates",
        "Quantised models were derived by quantisating full-precision weights of pre-trained models (<a class=\"ref-link\" id=\"cGong_et+al_2014_a\" href=\"#rGong_et+al_2014_a\">Gong et al, 2014</a>)",
        "We show the impact of clipping gradients and weights followed by batch normalisation hyper-parameters on convergence speed and accuracy of Binary Neural Networks",
        "We study the landscape of binary neural networks and evaluate the impact of various techniques on the accuracy and convergence performance of binary models",
        "We show that training binary models are harder and slower than the equivalent non-binary model"
    ],
    "key_statements": [
        "There is great interest in expanding usage of Deep Neural Networks (DNNs) from running remotely in the cloud to performing local on-device inference on resource-constrained devices (<a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\"><a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\">Sze et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\"><a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\">Lane & Warden, 2018</a></a>)",
        "We identify the essential techniques required for successful optimisation of binary models and show that end-to-end training of binary networks crucially relies on the optimiser taking advantage of second-moment gradient estimates",
        "Quantised models were derived by quantisating full-precision weights of pre-trained models (<a class=\"ref-link\" id=\"cGong_et+al_2014_a\" href=\"#rGong_et+al_2014_a\">Gong et al, 2014</a>)",
        "<a class=\"ref-link\" id=\"cHubara_et+al_2017_a\" href=\"#rHubara_et+al_2017_a\">Hubara et al (2017</a>) showed that in order to maintain model performance, quantisation must be incorporated as part of the training process. This is done by either performing additional training steps to fine-tune a quantised model or by directly learning quantised parameters. This is essential for Binary Neural Networks where binarising weights of pre-trained models result in significant loss in accuracy",
        "We show the impact of clipping gradients and weights followed by batch normalisation hyper-parameters on convergence speed and accuracy of Binary Neural Networks",
        "A possible hypothesis is that early stages of training binary models require more averaging for the optimiser to proceed in presence of binarisaton operation",
        "We study the landscape of binary neural networks and evaluate the impact of various techniques on the accuracy and convergence performance of binary models",
        "We show that training binary models are harder and slower than the equivalent non-binary model"
    ],
    "summary": [
        "There is great interest in expanding usage of Deep Neural Networks (DNNs) from running remotely in the cloud to performing local on-device inference on resource-constrained devices (<a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\"><a class=\"ref-link\" id=\"cSze_et+al_2017_a\" href=\"#rSze_et+al_2017_a\">Sze et al, 2017</a></a>; <a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\"><a class=\"ref-link\" id=\"cLane_2018_a\" href=\"#rLane_2018_a\">Lane & Warden, 2018</a></a>).",
        "We show that most of the commonly used tricks in training binary models, such as gradient and weight clipping, are only required during the final stages of the training to achieve the best performance.",
        "This is essential for BNNs where binarising weights of pre-trained models result in significant loss in accuracy.",
        "We show the impact of clipping gradients and weights followed by batch normalisation hyper-parameters on convergence speed and accuracy of BNNs. We finish by testing the effectiveness of some of the other commonly used tweaks used in training binary models.",
        "We observed great variance in convergence speed and model performance as a result of optimiser choice that goes beyond differences seen when training non-binary models.",
        "Similar to SGD, increasing momentum rate improves training speed significantly but results in worse final model accuracy.",
        "A possible hypothesis is that early stages of training binary models require more averaging for the optimiser to proceed in presence of binarisaton operation.",
        "While weight and gradient clipping help achieve better accuracy, our hypothesis is that they are only required in the later stages of training where the noise added by clipping weights and gradients increases the exploration of the optimiser.",
        "We tested this hypothesis by training a binary model in two stages: (1) using vanilla STE in the first stage with higher learning rates and (2) turning clippings back on when the accuracy stops improving by reducing learning rate.",
        "With vanilla STE the gradients are passed along to the full-precision proxies and the model is optimised as if the binarisation operations were not present.",
        "Our experiments show that this works well in terms of accuracy but converges considerably faster for ResNet-18 (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>) and VGG-10 architectures than if we had trained these binary models end-to-end.",
        "This result is encouraging because it turns the problem of learning binary models into a fine-tuning stage that can be applied to available pre-trained models.",
        "We study the landscape of binary neural networks and evaluate the impact of various techniques on the accuracy and convergence performance of binary models.",
        "ADAM for optimising the objective, (2) not using early stopping, (3) splitting the training into two stages, (4) removing gradient and weight clipping in the first stage and (5) reducing the averaging rate in Batch Normalisation layers in the second stage.",
        "For efficient training of Binary models we recommend: (1) using"
    ],
    "headline": "We empirically identify and study the effectiveness of the various ad-hoc techniques commonly used in the literature, providing best-practices for efficient training of binary models",
    "reference_links": [
        {
            "id": "Anderson_2018_a",
            "entry": "Alexander G. Anderson and Cory P. Berg. The high-dimensional geometry of binary neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Alexander%20G.%20Berg%2C%20Cory%20P.%20The%20high-dimensional%20geometry%20of%20binary%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Alexander%20G.%20Berg%2C%20Cory%20P.%20The%20high-dimensional%20geometry%20of%20binary%20neural%20networks%202018"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Bulat_2017_a",
            "entry": "Adrian Bulat et al. Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources. In International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bulat%2C%20Adrian%20Binarized%20convolutional%20landmark%20localizers%20for%20human%20pose%20estimation%20and%20face%20alignment%20with%20limited%20resources%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bulat%2C%20Adrian%20Binarized%20convolutional%20landmark%20localizers%20for%20human%20pose%20estimation%20and%20face%20alignment%20with%20limited%20resources%202017"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Zhaowei%20He%2C%20Xiaodong%20Sun%2C%20Jian%20Vasconcelos%2C%20Nuno%20Deep%20learning%20with%20low%20precision%20by%20half-wave%20gaussian%20quantization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Zhaowei%20He%2C%20Xiaodong%20Sun%2C%20Jian%20Vasconcelos%2C%20Nuno%20Deep%20learning%20with%20low%20precision%20by%20half-wave%20gaussian%20quantization%202017"
        },
        {
            "id": "Courbariaux_et+al_2015_a",
            "entry": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "Courbariaux_et+al_2016_a",
            "entry": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20jia%20Li%2C%20Li%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20jia%20Li%2C%20Li%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Gong_et+al_2014_a",
            "entry": "Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "Han_et+al_2017_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Hubara_et+al_2017_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869\u20136898, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubara%2C%20Itay%20Courbariaux%2C%20Matthieu%20Soudry%2C%20Daniel%20El-Yaniv%2C%20Ran%20Quantized%20neural%20networks%3A%20Training%20neural%20networks%20with%20low%20precision%20weights%20and%20activations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubara%2C%20Itay%20Courbariaux%2C%20Matthieu%20Soudry%2C%20Daniel%20El-Yaniv%2C%20Ran%20Quantized%20neural%20networks%3A%20Training%20neural%20networks%20with%20low%20precision%20weights%20and%20activations%202017"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning-Volume 37, pp. 448\u2013456. JMLR. org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%20448456%20JMLR%20org%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%20448456%20JMLR%20org%202015"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Krishnamoorthi_2018_a",
            "entry": "Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08342"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lane_2018_a",
            "entry": "N. D. Lane and P. Warden. The deep (learning) transformation of mobile and embedded computing. Computer, 51(5):12\u201316, May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lane%2C%20N.D.%20Warden%2C%20P.%20The%20deep%20%28learning%29%20transformation%20of%20mobile%20and%20embedded%20computing%202018-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lane%2C%20N.D.%20Warden%2C%20P.%20The%20deep%20%28learning%29%20transformation%20of%20mobile%20and%20embedded%20computing%202018-05"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The \u201dMNIST\u201d database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pp. 5811\u20135821, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hao%20De%2C%20Soham%20Xu%2C%20Zheng%20Studer%2C%20Christoph%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hao%20De%2C%20Soham%20Xu%2C%20Zheng%20Studer%2C%20Christoph%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In Advances in Neural Information Processing Systems, pp. 345\u2013353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Xiaofan%20Zhao%2C%20Cong%20Pan%2C%20Wei%20Towards%20accurate%20binary%20convolutional%20neural%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Xiaofan%20Zhao%2C%20Cong%20Pan%2C%20Wei%20Towards%20accurate%20binary%20convolutional%20neural%20network%202017"
        },
        {
            "id": "Louizos_et+al_2017_a",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3288\u20133298. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Mishra_2018_a",
            "entry": "Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve lowprecision network accuracy. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Asit%20Marr%2C%20Debbie%20Apprentice%3A%20Using%20knowledge%20distillation%20techniques%20to%20improve%20lowprecision%20network%20accuracy%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Asit%20Marr%2C%20Debbie%20Apprentice%3A%20Using%20knowledge%20distillation%20techniques%20to%20improve%20lowprecision%20network%20accuracy%202018"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Sutskever_et+al_2013_a",
            "entry": "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "Sze_et+al_2017_a",
            "entry": "Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295\u20132329, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sze%2C%20Vivienne%20Chen%2C%20Yu-Hsin%20Yang%2C%20Tien-Ju%20Emer%2C%20Joel%20S.%20Efficient%20processing%20of%20deep%20neural%20networks%3A%20A%20tutorial%20and%20survey%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sze%2C%20Vivienne%20Chen%2C%20Yu-Hsin%20Yang%2C%20Tien-Ju%20Emer%2C%20Joel%20S.%20Efficient%20processing%20of%20deep%20neural%20networks%3A%20A%20tutorial%20and%20survey%202017"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017"
        },
        {
            "id": "Xiang_et+al_2017_a",
            "entry": "Xu Xiang, Yanmin Qian, and Kai Yu. Binary deep neural networks for speech recognition. In Proc. Interspeech 2017, pp. 533\u2013537, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiang%2C%20Xu%20Qian%2C%20Yanmin%20Yu%2C%20Kai%20Binary%20deep%20neural%20networks%20for%20speech%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiang%2C%20Xu%20Qian%2C%20Yanmin%20Yu%2C%20Kai%20Binary%20deep%20neural%20networks%20for%20speech%20recognition%202017"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. DoReFaNet: Training low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. CoRR, abs/1612.01064, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        }
    ]
}
