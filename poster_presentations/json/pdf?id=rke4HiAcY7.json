{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "CAVEATS FOR INFORMATION BOTTLENECK",
        "author": "IN DETERMINISTIC SCENARIOS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rke4HiAcY7"
        },
        "abstract": "Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y . To do so, IB identifies an intermediate \u201cbottleneck\u201d variable T that has low mutual information I(X; T ) and high mutual information I(Y ; T ). The IB curve characterizes the set of bottleneck variables that achieve maximal I(Y ; T ) for a given I(X; T ), and is typically explored by maximizing the IB Lagrangian, I(Y ; T ) \u2212 \u03b2I(X; T ). In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of \u03b2; (2) there are \u201cuninteresting\u201d trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal. We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way. To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases. We demonstrate the three caveats on the MNIST dataset."
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "markov condition",
            "url": "https://en.wikipedia.org/wiki/markov_condition"
        },
        {
            "term": "data processing inequality",
            "url": "https://en.wikipedia.org/wiki/data_processing_inequality"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "classification problem",
            "url": "https://en.wikipedia.org/wiki/classification_problem"
        }
    ],
    "abbreviations": {
        "IB": "information bottleneck",
        "DPI": "data processing inequality",
        "SGD": "stochastic gradient descent",
        "dIB": "deterministic IB"
    },
    "highlights": [
        "The information bottleneck (IB) method (<a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\"><a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\">Tishby et al, 1999</a></a>) provides a principled way to extract information that is present in one variable that is relevant for predicting another variable",
        "We demonstrate three caveats for information bottleneck that appear whenever Y is a deterministic function of X: 1",
        "When Y is a deterministic function of X, the fact that a variable T is on the information bottleneck curve does not necessarily imply that T is an interesting compressed representation of X",
        "We showed that in any scenario where Y is a deterministic function of X \u2014 which includes many classification problems \u2014 information bottleneck demonstrates behavior that is qualitatively different from when the mapping from X to Y is stochastic",
        "In such cases: (1) the information bottleneck curve cannot be recovered by maximizing the information bottleneck Lagrangian I(Y ; T ) \u2212 \u03b2I(X; T ) while varying \u03b2; (2) all points on the information bottleneck curve contain \u201cuninteresting\u201d representations of inputs; (3) multi-layer classifiers that achieve zero probability of error cannot have a strict trade-off between prediction and compression among successive layers, contrary to a recent proposal",
        "Our results should not be taken to mean that the application of information bottleneck to supervised learning is without merit"
    ],
    "key_statements": [
        "The information bottleneck (IB) method (<a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\"><a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\">Tishby et al, 1999</a></a>) provides a principled way to extract information that is present in one variable that is relevant for predicting another variable",
        "We demonstrate three caveats for information bottleneck that appear whenever Y is a deterministic function of X: 1",
        "Issues that arise when Y is a deterministic function of X, and its arguments are complementary to ours",
        "As we show in Appendix C, even if the relationship between X and Y is not perfectly deterministic but close to being so, the three caveats discussed in this paper still apply in an approximate sense.\n3 THE information bottleneck CURVE WHEN Y IS A DETERMINISTIC FUNCTION OF X",
        "We show that such intuitions will not generally hold when Y is a deterministic function of X",
        "When Y is a deterministic function of X, the fact that a variable T is on the information bottleneck curve does not necessarily imply that T is an interesting compressed representation of X",
        "When Y is a deterministic function of X, for any \u201cinteresting\u201d T , there will be an \u201cuninteresting\u201d T\u03b1 that achieves the same compression I(X; T ) and prediction I(Y ; T ) values, and information bottleneck does not distinguish between the two",
        "The authors demonstrated a strict trade-off using an artificial classification dataset, in which Y was defined to be a noisy function of X. This outcome cannot generally hold when Y is a deterministic function of X",
        "Suppose that a classifier achieves 0 probability of error on training data. This event, which can only occur when Y is a deterministic function of X, is commonly observed in real-world deep neural networks (Zhang et al, 2017)",
        "In contrast to the information bottleneck Lagrangian, by optimizing the squared-information bottleneck functional (Fig. 4B), we discover solutions located along different points on the information bottleneck curve for different values of \u03b2",
        "The information bottleneck principle has attracted a great deal of attention in various fields, including information theory, cognitive science, and machine learning, in the context of classification using neural networks",
        "We showed that in any scenario where Y is a deterministic function of X \u2014 which includes many classification problems \u2014 information bottleneck demonstrates behavior that is qualitatively different from when the mapping from X to Y is stochastic",
        "In such cases: (1) the information bottleneck curve cannot be recovered by maximizing the information bottleneck Lagrangian I(Y ; T ) \u2212 \u03b2I(X; T ) while varying \u03b2; (2) all points on the information bottleneck curve contain \u201cuninteresting\u201d representations of inputs; (3) multi-layer classifiers that achieve zero probability of error cannot have a strict trade-off between prediction and compression among successive layers, contrary to a recent proposal",
        "Our results should not be taken to mean that the application of information bottleneck to supervised learning is without merit",
        "Our work shows that to achieve varying rates of compression, one should use a different objective function than the information bottleneck Lagrangian"
    ],
    "summary": [
        "The information bottleneck (IB) method (<a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\"><a class=\"ref-link\" id=\"cTishby_et+al_1999_a\" href=\"#rTishby_et+al_1999_a\">Tishby et al, 1999</a></a>) provides a principled way to extract information that is present in one variable that is relevant for predicting another variable.",
        "Many supervised learning architectures use some kind of intermediate representation to make predictions about the output, such as hidden layers in neural networks.",
        "One is given an empirical distribution p(x, y) and defines an intermediate representation T that obeys the Markov condition Y \u2212 X \u2212 T ; during training, cross-entropy loss is minimized, which is equivalent to maximizing a lower bound on I(Y ; T ) hold).",
        "Note that we make no assumptions about the relationship between X and Yunder q\u03b8(y|x), the distribution of predicted outputs given inputs implemented by the supervised learning architecture, and our analysis holds even if q\u03b8(y|x) is nondeterministic.",
        "Recall that we consider IB in terms of the empirical distribution of inputs, hidden layers, and outputs given the training data (Section 2).",
        "In contrast to the IB Lagrangian, by optimizing the squared-IB functional (Fig. 4B), we discover solutions located along different points on the IB curve for different values of \u03b2.",
        "By maximizing the squared-IB functional, we could find IB-optimal solutions along different points of the IB curve.",
        "This shows that hidden layer activity was not compressed, in that it retained information about X that was irrelevant for predicting the class Y , and fell onto the flat part of the IB curve.",
        "For \u03b2 = 0 runs, as in regular supervised learning, the activity of the all hidden layers is located on the flat part of the IB curve, demonstrating a lack of a strict trade-off between prediction and compression.",
        "We showed that in any scenario where Y is a deterministic function of X \u2014 which includes many classification problems \u2014 IB demonstrates behavior that is qualitatively different from when the mapping from X to Y is stochastic.",
        "In such cases: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian I(Y ; T ) \u2212 \u03b2I(X; T ) while varying \u03b2; (2) all points on the IB curve contain \u201cuninteresting\u201d representations of inputs; (3) multi-layer classifiers that achieve zero probability of error cannot have a strict trade-off between prediction and compression among successive layers, contrary to a recent proposal.",
        "Our work shows that to achieve varying rates of compression, one should use a different objective function than the IB Lagrangian."
    ],
    "headline": "We demonstrate three caveats when using information bottleneck in any situation where Y is a deterministic function of X: the information bottleneck curve cannot be recovered by maximizing the information bottleneck Lagrangian for different values of \u03b2; there are \u201cuninteresting\u201d trivial solutions at all points of the information bottleneck curve; and for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal",
    "reference_links": [
        {
            "id": "Achille_2018_a",
            "entry": "Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achille%2C%20Alessandro%20Soatto%2C%20Stefano%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018"
        },
        {
            "id": "Ahlswede_1975_a",
            "entry": "Rudolf Ahlswede and J\u00e1nos K\u00f6rner. Source coding with Side Information and a Converse for Degraded Broadcast Channels. IEEE Transaction on Information Theory, pp. 9, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahlswede%2C%20Rudolf%20K%C3%B6rner%2C%20J%C3%A1nos%20Source%20coding%20with%20Side%20Information%20and%20a%20Converse%20for%20Degraded%20Broadcast%20Channels%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahlswede%2C%20Rudolf%20K%C3%B6rner%2C%20J%C3%A1nos%20Source%20coding%20with%20Side%20Information%20and%20a%20Converse%20for%20Degraded%20Broadcast%20Channels%201975"
        },
        {
            "id": "Alemi_et+al_2016_a",
            "entry": "Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep Variational Information Bottleneck. arXiv preprint arXiv:1612.00410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "Alemi_et+al_2018_a",
            "entry": "Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information bottleneck. arXiv preprint arXiv:1807.00906, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.00906"
        },
        {
            "id": "Amjad_2018_a",
            "entry": "Rana Ali Amjad and Bernhard C Geiger. Learning representations for neural network-based classification using the information bottleneck principle. arXiv preprint arXiv:1802.09766, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09766"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04062"
        },
        {
            "id": "Cardinal_2003_a",
            "entry": "Jean Cardinal. Compression of side information. In icme, pp. 569\u2013572. IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cardinal%2C%20Jean%20Compression%20of%20side%20information%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cardinal%2C%20Jean%20Compression%20of%20side%20information%202003"
        },
        {
            "id": "Chalk_et+al_2016_a",
            "entry": "Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational information bottleneck. In Advances in Neural Information Processing Systems, pp. 1957\u20131965, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chalk%2C%20Matthew%20Marre%2C%20Olivier%20Tkacik%2C%20Gasper%20Relevant%20sparse%20codes%20with%20variational%20information%20bottleneck%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chalk%2C%20Matthew%20Marre%2C%20Olivier%20Tkacik%2C%20Gasper%20Relevant%20sparse%20codes%20with%20variational%20information%20bottleneck%202016"
        },
        {
            "id": "Thomas_2011_a",
            "entry": "Thomas A Courtade and Richard D Wesel. Multiterminal source coding with an entropy-based distortion measure. In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pp. 2040\u20132044. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%20A%20Courtade%20and%20Richard%20D%20Wesel%20Multiterminal%20source%20coding%20with%20an%20entropybased%20distortion%20measure%20In%20Information%20Theory%20Proceedings%20ISIT%202011%20IEEE%20International%20Symposium%20on%20pp%2020402044%20IEEE%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomas%20A%20Courtade%20and%20Richard%20D%20Wesel%20Multiterminal%20source%20coding%20with%20an%20entropybased%20distortion%20measure%20In%20Information%20Theory%20Proceedings%20ISIT%202011%20IEEE%20International%20Symposium%20on%20pp%2020402044%20IEEE%202011"
        },
        {
            "id": "Cover_2012_a",
            "entry": "Thomas M. Cover and Joy A. Thomas. Elements of information theory. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20information%20theory%202012"
        },
        {
            "id": "Dai_et+al_2018_a",
            "entry": "Bin Dai, Chen Zhu, and David Wipf. Compressing neural networks using the variational information bottleneck. arXiv preprint arXiv:1802.10399, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10399"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Gabri_et+al_2018_a",
            "entry": "Marylou Gabri\u00e9, Andre Manoel, Cl\u00e9ment Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, and Lenka Zdeborov\u00e1. Entropy and mutual information in models of deep neural networks. arXiv preprint arXiv:1805.09785, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09785"
        },
        {
            "id": "Gilad-Bachrach_et+al_2003_a",
            "entry": "Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby. An Information Theoretic Tradeoff between Complexity and Accuracy. In Gerhard Goos, Juris Hartmanis, Jan van Leeuwen, Bernhard Sch\u00f6lkopf, and Manfred K. Warmuth (eds.), Learning Theory and Kernel Machines, volume 2777, pp. 595\u2013609. Springer Berlin Heidelberg, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilad-Bachrach%2C%20Ran%20Navot%2C%20Amir%20Tishby%2C%20Naftali%20An%20Information%20Theoretic%20Tradeoff%20between%20Complexity%20and%20Accuracy%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilad-Bachrach%2C%20Ran%20Navot%2C%20Amir%20Tishby%2C%20Naftali%20An%20Information%20Theoretic%20Tradeoff%20between%20Complexity%20and%20Accuracy%202003"
        },
        {
            "id": "Goldfeld_et+al_2018_a",
            "entry": "Ziv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and Yury Polyanskiy. Estimating information flow in neural networks. arXiv preprint arXiv:1810.05728, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.05728"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kolchinsky_2017_a",
            "entry": "Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017. Corrected version available at https://arxiv.org/abs/1706.02419.",
            "url": "https://arxiv.org/abs/1706.02419",
            "arxiv_url": "https://arxiv.org/pdf/1706.02419"
        },
        {
            "id": "Kolchinsky_et+al_1705_a",
            "entry": "Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear Information Bottleneck. arXiv preprint arXiv:1705.02436, May 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02436"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Miettinen_1999_a",
            "entry": "Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12 of international series in operations research and management science, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miettinen%2C%20Kaisa%20Nonlinear%20multiobjective%20optimization%2C%20volume%2012%20of%20international%20series%20in%20operations%20research%20and%20management%20science%201999"
        },
        {
            "id": "Rockafellar_2015_a",
            "entry": "Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20Ralph%20Tyrell%20Convex%20analysis%202015"
        },
        {
            "id": "Saxe_et+al_2018_a",
            "entry": "AM Saxe, Y Bansal, J Dapello, M Advani, A Kolchinsky, BD Tracey, and DD Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018"
        },
        {
            "id": "Shamir_et+al_2010_a",
            "entry": "Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information bottleneck. Theoretical Computer Science, 411(29-30):2696\u20132711, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Sabato%2C%20Sivan%20Tishby%2C%20Naftali%20Learning%20and%20generalization%20with%20the%20information%20bottleneck%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Sabato%2C%20Sivan%20Tishby%2C%20Naftali%20Learning%20and%20generalization%20with%20the%20information%20bottleneck%202010"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Slonim_2000_a",
            "entry": "Noam Slonim and Naftali Tishby. Document clustering using word clusters via the information bottleneck method. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 208\u2013215. ACM, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Slonim%2C%20Noam%20Tishby%2C%20Naftali%20Document%20clustering%20using%20word%20clusters%20via%20the%20information%20bottleneck%20method%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Slonim%2C%20Noam%20Tishby%2C%20Naftali%20Document%20clustering%20using%20word%20clusters%20via%20the%20information%20bottleneck%20method%202000"
        },
        {
            "id": "Strouse_2017_a",
            "entry": "Dj Strouse and David J. Schwab. The Deterministic Information Bottleneck. Neural Computation, pp. 1\u201320, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dj%20Strouse%20and%20David%20J%20Schwab%20The%20Deterministic%20Information%20Bottleneck%20Neural%20Computation%20pp%20120%20April%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dj%20Strouse%20and%20David%20J%20Schwab%20The%20Deterministic%20Information%20Bottleneck%20Neural%20Computation%20pp%20120%20April%202017"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Tishby_et+al_1999_a",
            "entry": "N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In 37th Allerton Conf on Communication, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Pereira%2C%20F.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Pereira%2C%20F.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In Information Theory Workshop (ITW), 2015 IEEE, pp. 1\u20135. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "Vera_et+al_2018_a",
            "entry": "Matias Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of the information bottleneck in representation learning. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1580\u20131584. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vera%2C%20Matias%20Piantanida%2C%20Pablo%20Vega%2C%20Leonardo%20Rey%20The%20role%20of%20the%20information%20bottleneck%20in%20representation%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vera%2C%20Matias%20Piantanida%2C%20Pablo%20Vega%2C%20Leonardo%20Rey%20The%20role%20of%20the%20information%20bottleneck%20in%20representation%20learning%202018"
        },
        {
            "id": "Witsenhausen_1975_a",
            "entry": "H. Witsenhausen and A. Wyner. A conditional entropy bound for a pair of discrete random variables. IEEE Transactions on Information Theory, 21(5):493\u2013501, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Witsenhausen%2C%20H.%20Wyner%2C%20A.%20A%20conditional%20entropy%20bound%20for%20a%20pair%20of%20discrete%20random%20variables%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Witsenhausen%2C%20H.%20Wyner%2C%20A.%20A%20conditional%20entropy%20bound%20for%20a%20pair%20of%20discrete%20random%20variables%201975"
        },
        {
            "id": "Zaslavsky_et+al_2018_a",
            "entry": "Noga Zaslavsky, Charles Kemp, Terry Regier, and Naftali Tishby. Efficient compression in color naming and its evolution. Proceedings of the National Academy of Sciences, 115(31):7937\u20137942, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaslavsky%2C%20Noga%20Kemp%2C%20Charles%20Regier%2C%20Terry%20Tishby%2C%20Naftali%20Efficient%20compression%20in%20color%20naming%20and%20its%20evolution%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaslavsky%2C%20Noga%20Kemp%2C%20Charles%20Regier%2C%20Terry%20Tishby%2C%20Naftali%20Efficient%20compression%20in%20color%20naming%20and%20its%20evolution%202018"
        },
        {
            "id": "Zeitler_et+al_2008_a",
            "entry": "Georg Zeitler, Ralf Koetter, Gerhard Bauch, and Joerg Widmer. Design of network coding functions in multihop relay networks. In Turbo Codes and Related Topics, 2008 5th International Symposium on, pp. 249\u2013254.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeitler%2C%20Georg%20Koetter%2C%20Ralf%20Bauch%2C%20Gerhard%20Widmer%2C%20Joerg%20Design%20of%20network%20coding%20functions%20in%20multihop%20relay%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeitler%2C%20Georg%20Koetter%2C%20Ralf%20Bauch%2C%20Gerhard%20Widmer%2C%20Joerg%20Design%20of%20network%20coding%20functions%20in%20multihop%20relay%20networks%202008"
        },
        {
            "id": "Zhang_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. Zhengmin Zhang. Estimating Mutual Information Via Kolmogorov Distance. IEEE Transactions on Information Theory, 53(9):3280\u20133282, September 2007. ISSN 0018-9448. doi: 10.1109/TIT. 2007.903122. URL http://ieeexplore.ieee.org/document/4294175/.",
            "crossref": "https://dx.doi.org/10.1109/TIT.2007.903122",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TIT.2007.903122"
        }
    ]
}
