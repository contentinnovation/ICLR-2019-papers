{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EIDETIC 3D LSTM: A MODEL FOR VIDEO PREDICTION AND BEYOND",
        "author": "Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, Li Fei-Fei, 1Tsinghua University, 2Google AI, 3University of California, Merced, 4Stanford University",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1lKS2AqtX"
        },
        "abstract": "Spatiotemporal predictive learning, though long considered to be a promising selfsupervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency."
    },
    "keywords": [
        {
            "term": "predictive learning",
            "url": "https://en.wikipedia.org/wiki/predictive_learning"
        },
        {
            "term": "short term",
            "url": "https://en.wikipedia.org/wiki/short_term"
        },
        {
            "term": "physical interaction",
            "url": "https://en.wikipedia.org/wiki/physical_interaction"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "action recognition",
            "url": "https://en.wikipedia.org/wiki/action_recognition"
        },
        {
            "term": "long term",
            "url": "https://en.wikipedia.org/wiki/long_term"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks",
        "3D-CNNs": "3D Convolutional Neural networks",
        "3D-Conv": "3D convolution",
        "VPN": "video pixel network",
        "ST-LSTM": "spatiotemporal LSTM",
        "SSIM": "structural similarity index measure",
        "MSE": "mean squared error"
    },
    "highlights": [
        "A fundamental problem in spatiotemporal predictive learning is how to effectively learn good representations for video inference or reasoning",
        "We propose a \u201cdeeper\u201d integration of 3D convolution inside the LSTM unit in order to incorporate the convolutional features into the recurrent state transition over time",
        "The results show that the E3D-LSTM network is effective in modeling spatiotemporal data for video",
        "We report mean squared error at every time stamp in Table 4 where lower scores indicate better prediction results",
        "We presented the E3D-LSTM model based on 3D convolutional recurrent units for this task",
        "Experimental results demonstrate that the E3D-LSTM model performs favorably against the state-of-the-art methods on video prediction and early activity recognition tasks"
    ],
    "key_statements": [
        "A fundamental problem in spatiotemporal predictive learning is how to effectively learn good representations for video inference or reasoning",
        "Variants of 3D Convolutional Neural networks, such as Inflated 3D Convolutional Neural networks, have significantly increased action classification accuracy over the UCF 101 and Kinetics datasets. These 3D Convolutional Neural networks architectures have no recurrent structures but instead employ 3D convolution (3D-Conv) and 3D pooling operations to preserve temporal information of the input sequences which would be otherwise discarded in classical 2D convolution operations",
        "Motivated by the recent success of 3D Convolutional Neural networks, in this paper we propose a new model for spatiotemporal predictive learning based on both recurrent modeling and feedforward 3D convolution modeling",
        "We propose a new model called Eidetic 3D LSTM (E3D-LSTM) for spatiotemporal predictive learning",
        "We find that integrating the 3D convolution outside the LSTM unit performs noticeably worse than the baseline recurrent neural networks model",
        "We propose a \u201cdeeper\u201d integration of 3D convolution inside the LSTM unit in order to incorporate the convolutional features into the recurrent state transition over time",
        "We evaluate the proposed E3D-LSTM model on two tasks: future video prediction and early activity recognition",
        "The E3D-LSTM model is evaluated against the state-of-the-art methods including the ConvLSTM network (<a class=\"ref-link\" id=\"cShi_et+al_2015_a\" href=\"#rShi_et+al_2015_a\">Shi et al, 2015</a>), DFN (De Brabandere et al, 2016), CDNA (<a class=\"ref-link\" id=\"cFinn_et+al_2016_a\" href=\"#rFinn_et+al_2016_a\">Finn et al, 2016</a>), video pixel network baseline model with CNN decoders (<a class=\"ref-link\" id=\"cKalchbrenner_et+al_2017_a\" href=\"#rKalchbrenner_et+al_2017_a\">Kalchbrenner et al, 2017</a>), PredRNN (<a class=\"ref-link\" id=\"cWang_et+al_2017_a\" href=\"#rWang_et+al_2017_a\">Wang et al, 2017</a>), PredRNN++ (<a class=\"ref-link\" id=\"cWang_et+al_2018_b\" href=\"#rWang_et+al_2018_b\">Wang et al, 2018b</a>) and FRNN (<a class=\"ref-link\" id=\"cOliu_et+al_2018_a\" href=\"#rOliu_et+al_2018_a\">Oliu et al, 2018</a>)",
        "The structural similarity index measure ranges between \u22121 and 1, representing the similarity between the generated image and the ground truth",
        "The results show that the E3D-LSTM network is effective in modeling spatiotemporal data for video",
        "Consistent with the observations on the moving MNIST dataset, the E3D-LSTM model performs favorably against the state-of-the-art methods across three settings of predicting future 10 frames, 20 frames, and copy test. These empirical results demonstrate the effectiveness of the E3D-LSTM model for modeling spatiotemporal data",
        "We report mean squared error at every time stamp in Table 4 where lower scores indicate better prediction results",
        "We evaluate the E3D-LSTM model against the state-of-the-art feed-forward 3D convolution architectures including C3D/I3D (<a class=\"ref-link\" id=\"cDiba_et+al_2016_a\" href=\"#rDiba_et+al_2016_a\">Diba et al, 2016</a>; <a class=\"ref-link\" id=\"cCarreira_2017_a\" href=\"#rCarreira_2017_a\">Carreira & Zisserman, 2017</a>), Separable 3D Convolutional Neural networks (<a class=\"ref-link\" id=\"cXie_et+al_2018_a\" href=\"#rXie_et+al_2018_a\">Xie et al, 2018</a>; <a class=\"ref-link\" id=\"cQiu_et+al_2017_a\" href=\"#rQiu_et+al_2017_a\">Qiu et al, 2017</a>) and (2+1)D-CNN (<a class=\"ref-link\" id=\"cTran_et+al_2018_a\" href=\"#rTran_et+al_2018_a\">Tran et al, 2018</a>)",
        "We presented the E3D-LSTM model based on 3D convolutional recurrent units for this task",
        "We integrated 3D convolution into state transitions to perceive short-term motions and designed a memory attentive module controlled by recurrent gates to capture the long-term video frame interaction",
        "Experimental results demonstrate that the E3D-LSTM model performs favorably against the state-of-the-art methods on video prediction and early activity recognition tasks"
    ],
    "summary": [
        "A fundamental problem in spatiotemporal predictive learning is how to effectively learn good representations for video inference or reasoning.",
        "Beyond frames prediction, RNN based models are less effective in learning high-level video representations or capturing long-term relations.",
        "We integrate them in a unified network by applying layer normalization (<a class=\"ref-link\" id=\"cBa_et+al_2016_a\" href=\"#rBa_et+al_2016_a\">Ba et al, 2016</a>) to their element-wise sum, in order to mitigate the covariant shift and stabilize the training process, as it has been commonly used in RNNs. The hyper-parameter \u03c4 in Ctk\u2212\u03c4:t\u22121 decides how many historical memory states are attended by the recall gate Rt. To involve more long-term relations, in most experiments, we take C1k:t\u22121 as the inputs of the RECALL function and do not fix \u03c4 .",
        "We evaluate the proposed E3D-LSTM model on two tasks: future video prediction and early activity recognition.",
        "The E3D-LSTM model is evaluated against the state-of-the-art methods including the ConvLSTM network (<a class=\"ref-link\" id=\"cShi_et+al_2015_a\" href=\"#rShi_et+al_2015_a\">Shi et al, 2015</a>), DFN (De Brabandere et al, 2016), CDNA (<a class=\"ref-link\" id=\"cFinn_et+al_2016_a\" href=\"#rFinn_et+al_2016_a\">Finn et al, 2016</a>), VPN baseline model with CNN decoders (<a class=\"ref-link\" id=\"cKalchbrenner_et+al_2017_a\" href=\"#rKalchbrenner_et+al_2017_a\">Kalchbrenner et al, 2017</a>), PredRNN (<a class=\"ref-link\" id=\"cWang_et+al_2017_a\" href=\"#rWang_et+al_2017_a\">Wang et al, 2017</a>), PredRNN++ (<a class=\"ref-link\" id=\"cWang_et+al_2018_b\" href=\"#rWang_et+al_2018_b\">Wang et al, 2018b</a>) and FRNN (<a class=\"ref-link\" id=\"cOliu_et+al_2018_a\" href=\"#rOliu_et+al_2018_a\">Oliu et al, 2018</a>).",
        "Thanks to the eidetic 3D memory, our E3D-LSTM model captures the long-term video frame interactions and performs well in both metrics.",
        "Consistent with the observations on the moving MNIST dataset, the E3D-LSTM model performs favorably against the state-of-the-art methods across three settings of predicting future 10 frames, 20 frames, and copy test.",
        "To validate that the E3D-LSTM model can learn high-level video representations effectively, we carry out experiments on early activity recognition.",
        "We adopt the early activity recognition setting (<a class=\"ref-link\" id=\"cMa_et+al_2016_a\" href=\"#rMa_et+al_2016_a\">Ma et al, 2016</a>; <a class=\"ref-link\" id=\"cZeng_et+al_2017_a\" href=\"#rZeng_et+al_2017_a\">Zeng et al, 2017</a>; <a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\">Zhou et al, 2018</a>), where a model predicts an action type after observing the first 25% or 50% frames of each video.",
        "Table 5 shows the classification accuracy of the E3D-LSTM network against the stateof-the-art feed-forward 3D-CNNs. The E3D-LSTM model performs favorably against the other methods in two settings of using the first 25% and 50% frames, showing its effectiveness in learning high-level spatiotemporal representations.",
        "The proposed self-supervised auxiliary learning approach performs better than other alternatives, including using video prediction models as network initialization, or training the model under these two tasks with a fixed objective function ratio.",
        "We integrated 3D-Convs into state transitions to perceive short-term motions and designed a memory attentive module controlled by recurrent gates to capture the long-term video frame interaction.",
        "Experimental results demonstrate that the E3D-LSTM model performs favorably against the state-of-the-art methods on video prediction and early activity recognition tasks."
    ],
    "headline": "We present a new model, Eidetic 3D LSTM, that integrates 3D convolutions into recurrent neural networks",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bengio_et+al_2015_a",
            "entry": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Bhattacharjee_2017_a",
            "entry": "Prateep Bhattacharjee and Sukhendu Das. Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhattacharjee%2C%20Prateep%20Das%2C%20Sukhendu%20Temporal%20coherency%20based%20criteria%20for%20predicting%20video%20frames%20using%20deep%20multi-stage%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhattacharjee%2C%20Prateep%20Das%2C%20Sukhendu%20Temporal%20coherency%20based%20criteria%20for%20predicting%20video%20frames%20using%20deep%20multi-stage%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Carreira_2017_a",
            "entry": "Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "Brabandere_et+al_2016_a",
            "entry": "Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brabandere%2C%20Bert%20De%20Jia%2C%20Xu%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20Van%20Dynamic%20filter%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brabandere%2C%20Bert%20De%20Jia%2C%20Xu%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20Van%20Dynamic%20filter%20networks%202016"
        },
        {
            "id": "Denton_2018_a",
            "entry": "Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20Fergus%2C%20Rob%20Stochastic%20video%20generation%20with%20a%20learned%20prior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20Fergus%2C%20Rob%20Stochastic%20video%20generation%20with%20a%20learned%20prior%202018"
        },
        {
            "id": "Diba_et+al_2016_a",
            "entry": "Ali Diba, Ali Mohammad Pazandeh, and Luc Van Gool. Efficient two-stream motion and appearance 3d cnns for video classification. In ECCV Workshop, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diba%2C%20Ali%20Pazandeh%2C%20Ali%20Mohammad%20Gool%2C%20Luc%20Van%20Efficient%20two-stream%20motion%20and%20appearance%203d%20cnns%20for%20video%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diba%2C%20Ali%20Pazandeh%2C%20Ali%20Mohammad%20Gool%2C%20Luc%20Van%20Efficient%20two-stream%20motion%20and%20appearance%203d%20cnns%20for%20video%20classification%202016"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Goodfellow%2C%20Ian%20Levine%2C%20Sergey%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \"something something\" video database for learning and evaluating visual common sense. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Raghav%20Kahou%2C%20Samira%20Ebrahimi%20Michalski%2C%20Vincent%20Materzynska%2C%20Joanna%20The%20%22something%20something%22%20video%20database%20for%20learning%20and%20evaluating%20visual%20common%20sense%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Raghav%20Kahou%2C%20Samira%20Ebrahimi%20Michalski%2C%20Vincent%20Materzynska%2C%20Joanna%20The%20%22something%20something%22%20video%20database%20for%20learning%20and%20evaluating%20visual%20common%20sense%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computaiton, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Ji_et+al_2013_a",
            "entry": "Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. TPAMI, 35(1):221\u2013231, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203d%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203d%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013"
        },
        {
            "id": "Kalchbrenner_et+al_2017_a",
            "entry": "Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nal%20Kalchbrenner%20Aaron%20van%20den%20Oord%20Karen%20Simonyan%20Ivo%20Danihelka%20Oriol%20Vinyals%20Alex%20Graves%20and%20Koray%20Kavukcuoglu%20Video%20pixel%20networks%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nal%20Kalchbrenner%20Aaron%20van%20den%20Oord%20Karen%20Simonyan%20Ivo%20Danihelka%20Oriol%20Vinyals%20Alex%20Graves%20and%20Koray%20Kavukcuoglu%20Video%20pixel%20networks%20In%20ICML%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Zhouhan%20Feng%2C%20Minwei%20dos%20Santos%2C%20Cicero%20Nogueira%20Yu%2C%20Mo%20and%20Yoshua%20Bengio.%20A%20structured%20self-attentive%20sentence%20embedding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Zhouhan%20Feng%2C%20Minwei%20dos%20Santos%2C%20Cicero%20Nogueira%20Yu%2C%20Mo%20and%20Yoshua%20Bengio.%20A%20structured%20self-attentive%20sentence%20embedding%202017"
        },
        {
            "id": "Lu_et+al_2017_a",
            "entry": "Chaochao Lu, Michael Hirsch, and Bernhard Sch\u00f6lkopf. Flexible spatio-temporal networks for video prediction. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Chaochao%20Hirsch%2C%20Michael%20Sch%C3%B6lkopf%2C%20Bernhard%20Flexible%20spatio-temporal%20networks%20for%20video%20prediction%202017"
        },
        {
            "id": "Ma_et+al_2016_a",
            "entry": "Shugao Ma, Leonid Sigal, and Stan Sclaroff. Learning activity progression in lstms for activity detection and early detection. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Shugao%20Sigal%2C%20Leonid%20Sclaroff%2C%20Stan%20Learning%20activity%20progression%20in%20lstms%20for%20activity%20detection%20and%20early%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Shugao%20Sigal%2C%20Leonid%20Sclaroff%2C%20Stan%20Learning%20activity%20progression%20in%20lstms%20for%20activity%20detection%20and%20early%20detection%202016"
        },
        {
            "id": "Mathieu_et+al_2016_a",
            "entry": "Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "Oliu_et+al_2018_a",
            "entry": "Marc Oliu, Javier Selva, and Sergio Escalera. Folded recurrent neural networks for future video prediction. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliu%2C%20Marc%20Selva%2C%20Javier%20Escalera%2C%20Sergio%20Folded%20recurrent%20neural%20networks%20for%20future%20video%20prediction%202018"
        },
        {
            "id": "Qiu_et+al_2017_a",
            "entry": "Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiu%2C%20Zhaofan%20Yao%2C%20Ting%20Mei%2C%20Tao%20Learning%20spatio-temporal%20representation%20with%20pseudo-3d%20residual%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qiu%2C%20Zhaofan%20Yao%2C%20Ting%20Mei%2C%20Tao%20Learning%20spatio-temporal%20representation%20with%20pseudo-3d%20residual%20networks%202017"
        },
        {
            "id": "Schuldt_et+al_2004_a",
            "entry": "Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm approach. In ICPR, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuldt%2C%20Christian%20Laptev%2C%20Ivan%20Caputo%2C%20Barbara%20Recognizing%20human%20actions%3A%20a%20local%20svm%20approach%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuldt%2C%20Christian%20Laptev%2C%20Ivan%20Caputo%2C%20Barbara%20Recognizing%20human%20actions%3A%20a%20local%20svm%20approach%202004"
        },
        {
            "id": "Shi_et+al_2015_a",
            "entry": "Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Xingjian%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Dit-Yan%20Yeung%2C%20Wai-Kin%20Wong%2C%20and%20Wang-chun%20Woo.%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Xingjian%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Dit-Yan%20Yeung%2C%20Wai-Kin%20Wong%2C%20and%20Wang-chun%20Woo.%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015"
        },
        {
            "id": "Srivastava_et+al_2015_a",
            "entry": "Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using lstms. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Mansimov%2C%20Elman%20Salakhutdinov%2C%20Ruslan%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Mansimov%2C%20Elman%20Salakhutdinov%2C%20Ruslan%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tran_et+al_2015_a",
            "entry": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015"
        },
        {
            "id": "Tran_et+al_2018_a",
            "entry": "Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Wang%2C%20Heng%20Torresani%2C%20Lorenzo%20Ray%2C%20Jamie%20A%20closer%20look%20at%20spatiotemporal%20convolutions%20for%20action%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Wang%2C%20Heng%20Torresani%2C%20Lorenzo%20Ray%2C%20Jamie%20A%20closer%20look%20at%20spatiotemporal%20convolutions%20for%20action%20recognition%202018"
        },
        {
            "id": "Tulyakov_et+al_2018_a",
            "entry": "Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tulyakov%2C%20Sergey%20Liu%2C%20Ming-Yu%20Yang%2C%20Xiaodong%20Kautz%2C%20Jan%20Mocogan%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "Villegas_et+al_2017_a",
            "entry": "Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and content for natural video sequence prediction. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017"
        },
        {
            "id": "Vondrick_et+al_2016_a",
            "entry": "Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "Wang_2018_a",
            "entry": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiaolong%20Girshick%2C%20Ross%20Abhinav%20Gupta%2C%20and%20Kaiming%20He.%20Non-local%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiaolong%20Girshick%2C%20Ross%20Abhinav%20Gupta%2C%20and%20Kaiming%20He.%20Non-local%20neural%20networks%202018"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and S Yu Philip. Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yunbo%20Long%2C%20Mingsheng%20Wang%2C%20Jianmin%20Gao%2C%20Zhifeng%20Predrnn%3A%20Recurrent%20neural%20networks%20for%20predictive%20learning%20using%20spatiotemporal%20lstms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yunbo%20Long%2C%20Mingsheng%20Wang%2C%20Jianmin%20Gao%2C%20Zhifeng%20Predrnn%3A%20Recurrent%20neural%20networks%20for%20predictive%20learning%20using%20spatiotemporal%20lstms%202017"
        },
        {
            "id": "Wang_et+al_2018_b",
            "entry": "Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In ICML, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yunbo%20Gao%2C%20Zhifeng%20Long%2C%20Mingsheng%20Wang%2C%20Jianmin%20Predrnn%2B%2B%3A%20Towards%20a%20resolution%20of%20the%20deep-in-time%20dilemma%20in%20spatiotemporal%20predictive%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yunbo%20Gao%2C%20Zhifeng%20Long%2C%20Mingsheng%20Wang%2C%20Jianmin%20Predrnn%2B%2B%3A%20Towards%20a%20resolution%20of%20the%20deep-in-time%20dilemma%20in%20spatiotemporal%20predictive%20learning%202018"
        },
        {
            "id": "Zhou_et+al_2004_a",
            "entry": "Zhou Wang, A. C Bovik, H. R Sheikh, and E. P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%20Wang%2C%20A.C.Bovik%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%20Wang%2C%20A.C.Bovik%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004"
        },
        {
            "id": "Wichers_et+al_2018_a",
            "entry": "Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical long-term video prediction without supervision. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wichers%2C%20Nevan%20Villegas%2C%20Ruben%20Erhan%2C%20Dumitru%20Lee%2C%20Honglak%20Hierarchical%20long-term%20video%20prediction%20without%20supervision%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wichers%2C%20Nevan%20Villegas%2C%20Ruben%20Erhan%2C%20Dumitru%20Lee%2C%20Honglak%20Hierarchical%20long-term%20video%20prediction%20without%20supervision%202018"
        },
        {
            "id": "Xie_et+al_2018_a",
            "entry": "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning for video understanding. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Sun%2C%20Chen%20Huang%2C%20Jonathan%20Tu%2C%20Zhuowen%20Rethinking%20spatiotemporal%20feature%20learning%20for%20video%20understanding%202018"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Jingwei Xu, Bingbing Ni, Zefan Li, Shuo Cheng, and Xiaokang Yang. Structure preserving video prediction. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Jingwei%20Ni%2C%20Bingbing%20Li%2C%20Zefan%20Cheng%2C%20Shuo%20Structure%20preserving%20video%20prediction%202018"
        },
        {
            "id": "Zeng_et+al_2017_a",
            "entry": "Kuo-Hao Zeng, William B Shen, De-An Huang, Min Sun, and Juan Carlos Niebles. Visual forecasting by imitating dynamics in natural sequences. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeng%2C%20Kuo-Hao%20Shen%2C%20William%20B.%20Huang%2C%20De-An%20Sun%2C%20Min%20Visual%20forecasting%20by%20imitating%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeng%2C%20Kuo-Hao%20Shen%2C%20William%20B.%20Huang%2C%20De-An%20Sun%2C%20Min%20Visual%20forecasting%20by%20imitating%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd flows prediction. In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Junbo%20Zheng%2C%20Yu%20Qi%2C%20Dekang%20Deep%20spatio-temporal%20residual%20networks%20for%20citywide%20crowd%20flows%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Junbo%20Zheng%2C%20Yu%20Qi%2C%20Dekang%20Deep%20spatio-temporal%20residual%20networks%20for%20citywide%20crowd%20flows%20prediction%202017"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Bolei Zhou, Alex Andonian, and Antonio Torralba. Temporal relational reasoning in videos. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Andonian%2C%20Alex%20Torralba%2C%20Antonio%20Temporal%20relational%20reasoning%20in%20videos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Andonian%2C%20Alex%20Torralba%2C%20Antonio%20Temporal%20relational%20reasoning%20in%20videos%202018"
        }
    ]
}
