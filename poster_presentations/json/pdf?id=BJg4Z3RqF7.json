{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNSUPERVISED ADVERSARIAL IMAGE RECONSTRUCTION",
        "author": "Arthur Pajot, a, Emmanuel de B\u00e9zenac,a, Patrick Gallinari a, b {arthur.pajot, emmanuel.de-bezenac, patrick.gallinari}@lip,.fr a Sorbonne Universit\u00e9s, UMR 7606, LIP, F-75005 Paris, France b Criteo AI Lab, Paris, France",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJg4Z3RqF7"
        },
        "abstract": "We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the maximum a posteriori estimate of the signal given each measurement, and propose a general framework for the reconstruction problem. We use a formulation of generative adversarial networks, where the generator takes as input a corrupted observation in order to produce realistic reconstructions, and add a penalty term tying the reconstruction to the associated observation. We evaluate our reconstructions on several image datasets with different types of corruptions. The proposed approach yields better results than alternative baselines, and comparable performance with model variants trained with additional supervision."
    },
    "keywords": [
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "mean square error",
            "url": "https://en.wikipedia.org/wiki/mean_square_error"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "prior information",
            "url": "https://en.wikipedia.org/wiki/prior_information"
        },
        {
            "term": "signal recovery",
            "url": "https://en.wikipedia.org/wiki/signal_recovery"
        }
    ],
    "abbreviations": {
        "MAP": "maximum a posteriori",
        "MSE": "mean square error"
    },
    "highlights": [
        "Many real world applications require acquiring information about the state of some physical system from incomplete and inaccurate measurements",
        "In infrared satellite imagery, one has to deal with the presence of clouds and a variety of other external factors perturbing the acquisition of temperature maps",
        "We report mean square error (MSE) scores between the reconstructed xand the true signal x used to generate the input y",
        "Table 1 shows the mean square error computed on the test set, a randomly selected subset of CelebA comprised of 40000 images",
        "Because the Conditional AmbientGan model is too computationally expensive, we only report the mean square error on 40 randomly chosen samples of the test set",
        "Figure 4 shows reconstructions obtained from different models on the CelebA dataset.We observe that Conditional AmbientGAN yields visually poor results, especially for the Remove-Pixel and Remove-Pixel-Channel measurement processes"
    ],
    "key_statements": [
        "Many real world applications require acquiring information about the state of some physical system from incomplete and inaccurate measurements",
        "In infrared satellite imagery, one has to deal with the presence of clouds and a variety of other external factors perturbing the acquisition of temperature maps",
        "A common approach is to use handcrafted, analytically tractable priors (Cand\u00e8s et al (2005), <a class=\"ref-link\" id=\"cMota_et+al_2017_a\" href=\"#rMota_et+al_2017_a\">Mota et al (2017</a>)). This approach is limited to situations for which the underlying signal structure can be described, which are rarely observed in the wild",
        "We address the problem of image reconstruction in an unsupervised setting, when only corrupted observations are available, together with some prior information on the nature of the measurement process",
        "Note that even if the generation process of the observations y in AmbientGAN is similar to the one considered in this paper, the objective is different: when the aim of AmbientGAN is to learn a distribution of the underlying signal by sampling a latent space, ours is to reconstruct corrupted signals",
        "We report mean square error (MSE) scores between the reconstructed xand the true signal x used to generate the input y",
        "Table 1 shows the mean square error computed on the test set, a randomly selected subset of CelebA comprised of 40000 images",
        "Because the Conditional AmbientGan model is too computationally expensive, we only report the mean square error on 40 randomly chosen samples of the test set",
        "Table 1 \u2013 : Average mean square error of neural network based models on the test set of CelebA, for different measurement processes",
        "Figure 4 shows reconstructions obtained from different models on the CelebA dataset.We observe that Conditional AmbientGAN yields visually poor results, especially for the Remove-Pixel and Remove-Pixel-Channel measurement processes"
    ],
    "summary": [
        "Many real world applications require acquiring information about the state of some physical system from incomplete and inaccurate measurements.",
        "We address the problem of image reconstruction in an unsupervised setting, when only corrupted observations are available, together with some prior information on the nature of the measurement process.",
        "Equation (7) shows that the likelihood term can be evaluated by first sampling a measurement y conditioned on a corruption parameter \u03b8 and signal x, and constrain G such that y \u2212 F (G(y); \u03b8)",
        "In AmbientGAN, a generator is trained to produce uncorrupted signal samples from a latent code so that the generated signals when corrupted are indistinguishable from the observation measurements.",
        "Note that even if the generation process of the observations y in AmbientGAN is similar to the one considered in this paper, the objective is different: when the aim of AmbientGAN is to learn a distribution of the underlying signal by sampling a latent space, ours is to reconstruct corrupted signals.",
        "In Section 3.1, we have shown that it is possible to maximize the average log-likelihood term in equation (4), given that we can sample from the unknown prior distribution pX .",
        "The context is the same as for our model: the measurement process F is assumed known, there is no access to samples from the uncorrupted signal distribution pX , but only to their corrupted counterpart pY .",
        "This baseline is similar to our model but instead of discriminating between a measurement from the data y and a simulated measurement y, we directly discriminate between samples x from the signal distribution and the output of the reconstruction network x.",
        "Table 1 \u2013 : Average mean square error of neural network based models on the test set of CelebA, for different measurement processes.",
        "Figure 4 shows reconstructions obtained from different models on the CelebA dataset.We observe that Conditional AmbientGAN yields visually poor results, especially for the Remove-Pixel and Remove-Pixel-Channel measurement processes.",
        "They too use a generative model of the signal trained in an adversarial fashion using samples from signal distribution to constrain their reconstructions.",
        "Given a measurement from which we wish to reconstruct the signal, it is inverted by finding the latent input code that generated the uncorrupted image, by minimizing the mean square error between the corrupted reconstruction and the measurement.",
        "Another interesting research direction would be to make our reconstruction network stochastic, in order to approximate the true posterior of the signal given the measurement, and to obtain uncertainty estimates."
    ],
    "headline": "We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting",
    "reference_links": [
        {
            "id": "Alkinani_2017_a",
            "entry": "Monagi H. Alkinani and Mahmoud R. El-Sakka. Patch-based models and algorithms for image denoising: a comparative review between patch-based images denoising methods for additive noise reduction. EURASIP Journal on Image and Video Processing, 2017(1):58, Aug 2017. ISSN 1687-5281. doi: 10.1186/s13640-017-0203-4. URL https://doi.org/10.1186/s13640-017-0203-4.",
            "crossref": "https://dx.doi.org/10.1186/s13640-017-0203-4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1186/s13640-017-0203-4"
        },
        {
            "id": "Almahairi_et+al_2018_a",
            "entry": "Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint arXiv:1802.10151, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10151"
        },
        {
            "id": "Asim_et+al_2018_a",
            "entry": "Muhammad Asim, Fahad Shamshad, and Ali Ahmed. Solving bilinear inverse problems using deep generative priors. CoRR, abs/1802.04073, 2018. URL http://arxiv.org/abs/1802.04073.",
            "url": "http://arxiv.org/abs/1802.04073",
            "arxiv_url": "https://arxiv.org/pdf/1802.04073"
        },
        {
            "id": "Bora_et+al_2017_a",
            "entry": "Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed Sensing using Generative Models. arXiv:1703.03208 [cs, math, stat], March 2017. URL http://arxiv.org/abs/1703.03208.arXiv:1703.03208.",
            "url": "http://arxiv.org/abs/1703.03208.arXiv:1703.03208",
            "arxiv_url": "https://arxiv.org/pdf/1703.03208"
        },
        {
            "id": "Bora_et+al_2018_a",
            "entry": "Ashish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from lossy measurements. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hy7fDog0b.",
            "url": "https://openreview.net/forum?id=Hy7fDog0b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bora%2C%20Ashish%20Price%2C%20Eric%20Dimakis%2C%20Alexandros%20G.%20AmbientGAN%3A%20Generative%20models%20from%20lossy%20measurements%202018"
        },
        {
            "id": "Boyat_2015_a",
            "entry": "Ajay Kumar Boyat and Brijendra Kumar Joshi. A review paper: Noise models in digital image processing. CoRR, abs/1505.03489, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.03489"
        },
        {
            "id": "Cand_et+al_0000_a",
            "entry": "Emmanuel J. Cand\u00e8s, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207\u2013 1223, 2005. doi: 10.1002/cpa.20124. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20124.",
            "crossref": "https://dx.doi.org/10.1002/cpa.20124",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1002/cpa.20124"
        },
        {
            "id": "Chambolle_2004_a",
            "entry": "Antonin Chambolle. An Algorithm for Total Variation Minimization and Applications. Journal of Mathematical Imaging and Vision, 20(1):89\u201397, jan 2004. ISSN 1573-7683. doi: 10.1023/B:JMIV.0000011325.36760.1e. URL https://doi.org/10.1023/B:JMIV.0000011325.36760.1e.",
            "crossref": "https://dx.doi.org/10.1023/B:JMIV.0000011325.36760.1e",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1023/B%3AJMIV.0000011325.36760.1e"
        },
        {
            "id": "Damelin_2018_a",
            "entry": "SB Damelin and NS Hoang. On surface completion and image inpainting by biharmonic functions: Numerical aspects. International Journal of Mathematics and Mathematical Sciences, 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damelin%2C%20S.B.%20Hoang%2C%20N.S.%20On%20surface%20completion%20and%20image%20inpainting%20by%20biharmonic%20functions%3A%20Numerical%20aspects%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damelin%2C%20S.B.%20Hoang%2C%20N.S.%20On%20surface%20completion%20and%20image%20inpainting%20by%20biharmonic%20functions%3A%20Numerical%20aspects%202018"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR, abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.",
            "url": "http://arxiv.org/abs/1605.08803",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201914, pp. 2672\u20132680, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969033.2969125.",
            "url": "http://dl.acm.org/citation.cfm?id=2969033.2969125",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778, 2016. URL https://doi.org/10.1109/ CVPR.2016.90.",
            "url": "https://doi.org/10.1109/CVPR.2016.90",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06-27"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448\u2013456, 2015. URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.",
            "url": "http://jmlr.org/proceedings/papers/v37/ioffe15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20ICML%202015%20Lille%20France%20611%20July%202015%20pp%20448456%202015%20URL%20httpjmlrorgproceedingspapersv37ioffe15html"
        },
        {
            "id": "Isola_et+al_2016_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016. URL http://arxiv.org/abs/1611.07004.",
            "url": "http://arxiv.org/abs/1611.07004",
            "arxiv_url": "https://arxiv.org/pdf/1611.07004"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. URL http://arxiv.org/abs/1312.6114.",
            "url": "http://arxiv.org/abs/1312.6114",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Lample_et+al_2017_a",
            "entry": "Guillaume Lample, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. CoRR, abs/1711.00043, 2017. URL http://arxiv.org/abs/1711.00043.",
            "url": "http://arxiv.org/abs/1711.00043",
            "arxiv_url": "https://arxiv.org/pdf/1711.00043"
        },
        {
            "id": "Ledig_et+al_2016_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. CoRR, abs/1609.04802, 2016. URL http://arxiv.org/abs/1609.04802.",
            "url": "http://arxiv.org/abs/1609.04802",
            "arxiv_url": "https://arxiv.org/pdf/1609.04802"
        },
        {
            "id": "Lehtinen_et+al_2018_a",
            "entry": "Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning Image Restoration without Clean Data. arXiv:1803.04189 [cs, stat], March 2018. URL http://arxiv.org/abs/1803.04189.arXiv:1803.04189.",
            "url": "http://arxiv.org/abs/1803.04189.arXiv:1803.04189",
            "arxiv_url": "https://arxiv.org/pdf/1803.04189"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Mardani_et+al_2017_a",
            "entry": "Morteza Mardani, Enhao Gong, Joseph Y. Cheng, Shreyas Vasanawala, Greg Zaharchuk, Marcus T. Alley, Neil Thakur, Song Han, William J. Dally, John M. Pauly, and Lei Xing. Deep generative adversarial networks for compressed sensing automates MRI. CoRR, abs/1706.00051, 2017. URL http://arxiv.org/abs/1706.00051.",
            "url": "http://arxiv.org/abs/1706.00051",
            "arxiv_url": "https://arxiv.org/pdf/1706.00051"
        },
        {
            "id": "Marin_et+al_2018_a",
            "entry": "Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. Recipe1m: A dataset for learning cross-modal embeddings for cooking recipes and food images. arXiv preprint arXiv:1810.06553, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.06553"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. URL http://arxiv.org/abs/1802.05957.",
            "url": "http://arxiv.org/abs/1802.05957",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "Mota_et+al_2017_a",
            "entry": "Jo\u00e3o FC Mota, Nikos Deligiannis, and Miguel RD Rodrigues. Compressed sensing with prior information: Strategies, geometry, and bounds. IEEE Transactions on Information Theory, 63(7): 4472\u20134496, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mota%2C%20Jo%C3%A3o%20F.C.%20Deligiannis%2C%20Nikos%20Rodrigues%2C%20Miguel%20R.D.%20Compressed%20sensing%20with%20prior%20information%3A%20Strategies%2C%20geometry%2C%20and%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mota%2C%20Jo%C3%A3o%20F.C.%20Deligiannis%2C%20Nikos%20Rodrigues%2C%20Miguel%20R.D.%20Compressed%20sensing%20with%20prior%20information%3A%20Strategies%2C%20geometry%2C%20and%20bounds%202017"
        },
        {
            "id": "Stuart_2010_a",
            "entry": "A. M. Stuart. Inverse problems: A bayesian perspective. Acta Numerica, 19:451\u2013559, 2010. doi: 10.1017/S0962492910000061.",
            "crossref": "https://dx.doi.org/10.1017/S0962492910000061",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1017/S0962492910000061"
        },
        {
            "id": "S_et+al_2016_a",
            "entry": "Casper Kaae S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz\u00e1r. Amortised MAP Inference for Image Super-resolution. arXiv:1610.04490 [cs, stat], October 2016. URL http://arxiv.org/abs/1610.04490.arXiv:1610.04490.",
            "url": "http://arxiv.org/abs/1610.04490.arXiv:1610.04490",
            "arxiv_url": "https://arxiv.org/pdf/1610.04490"
        },
        {
            "id": "Tripathi_et+al_2018_a",
            "entry": "Subarna Tripathi, Zachary C. Lipton, and Truong Q. Nguyen. Correction by projection: Denoising images with generative adversarial networks. CoRR, abs/1803.04477, 2018. URL http://arxiv.org/abs/1803.04477.",
            "url": "http://arxiv.org/abs/1803.04477",
            "arxiv_url": "https://arxiv.org/pdf/1803.04477"
        },
        {
            "id": "Ulyanov_et+al_2017_a",
            "entry": "Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. CoRR, abs/1711.10925, 2017. URL http://arxiv.org/abs/1711.10925.",
            "url": "http://arxiv.org/abs/1711.10925",
            "arxiv_url": "https://arxiv.org/pdf/1711.10925"
        },
        {
            "id": "Veen_et+al_2018_a",
            "entry": "David Van Veen, Ajil Jalal, Eric Price, Sriram Vishwanath, and Alexandros G. Dimakis. Compressed Sensing with Deep Image Prior and Learned Regularization. arXiv:1806.06438 [cs, math, stat], June 2018. URL http://arxiv.org/abs/1806.06438.arXiv:1806.06438.",
            "url": "http://arxiv.org/abs/1806.06438.arXiv:1806.06438",
            "arxiv_url": "https://arxiv.org/pdf/1806.06438"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a largescale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. URL http://arxiv.org/abs/1506.03365.",
            "url": "http://arxiv.org/abs/1506.03365",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative adversarial networks. CoRR, abs/1805.08318, 2018. URL http://arxiv.org/abs/1805.08318.",
            "url": "http://arxiv.org/abs/1805.08318",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv:1703.10593 [cs], March 2017. URL http://arxiv.org/abs/1703.10593.arXiv:1703.10593.",
            "url": "http://arxiv.org/abs/1703.10593.arXiv:1703.10593",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "2",
            "entry": "2. The general measurement process in equation (1), Y = F (X; \u0398) + E induces log pY |X,\u0398(y|G(y), \u03b8) to yield a simple analytic expression: log p(y|G(y), \u03b8)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20general%20measurement%20process%20in%20equation%201%20Y%20%20F%20X%20%CE%98%20%20E%20induces%20log%20pY%20X%CE%98yGy%20%CE%B8%20to%20yield%20a%20simple%20analytic%20expression%20log%20pyGy%20%CE%B8",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20general%20measurement%20process%20in%20equation%201%20Y%20%20F%20X%20%CE%98%20%20E%20induces%20log%20pY%20X%CE%98yGy%20%CE%B8%20to%20yield%20a%20simple%20analytic%20expression%20log%20pyGy%20%CE%B8"
        },
        {
            "id": "3",
            "entry": "3. The likelihood term EpY log pY |X (y|G(y)) can then be replaced by Network architecture. Our network architectures are inspired by the Self-Attention GAN architecture in Zhang et al. (2018). They use residual networks (He et al. (2016)), where each residual block of the generator and discriminator is comprised of 2 repeated sequences of batch normalization (Ioffe & Szegedy (2015)), ReLU activation, spectral normalization (Miyato et al. (2018)) and 3 \u00d7 3 convolutional layers. For the discriminator, we use the same as Zhang et al. (2018), and for reconstruction network G, we propose an image-to-image variant of their generator. We have not added downsampling layers: we have found that they degraded the overall model\u2019s performance. For corruption processes that yield observations that are very correlated with the input, such as Patch Band and Convolve-Noise, we have found that using G(y):= y + Net(y) for the reconstruction network allows us to initialize G close to identity, accelerates training and augments the overall quality of the samples.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20likelihood%20term%20EpY%20log%20pY%20X%20yGy%20can%20then%20be%20replaced%20by%20Network%20architecture%20Our%20network%20architectures%20are%20inspired%20by%20the%20SelfAttention%20GAN%20architecture%20in%20Zhang%20et%20al%202018%20They%20use%20residual%20networks%20He%20et%20al%202016%20where%20each%20residual%20block%20of%20the%20generator%20and%20discriminator%20is%20comprised%20of%202%20repeated%20sequences%20of%20batch%20normalization%20Ioffe%20%20Szegedy%202015%20ReLU%20activation%20spectral%20normalization%20Miyato%20et%20al%202018%20and%203%20%203%20convolutional%20layers%20For%20the%20discriminator%20we%20use%20the%20same%20as%20Zhang%20et%20al%202018%20and%20for%20reconstruction%20network%20G%20we%20propose%20an%20imagetoimage%20variant%20of%20their%20generator%20We%20have%20not%20added%20downsampling%20layers%20we%20have%20found%20that%20they%20degraded%20the%20overall%20models%20performance%20For%20corruption%20processes%20that%20yield%20observations%20that%20are%20very%20correlated%20with%20the%20input%20such%20as%20Patch%20Band%20and%20ConvolveNoise%20we%20have%20found%20that%20using%20Gy%20y%20%20Nety%20for%20the%20reconstruction%20network%20allows%20us%20to%20initialize%20G%20close%20to%20identity%20accelerates%20training%20and%20augments%20the%20overall%20quality%20of%20the%20samples",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20likelihood%20term%20EpY%20log%20pY%20X%20yGy%20can%20then%20be%20replaced%20by%20Network%20architecture%20Our%20network%20architectures%20are%20inspired%20by%20the%20SelfAttention%20GAN%20architecture%20in%20Zhang%20et%20al%202018%20They%20use%20residual%20networks%20He%20et%20al%202016%20where%20each%20residual%20block%20of%20the%20generator%20and%20discriminator%20is%20comprised%20of%202%20repeated%20sequences%20of%20batch%20normalization%20Ioffe%20%20Szegedy%202015%20ReLU%20activation%20spectral%20normalization%20Miyato%20et%20al%202018%20and%203%20%203%20convolutional%20layers%20For%20the%20discriminator%20we%20use%20the%20same%20as%20Zhang%20et%20al%202018%20and%20for%20reconstruction%20network%20G%20we%20propose%20an%20imagetoimage%20variant%20of%20their%20generator%20We%20have%20not%20added%20downsampling%20layers%20we%20have%20found%20that%20they%20degraded%20the%20overall%20models%20performance%20For%20corruption%20processes%20that%20yield%20observations%20that%20are%20very%20correlated%20with%20the%20input%20such%20as%20Patch%20Band%20and%20ConvolveNoise%20we%20have%20found%20that%20using%20Gy%20y%20%20Nety%20for%20the%20reconstruction%20network%20allows%20us%20to%20initialize%20G%20close%20to%20identity%20accelerates%20training%20and%20augments%20the%20overall%20quality%20of%20the%20samples"
        },
        {
            "id": "Hyperparameters_2018_a",
            "entry": "Hyperparameters. Hyperparameters have been selected on the validation set, based on the mean square error between the reconstructions xand the image x. As in Zhang et al. (2018), we use imbalanced learning rates for the generator and the discriminator (0.0001 and 0.0004, respectively), using the Adam optimizer (Kingma & Ba (2014)), using \u03b21 = 0 and \u03b22 = 0.9. The weights are initialized using orthogonal initialization. We set \u03bb = 2, and exponentially decay the learning rate every 400 iterations, setting the decay factor to 0.995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyperparameters%20Hyperparameters%20have%20been%20selected%20on%20the%20validation%20set%2C%20based%20on%20the%20mean%20square%20error%20between%20the%20reconstructions%20xand%20the%20image%20x.%20As%20in%20Zhang%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyperparameters%20Hyperparameters%20have%20been%20selected%20on%20the%20validation%20set%2C%20based%20on%20the%20mean%20square%20error%20between%20the%20reconstructions%20xand%20the%20image%20x.%20As%20in%20Zhang%202018"
        }
    ]
}
