{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "RELAXED QUANTIZATION FOR DISCRETIZED NEURAL NETWORKS",
        "author": "Christos Louizos, University of Amsterdam TNO Intelligent Imaging c.louizos@uva.nl",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkxjYoCqKX"
        },
        "abstract": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification."
    },
    "keywords": [
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "low bit",
            "url": "https://en.wikipedia.org/wiki/low_bit"
        },
        {
            "term": "low precision",
            "url": "https://en.wikipedia.org/wiki/low_precision"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "RQ": "Relaxed Quantization",
        "BOPs": "bit operations"
    },
    "highlights": [
        "Neural networks excel in a variety of large scale problems due to their highly flexible parametric nature",
        "Where b is the number of available bits that allow for K = 2b possible integer values. By construction this grid of values is agnostic to the input signal x and suboptimal; to allow for the grid to adapt to x we introduce two free parameters, a scale \u03b1 and an offset \u03b2",
        "We have introduced Relaxed Quantization (RQ), a powerful and versatile algorithm for learning low-bit neural networks using a uniform quantization scheme",
        "We have extensively evaluated Relaxed Quantization on various image classification benchmarks and have shown that it allows for the better trade-offs between accuracy and bit operations per second",
        "Future hardware might enable us to cheaply do non-uniform quantization, for which this method can be extended. (<a class=\"ref-link\" id=\"cLai_et+al_2017_a\" href=\"#rLai_et+al_2017_a\">Lai et al, 2017</a>; <a class=\"ref-link\" id=\"cOrtiz_et+al_2018_a\" href=\"#rOrtiz_et+al_2018_a\">Ortiz et al, 2018</a>) for example, show the benefits of low-bit floating point weights that can be efficiently implemented in hardware"
    ],
    "key_statements": [
        "Neural networks excel in a variety of large scale problems due to their highly flexible parametric nature",
        "A workaround to the discontinuity are the \u201cpseudo-gradients\u201d according to the straight-through estimator (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a>), which have been successfully used for training low-bit width architectures at e.g. <a class=\"ref-link\" id=\"cHubara_et+al_2016_a\" href=\"#rHubara_et+al_2016_a\">Hubara et al (2016</a>); <a class=\"ref-link\" id=\"cZhu_et+al_2016_a\" href=\"#rZhu_et+al_2016_a\">Zhu et al (2016</a>)",
        "The contributions of this paper are four-fold: First, we show how to make the set of quantization targets part of the training process such that we can optimize them with gradient descent",
        "We introduce a way to discretize the network by converting distributions over the weights and activations to categorical distributions over the quantization grid",
        "Where b is the number of available bits that allow for K = 2b possible integer values. By construction this grid of values is agnostic to the input signal x and suboptimal; to allow for the grid to adapt to x we introduce two free parameters, a scale \u03b1 and an offset \u03b2",
        "In order to make sure that only grid-points are sampled, we propose an alternative algorithm Relaxed Quantization ST in which we use the variant of the straight-through (ST) estimator proposed in <a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al (2016</a>)",
        "For the first task we considered the toy LeNet-5 network trained on MNIST with the 32C5 - MP2 - 64C5 - MP2 - 512FC - Softmax architecture and the VGG 2x(128C3) - MP2 - 2x(256C3) - MP2 - 2x(512C3) - MP2 - 1024FC - Softmax architecture on the CIFAR 10 dataset",
        "In order to demonstrate the effectiveness of our proposed approach on large scale tasks we considered the task of quantizing a Resnet-18 (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>) as well as a Mobilenet (<a class=\"ref-link\" id=\"cHoward_et+al_2017_a\" href=\"#rHoward_et+al_2017_a\">Howard et al, 2017</a>) trained on the Imagenet (ILSVRC2012) dataset",
        "Figure 4 compares a wide range of methods in terms of accuracy and bit operations",
        "We choose to compare only against methods that employ fixed-point quantization on Resnet-18 and Mobilenet, do not compare with non-uniform quantization techniques, such as the one described at <a class=\"ref-link\" id=\"cBaskin_et+al_2018_a\" href=\"#rBaskin_et+al_2018_a\">Baskin et al (2018</a>)",
        "AWith batch normalization after convolution bLast layer in full precision in this case is defined as the initial grid used for fine-tuning with Relaxed Quantization",
        "SYQ, employs \u201cbucketing\u201d and Apprentice uses distillation, both of which can be combined with Relaxed Quantization and improve performance",
        "We have introduced Relaxed Quantization (RQ), a powerful and versatile algorithm for learning low-bit neural networks using a uniform quantization scheme",
        "We have extensively evaluated Relaxed Quantization on various image classification benchmarks and have shown that it allows for the better trade-offs between accuracy and bit operations per second",
        "Future hardware might enable us to cheaply do non-uniform quantization, for which this method can be extended. (<a class=\"ref-link\" id=\"cLai_et+al_2017_a\" href=\"#rLai_et+al_2017_a\">Lai et al, 2017</a>; <a class=\"ref-link\" id=\"cOrtiz_et+al_2018_a\" href=\"#rOrtiz_et+al_2018_a\">Ortiz et al, 2018</a>) for example, show the benefits of low-bit floating point weights that can be efficiently implemented in hardware"
    ],
    "summary": [
        "Neural networks excel in a variety of large scale problems due to their highly flexible parametric nature.",
        "We introduce a way to discretize the network by converting distributions over the weights and activations to categorical distributions over the quantization grid.",
        "The proposed quantizer comprises four elements: a vocabulary, its noise model and the resulting discretization procedure, as well as a final relaxation step to enable gradient based optimization.",
        "Compared to a standard neural network or stochastic rounding approaches, the proposed procedure can be infeasible for larger models and datasets.",
        "Discretizing this distribution to a categorical over the quantization grid assigns probabilities to the two closest grid points as in stochastic rounding, following Equation 2: x x x xx p(x = \u03b1 |x) = P (x \u2264 ( \u03b1 + \u03b1/2)) \u2212 P (x < ( \u03b1 \u2212 \u03b1/2)) = \u2212 .",
        "RQ ST further allows sampling of not only the two closest grid points, but has support for more distant ones depending on the estimated input noise \u03c3.",
        "Within the body of work that considers quantizing weights and activations fall papers using stochastic rounding (<a class=\"ref-link\" id=\"cGupta_et+al_2015_a\" href=\"#rGupta_et+al_2015_a\">Gupta et al, 2015</a>; <a class=\"ref-link\" id=\"cHubara_et+al_2016_a\" href=\"#rHubara_et+al_2016_a\"><a class=\"ref-link\" id=\"cHubara_et+al_2016_a\" href=\"#rHubara_et+al_2016_a\">Hubara et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGysel_et+al_2018_a\" href=\"#rGysel_et+al_2018_a\">Gysel et al, 2018</a>; <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al, 2018</a>).",
        "Another line of work considers binarizing (<a class=\"ref-link\" id=\"cCourbariaux_et+al_2015_a\" href=\"#rCourbariaux_et+al_2015_a\">Courbariaux et al, 2015</a>; <a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\">Zhou et al, 2018</a></a>) or ternarizing (<a class=\"ref-link\" id=\"cLi_et+al_2016_a\" href=\"#rLi_et+al_2016_a\">Li et al, 2016</a>; <a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhou_et+al_2018_a\" href=\"#rZhou_et+al_2018_a\">Zhou et al, 2018</a></a>) weights and activations (<a class=\"ref-link\" id=\"cHubara_et+al_2016_a\" href=\"#rHubara_et+al_2016_a\"><a class=\"ref-link\" id=\"cHubara_et+al_2016_a\" href=\"#rHubara_et+al_2016_a\">Hubara et al, 2016</a></a>; Rastegari et al, 2016; <a class=\"ref-link\" id=\"cZhou_et+al_2016_a\" href=\"#rZhou_et+al_2016_a\">Zhou et al, 2016</a>) via the straight-through gradient estimator (<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013</a>); these allow for fast implementations of convolutions using only bit-shift operations.",
        "In BOPs, the impact of not quantizing the first layer in, for example, the Resnet-18 model on Imagenet, becomes apparent: keeping the first layer in full precision requires roughly 1.3 times as many BOPs for one forward pass through the whole network compared to quantizing all weights and activations to 5 bits.",
        "Figure 4 compares a wide range of methods in terms of accuracy and BOPs. We choose to compare only against methods that employ fixed-point quantization on Resnet-18 and Mobilenet, do not compare with non-uniform quantization techniques, such as the one described at <a class=\"ref-link\" id=\"cBaskin_et+al_2018_a\" href=\"#rBaskin_et+al_2018_a\">Baskin et al (2018</a>).",
        "In order to accurately reflect this folding at test time, future work on the proposed algorithm will emulate folded batchnorm at training time and learn the corresponding quantization grid of the modified kernel and bias."
    ],
    "headline": "In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Achterhold_et+al_2018_a",
            "entry": "Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network quantization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ry-TW-WAb.",
            "url": "https://openreview.net/forum?id=ry-TW-WAb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achterhold%2C%20Jan%20Koehler%2C%20Jan%20Mathias%20Schmeink%2C%20Anke%20Genewein%2C%20Tim%20Variational%20network%20quantization%202018"
        },
        {
            "id": "Baskin_et+al_2018_a",
            "entry": "Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for the quantization of neural networks. arXiv preprint arXiv:1804.10969, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.10969"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. arXiv preprint arXiv:1702.00953, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00953"
        },
        {
            "id": "Chollet_2015_a",
            "entry": "Francois Chollet et al. Keras. https://keras.io, 2015.",
            "url": "https://keras.io"
        },
        {
            "id": "Courbariaux_et+al_2015_a",
            "entry": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "Faraone_et+al_2018_a",
            "entry": "Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip HW Leong. Syq: Learning symmetric quantization for efficient deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4300\u20134309, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faraone%2C%20Julian%20Fraser%2C%20Nicholas%20Blott%2C%20Michaela%20Leong%2C%20Philip%20H.W.%20Syq%3A%20Learning%20symmetric%20quantization%20for%20efficient%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faraone%2C%20Julian%20Fraser%2C%20Nicholas%20Blott%2C%20Michaela%20Leong%2C%20Philip%20H.W.%20Syq%3A%20Learning%20symmetric%20quantization%20for%20efficient%20deep%20neural%20networks%202018"
        },
        {
            "id": "Gupta_et+al_2015_a",
            "entry": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737\u20131746, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015"
        },
        {
            "id": "Gysel_et+al_2018_a",
            "entry": "Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2018. doi: 10.1109/TNNLS.2018.2808319.",
            "crossref": "https://dx.doi.org/10.1109/TNNLS.2018.2808319",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TNNLS.2018.2808319"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Hubara_et+al_2016_a",
            "entry": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07061"
        },
        {
            "id": "Jacob_et+al_2017_a",
            "entry": "Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. arXiv preprint arXiv:1712.05877, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05877"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lai_et+al_2017_a",
            "entry": "Liangzhen Lai, Naveen Suda, and Vikas Chandra. Deep convolutional neural network inference with floating-point weights and fixed-point activations. arXiv preprint arXiv:1703.03073, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03073"
        },
        {
            "id": "Lecun_et+al_1989_a",
            "entry": "Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems 2, NIPS 1989, volume 2, pp. 598\u2013605. Morgan-Kaufmann Publishers, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201989"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.04711"
        },
        {
            "id": "Louizos_et+al_0000_a",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. arXiv preprint arXiv:1705.08665, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08665"
        },
        {
            "id": "Louizos_et+al_0000_b",
            "entry": "Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l0 regularization. arXiv preprint arXiv:1712.01312, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01312"
        },
        {
            "id": "Maddison_2016_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Mishra_2017_a",
            "entry": "Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. arXiv preprint arXiv:1711.05852, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05852"
        },
        {
            "id": "Mishra_et+al_2017_b",
            "entry": "Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn: wide reduced-precision networks. arXiv preprint arXiv:1709.01134, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01134"
        },
        {
            "id": "Ortiz_et+al_2018_a",
            "entry": "Marc Ortiz, Adrian Cristal, Eduard Ayguade, and Marc Casas. Low-precision floating-point schemes for neural network training. arXiv preprint arXiv:1804.05267, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05267"
        },
        {
            "id": "Peters_2018_a",
            "entry": "Jorn WT Peters and Max Welling. Probabilistic binary neural networks. arXiv preprint arXiv:1809.03368, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.03368"
        },
        {
            "id": "Polino_et+al_2018_a",
            "entry": "Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05668"
        },
        {
            "id": "Shayer_et+al_2018_a",
            "entry": "Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BySRH6CpW.",
            "url": "https://openreview.net/forum?id=BySRH6CpW",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shayer%2C%20Oran%20Levi%2C%20Dan%20Fetaya%2C%20Ethan%20Learning%20discrete%20weights%20using%20the%20local%20reparameterization%20trick%202018"
        },
        {
            "id": "Sheng_et+al_2018_a",
            "entry": "Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic. A quantization-friendly separable convolution for mobilenets. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sheng%2C%20Tao%20Feng%2C%20Chen%20Zhuo%2C%20Shaojie%20Zhang%2C%20Xiaopeng%20and%20Mickey%20Aleksic.%20A%20quantization-friendly%20separable%20convolution%20for%20mobilenets%202018"
        },
        {
            "id": "Soudry_et+al_2014_a",
            "entry": "Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In Advances in Neural Information Processing Systems, pp. 963\u2013971, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20Daniel%20Hubara%2C%20Itay%20Meir%2C%20Ron%20Expectation%20backpropagation%3A%20Parameter-free%20training%20of%20multilayer%20neural%20networks%20with%20continuous%20or%20discrete%20weights%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20Daniel%20Hubara%2C%20Itay%20Meir%2C%20Ron%20Expectation%20backpropagation%3A%20Parameter-free%20training%20of%20multilayer%20neural%20networks%20with%20continuous%20or%20discrete%20weights%202014"
        },
        {
            "id": "Staines_2012_a",
            "entry": "Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.4507"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJGXzmspb.",
            "url": "https://openreview.net/forum?id=HJGXzmspb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Shuang%20Li%2C%20Guoqi%20Chen%2C%20Feng%20Shi%2C%20Luping%20Training%20and%20inference%20with%20integers%20in%20deep%20neural%20networks%202018"
        },
        {
            "id": "Yin_et+al_2016_a",
            "entry": "Penghang Yin, Shuai Zhang, Yingyong Qi, and Jack Xin. Quantization and training of low bit-width convolutional neural networks for object detection. arXiv preprint arXiv:1612.06052, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.06052"
        },
        {
            "id": "Yin_et+al_2018_a",
            "entry": "Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Binaryrelax: A relaxation approach for training deep neural networks with quantized weights. SIAM Journal on Imaging Sciences, 11(4):2205\u20132223, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Penghang%20Zhang%2C%20Shuai%20Lyu%2C%20Jiancheng%20Osher%2C%20Stanley%20Binaryrelax%3A%20A%20relaxation%20approach%20for%20training%20deep%20neural%20networks%20with%20quantized%20weights%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Penghang%20Zhang%2C%20Shuai%20Lyu%2C%20Jiancheng%20Osher%2C%20Stanley%20Binaryrelax%3A%20A%20relaxation%20approach%20for%20training%20deep%20neural%20networks%20with%20quantized%20weights%202018"
        },
        {
            "id": "Yin_et+al_2019_a",
            "entry": "Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Blended coarse gradient descent for full quantization of deep neural networks. Research in the Mathematical Sciences, 6(1):14, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Penghang%20Zhang%2C%20Shuai%20Lyu%2C%20Jiancheng%20Osher%2C%20Stanley%20Blended%20coarse%20gradient%20descent%20for%20full%20quantization%20of%20deep%20neural%20networks%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Penghang%20Zhang%2C%20Shuai%20Lyu%2C%20Jiancheng%20Osher%2C%20Stanley%20Blended%20coarse%20gradient%20descent%20for%20full%20quantization%20of%20deep%20neural%20networks%202019"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Dongqing%20Yang%2C%20Jiaolong%20Ye%2C%20Dongqiangzi%20Hua%2C%20Gang%20Lq-nets%3A%20Learned%20quantization%20for%20highly%20accurate%20and%20compact%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Dongqing%20Yang%2C%20Jiaolong%20Ye%2C%20Dongqiangzi%20Hua%2C%20Gang%20Lq-nets%3A%20Learned%20quantization%20for%20highly%20accurate%20and%20compact%20deep%20neural%20networks%202018"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03044"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong Chen. Explicit loss-error-aware quantization for low-bit deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9426\u20139435, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Aojun%20Yao%2C%20Anbang%20Wang%2C%20Kuan%20Chen%2C%20Yurong%20Explicit%20loss-error-aware%20quantization%20for%20low-bit%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Aojun%20Yao%2C%20Anbang%20Wang%2C%20Kuan%20Chen%2C%20Yurong%20Explicit%20loss-error-aware%20quantization%20for%20low-bit%20deep%20neural%20networks%202018"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        },
        {
            "id": "The_2018_a",
            "entry": "The moving averages of layer statistics that are aggregated during the training phase for the batch normalization do not necessarily reflect the statistics of the quantized model accurately. Even though RQ aims to minimize the gap between training and testing phase, we found that the aggregated statistics in combination with the learned scale and shift parameters of batch normalization lead to decreased test performance. In order to avoid this drop in accuracy, we apply the insights from (Peters & Welling, 2018) and recompute the statistics of the quantized model before reporting the final test error rate. The final models were determined through early stopping using the validation loss computed with minibatch statistics, in case the model uses batch normalization.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20moving%20averages%20of%20layer%20statistics%20that%20are%20aggregated%20during%20the%20training%20phase%20for%20the%20batch%20normalization%20do%20not%20necessarily%20reflect%20the%20statistics%20of%20the%20quantized%20model%20accurately%20Even%20though%20RQ%20aims%20to%20minimize%20the%20gap%20between%20training%20and%20testing%20phase%20we%20found%20that%20the%20aggregated%20statistics%20in%20combination%20with%20the%20learned%20scale%20and%20shift%20parameters%20of%20batch%20normalization%20lead%20to%20decreased%20test%20performance%20In%20order%20to%20avoid%20this%20drop%20in%20accuracy%20we%20apply%20the%20insights%20from%20Peters%20%20Welling%202018%20and%20recompute%20the%20statistics%20of%20the%20quantized%20model%20before%20reporting%20the%20final%20test%20error%20rate%20The%20final%20models%20were%20determined%20through%20early%20stopping%20using%20the%20validation%20loss%20computed%20with%20minibatch%20statistics%20in%20case%20the%20model%20uses%20batch%20normalization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20moving%20averages%20of%20layer%20statistics%20that%20are%20aggregated%20during%20the%20training%20phase%20for%20the%20batch%20normalization%20do%20not%20necessarily%20reflect%20the%20statistics%20of%20the%20quantized%20model%20accurately%20Even%20though%20RQ%20aims%20to%20minimize%20the%20gap%20between%20training%20and%20testing%20phase%20we%20found%20that%20the%20aggregated%20statistics%20in%20combination%20with%20the%20learned%20scale%20and%20shift%20parameters%20of%20batch%20normalization%20lead%20to%20decreased%20test%20performance%20In%20order%20to%20avoid%20this%20drop%20in%20accuracy%20we%20apply%20the%20insights%20from%20Peters%20%20Welling%202018%20and%20recompute%20the%20statistics%20of%20the%20quantized%20model%20before%20reporting%20the%20final%20test%20error%20rate%20The%20final%20models%20were%20determined%20through%20early%20stopping%20using%20the%20validation%20loss%20computed%20with%20minibatch%20statistics%20in%20case%20the%20model%20uses%20batch%20normalization"
        },
        {
            "id": "For_for 200_a",
            "entry": "For the MNIST experiment we rescaled the input to the [-1, 1] range, employed no regularization and the network was trained with Adam (Kingma & Ba, 2014) and a batch size of 128. We used a local grid whenever the bit width was larger than 2 for both, weights and biases (shared grid parameters), as well as for the ouputs of the ReLU, with \u03b4 = 3. For the 8 and 4 bit networks we used a temperature \u03bb of 2 whereas for the 2 bit models we used a temperature of 1 for RQ. We trained the 8 and 4 bit networks for 100 epochs using a learning rate of 1e-3 and the 2 bit networks for 200 epochs with a learning rate of 5e-4. In all of the cases the learning rate was annealed to zero during the last 50 epochs.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20MNIST%20experiment%20we%20rescaled%20the%20input%20to%20the%201%201%20range%20employed%20no%20regularization%20and%20the%20network%20was%20trained%20with%20Adam%20Kingma%20%20Ba%202014%20and%20a%20batch%20size%20of%20128%20We%20used%20a%20local%20grid%20whenever%20the%20bit%20width%20was%20larger%20than%202%20for%20both%20weights%20and%20biases%20shared%20grid%20parameters%20as%20well%20as%20for%20the%20ouputs%20of%20the%20ReLU%20with%20%CE%B4%20%203%20For%20the%208%20and%204%20bit%20networks%20we%20used%20a%20temperature%20%CE%BB%20of%202%20whereas%20for%20the%202%20bit%20models%20we%20used%20a%20temperature%20of%201%20for%20RQ%20We%20trained%20the%208%20and%204%20bit%20networks%20for%20100%20epochs%20using%20a%20learning%20rate%20of%201e3%20and%20the%202%20bit%20networks%20for%20200%20epochs%20with%20a%20learning%20rate%20of%205e4%20In%20all%20of%20the%20cases%20the%20learning%20rate%20was%20annealed%20to%20zero%20during%20the%20last%2050%20epochs",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20the%20MNIST%20experiment%20we%20rescaled%20the%20input%20to%20the%201%201%20range%20employed%20no%20regularization%20and%20the%20network%20was%20trained%20with%20Adam%20Kingma%20%20Ba%202014%20and%20a%20batch%20size%20of%20128%20We%20used%20a%20local%20grid%20whenever%20the%20bit%20width%20was%20larger%20than%202%20for%20both%20weights%20and%20biases%20shared%20grid%20parameters%20as%20well%20as%20for%20the%20ouputs%20of%20the%20ReLU%20with%20%CE%B4%20%203%20For%20the%208%20and%204%20bit%20networks%20we%20used%20a%20temperature%20%CE%BB%20of%202%20whereas%20for%20the%202%20bit%20models%20we%20used%20a%20temperature%20of%201%20for%20RQ%20We%20trained%20the%208%20and%204%20bit%20networks%20for%20100%20epochs%20using%20a%20learning%20rate%20of%201e3%20and%20the%202%20bit%20networks%20for%20200%20epochs%20with%20a%20learning%20rate%20of%205e4%20In%20all%20of%20the%20cases%20the%20learning%20rate%20was%20annealed%20to%20zero%20during%20the%20last%2050%20epochs"
        }
    ]
}
