{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "QUERY-EFFICIENT HARD-LABEL BLACK-BOX ATTACK: AN OPTIMIZATION-BASED APPROACH",
        "author": "Minhao Cheng, Huan Zhang & Cho-Jui Hsieh Department of Computer Science University of California, Los Angeles {mhcheng,huanzhang,chohsieh}@cs.ucla.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJlk6iRqKX"
        },
        "abstract": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions. This is a very challenging problem since the direct extension of state-of-the-art white-box attacks (e.g., C&W or PGD) to the hard-label black-box setting will require minimizing a non-continuous step function, which is combinatorial and cannot be solved by a gradient-based optimizer. The only two current approaches are based on random walk on the boundary (<a class=\"ref-link\" id=\"cBrendel_et+al_2017_a\" href=\"#rBrendel_et+al_2017_a\">Brendel et al., 2017</a>) and random trials to evaluate the loss function (<a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\">Ilyas et al., 2018</a>), which require lots of queries and lacks convergence guarantees. We propose a novel way to formulate the hard-label black-box attack as a real-valued optimization problem which is usually continuous and can be solved by any zeroth order optimization algorithm, such as randomized gradientfree method (Nesterov & Spokoiny, 2017). We demonstrate that our proposed method outperforms the previous stochastic approaches to attacking convolutional neural networks on MNIST, CIFAR, and ImageNet datasets. More interestingly, the proposed algorithm can also be used to attack other discrete and non-continuous machine learning models, such as Gradient Boosting Decision Trees."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "PGD": "projected gradient descent",
        "RGF": "Randomized Gradient-Free",
        "CNN": "convolutional neural network",
        "GBDT": "gradient booting decision tree",
        "ASR": "attack success rate"
    },
    "highlights": [
        "It has been observed recently that machine learning algorithms, especially deep neural networks, are vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a></a>; Szegedy et al, 2013; Moosavi-Dezfooli et al.; Moosavi Dezfooli et al, 2016; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\"><a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\">Cheng et al, 2018</a></a>)",
        "The objective function of this reformulation cannot be written in an analytical form, we show how to use model queries to evaluate its function value and any zeroth order optimization algorithm can be applied to solve it",
        "We show our algorithm can be successfully used to attack hard-label black-box convolutional neural network models on MNIST, CIFAR, and ImageNet with far less number of queries compared to the state-of-art algorithm both in L2 and L\u221e metric",
        "In Figure 3b and Figure 1a, we show decision boundaries generated by gradient booting decision tree and neural network classifier, which are not continuous",
        "We show that even with good interpretability and a similar prediction accuracy with convolution neural network, the gradient booting decision tree models are vulnerable under our Opt-attack",
        "We propose a generic and optimization-based hard-label black-box attack algorithm, which can be applied to discrete and non-continuous models other than neural networks, such as the gradient boosting decision tree"
    ],
    "key_statements": [
        "It has been observed recently that machine learning algorithms, especially deep neural networks, are vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a></a>; Szegedy et al, 2013; Moosavi-Dezfooli et al.; Moosavi Dezfooli et al, 2016; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\"><a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\">Cheng et al, 2018</a></a>)",
        "The objective function of this reformulation cannot be written in an analytical form, we show how to use model queries to evaluate its function value and any zeroth order optimization algorithm can be applied to solve it",
        "We show our algorithm can be successfully used to attack hard-label black-box convolutional neural network models on MNIST, CIFAR, and ImageNet with far less number of queries compared to the state-of-art algorithm both in L2 and L\u221e metric",
        "In Figure 3b and Figure 1a, we show decision boundaries generated by gradient booting decision tree and neural network classifier, which are not continuous",
        "We show that even with good interpretability and a similar prediction accuracy with convolution neural network, the gradient booting decision tree models are vulnerable under our Opt-attack",
        "We propose a generic and optimization-based hard-label black-box attack algorithm, which can be applied to discrete and non-continuous models other than neural networks, such as the gradient boosting decision tree"
    ],
    "summary": [
        "It has been observed recently that machine learning algorithms, especially deep neural networks, are vulnerable to adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a></a></a>; Szegedy et al, 2013; Moosavi-Dezfooli et al.; Moosavi Dezfooli et al, 2016; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018a</a></a>; <a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\"><a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\">Cheng et al, 2018</a></a>).",
        "We assume that the model is not revealed and the attacker can only make queries to acquire the corresponding hard-label decision instead of the probability outputs.",
        "We make hard-label black-box attacks query-efficient by reformulating the attack as a novel realvalued optimization problem, which is usually continuous and much easier to solve.",
        "The objective function of this reformulation cannot be written in an analytical form, we show how to use model queries to evaluate its function value and any zeroth order optimization algorithm can be applied to solve it.",
        "We show our algorithm can be successfully used to attack hard-label black-box CNN models on MNIST, CIFAR, and ImageNet with far less number of queries compared to the state-of-art algorithm both in L2 and L\u221e metric.",
        "Instead, (<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>) considers the score-based black-box setting, where attackers can use x to query the softmax layer output in addition to the final classification result.",
        "We introduce a novel way to re-formulate hard-label black-box attack as another optimization problem, show how to evaluate the function value using hard-label queries, and apply a zeroth order optimization algorithm to solve it.",
        "To solve the optimization problem (6) for which we can only evaluate function value instead of gradient, zeroth order optimization algorithms can be naturally applied.",
        "We test the performance of our hard-label black-box attack algorithm on convolutional neural network (CNN) models and compare with Boundary attack (<a class=\"ref-link\" id=\"cBrendel_et+al_2017_a\" href=\"#rBrendel_et+al_2017_a\">Brendel et al, 2017</a>), Limited attack (<a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\">Ilyas et al, 2018</a>) and a random trail baseline.",
        "For black-box attack algorithms, we report average and median number of queries for comparison.",
        "On MNIST data, we are able to reduce the number of queries by 3-4 folds, and Boundary-attack converges to worse solutions in all the 3 datasets.",
        "On CIFAR, our algorithm has similar efficiency with Boundary-attack at the first 60,000 queries, but converges to a slightly worse solution.",
        "To evaluate our method\u2019s ability to attack models with discrete decision functions, we conduct our untargeted attack on gradient booting decision tree (GBDT).",
        "We propose a generic and optimization-based hard-label black-box attack algorithm, which can be applied to discrete and non-continuous models other than neural networks, such as the gradient boosting decision tree.",
        "Our attack achieves smaller or similar distortion using 3-4 times less queries compared with the state-of-the-art algorithms."
    ],
    "headline": "We study the problem of attacking machine learning models in the hard-label black-box setting, where no model information is revealed except that the attacker can make queries to probe the corresponding hard-label decisions",
    "reference_links": [
        {
            "id": "Alzantot_et+al_2018_a",
            "entry": "Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, and Mani Srivastava. Genattack: Practical black-box attacks with gradient-free optimization. arXiv preprint arXiv:1805.11090, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11090"
        },
        {
            "id": "Baldi_et+al_2014_a",
            "entry": "Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5:4308, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014"
        },
        {
            "id": "Bhagoji_et+al_2017_a",
            "entry": "Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. arXiv preprint arXiv:1712.09491, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09491"
        },
        {
            "id": "Brendel_et+al_2017_a",
            "entry": "Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04248"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Attacking visual language grounding with adversarial examples: A case study on neural image captioning. In ACL, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Hongge%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Attacking%20visual%20language%20grounding%20with%20adversarial%20examples%3A%20A%20case%20study%20on%20neural%20image%20captioning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Hongge%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Attacking%20visual%20language%20grounding%20with%20adversarial%20examples%3A%20A%20case%20study%20on%20neural%20image%20captioning%202018"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15\u201326. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017"
        },
        {
            "id": "Chen_et+al_2018_b",
            "entry": "Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. In AAAI, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Sharma%2C%20Yash%20Zhang%2C%20Huan%20Yi%2C%20Jinfeng%20Ead%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Sharma%2C%20Yash%20Zhang%2C%20Huan%20Yi%2C%20Jinfeng%20Ead%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%202018"
        },
        {
            "id": "Cheng_et+al_2018_a",
            "entry": "Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. CoRR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Minhao%20Yi%2C%20Jinfeng%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Seq2sick%3A%20Evaluating%20the%20robustness%20of%20sequence-to-sequence%20models%20with%20adversarial%20examples%202018"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Ilyas_et+al_2018_a",
            "entry": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. arXiv preprint arXiv:1804.08598, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08598"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017"
        },
        {
            "id": "Madry_et+al_2018_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "Dezfooli_et+al_2016_a",
            "entry": "Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Nesterov_2011_a",
            "entry": "Yurii Nesterov. Random gradient-free minimization of convex functions. Technical report, 2011. Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Random%20gradient-free%20minimization%20of%20convex%20functions%202011"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Foundations of Computational Mathematics, 17(2):527\u2013566, 2017. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Patrick%20McDaniel%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Foundations%20of%20Computational%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Patrick%20McDaniel%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Foundations%20of%20Computational%202017"
        },
        {
            "id": "Swami_2017_a",
            "entry": "Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818\u20132826, 2016. Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. CoRR, abs/1805.11770, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        }
    ]
}
