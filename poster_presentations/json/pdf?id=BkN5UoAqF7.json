{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SAMPLE EFFICIENT IMITATION LEARNING FOR CONTINUOUS CONTROL",
        "author": "Fumihiro Sasaki, Tetsuya Yohira & Atsuo Kawaguchi Ricoh Company, Ltd. {fumihiro.fs.sasaki,tetsuya.yohira,atsuo.kawaguchi}@jp.ricoh.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkN5UoAqF7"
        },
        "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. In this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        }
    ],
    "abbreviations": {
        "IL": "Imitation learning",
        "GAIL": "generative adversarial imitation learning",
        "AIL": "adversarial IL",
        "Off-PAC": "off-policy actor-critic",
        "RL": "reinforcement learning",
        "BC": "behavioral cloning",
        "IRL": "Inverse Reinforcement Learning",
        "cGANs": "conditional generative adversarial networks",
        "GAN": "generative adversarial networks",
        "GCL": "Guided Cost Learning"
    },
    "highlights": [
        "Recent advances in reinforcement learning (RL) have achieved super-human performance on several domains (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>)",
        "Since it is generally hard to model a variety of real-world environments with an algorithm, and the state-action pairs in a vast majority of realworld applications such as robotics control can be naturally represented in continuous spaces, we focus on model-free Imitation learning for continuous control",
        "We propose an Imitation learning algorithm for continuous control to improve the sample complexity of the existing adversarial IL methods",
        "We conclude that our algorithm is competitive with generative adversarial imitation learning with regards to performance",
        "We proposed a model-free Imitation learning algorithm for continuous control",
        "Experimental results showed that our algorithm achieves competitive performance with generative adversarial imitation learning while significantly reducing the environment interactions"
    ],
    "key_statements": [
        "Recent advances in reinforcement learning (RL) have achieved super-human performance on several domains (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>)",
        "Since it is generally hard to model a variety of real-world environments with an algorithm, and the state-action pairs in a vast majority of realworld applications such as robotics control can be naturally represented in continuous spaces, we focus on model-free Imitation learning for continuous control",
        "We propose an Imitation learning algorithm for continuous control to improve the sample complexity of the existing adversarial IL methods",
        "Experimental results show that our algorithm enables the learner to imitate the expert behavior as well as generative adversarial imitation learning does while significantly reducing the environment interactions",
        "Ablation experimental results show that (a) adopting the off-policy scheme requires about 100 times fewer environment interactions to imitate the expert behavior than the one on-policy Imitation learning algorithms require, (b) omitting the reward learning makes the training stable and faster, and (c) bounding action values makes the training faster.\n1Throughout this paper, we refer to \u201cnumber of interactions\u201c as the number of state-action pairs obtained through interaction between the learner and the environment during training the learner.\n2.1",
        "We briefly describe objectives of reinforcement learning, Inverse Reinforcement Learning, and adversarial IL below",
        "We train an agent on each task by TRPO (<a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\">Schulman et al, 2015a</a>) using the rewards defined in the OpenAI Gym (<a class=\"ref-link\" id=\"cBrockman_et+al_2016_a\" href=\"#rBrockman_et+al_2016_a\">Brockman et al, 2016</a>), we use the resulting agent with a stochastic policy as the expert for the Imitation learning algorithms",
        "If an Imitation learning algorithm succeeds to imitate the expert behavior in the dense sampling setup whereas it fails in the sparse sampling setup, we evaluate the algorithm as sample inefficient in terms of the expert demonstration",
        "We conclude that our algorithm is competitive with generative adversarial imitation learning with regards to performance",
        "The curves on the bottom row in Figure 3 show that our algorithm trains the learner much more efficiently than generative adversarial imitation learning does in terms of the environment interaction",
        "We proposed a model-free Imitation learning algorithm for continuous control",
        "Experimental results showed that our algorithm achieves competitive performance with generative adversarial imitation learning while significantly reducing the environment interactions"
    ],
    "summary": [
        "Recent advances in reinforcement learning (RL) have achieved super-human performance on several domains (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\"><a class=\"ref-link\" id=\"cLillicrap_et+al_2015_a\" href=\"#rLillicrap_et+al_2015_a\">Lillicrap et al, 2015</a></a>).",
        "Even if enough demonstration is given by the expert before training the learner, the AIL methods require a large number of state-action pairs obtained through the interaction between the learner and the environment1.",
        "Experimental results show that our algorithm enables the learner to imitate the expert behavior as well as GAIL does while significantly reducing the environment interactions.",
        "Ablation experimental results show that (a) adopting the off-policy scheme requires about 100 times fewer environment interactions to imitate the expert behavior than the one on-policy IL algorithms require, (b) omitting the reward learning makes the training stable and faster, and (c) bounding action values makes the training faster.",
        "Given the reward function R, the objective of RL with parameterized stochastic policies \u03c0\u03b8 : S\u00d7A \u2192 [0, 1] is defined as follows: RL(R) = arg max\u03b8 J",
        "As mentioned in Section.1, our algorithm (a) adopts Off-PAC algorithms to train the learner policy, (b) estimates state-action value without learning the reward functions, and (c) represents the stochastic policy function so that its outputs are bounded.",
        "A value function approximator Q\u03c0\u03b8 following the learner policy \u03c0\u03b8 can be formed as a log probability.",
        "Suppose the optimal reward function R\u03c9\u2217 (s, a) = log r\u03c9\u2217 (s, a) for the objective (8) can be obtained, the Bellman equation (11) can be rewritten as follows: log r\u03c9\u2217 = log q\u03c0\u03b8,\u03bd \u2212 Est+1\u223cT ,at+1\u223c\u03c0\u03b8 log q\u03c0\u03b3\u03b8,\u03bd",
        "To learn the value function approximator Q\u03c0\u03b8,\u03bd, we adopt a behavior policy \u03b2 as \u03c0 in the second term in objective (18) We employ a mixture of the past learner policies as \u03b2 and a replay buffer B\u03b2 (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) to perform sampling st \u223c \u03c1\u03c0, at \u223c \u03c0 and st+1 \u223c T .",
        "We train an agent on each task by TRPO (<a class=\"ref-link\" id=\"cSchulman_et+al_2015_a\" href=\"#rSchulman_et+al_2015_a\">Schulman et al, 2015a</a>) using the rewards defined in the OpenAI Gym (<a class=\"ref-link\" id=\"cBrockman_et+al_2016_a\" href=\"#rBrockman_et+al_2016_a\">Brockman et al, 2016</a>), we use the resulting agent with a stochastic policy as the expert for the IL algorithms.",
        "The curves on the bottom row in Figure 3 show that our algorithm trains the learner much more efficiently than GAIL does in terms of the environment interaction.",
        "The result with Ours+GP, which denotes a variant of Ours that adopts the Gaussian policy, suggests that bounding action values described in 3.2 makes the training faster and stable.",
        "Experimental results showed that our algorithm achieves competitive performance with GAIL while significantly reducing the environment interactions"
    ],
    "headline": "We propose a model-free Imitation learning algorithm for continuous control",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning, pp. 1, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine%20Learning%20pp%201%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine%20Learning%20pp%201%202004"
        },
        {
            "id": "Baram_et+al_2017_a",
            "entry": "Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end differentiable adversarial imitation learning. In International Conference on Machine Learning, pp. 390\u2013399, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baram%2C%20Nir%20Anschel%2C%20Oron%20Caspi%2C%20Itai%20Mannor%2C%20Shie%20End-to-end%20differentiable%20adversarial%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baram%2C%20Nir%20Anschel%2C%20Oron%20Caspi%2C%20Itai%20Mannor%2C%20Shie%20End-to-end%20differentiable%20adversarial%20imitation%20learning%202017"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Chou_et+al_2017_a",
            "entry": "Daniel Maturana Chou, Po-Wei and Sebastian Scherer. Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution. In International Conference on Machine Learning, pp. 834\u2013843, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chou%2C%20Daniel%20Maturana%20Po-Wei%20Scherer%2C%20Sebastian%20Improving%20stochastic%20policy%20gradients%20in%20continuous%20control%20with%20deep%20reinforcement%20learning%20using%20the%20beta%20distribution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chou%2C%20Daniel%20Maturana%20Po-Wei%20Scherer%2C%20Sebastian%20Improving%20stochastic%20policy%20gradients%20in%20continuous%20control%20with%20deep%20reinforcement%20learning%20using%20the%20beta%20distribution%202017"
        },
        {
            "id": "Daume_et+al_2009_a",
            "entry": "John Langford Daume, Hal and Daniel Marcu. Search-based structured prediction. In Machine learning, pp. 297\u2013325, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daume%2C%20John%20Langford%20Hal%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daume%2C%20John%20Langford%20Hal%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009"
        },
        {
            "id": "Degris_et+al_2012_a",
            "entry": "Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1205.4839"
        },
        {
            "id": "Finn_et+al_0000_a",
            "entry": "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03852"
        },
        {
            "id": "Finn_et+al_0000_b",
            "entry": "Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pp. 49\u201358, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Katie Luo Fu, Justin and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Hausman_et+al_2017_a",
            "entry": "Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph Lim. Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. arXiv preprint arXiv:1705.10479, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10479"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Hester_et+al_2017_a",
            "entry": "Vecerik M. Pietquin O. Lanctot M. Schaul T. Piot B. Horgan D. Quan J. Sendonaris A. DulacArnold G. Hester, T. and I Osband. Deep q-learning from demonstrations. arXiv preprint arXiv:1704.03732, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03732"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "G Hinton, N Srivastava, and K Swersky. Rmsprop: Divide the gradient by a running average of its recent magnitude. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Srivastava%2C%20N.%20Swersky%2C%20K.%20Rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. pp. 3815\u20133825, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yunzhu%20Song%2C%20Jiaming%20Ermon%2C%20Stefano%20Infogail%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Maas_et+al_2013_a",
            "entry": "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, volume 30, pp. 3, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, pp. 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Pomerleau_1991_a",
            "entry": "Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. volume 3, pp. 88\u201397. MIT Press, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pomerleau%2C%20Dean%20A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%201991"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Gordon G. Ross, S. and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, pp. 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Gordon%20G.%20S.%20Bagnell%2C%20D.%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Gordon%20G.%20S.%20Bagnell%2C%20D.%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Ross_2010_a",
            "entry": "Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 661\u2013668, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Bagnell%2C%20Drew%20Efficient%20reductions%20for%20imitation%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Bagnell%2C%20Drew%20Efficient%20reductions%20for%20imitation%20learning%202010"
        },
        {
            "id": "Ross_2014_a",
            "entry": "Stephane Ross and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. arXiv preprint arXiv:1406.5979, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5979"
        },
        {
            "id": "Russell_1998_a",
            "entry": "Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 101\u2013103, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_0000_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Syed_et+al_2008_a",
            "entry": "Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In International Conference on Machine Learning, pp. 1032\u20131039, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syed%2C%20Umar%20Bowling%2C%20Michael%20Schapire%2C%20Robert%20E.%20Apprenticeship%20learning%20using%20linear%20programming%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syed%2C%20Umar%20Bowling%2C%20Michael%20Schapire%2C%20Robert%20E.%20Apprenticeship%20learning%20using%20linear%20programming%202008"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Association for the Advancement of Artificial Intelligence, pp. 1433\u2013 1438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
