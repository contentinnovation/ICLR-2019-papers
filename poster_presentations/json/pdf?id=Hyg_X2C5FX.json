{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GAN DISSECTION: VISUALIZING AND UNDERSTANDING GENERATIVE ADVERSARIAL NETWORKS",
        "author": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba, 1Massachusetts Institute of Technology, 2MIT-IBM Watson AI Lab, 3IBM Research, 4The Chinese University of Hong Kong",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hyg_X2C5FX"
        },
        "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models."
    },
    "keywords": [
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "ACE": "average causal effect",
        "FID": "Frechet Inception Distance"
    },
    "highlights": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) have been able to produce photorealistic images, often indistinguishable from real images",
        "To produce a church image (Figure 1a), what knowledge does a Generative Adversarial Networks need to learn? Alternatively, when a Generative Adversarial Networks sometimes produces terribly unrealistic images (Figure 1f), what causes the mistakes? Why does one Generative Adversarial Networks variant work better than another? What fundamental differences are encoded in their weights?",
        "We study where we can insert object concepts in new images and how this intervention interacts with other objects in the image (Figure 1d)",
        "We show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, Generative Adversarial Networks variants and datasets; to debugging and improving Generative Adversarial Networks by locating and ablating \u201cartifact\u201d units (Figure 1e); to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control.\n2 RELATED WORK",
        "Interpretable units for different scene categories The set of all object classes matched by the units of a Generative Adversarial Networks provides a map of what a Generative Adversarial Networks has learned about the data",
        "By carefully examining representation units, we have found that many parts of Generative Adversarial Networks representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) have been able to produce photorealistic images, often indistinguishable from real images",
        "To produce a church image (Figure 1a), what knowledge does a Generative Adversarial Networks need to learn? Alternatively, when a Generative Adversarial Networks sometimes produces terribly unrealistic images (Figure 1f), what causes the mistakes? Why does one Generative Adversarial Networks variant work better than another? What fundamental differences are encoded in their weights?",
        "We study where we can insert object concepts in new images and how this intervention interacts with other objects in the image (Figure 1d)",
        "We show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, Generative Adversarial Networks variants and datasets; to debugging and improving Generative Adversarial Networks by locating and ablating \u201cartifact\u201d units (Figure 1e); to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control.\n2 RELATED WORK",
        "Interpretable units for different scene categories The set of all object classes matched by the units of a Generative Adversarial Networks provides a map of what a Generative Adversarial Networks has learned about the data",
        "The units that emerge are object classes appropriate to the scene type: for example, when we examine a Generative Adversarial Networks trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools",
        "We find that the Generative Adversarial Networks allows doors to be added in buildings, in plausible locations such as where a window is present, or where bricks are present",
        "By carefully examining representation units, we have found that many parts of Generative Adversarial Networks representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) have been able to produce photorealistic images, often indistinguishable from real images.",
        "Our work provides the first systematic analysis for understanding the internal representations of GANs. we show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, GAN variants and datasets; to debugging and improving GANs by locating and ablating \u201cartifact\u201d units (Figure 1e); to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control.",
        "Dissection measures agreement between a unit u and a concept c by comparing its thresholded upsampled heatmap with a semantic segmentation of the generated image sc(x).",
        "For image classification networks, <a class=\"ref-link\" id=\"cBau_et+al_2017_a\" href=\"#rBau_et+al_2017_a\">Bau et al (2017</a>) has observed that many units can approximately locate emergent object classes when the units are upsampled and thresholded.",
        "To answer the above question about causality, we probe the network using interventions: we test whether a set of units U in r cause the generation of c by forcing the units of U on and off.",
        "We wish to identify a set of units U that maximize the average causal effect \u03b4U\u2192c for an object class c.",
        "We start with a set of dominant object classes and use intervention to locate causal units that can remove and insert objects in different images (Section 4.3 and 4.4).",
        "Interpretable units for different scene categories The set of all object classes matched by the units of a GAN provides a map of what a GAN has learned about the data.",
        "We confirm that providing minibatch stddev statistics to the discriminator increases not only the realism of results, but the diversity of concepts represented by units: the number of types of objects, parts, and materials matching units increases by more than 40%.",
        "In Figure 9 we apply the method in Section 3.2 to identify sets of 20 units that have causal effects on common object classes in conference rooms scenes.",
        "Figure 11 shows the effect of inserting 20 layer4 causal door units in church scenes.",
        "The bar chart in Figure 11 shows average causal effects of insertions of door units, conditioned on the background object class at the location of the intervention.",
        "By carefully examining representation units, we have found that many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output."
    ],
    "headline": "We present an analytic framework to visualize and understand Generative Adversarial Networks at the unit-, object-, and scene-level",
    "reference_links": [
        {
            "id": "Bach_et+al_2015_a",
            "entry": "Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gregoire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gregoire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "Bau_et+al_2017_a",
            "entry": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR, 2017. 3, 4, 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017"
        },
        {
            "id": "Borji_2018_a",
            "entry": "Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446, 2018. 10",
            "arxiv_url": "https://arxiv.org/pdf/1802.03446"
        },
        {
            "id": "Bowers_et+al_2016_a",
            "entry": "Jeffrey S Bowers, Ivan I Vankov, Markus F Damian, and Colin J Davis. Why do some neurons in cortex respond to information in a selective manner? insights from artificial neural networks. Cognition, 148:47\u201363, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowers%2C%20Jeffrey%20S.%20Vankov%2C%20Ivan%20I.%20Damian%2C%20Markus%20F.%20Davis%2C%20Colin%20J.%20Why%20do%20some%20neurons%20in%20cortex%20respond%20to%20information%20in%20a%20selective%20manner%3F%20insights%20from%20artificial%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowers%2C%20Jeffrey%20S.%20Vankov%2C%20Ivan%20I.%20Damian%2C%20Markus%20F.%20Davis%2C%20Colin%20J.%20Why%20do%20some%20neurons%20in%20cortex%20respond%20to%20information%20in%20a%20selective%20manner%3F%20insights%20from%20artificial%20neural%20networks%202016"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. In ICLR, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017"
        },
        {
            "id": "Dasgupta_et+al_2018_a",
            "entry": "Sanjoy Dasgupta, Timothy C Sheehan, Charles F Stevens, and Saket Navlakha. A neural data structure for novelty detection. Proceedings of the National Academy of Sciences, 115(51):13093\u201313098, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20Sanjoy%20Sheehan%2C%20Timothy%20C.%20Stevens%2C%20Charles%20F.%20Navlakha%2C%20Saket%20A%20neural%20data%20structure%20for%20novelty%20detection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20Sanjoy%20Sheehan%2C%20Timothy%20C.%20Stevens%2C%20Charles%20F.%20Navlakha%2C%20Saket%20A%20neural%20data%20structure%20for%20novelty%20detection%202018"
        },
        {
            "id": "Denton_et+al_2015_a",
            "entry": "Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20L.%20Chintala%2C%20Soumith%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20L.%20Chintala%2C%20Soumith%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In ICLR, 2017. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20real%20nvp%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20real%20nvp%202017"
        },
        {
            "id": "Donahue_et+al_2017_a",
            "entry": "Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017. 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Krahenbuhl%2C%20Philipp%20Darrell%2C%20Trevor%20Adversarial%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Krahenbuhl%2C%20Philipp%20Darrell%2C%20Trevor%20Adversarial%20feature%20learning%202017"
        },
        {
            "id": "Dosovitskiy_2016_a",
            "entry": "Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In NIPS, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Generating%20images%20with%20perceptual%20similarity%20metrics%20based%20on%20deep%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20Alexey%20Brox%2C%20Thomas%20Generating%20images%20with%20perceptual%20similarity%20metrics%20based%20on%20deep%20networks%202016"
        },
        {
            "id": "Dumoulin_et+al_2017_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017. 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20Vincent%20Belghazi%2C%20Ishmael%20Poole%2C%20Ben%20Lamb%2C%20Alex%20Adversarially%20learned%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20Vincent%20Belghazi%2C%20Ishmael%20Poole%2C%20Ben%20Lamb%2C%20Alex%20Adversarially%20learned%20inference%202017"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017. 16",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017. 7, 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Hoffman_et+al_2018_a",
            "entry": "Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Judy%20Tzeng%2C%20Eric%20Park%2C%20Taesung%20Zhu%2C%20Jun-Yan%20Cycada%3A%20Cycle-consistent%20adversarial%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Judy%20Tzeng%2C%20Eric%20Park%2C%20Taesung%20Zhu%2C%20Jun-Yan%20Cycada%3A%20Cycle-consistent%20adversarial%20domain%20adaptation%202018"
        },
        {
            "id": "Holland_1988_a",
            "entry": "Paul W Holland. Causal inference, path analysis and recursive structural equations models. ETS Research Report Series, 1988(1):i\u201350, 1988. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holland%2C%20Paul%20W.%20Causal%20inference%2C%20path%20analysis%20and%20recursive%20structural%20equations%20models%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holland%2C%20Paul%20W.%20Causal%20inference%2C%20path%20analysis%20and%20recursive%20structural%20equations%20models%201988"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Karpathy_et+al_2016_a",
            "entry": "Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. In ICLR, 2016. 3, 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Johnson%2C%20Justin%20Fei-Fei%2C%20Li%20Visualizing%20and%20understanding%20recurrent%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Johnson%2C%20Justin%20Fei-Fei%2C%20Li%20Visualizing%20and%20understanding%20recurrent%20networks%202016"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018. 2, 6, 8, 16",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, and Martin Wattenberg. Tcav: Relative concept importance testing with linear concept activation vectors. arXiv preprint arXiv:1711.11279, 2017. 3",
            "arxiv_url": "https://arxiv.org/pdf/1711.11279"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "Lundberg_2017_a",
            "entry": "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NIPS, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "Mahendran_2015_a",
            "entry": "Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In CVPR, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015"
        },
        {
            "id": "Mathieu_et+al_2016_a",
            "entry": "Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959, 2018. 3",
            "arxiv_url": "https://arxiv.org/pdf/1803.06959"
        },
        {
            "id": "Olah_et+al_2018_a",
            "entry": "Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olah%2C%20Chris%20Satyanarayan%2C%20Arvind%20Johnson%2C%20Ian%20Carter%2C%20Shan%20The%20building%20blocks%20of%20interpretability%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olah%2C%20Chris%20Satyanarayan%2C%20Arvind%20Johnson%2C%20Ian%20Carter%2C%20Shan%20The%20building%20blocks%20of%20interpretability%202018"
        },
        {
            "id": "Pearl_2009_a",
            "entry": "Judea Pearl. Causality. Cambridge university press, 2009. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20Judea%20Causality%202009"
        },
        {
            "id": "Quiroga_2012_a",
            "entry": "Rodrigo Quian Quiroga. Concept cells: the building blocks of declarative memory functions. Nature Reviews Neuroscience, 13(8):587, 2012. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quiroga%2C%20Rodrigo%20Quian%20Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quiroga%2C%20Rodrigo%20Quian%20Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions%202012"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 1, 2, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Selvaraju_et+al_2017_a",
            "entry": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Selvaraju%2C%20Ramprasaath%20R.%20Cogswell%2C%20Michael%20Das%2C%20Abhishek%20Vedantam%2C%20Ramakrishna%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Selvaraju%2C%20Ramprasaath%20R.%20Cogswell%2C%20Michael%20Das%2C%20Abhishek%20Vedantam%2C%20Ramakrishna%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017"
        },
        {
            "id": "Simonyan_et+al_2014_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR Workshop, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014"
        },
        {
            "id": "Strobelt_et+al_2018_a",
            "entry": "Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M. Rush. LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE TVCG, 24(1): 667\u2013676, Jan 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strobelt%2C%20Hendrik%20Gehrmann%2C%20Sebastian%20Pfister%2C%20Hanspeter%20Rush%2C%20Alexander%20M.%20LSTMVis%3A%20A%20tool%20for%20visual%20analysis%20of%20hidden%20state%20dynamics%20in%20recurrent%20neural%20networks%202018-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strobelt%2C%20Hendrik%20Gehrmann%2C%20Sebastian%20Pfister%2C%20Hanspeter%20Rush%2C%20Alexander%20M.%20LSTMVis%3A%20A%20tool%20for%20visual%20analysis%20of%20hidden%20state%20dynamics%20in%20recurrent%20neural%20networks%202018-01"
        },
        {
            "id": "Sundararajan_et+al_2017_a",
            "entry": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In PMLR, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. In NIPS, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Liu%2C%20Guilin%20Video-to-video%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Liu%2C%20Guilin%20Video-to-video%20synthesis%202018"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive generation via adversary for object detection. In CVPR, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiaolong%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20A-fast-rcnn%3A%20Hard%20positive%20generation%20via%20adversary%20for%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiaolong%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20A-fast-rcnn%3A%20Hard%20positive%20generation%20via%20adversary%20for%20object%20detection%202017"
        },
        {
            "id": "Wijaya_et+al_2017_a",
            "entry": "Dedy Rahman Wijaya, Riyanarto Sarno, and Enny Zulaika. Information quality ratio as a novel metric for mother wavelet selection. Chemometrics and Intelligent Laboratory Systems, 160:59\u201371, 2017. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wijaya%2C%20Dedy%20Rahman%20Sarno%2C%20Riyanarto%20Zulaika%2C%20Enny%20Information%20quality%20ratio%20as%20a%20novel%20metric%20for%20mother%20wavelet%20selection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wijaya%2C%20Dedy%20Rahman%20Sarno%2C%20Riyanarto%20Zulaika%2C%20Enny%20Information%20quality%20ratio%20as%20a%20novel%20metric%20for%20mother%20wavelet%20selection%202017"
        },
        {
            "id": "Xiao_et+al_2018_a",
            "entry": "Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Tete%20Liu%2C%20Yingcheng%20Zhou%2C%20Bolei%20Jiang%2C%20Yuning%20Unified%20perceptual%20parsing%20for%20scene%20understanding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Tete%20Liu%2C%20Yingcheng%20Zhou%2C%20Bolei%20Jiang%2C%20Yuning%20Unified%20perceptual%20parsing%20for%20scene%20understanding%202018"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 6",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "Yuste_2015_a",
            "entry": "Rafael Yuste. From the neuron doctrine to neural networks. Nature reviews neuroscience, 16(8):487, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuste%2C%20Rafael%20From%20the%20neuron%20doctrine%20to%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuste%2C%20Rafael%20From%20the%20neuron%20doctrine%20to%20neural%20networks%202015"
        },
        {
            "id": "Zeiler_2014_a",
            "entry": "Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. 3, 6, 10",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018a. 1, 2",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "Zhang_et+al_2018_b",
            "entry": "Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural networks. In CVPR, 2018b. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Quanshi%20Wu%2C%20Ying%20Nian%20Zhu%2C%20Song-Chun%20Interpretable%20convolutional%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Quanshi%20Wu%2C%20Ying%20Nian%20Zhu%2C%20Song-Chun%20Interpretable%20convolutional%20neural%20networks%202018"
        },
        {
            "id": "Zhou_et+al_2015_a",
            "entry": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. In ICLR, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20Agata%20Oliva%2C%20Aude%20Object%20detectors%20emerge%20in%20deep%20scene%20cnns%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20Agata%20Oliva%2C%20Aude%20Object%20detectors%20emerge%20in%20deep%20scene%20cnns%202015"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Zhao%2C%20Hang%20Puig%2C%20Xavier%20Fidler%2C%20Sanja%20Scene%20parsing%20through%20ade20k%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Zhao%2C%20Hang%20Puig%2C%20Xavier%20Fidler%2C%20Sanja%20Scene%20parsing%20through%20ade20k%20dataset%202017"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting deep visual representations via network dissection. PAMI, 2018a. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Bau%2C%20David%20Oliva%2C%20Aude%20Torralba%2C%20Antonio%20Interpreting%20deep%20visual%20representations%20via%20network%20dissection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Bau%2C%20David%20Oliva%2C%20Aude%20Torralba%2C%20Antonio%20Interpreting%20deep%20visual%20representations%20via%20network%20dissection%202018"
        },
        {
            "id": "Zhou_et+al_2018_b",
            "entry": "Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In ECCV, pp. 119\u2013134, 2018b. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Sun%2C%20Yiyou%20Bau%2C%20David%20Torralba%2C%20Antonio%20Interpretable%20basis%20decomposition%20for%20visual%20explanation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Sun%2C%20Yiyou%20Bau%2C%20David%20Torralba%2C%20Antonio%20Interpretable%20basis%20decomposition%20for%20visual%20explanation%202018"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning dense correspondence via 3d-guided cycle consistency. In CVPR, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Tinghui%20Krahenbuhl%2C%20Philipp%20Aubry%2C%20Mathieu%20Qixing%20Huang%2C%20and%20Alexei%20A%20Efros.%20Learning%20dense%20correspondence%20via%203d-guided%20cycle%20consistency%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Tinghui%20Krahenbuhl%2C%20Philipp%20Aubry%2C%20Mathieu%20Qixing%20Huang%2C%20and%20Alexei%20A%20Efros.%20Learning%20dense%20correspondence%20via%203d-guided%20cycle%20consistency%202016"
        },
        {
            "id": "Zhu_et+al_2016_a",
            "entry": "Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Krahenbuhl%2C%20Philipp%20Shechtman%2C%20Eli%20Efros%2C%20Alexei%20A.%20Generative%20visual%20manipulation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Krahenbuhl%2C%20Philipp%20Shechtman%2C%20Eli%20Efros%2C%20Alexei%20A.%20Generative%20visual%20manipulation%202016"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        }
    ]
}
