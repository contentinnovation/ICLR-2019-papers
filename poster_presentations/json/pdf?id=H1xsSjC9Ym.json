{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TO UNDERSTAND GOAL SPECIFICATIONS BY MODELLING REWARD",
        "author": "Dzmitry Bahdanau, Mila, Universitede Montreal dimabgv@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1xsSjC9Ym"
        },
        "abstract": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations\u2014and for instructions\u2014not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "3d world",
            "url": "https://en.wikipedia.org/wiki/3D_World"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "Conditional Random Field",
            "url": "https://en.wikipedia.org/wiki/Conditional_Random_Field"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "IRL": "inverse reinforcement learning",
        "AGILE": "Adversarial Goal-Induced Learning from Examples",
        "NMN": "Neural Module Networks",
        "RP": "reward prediction",
        "CRF": "Conditional Random Field"
    },
    "highlights": [
        "Developing agents that can learn to follow user instructions pertaining to an environment is a longstanding goal of AI research (<a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\"><a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\">Winograd, 1972</a></a>)",
        "We show that the learning speed and performance of A3C agents trained with Adversarial Goal-Induced Learning from Examples reward models is superior to A3C agents trained against environment reward, and comparable to that of true-reward A3C agents supplemented by auxiliary unsupervised reward prediction objectives",
        "The difference lies in the source of the reward: we introduce an additional discriminator network D\u03c6, the reward model, whose purpose is to define a meaningful reward function for training \u03c0\u03b8",
        "We propose to reuse a reward model trained in Adversarial Goal-Induced Learning from Examples as a reward function for training or fine-tuning policies",
        "We have proposed Adversarial Goal-Induced Learning from Examples, a framework for training instruction-conditional reinforcement learning agents using rewards from learned reward models, which are jointly trained from data provided by both experts and the agent being trained, rather than reward provided by an instruction interpreter within the environment",
        "The fact that Adversarial Goal-Induced Learning from Examples objective attenuates learning issues due to the sparsity of reward states within episodes in a manner similar to reward prediction suggests that the reward model within Adversarial Goal-Induced Learning from Examples learns some form of shaped reward (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al, 1999</a>), and could serve not only in the cases where a reward function need to be learned in the absence of true reward, but in cases where environment reward is defined but sparse"
    ],
    "key_statements": [
        "Developing agents that can learn to follow user instructions pertaining to an environment is a longstanding goal of AI research (<a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\"><a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\">Winograd, 1972</a></a>)",
        "In each of these cases, being able to reward an agent for successfully completing a task specified by an instruction requires the implementation of a full interpreter of the instruction language",
        "This interpreter must be able to evaluate the instruction against environment states to determine when reward must be granted to the agent, and in doing so requires full knowledge of the Figure 1: Different valid goal states for semantics of the instruction language relative to the envi- the instruction \u201cbuild an L-like shape ronment",
        "Each of them can be interpreted as a result of successfully executing the instruction \u201cbuild an L-like shape from red blocks\u201d, despite the fact that these arrangements differ in the location and the orientation of the target shape, as well as in the positioning of the irrelevant blue blocks",
        "We introduce a framework\u2014Adversarial Goal-Induced Learning from Examples (AGILE)\u2014for jointly training an instruction-conditional reward model using expert examples of completed instructions alongside a policy which will learn to complete instructions by maximising the -modelled reward",
        "We show that the learning speed and performance of A3C agents trained with Adversarial Goal-Induced Learning from Examples reward models is superior to A3C agents trained against environment reward, and comparable to that of true-reward A3C agents supplemented by auxiliary unsupervised reward prediction objectives",
        "Without us ever having to implement the reward function, the agent trained within Adversarial Goal-Induced Learning from Examples learns to construct arrangements as instructed",
        "The difference lies in the source of the reward: we introduce an additional discriminator network D\u03c6, the reward model, whose purpose is to define a meaningful reward function for training \u03c0\u03b8",
        "We propose to reuse a reward model trained in Adversarial Goal-Induced Learning from Examples as a reward function for training or fine-tuning policies",
        "Because the language of our instructions is generated from a simple grammar, we perform most of our experiments using policy and reward model networks that are constructed using the Neural Module Networks (<a class=\"ref-link\" id=\"cAndreas_et+al_2016_a\" href=\"#rAndreas_et+al_2016_a\">Andreas et al, 2016</a>) paradigm",
        "The policy and reward model trained within Adversarial Goal-Induced Learning from Examples have to infer this specific sense of what these spatial relations mean from goal-state examples, while the baseline agent is allowed to access our programmed ground-truth reward",
        "Adversarial Goal-Induced Learning from Examples-A3C achieved nearly perfect performance (99.5%). We found this to be a very promising result, since within Adversarial Goal-Induced Learning from Examples, we induce the reward function from a limited set of examples",
        "When we used larger values of \u03c1 Adversarial Goal-Induced Learning from Examples-A3C training started quicker but after 100-200 million steps the performance started to deteriorate, while it remained stable with \u03c1 = 25%. Data efficiency These results suggest that the Adversarial Goal-Induced Learning from Examples reward model was able to induce a near perfect reward function from a limited set of instruction, goal-state pairs",
        "We found that with a training set of only 8000 examples, the Adversarial Goal-Induced Learning from Examples-A3C agent could reach a performance of 60%",
        "Generalization to Unseen Instructions In the experiments we have reported so far the Adversarial Goal-Induced Learning from Examples agent was trained on all 990 possible GridLU-Relation instructions",
        "<a class=\"ref-link\" id=\"cJanner_et+al_2017_a\" href=\"#rJanner_et+al_2017_a\">Janner et al (2017</a>) and <a class=\"ref-link\" id=\"cMisra_et+al_2017_a\" href=\"#rMisra_et+al_2017_a\">Misra et al (2017</a>) train reinforcement learning agents to produce goal-states given instructions. These approaches are constrained by the difficulty of programming language-related reward functions, a task that requires an programming expert, detailed access to the state of the environment and hard choices above how language should map to the world",
        "Adversarial Goal-Induced Learning from Examples differs from these approaches in that goal-states are only used to train the reward module, which we show generalises to new environment configurations or instructions, relative to those seen in the expert data",
        "We have proposed Adversarial Goal-Induced Learning from Examples, a framework for training instruction-conditional reinforcement learning agents using rewards from learned reward models, which are jointly trained from data provided by both experts and the agent being trained, rather than reward provided by an instruction interpreter within the environment",
        "As well as a means to learn from a potentially more prevalent form of data, our experiments demonstrate that policies trained in the Adversarial Goal-Induced Learning from Examples framework perform comparably with and can learn as fast as those trained against ground-truth reward and additional auxiliary tasks",
        "The fact that Adversarial Goal-Induced Learning from Examples objective attenuates learning issues due to the sparsity of reward states within episodes in a manner similar to reward prediction suggests that the reward model within Adversarial Goal-Induced Learning from Examples learns some form of shaped reward (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al, 1999</a>), and could serve not only in the cases where a reward function need to be learned in the absence of true reward, but in cases where environment reward is defined but sparse",
        "While there is a large gap to be closed between the sort of tasks and language experimented with in this paper and those which might be presented in \u201creal world\u201d situations or more complex environments, our results provide an encouraging first step in this direction"
    ],
    "summary": [
        "Developing agents that can learn to follow user instructions pertaining to an environment is a longstanding goal of AI research (<a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\"><a class=\"ref-link\" id=\"cWinograd_1972_a\" href=\"#rWinograd_1972_a\">Winograd, 1972</a></a>).",
        "Without us ever having to implement the reward function, the agent trained within AGILE learns to construct arrangements as instructed.",
        "We jointly learn this reward model alongside the policy by training it to predict whether a given state s is a goal state for a given instruction c or not.",
        "Because the language of our instructions is generated from a simple grammar, we perform most of our experiments using policy and reward model networks that are constructed using the NMN (<a class=\"ref-link\" id=\"cAndreas_et+al_2016_a\" href=\"#rAndreas_et+al_2016_a\">Andreas et al, 2016</a>) paradigm.",
        "The policy and reward model trained within AGILE have to infer this specific sense of what these spatial relations mean from goal-state examples, while the baseline agent is allowed to access our programmed ground-truth reward.",
        "Data efficiency These results suggest that the AGILE reward model was able to induce a near perfect reward function from a limited set of instruction, goal-state pairs.",
        "To test whether the AGILE framework is robust to changes to the environment dynamics, we first trained the policy and reward model as normal and modified the effective physics of the world by making all red square objects immovable.",
        "We built a generator to produce random instantiations of these goal-state classes, as positive examples for the reward model.",
        "These approaches are constrained by the difficulty of programming language-related reward functions, a task that requires an programming expert, detailed access to the state of the environment and hard choices above how language should map to the world.",
        "AGILE differs from these approaches in that goal-states are only used to train the reward module, which we show generalises to new environment configurations or instructions, relative to those seen in the expert data.",
        "This opens up new possibilities for training language-aware agents: in the real world, and even in rich simulated environments (<a class=\"ref-link\" id=\"cBrodeur_et+al_2017_a\" href=\"#rBrodeur_et+al_2017_a\">Brodeur et al, 2017</a>; <a class=\"ref-link\" id=\"cWu_et+al_2018_a\" href=\"#rWu_et+al_2018_a\">Wu et al, 2018</a>), acquiring such data via human annotation would often be much more viable than defining and implementing reward functions programmatically.",
        "As well as a means to learn from a potentially more prevalent form of data, our experiments demonstrate that policies trained in the AGILE framework perform comparably with and can learn as fast as those trained against ground-truth reward and additional auxiliary tasks."
    ],
    "headline": "We present a framework within which instruction-conditional reinforcement learning agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples",
    "reference_links": [
        {
            "id": "Abbeel_2004_a",
            "entry": "Pieter Abbeel and Andrew Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, 2004. URL http://doi.acm.org/10.1145/1015330.1015430.",
            "url": "http://doi.acm.org/10.1145/1015330.1015430",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20Pieter%20Ng%2C%20Andrew%20Y.%20Apprenticeship%20Learning%20via%20Inverse%20Reinforcement%20Learning%202004"
        },
        {
            "id": "Andreas_2015_a",
            "entry": "Jacob Andreas and Dan Klein. Alignment-Based Compositional Semantics for Instruction Following. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Alignment-Based%20Compositional%20Semantics%20for%20Instruction%20Following%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Alignment-Based%20Compositional%20Semantics%20for%20Instruction%20Following%202015"
        },
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural Module Networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL http://arxiv.org/abs/1511.02799.",
            "url": "http://arxiv.org/abs/1511.02799",
            "arxiv_url": "https://arxiv.org/pdf/1511.02799"
        },
        {
            "id": "Artzi_2013_a",
            "entry": "Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49\u201362, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013"
        },
        {
            "id": "Brodeur_et+al_2017_a",
            "entry": "Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, and Aaron Courville. HoME: a Household Multimodal Environment. arXiv:1711.11017 [cs, eess], November 2017. URL http://arxiv.org/abs/1711.11017.arXiv:1711.11017.",
            "url": "http://arxiv.org/abs/1711.11017.arXiv:1711.11017",
            "arxiv_url": "https://arxiv.org/pdf/1711.11017"
        },
        {
            "id": "Chaplot_et+al_2018_a",
            "entry": "Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-Attention Architectures for Task-Oriented Language Grounding. In Proceedings of 32nd AAAI Conference on Artificial Intelligence, 2018. URL http://arxiv.org/abs/1706.07230.",
            "url": "http://arxiv.org/abs/1706.07230",
            "arxiv_url": "https://arxiv.org/pdf/1706.07230"
        },
        {
            "id": "Chen_2011_a",
            "entry": "David L. Chen and Raymond J. Mooney. Learning to Interpret Natural Language Navigation Instructions from Observations. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, pp. 859\u2013865, 2011. URL http://dl.acm.org/citation.cfm?id=2900423.2900560.",
            "url": "http://dl.acm.org/citation.cfm?id=2900423.2900560",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20David%20L.%20Mooney%2C%20Raymond%20J.%20Learning%20to%20Interpret%20Natural%20Language%20Navigation%20Instructions%20from%20Observations%202011"
        },
        {
            "id": "Christiano_et+al_2017_a",
            "entry": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pp. 4302\u20134310, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017"
        },
        {
            "id": "Denil_et+al_2017_a",
            "entry": "Misha Denil, Sergio Gmez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Programmable Agents. arXiv:1706.06383 [cs, stat], June 2017. URL http://arxiv.org/abs/1706.06383.",
            "url": "http://arxiv.org/abs/1706.06383",
            "arxiv_url": "https://arxiv.org/pdf/1706.06383"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. In International Conference on Learning Representations, September 2018. URL https://openreview.net/forum?id=r1lq1hRqYQ.",
            "url": "https://openreview.net/forum?id=r1lq1hRqYQ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Korattikara%2C%20Anoop%20Levine%2C%20Sergey%20Guadarrama%2C%20Sergio%20From%20Language%20to%20Goals%3A%20Inverse%20Reinforcement%20Learning%20for%20Vision-Based%20Instruction%20Following%202018-09"
        },
        {
            "id": "Ganin_et+al_2018_a",
            "entry": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. Synthesizing Programs for Images using Reinforced Adversarial Learning. arXiv:1804.01118 [cs, stat], April 2018. URL http://arxiv.org/abs/1804.01118.arXiv:1804.01118.",
            "url": "http://arxiv.org/abs/1804.01118.arXiv:1804.01118",
            "arxiv_url": "https://arxiv.org/pdf/1804.01118"
        },
        {
            "id": "Hermann_et+al_2017_a",
            "entry": "Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, and Phil Blunsom. Grounded Language Learning in a Simulated 3d World. arXiv:1706.06551 [cs, stat], June 2017. URL http://arxiv.org/abs/1706.06551.",
            "url": "http://arxiv.org/abs/1706.06551",
            "arxiv_url": "https://arxiv.org/pdf/1706.06551"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In ICLR, November 2016. URL http://arxiv.org/abs/1611.05397.",
            "url": "http://arxiv.org/abs/1611.05397",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "Janner_et+al_2017_a",
            "entry": "Michael Janner, Karthik Narasimhan, and Regina Barzilay. Representation Learning for Grounded Spatial Reasoning. Transactions of the Association for Computational Linguistics, July 2017. URL http://arxiv.org/abs/1707.03938.",
            "url": "http://arxiv.org/abs/1707.03938",
            "arxiv_url": "https://arxiv.org/pdf/1707.03938"
        },
        {
            "id": "Knox_2009_a",
            "entry": "W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The TAMER framework. In International Conference on Knowledge Capture, pp. 9\u201316, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knox%2C%20W.Bradley%20Stone%2C%20Peter%20Interactively%20shaping%20agents%20via%20human%20reinforcement%3A%20The%20TAMER%20framework%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knox%2C%20W.Bradley%20Stone%2C%20Peter%20Interactively%20shaping%20agents%20via%20human%20reinforcement%3A%20The%20TAMER%20framework%202009"
        },
        {
            "id": "Macglashan_et+al_2015_a",
            "entry": "James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael L. Littman, Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang. Grounding english commands to reward functions. In Robotics: Science and Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacGlashan%2C%20James%20Babes-Vroman%2C%20Monica%20desJardins%2C%20Marie%20Littman%2C%20Michael%20L.%20Grounding%20english%20commands%20to%20reward%20functions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacGlashan%2C%20James%20Babes-Vroman%2C%20Monica%20desJardins%2C%20Marie%20Littman%2C%20Michael%20L.%20Grounding%20english%20commands%20to%20reward%20functions%202015"
        },
        {
            "id": "Mei_et+al_2016_a",
            "entry": "Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. In Proceedings of the AAAI Conference on Artificial Intelligence, 2016. URL http://arxiv.org/abs/1506.04089.",
            "url": "http://arxiv.org/abs/1506.04089",
            "arxiv_url": "https://arxiv.org/pdf/1506.04089"
        },
        {
            "id": "Misra_et+al_2017_a",
            "entry": "Dipendra Misra, John Langford, and Yoav Artzi. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning. In arXiv:1704.08795 [cs], April 2017. URL http://arxiv.org/abs/1704.08795.",
            "url": "http://arxiv.org/abs/1704.08795",
            "arxiv_url": "https://arxiv.org/pdf/1704.08795"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y. Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. In in Proc. 17th International Conf. on Machine Learning, pp. 663\u2013670. Morgan Kaufmann, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20Inverse%20Reinforcement%20Learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20Inverse%20Reinforcement%20Learning%202000"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning. In Proceedings of The 34st International Conference on Machine Learning, June 2017. URL http://arxiv.org/abs/1706.05064.",
            "url": "http://arxiv.org/abs/1706.05064",
            "arxiv_url": "https://arxiv.org/pdf/1706.05064"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Perez_et+al_2017_a",
            "entry": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer. In In Proceedings of the AAAI Conference on Artificial Intelligence, 2017. URL http://arxiv.org/abs/1709.07871.",
            "url": "http://arxiv.org/abs/1709.07871",
            "arxiv_url": "https://arxiv.org/pdf/1709.07871"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014. URL http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf.",
            "url": "http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Stadie_et+al_2017_a",
            "entry": "Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-Person Imitation Learning. In ICLR, March 2017. URL http://arxiv.org/abs/1703.01703.",
            "url": "http://arxiv.org/abs/1703.01703",
            "arxiv_url": "https://arxiv.org/pdf/1703.01703"
        },
        {
            "id": "Tellex_et+al_2011_a",
            "entry": "Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. In Twenty-Fifth AAAI Conference on Artificial Intelligence, August 2011. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3623.",
            "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3623",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tellex%2C%20Stefanie%20Kollar%2C%20Thomas%20Dickerson%2C%20Steven%20Walter%2C%20Matthew%20R.%20Understanding%20Natural%20Language%20Commands%20for%20Robotic%20Navigation%20and%20Mobile%20Manipulation%202011-08"
        },
        {
            "id": "Vogel_2010_a",
            "entry": "Adam Vogel and Dan Jurafsky. Learning to Follow Navigational Directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 806\u2013814. Association for Computational Linguistics, 2010. URL http://dl.acm.org/citation.cfm?id=1858681.1858764.",
            "url": "http://dl.acm.org/citation.cfm?id=1858681.1858764",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vogel%2C%20Adam%20Jurafsky%2C%20Dan%20Learning%20to%20Follow%20Navigational%20Directions%202010"
        },
        {
            "id": "Warnell_et+al_2017_a",
            "entry": "Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER: Interactive agent shaping in high-dimensional state spaces. arXiv preprint arXiv:1709.10163, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10163"
        },
        {
            "id": "Williams_et+al_2018_a",
            "entry": "Edward C Williams, Nakul Gopalan, Mine Rhee, and Stefanie Tellex. Learning to parse natural language to grounded reward functions with weak supervision. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1\u20137. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Edward%20C.%20Gopalan%2C%20Nakul%20Rhee%2C%20Mine%20Tellex%2C%20Stefanie%20Learning%20to%20parse%20natural%20language%20to%20grounded%20reward%20functions%20with%20weak%20supervision%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Edward%20C.%20Gopalan%2C%20Nakul%20Rhee%2C%20Mine%20Tellex%2C%20Stefanie%20Learning%20to%20parse%20natural%20language%20to%20grounded%20reward%20functions%20with%20weak%20supervision%202018"
        },
        {
            "id": "Wilson_et+al_2012_a",
            "entry": "Aaron Wilson, Alan Fern, and Prasad Tadepalli. A Bayesian approach for policy learning from trajectory preference queries. In Advances in Neural Information Processing Systems, pp. 1133\u2013 1141, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Tadepalli%2C%20Prasad%20A%20Bayesian%20approach%20for%20policy%20learning%20from%20trajectory%20preference%20queries%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Tadepalli%2C%20Prasad%20A%20Bayesian%20approach%20for%20policy%20learning%20from%20trajectory%20preference%20queries%202012"
        },
        {
            "id": "Winograd_1971_a",
            "entry": "Terry Winograd. Procedures as a representation for data in a computer program for understanding natural language. Technical report, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Winograd%2C%20Terry%20Procedures%20as%20a%20representation%20for%20data%20in%20a%20computer%20program%20for%20understanding%20natural%20language%201971"
        },
        {
            "id": "Winograd_1972_a",
            "entry": "Terry Winograd. Understanding natural language. Cognitive Psychology, 3(1):1\u2013191, 1972. doi: 10. 1016/0010-0285(72)90002-3. URL http://linkinghub.elsevier.com/retrieve/pii/0010028572900023.",
            "crossref": "https://dx.doi.org/10.1016/0010-0285(72)90002-3",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/0010-0285%2872%2990002-3"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building Generalizable Agents with a Realistic and Rich 3d Environment. arXiv:1801.02209 [cs], January 2018. URL http://arxiv.org/abs/1801.02209.arXiv:1801.02209.",
            "url": "http://arxiv.org/abs/1801.02209.arXiv:1801.02209",
            "arxiv_url": "https://arxiv.org/pdf/1801.02209"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Haonan Yu, Haochao Zhang, and Wei Xu. Interactive Grounded Language Acquisition and Generalization in 2d Environment. In ICLR, 2018. URL https://openreview.net/forum?id= H1UOm4gA-&noteId=H1UOm4gA-.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Haonan%20Zhang%2C%20Haochao%20Xu%2C%20Wei%20Interactive%20Grounded%20Language%20Acquisition%20and%20Generalization%20in%202d%20Environment%202018"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum Entropy Inverse Reinforcement Learning. In Proc. AAAI, pp. 1433\u20131438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20Entropy%20Inverse%20Reinforcement%20Learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20Entropy%20Inverse%20Reinforcement%20Learning%202008"
        }
    ]
}
