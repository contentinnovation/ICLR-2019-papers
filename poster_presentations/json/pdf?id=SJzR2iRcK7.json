{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MULTI-CLASS CLASSIFICATION WITHOUT MULTICLASS LABELS",
        "author": "Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira, 1Georgia Institute of Technology 2Georgia Tech Research Institute",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJzR2iRcK7"
        },
        "abstract": "This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option."
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "binary classifier",
            "url": "https://en.wikipedia.org/wiki/binary_classifier"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "MCL": "Meta Classification Likelihood",
        "SPN": "similarity prediction network",
        "NMI": "normalized mutual information"
    },
    "highlights": [
        "One of the most common settings for machine learning is classification, which involves learning a function f to map the input data x to a class label y \u2208 {1, 2, .., C}",
        "We make the following contributions: 1) We analyze the problem setting and show that the loss we can use for this encapsulation can be derived, and we present an intuitive probabilistic graphical model interpretation for doing so, 2) We evaluate its performance compared to vanilla supervised learning of neural networks which uses multi-class labels, and visualize the loss landscape to better understand the underlying optimization difficulty, and 3) We demonstrate support for learning classifiers in more challenging problem domains, e.g. in unsupervised cross-task transfer and semi-supervised learning",
        "We presented a new strategy to learn a multi-class classification via a binary decision problem",
        "We formulate the problem setting via a probabilistic graphical model and derive a simple likelihood objective that can be effectively optimized via neural networks",
        "We show how this same framework can be used for three learning paradigms: supervised learning, unsupervised cross-task transfer learning, and semi-supervised learning",
        "Results show comparable or improved results over state of the art, especially in the challenging unsupervised cross-task setting. This demonstrates the power of using pairwise similarity as weak labels to relax the requirement of class-specific labeling"
    ],
    "key_statements": [
        "One of the most common settings for machine learning is classification, which involves learning a function f to map the input data x to a class label y \u2208 {1, 2, .., C}",
        "Despite the success of deep learning, a neural network demands a large amount of class-specific labels for learning a discriminative model, i.e. P (y|x)",
        "We make the following contributions: 1) We analyze the problem setting and show that the loss we can use for this encapsulation can be derived, and we present an intuitive probabilistic graphical model interpretation for doing so, 2) We evaluate its performance compared to vanilla supervised learning of neural networks which uses multi-class labels, and visualize the loss landscape to better understand the underlying optimization difficulty, and 3) We demonstrate support for learning classifiers in more challenging problem domains, e.g. in unsupervised cross-task transfer and semi-supervised learning",
        "In figure 2, we show the graphical model for our problem, where classspecific labels Y are latent while pairwise similarities S are observed",
        "We propose a new strategy to obtain the S for semi-supervised learning",
        "Results and discussion: The results in Table 1 show that Meta Classification Likelihood achieves similar classification performance as CE with different network depths and three datasets",
        "Unlike KCL, Meta Classification Likelihood can converge to a performance close to CE with random initialization in all of our experiments",
        "Meta Classification Likelihood outperforms CE with a deeper network",
        "Such a result indicates that Meta Classification Likelihood is less prone to overfitting.\n5.3",
        "The results shown in Table 2 and 3 demonstrate a clear advantage for Meta Classification Likelihood over other methods",
        "The advantage of Meta Classification Likelihood over KCL is not due to the ease of optimization, since the network is shallow in the Omniglot experiment and the network is pre-trained in the ImageNet experiment",
        "The advantage may due to the fact that Meta Classification Likelihood is free of hyper-parameters and so performs better than KCL which uses a heuristic threshold (\u03c3 = 2) (<a class=\"ref-link\" id=\"cHsu_2016_a\" href=\"#rHsu_2016_a\">Hsu & Kira, 2016</a>) for its margin",
        "The similarity prediction network-Meta Classification Likelihood uses the same strategy presented in the Section 4.2 for unsupervised learning, except that the similarity prediction network is trained with only the labeled portion (e.g. 4k labeled data) of CIFAR10",
        "Pseudo-Meta Classification Likelihood is free of hyperparameter, which is a very appealing characteristic for learning with few data",
        "We presented a new strategy to learn a multi-class classification via a binary decision problem",
        "We formulate the problem setting via a probabilistic graphical model and derive a simple likelihood objective that can be effectively optimized via neural networks",
        "We show how this same framework can be used for three learning paradigms: supervised learning, unsupervised cross-task transfer learning, and semi-supervised learning",
        "Results show comparable or improved results over state of the art, especially in the challenging unsupervised cross-task setting. This demonstrates the power of using pairwise similarity as weak labels to relax the requirement of class-specific labeling"
    ],
    "summary": [
        "One of the most common settings for machine learning is classification, which involves learning a function f to map the input data x to a class label y \u2208 {1, 2, .., C}.",
        "When using a neural network with softmax outputs for the multi-class classifier, the proposed scheme can learn a discriminative model with only pairwise information.",
        "We make the following contributions: 1) We analyze the problem setting and show that the loss we can use for this encapsulation can be derived, and we present an intuitive probabilistic graphical model interpretation for doing so, 2) We evaluate its performance compared to vanilla supervised learning of neural networks which uses multi-class labels, and visualize the loss landscape to better understand the underlying optimization difficulty, and 3) We demonstrate support for learning classifiers in more challenging problem domains, e.g. in unsupervised cross-task transfer and semi-supervised learning.",
        "The method transfers the pairwise similarity as the meta knowledge to an unlabeled dataset of different classes.",
        "<a class=\"ref-link\" id=\"cHsu_et+al_2018_a\" href=\"#rHsu_et+al_2018_a\">Hsu et al (2018</a>) proposes a method in which a similarity prediction network (SPN) can be learned from a labeled auxiliary dataset.",
        "The learning objective is the sum of the multi-class cross-entropy and Pseudo-MCL, so the mapping between output nodes and classes are automatically decided by the supervised part of learning.",
        "We use a standard gradient-based method for training a neural network by optimizing the learning criterion.",
        "This section empirically compares MCL to multi-class cross-entropy (CE) and the strong baseline using pairwise similarity (Kullback\u2013Leibler divergence based contrastive loss (KCL) (<a class=\"ref-link\" id=\"cHsu_2016_a\" href=\"#rHsu_2016_a\">Hsu & Kira, 2016</a>; <a class=\"ref-link\" id=\"cHsu_et+al_2018_a\" href=\"#rHsu_et+al_2018_a\"><a class=\"ref-link\" id=\"cHsu_et+al_2018_a\" href=\"#rHsu_et+al_2018_a\">Hsu et al, 2018</a></a>)), in a supervised learning setting.",
        "Since the learning objectives KCL and MCL both work on pairs of inputs, we have a pairwise enumeration layer (<a class=\"ref-link\" id=\"cHsu_et+al_2018_a\" href=\"#rHsu_et+al_2018_a\"><a class=\"ref-link\" id=\"cHsu_et+al_2018_a\" href=\"#rHsu_et+al_2018_a\">Hsu et al, 2018</a></a>) between the network outputs and the loss function.",
        "This setting is the same as a multi-class classification task, except no labels are provided in the target dataset.",
        "The procedure uses ImageN et882 for learning the similarity prediction function and randomly samples 30 classes (\u223c39k images) from ImageN et118 for the unlabeled target data.",
        "We evaluate the semi-supervised learning performance of the Pseudo-MCL on the standard benchmark dataset CIFAR-10.",
        "The SPN-MCL uses the same strategy presented in the Section 4.2 for unsupervised learning, except that the SPN is trained with only the labeled portion (e.g. 4k labeled data) of CIFAR10 .",
        "All the semi-supervised methods are trained with initial learning rate 0.001 and have a decay with factor 0.1 at epoch 150 and 250 for a total of 300 epochs.",
        "We presented a new strategy to learn a multi-class classification via a binary decision problem.",
        "We hope the presented perspective of meta classification inspires additional approaches to learning with fewer labeled data as well as application to domains where weak labels are easier to obtain"
    ],
    "headline": "We demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings",
    "reference_links": [
        {
            "id": "Allwein_et+al_2000_a",
            "entry": "Erin L Allwein, Robert E Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec):113\u2013141, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allwein%2C%20Erin%20L.%20Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Reducing%20multiclass%20to%20binary%3A%20A%20unifying%20approach%20for%20margin%20classifiers%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allwein%2C%20Erin%20L.%20Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Reducing%20multiclass%20to%20binary%3A%20A%20unifying%20approach%20for%20margin%20classifiers%202000"
        },
        {
            "id": "Amid_et+al_2016_a",
            "entry": "Ehsan Amid, Aristides Gionis, and Antti Ukkonen. Semi-supervised kernel metric learning using relative comparisons. arXiv preprint arXiv:1612.00086, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00086"
        },
        {
            "id": "Anand_et+al_1995_a",
            "entry": "Rangachari Anand, Kishan Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. Efficient classification for multiclass problems using modular neural networks. IEEE Transactions on Neural Networks, 6 (1):117\u2013124, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anand%2C%20Rangachari%20Mehrotra%2C%20Kishan%20Mohan%2C%20Chilukuri%20K.%20Ranka%2C%20Sanjay%20Efficient%20classification%20for%20multiclass%20problems%20using%20modular%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anand%2C%20Rangachari%20Mehrotra%2C%20Kishan%20Mohan%2C%20Chilukuri%20K.%20Ranka%2C%20Sanjay%20Efficient%20classification%20for%20multiclass%20problems%20using%20modular%20neural%20networks%201995"
        },
        {
            "id": "Anand_et+al_2014_a",
            "entry": "Saket Anand, Sushil Mittal, Oncel Tuzel, and Peter Meer. Semi-supervised kernel mean shift clustering. IEEE transactions on pattern analysis and machine intelligence, 36(6):1201\u20131215, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anand%2C%20Saket%20Mittal%2C%20Sushil%20Tuzel%2C%20Oncel%20Meer%2C%20Peter%20Semi-supervised%20kernel%20mean%20shift%20clustering%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anand%2C%20Saket%20Mittal%2C%20Sushil%20Tuzel%2C%20Oncel%20Meer%2C%20Peter%20Semi-supervised%20kernel%20mean%20shift%20clustering%202014"
        },
        {
            "id": "Antoine_et+al_2012_a",
            "entry": "Violaine Antoine, Benjamin Quost, M-H Masson, and Thierry Denoeux. Cecm: Constrained evidential c-means algorithm. Computational Statistics & Data Analysis, 56(4):894\u2013914, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antoine%2C%20Violaine%20Benjamin%20Quost%2C%20M.-H.Masson%20Denoeux%2C%20Thierry%20Cecm%3A%20Constrained%20evidential%20c-means%20algorithm%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antoine%2C%20Violaine%20Benjamin%20Quost%2C%20M.-H.Masson%20Denoeux%2C%20Thierry%20Cecm%3A%20Constrained%20evidential%20c-means%20algorithm%202012"
        },
        {
            "id": "Bao_et+al_2018_a",
            "entry": "Han Bao, Gang Niu, and Masashi Sugiyama. Classification from pairwise similarity and unlabeled data. In Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bao%2C%20Han%20Niu%2C%20Gang%20Sugiyama%2C%20Masashi%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bao%2C%20Han%20Niu%2C%20Gang%20Sugiyama%2C%20Masashi%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018"
        },
        {
            "id": "Bilenko_et+al_2004_a",
            "entry": "Mikhail Bilenko, Sugato Basu, and Raymond J Mooney. Integrating constraints and metric learning in semi-supervised clustering. In Proceedings of the twenty-first international conference on Machine learning, pp. 11. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bilenko%2C%20Mikhail%20Basu%2C%20Sugato%20Mooney%2C%20Raymond%20J.%20Integrating%20constraints%20and%20metric%20learning%20in%20semi-supervised%20clustering%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bilenko%2C%20Mikhail%20Basu%2C%20Sugato%20Mooney%2C%20Raymond%20J.%20Integrating%20constraints%20and%20metric%20learning%20in%20semi-supervised%20clustering%202004"
        },
        {
            "id": "Cai_et+al_2009_a",
            "entry": "Deng Cai, Xiaofei He, Xuanhui Wang, Hujun Bao, and Jiawei Han. Locality preserving nonnegative matrix factorization. In IJCAI, volume 9, pp. 1010\u20131015, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Deng%20He%2C%20Xiaofei%20Wang%2C%20Xuanhui%20Bao%2C%20Hujun%20Locality%20preserving%20nonnegative%20matrix%20factorization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Deng%20He%2C%20Xiaofei%20Wang%2C%20Xuanhui%20Bao%2C%20Hujun%20Locality%20preserving%20nonnegative%20matrix%20factorization%202009"
        },
        {
            "id": "Chen_2011_a",
            "entry": "Xinlei Chen and Deng Cai. Large scale spectral clustering with landmark-based representation. In AAAI, volume 5, pp. 14, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xinlei%20Cai%2C%20Deng%20Large%20scale%20spectral%20clustering%20with%20landmark-based%20representation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xinlei%20Cai%2C%20Deng%20Large%20scale%20spectral%20clustering%20with%20landmark-based%20representation%202011"
        },
        {
            "id": "Davis_et+al_2007_a",
            "entry": "Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. Information-theoretic metric learning. In Proceedings of the 24th international conference on Machine learning, pp. 209\u2013216. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davis%2C%20Jason%20V.%20Kulis%2C%20Brian%20Jain%2C%20Prateek%20Sra%2C%20Suvrit%20Information-theoretic%20metric%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davis%2C%20Jason%20V.%20Kulis%2C%20Brian%20Jain%2C%20Prateek%20Sra%2C%20Suvrit%20Information-theoretic%20metric%20learning%202007"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Freund_1997_a",
            "entry": "Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997"
        },
        {
            "id": "Robin_2003_a",
            "entry": "Johannes F\u00fcrnkranz. Round robin ensembles. Intelligent Data Analysis, 7(5):385\u2013403, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johannes%20F%C3%BCrnkranz%20Round%20robin%20ensembles%20Intelligent%20Data%20Analysis%2075385403%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johannes%20F%C3%BCrnkranz%20Round%20robin%20ensembles%20Intelligent%20Data%20Analysis%2075385403%202003"
        },
        {
            "id": "Galar_et+al_2011_a",
            "entry": "Mikel Galar, Alberto Fern\u00e1ndez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. An overview of ensemble methods for binary classifiers in multi-class problems: Experimental study on one-vs-one and one-vs-all schemes. Pattern Recognition, 44(8):1761\u20131776, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galar%2C%20Mikel%20Fern%C3%A1ndez%2C%20Alberto%20Barrenechea%2C%20Edurne%20Bustince%2C%20Humberto%20An%20overview%20of%20ensemble%20methods%20for%20binary%20classifiers%20in%20multi-class%20problems%3A%20Experimental%20study%20on%20one-vs-one%20and%20one-vs-all%20schemes%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Galar%2C%20Mikel%20Fern%C3%A1ndez%2C%20Alberto%20Barrenechea%2C%20Edurne%20Bustince%2C%20Humberto%20An%20overview%20of%20ensemble%20methods%20for%20binary%20classifiers%20in%20multi-class%20problems%3A%20Experimental%20study%20on%20one-vs-one%20and%20one-vs-all%20schemes%202011"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6544"
        },
        {
            "id": "Hastie_1998_a",
            "entry": "Trevor Hastie and Robert Tibshirani. Classification by pairwise coupling. In Advances in neural information processing systems, pp. 507\u2013513, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Classification%20by%20pairwise%20coupling%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Classification%20by%20pairwise%20coupling%201998"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Hsu_2016_a",
            "entry": "Yen-Chang Hsu and Zsolt Kira. Neural network-based clustering using pairwise constraints. International Conference on Learning Representations (ICLR) workshop, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Yen-Chang%20Kira%2C%20Zsolt%20Neural%20network-based%20clustering%20using%20pairwise%20constraints.%20International%20Conference%20on%20Learning%20Representations%20%28ICLR%29%20workshop%202016"
        },
        {
            "id": "Hsu_et+al_2018_a",
            "entry": "Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across domains and tasks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Yen-Chang%20Lv%2C%20Zhaoyang%20Kira%2C%20Zsolt%20Learning%20to%20cluster%20in%20order%20to%20transfer%20across%20domains%20and%20tasks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Yen-Chang%20Lv%2C%20Zhaoyang%20Kira%2C%20Zsolt%20Learning%20to%20cluster%20in%20order%20to%20transfer%20across%20domains%20and%20tasks%202018"
        },
        {
            "id": "Im_et+al_2016_a",
            "entry": "Daniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of deep network loss surfaces. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Im%2C%20Daniel%20Jiwoong%20Tao%2C%20Michael%20Branson%2C%20Kristin%20An%20empirical%20analysis%20of%20deep%20network%20loss%20surfaces%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Kuhn_1955_a",
            "entry": "Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuhn%2C%20Harold%20W.%20The%20hungarian%20method%20for%20the%20assignment%20problem%201955",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuhn%2C%20Harold%20W.%20The%20hungarian%20method%20for%20the%20assignment%20problem%201955"
        },
        {
            "id": "Laine_2017_a",
            "entry": "Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The mnist database of handwritten digits. 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20The%20mnist%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "Lecun_et+al_1998_b",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lee_2013_a",
            "entry": "Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Dong-Hyun%20Pseudo-label%3A%20The%20simple%20and%20efficient%20semi-supervised%20learning%20method%20for%20deep%20neural%20networks%202013"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09913"
        },
        {
            "id": "Macqueen_0000_a",
            "entry": "James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pp. 281\u2013297.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacQueen%2C%20James%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacQueen%2C%20James%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations"
        },
        {
            "id": "Oakland_1967_a",
            "entry": "Oakland, CA, USA., 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oakland%20CA%20USA%201967"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Maeda%2C%20Shin-ichi%20Ishii%2C%20Shin%20Koyama%2C%20Masanori%20Virtual%20adversarial%20training%3A%20a%20regularization%20method%20for%20supervised%20and%20semi-supervised%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Maeda%2C%20Shin-ichi%20Ishii%2C%20Shin%20Koyama%2C%20Masanori%20Virtual%20adversarial%20training%3A%20a%20regularization%20method%20for%20supervised%20and%20semi-supervised%20learning%202018"
        },
        {
            "id": "Oliver_et+al_2018_a",
            "entry": "Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. arXiv preprint arXiv:1804.09170, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09170"
        },
        {
            "id": "Rangapuram_2012_a",
            "entry": "Syama Sundar Rangapuram and Matthias Hein. Constrained 1-spectral clustering. In AISTATS, volume 30, pp. 90, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangapuram%2C%20Syama%20Sundar%20Hein%2C%20Matthias%20Constrained%201-spectral%20clustering%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangapuram%2C%20Syama%20Sundar%20Hein%2C%20Matthias%20Constrained%201-spectral%20clustering%202012"
        },
        {
            "id": "Rifkin_2004_a",
            "entry": "Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. Journal of machine learning research, 5(Jan):101\u2013141, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rifkin%2C%20Ryan%20Klautau%2C%20Aldebaro%20In%20defense%20of%20one-vs-all%20classification%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rifkin%2C%20Ryan%20Klautau%2C%20Aldebaro%20In%20defense%20of%20one-vs-all%20classification%202004"
        },
        {
            "id": "Sajjadi_et+al_2016_a",
            "entry": "Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163\u20131171, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016"
        },
        {
            "id": "Schapire_1999_a",
            "entry": "Robert E Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. Machine learning, 37(3):297\u2013336, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Improved%20boosting%20algorithms%20using%20confidence-rated%20predictions%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Improved%20boosting%20algorithms%20using%20confidence-rated%20predictions%201999"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Strehl_2002_a",
            "entry": "Alexander Strehl and Joydeep Ghosh. Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions. Journal of machine learning research, 3(Dec):583\u2013617, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20Ghosh%2C%20Joydeep%20Cluster%20ensembles%E2%80%94a%20knowledge%20reuse%20framework%20for%20combining%20multiple%20partitions%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20Ghosh%2C%20Joydeep%20Cluster%20ensembles%E2%80%94a%20knowledge%20reuse%20framework%20for%20combining%20multiple%20partitions%202002"
        },
        {
            "id": "Tarvainen_2017_a",
            "entry": "Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195\u20131204, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Timothy%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Timothy%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Wang_et+al_2014_a",
            "entry": "Xiang Wang, Buyue Qian, and Ian Davidson. On constrained spectral clustering and its applications. Data Mining and Knowledge Discovery, pp. 1\u201330, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiang%20Qian%2C%20Buyue%20Davidson%2C%20Ian%20On%20constrained%20spectral%20clustering%20and%20its%20applications.%20Data%20Mining%20and%20Knowledge%20Discovery%202014"
        },
        {
            "id": "Weston_1998_a",
            "entry": "Jason Weston and Chris Watkins. Multi-class support vector machines. Technical report, Citeseer, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weston%2C%20Jason%20Watkins%2C%20Chris%20Multi-class%20support%20vector%20machines%201998"
        },
        {
            "id": "Wu_et+al_2004_a",
            "entry": "Ting-Fan Wu, Chih-Jen Lin, and Ruby C Weng. Probability estimates for multi-class classification by pairwise coupling. Journal of Machine Learning Research, 5(Aug):975\u20131005, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Ting-Fan%20Lin%2C%20Chih-Jen%20Weng%2C%20Ruby%20C.%20Probability%20estimates%20for%20multi-class%20classification%20by%20pairwise%20coupling%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Ting-Fan%20Lin%2C%20Chih-Jen%20Weng%2C%20Ruby%20C.%20Probability%20estimates%20for%20multi-class%20classification%20by%20pairwise%20coupling%202004"
        },
        {
            "id": "Xing_2003_a",
            "entry": "Eric P Xing, Michael I Jordan, Stuart J Russell, and Andrew Y Ng. Distance metric learning with application to clustering with side-information. In Advances in neural information processing systems, pp. 521\u2013528, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eric%20P%20Xing%20Michael%20I%20Jordan%20Stuart%20J%20Russell%20and%20Andrew%20Y%20Ng%20Distance%20metric%20learning%20with%20application%20to%20clustering%20with%20sideinformation%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20521528%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eric%20P%20Xing%20Michael%20I%20Jordan%20Stuart%20J%20Russell%20and%20Andrew%20Y%20Ng%20Distance%20metric%20learning%20with%20application%20to%20clustering%20with%20sideinformation%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20521528%202003"
        },
        {
            "id": "Yang_et+al_2010_a",
            "entry": "Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering using local discriminant models and global integration. IEEE Transactions on Image Processing, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yi%20Xu%2C%20Dong%20Nie%2C%20Feiping%20Yan%2C%20Shuicheng%20Image%20clustering%20using%20local%20discriminant%20models%20and%20global%20integration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yi%20Xu%2C%20Dong%20Nie%2C%20Feiping%20Yan%2C%20Shuicheng%20Image%20clustering%20using%20local%20discriminant%20models%20and%20global%20integration%202010"
        }
    ]
}
