{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "BUILDING DYNAMIC KNOWLEDGE GRAPHS FROM TEXT USING MACHINE READING COMPREHENSION",
        "author": "Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew McCallum, 1College of Information and Computer Sciences University of Massachusetts, Amherst {rajarshi, mccallum}@cs.umass.edu 2Microsoft Research Montreal Montreal, Quebec, Canada {tsendsuren.munkhdalai,eric.yuan, adam.trischler}@microsoft.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1lhbnRqF7"
        },
        "abstract": "We propose a neural machine reading model that constructs dynamic knowledge graphs from procedural text. It builds these graphs recurrently for each step of the described procedure, and uses them to track the evolving states of participant entities. We harness and extend a recently proposed machine reading comprehension (MRC) model to query for entity states, since these states are generally communicated in spans of text and MRC models perform well in extracting entity-centric spans. The explicit, structured, and evolving knowledge graph representations that our model constructs can be used in downstream question answering tasks to improve machine comprehension of text, as we demonstrate empirically. On two comprehension tasks from the recently proposed PROPARA dataset (<a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al., 2018</a>), our model achieves state-of-the-art results. The model also outperforms previous approaches on the RECIPES dataset (<a class=\"ref-link\" id=\"cKiddon_et+al_2015_a\" href=\"#rKiddon_et+al_2015_a\">Kiddon et al., 2015</a>), which suggests it may apply broadly to procedural text. Finally, we present some evidence that the model\u2019s graphical representations help it to impose commonsense constraints on its predictions."
    },
    "keywords": [
        {
            "term": "knowledge base",
            "url": "https://en.wikipedia.org/wiki/knowledge_base"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "graphical representation",
            "url": "https://en.wikipedia.org/wiki/graphical_representation"
        },
        {
            "term": "knowledge graphs",
            "url": "https://en.wikipedia.org/wiki/Knowledge_Graph"
        }
    ],
    "abbreviations": {
        "MRC": "machine reading comprehension",
        "KGs": "knowledge graphs",
        "QRN": "query reduction networks",
        "NPN": "Neural Process Networks",
        "KB": "knowledge base",
        "RNN": "recurrent neural network"
    },
    "highlights": [
        "Building knowledge graphs (KGs) from text is a long-standing goal in artificial intelligence research",
        "This paper introduces a neural machine-reading model, knowledge graphs-machine reading comprehension, that (i) explicitly constructs dynamic knowledge graphs to track state changes in procedural text and conditions on its own constructed knowledge graphs to improve downstream question answering on the text",
        "We evaluate our model (KG-machine reading comprehension) on the above two PROPARA tasks and find that the same model significantly outperforms the previous state of the art",
        "We evaluate knowledge graphs-machine reading comprehension on the two PROPARA tasks proposed by <a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al (2018</a>) and <a class=\"ref-link\" id=\"cTandon_et+al_2018_a\" href=\"#rTandon_et+al_2018_a\">Tandon et al (2018</a>), respectively, and find that our single model outperforms each of the above models on their respective tasks of focus",
        "We evaluate knowledge graphs-machine reading comprehension on the above two tasks by querying the knowledge graphs it builds at each time-step; we use the official evaluation pipeline2 for each task",
        "We proposed a neural machine-reading model that constructs dynamic knowledge graphs from text to track locations of participant entities in procedural text"
    ],
    "key_statements": [
        "Building knowledge graphs (KGs) from text is a long-standing goal in artificial intelligence research",
        "This paper introduces a neural machine-reading model, knowledge graphs-machine reading comprehension, that (i) explicitly constructs dynamic knowledge graphs to track state changes in procedural text and conditions on its own constructed knowledge graphs to improve downstream question answering on the text",
        "We evaluate our model (KG-machine reading comprehension) on the above two PROPARA tasks and find that the same model significantly outperforms the previous state of the art",
        "We evaluate knowledge graphs-machine reading comprehension on the two PROPARA tasks proposed by <a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al (2018</a>) and <a class=\"ref-link\" id=\"cTandon_et+al_2018_a\" href=\"#rTandon_et+al_2018_a\">Tandon et al (2018</a>), respectively, and find that our single model outperforms each of the above models on their respective tasks of focus",
        "We evaluate knowledge graphs-machine reading comprehension on the recently released PROPARA dataset (<a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al, 2018</a>), which comprises procedural text about scientific processes",
        "<a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al (2018</a>) and <a class=\"ref-link\" id=\"cTandon_et+al_2018_a\" href=\"#rTandon_et+al_2018_a\">Tandon et al (2018</a>) propose different models to solve each of these tasks separately, whereas we evaluate the same model, knowledge graphs-machine reading comprehension, on both tasks",
        "We evaluate knowledge graphs-machine reading comprehension on the above two tasks by querying the knowledge graphs it builds at each time-step; we use the official evaluation pipeline2 for each task",
        "We analyzed knowledge graphs-machine reading comprehension\u2019s outputs and were surprised to discover that our model learns these commonsense constraints from the data in an end-to-end fashion, as we show quantitatively in \u00a75.4",
        "We find that knowledge graphs-machine reading comprehension, like PROSTRUCT, does not violate any Type 1 or Type 2 constraints",
        "The step-by-step output of both PROGLOBAL (<a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al (2018</a>)) and knowledge graphs-machine reading comprehension is shown in Table 7, where we track the state of entity blood across six sentences",
        "We proposed a neural machine-reading model that constructs dynamic knowledge graphs from text to track locations of participant entities in procedural text"
    ],
    "summary": [
        "Building knowledge graphs (KGs) from text is a long-standing goal in artificial intelligence research.",
        "Since we expect answers about entity states to change over the course of the text, our model\u2019s MRC component conditions on the evolving graph at the current time step.",
        "After the graph has been updated with the new states of all entities, our model updates each entity representation with information about its state.",
        "The querying process conditions on both the input text and the constructed knowledge graph from the previous time step.",
        "Conditioning on the span vectors for all entities, the model constructs the graph Gt by updating graph Gt\u22121 from the previous time step.",
        "The update to an entity\u2019s node representation with new location information arises from a hard decision made by the MRC model, whereas co-reference between entities across time steps is resolved with soft attention.",
        "When queried about the current location of an entity, the MRC module (\u00a7 4.2) returns a span of text as the answer, whose representation is later used to update the appropriate node vector in the graph.",
        "\u039bi,t = gi\u03c8i,t + (1 \u2212 gi)\u03c8i,t , where \u039bt\u22121 = [\u03bbi,t ]iN=1 \u2208 RN\u00d7d is a matrix of location node representations from the previous time step and \u03c8i,t is the location span vector output by the MRC module.",
        "Since different instances of the same location can be predicted for multiple entities, we perform a co-reference disambiguation within each time step using a self-attention mechanism: ui,t = softmax(\u039bt \u03bbi,t ) (2)",
        "We teacher-force the model at training time by updating the location-node representations with the encoding of the correct span.",
        "For the last two variations, we train the MRC model in isolation and predict location spans from the current sentence or paragraph prefix text.",
        "We can see that training the MRC model on paragraph prefixes already provides a good starting performance of 40.83% micro-average, which is significantly boosted by the recurrent graph module and graph conditioning up to 47.64%.",
        "The step-by-step output of both PROGLOBAL (<a class=\"ref-link\" id=\"cDalvi_et+al_2018_a\" href=\"#rDalvi_et+al_2018_a\">Dalvi et al (2018</a>)) and KG-MRC is shown in Table 7, where we track the state of entity blood across six sentences.",
        "We proposed a neural machine-reading model that constructs dynamic knowledge graphs from text to track locations of participant entities in procedural text.",
        "KG-MRC, achieves state-of-theart results on two question-answering tasks from the PROPARA dataset and one from the RECIPES dataset.",
        "We will extend the model to construct more general knowledge graphs with multiple relation types"
    ],
    "headline": "We propose a neural machine reading model that constructs dynamic knowledge graphs from procedural text",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Bansal_et+al_2017_a",
            "entry": "Trapit Bansal, Arvind Neelakantan, and Andrew McCallum. Relnet: End-to-end modeling of entities & relations. In AKBC, NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bansal%2C%20Trapit%20Neelakantan%2C%20Arvind%20McCallum%2C%20Andrew%20Relnet%3A%20End-to-end%20modeling%20of%20entities%20%26%20relations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bansal%2C%20Trapit%20Neelakantan%2C%20Arvind%20McCallum%2C%20Andrew%20Relnet%3A%20End-to-end%20modeling%20of%20entities%20%26%20relations%202017"
        },
        {
            "id": "Berant_et+al_2014_a",
            "entry": "Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, and Christopher D Manning. Modeling biological processes for reading comprehension. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berant%2C%20Jonathan%20Srikumar%2C%20Vivek%20Chen%2C%20Pei-Chun%20Linden%2C%20Abby%20Vander%20Modeling%20biological%20processes%20for%20reading%20comprehension%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berant%2C%20Jonathan%20Srikumar%2C%20Vivek%20Chen%2C%20Pei-Chun%20Linden%2C%20Abby%20Vander%20Modeling%20biological%20processes%20for%20reading%20comprehension%202014"
        },
        {
            "id": "Bosselut_et+al_2018_a",
            "entry": "Antoine Bosselut, Omer Levy, Ari Holtzman, Corin Ennis, Dieter Fox, and Yejin Choi. Simulating action dynamics with neural process networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bosselut%2C%20Antoine%20Levy%2C%20Omer%20Holtzman%2C%20Ari%20Ennis%2C%20Corin%20Simulating%20action%20dynamics%20with%20neural%20process%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bosselut%2C%20Antoine%20Levy%2C%20Omer%20Holtzman%2C%20Ari%20Ennis%2C%20Corin%20Simulating%20action%20dynamics%20with%20neural%20process%20networks%202018"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Danqi%20Fisch%2C%20Adam%20Weston%2C%20Jason%20and%20Antoine%20Bordes.%20Reading%20wikipedia%20to%20answer%20opendomain%20questions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Danqi%20Fisch%2C%20Adam%20Weston%2C%20Jason%20and%20Antoine%20Bordes.%20Reading%20wikipedia%20to%20answer%20opendomain%20questions%202017"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Dalvi_et+al_2018_a",
            "entry": "Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalvi%2C%20Bhavana%20Huang%2C%20Lifu%20Tandon%2C%20Niket%20Yih%2C%20Wen-tau%20Tracking%20state%20changes%20in%20procedural%20text%3A%20a%20challenge%20dataset%20and%20models%20for%20process%20paragraph%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalvi%2C%20Bhavana%20Huang%2C%20Lifu%20Tandon%2C%20Niket%20Yih%2C%20Wen-tau%20Tracking%20state%20changes%20in%20procedural%20text%3A%20a%20challenge%20dataset%20and%20models%20for%20process%20paragraph%20comprehension%202018"
        },
        {
            "id": "Das_et+al_2017_a",
            "entry": "Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. Question answering on knowledge bases and text using universal schema and memory networks. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20Rajarshi%20Zaheer%2C%20Manzil%20Reddy%2C%20Siva%20McCallum%2C%20Andrew%20Question%20answering%20on%20knowledge%20bases%20and%20text%20using%20universal%20schema%20and%20memory%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20Rajarshi%20Zaheer%2C%20Manzil%20Reddy%2C%20Siva%20McCallum%2C%20Andrew%20Question%20answering%20on%20knowledge%20bases%20and%20text%20using%20universal%20schema%20and%20memory%20networks%202017"
        },
        {
            "id": "Edwards_2016_a",
            "entry": "Michael Edwards and Xianghua Xie. Graph based convolutional neural network. arXiv preprint arXiv:1609.08965, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08965"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Henaff_et+al_2017_a",
            "entry": "Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henaff%2C%20Mikael%20Weston%2C%20Jason%20Szlam%2C%20Arthur%20Bordes%2C%20Antoine%20Tracking%20the%20world%20state%20with%20recurrent%20entity%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henaff%2C%20Mikael%20Weston%2C%20Jason%20Szlam%2C%20Arthur%20Bordes%2C%20Antoine%20Tracking%20the%20world%20state%20with%20recurrent%20entity%20networks%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Joulin_et+al_2016_a",
            "entry": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03651"
        },
        {
            "id": "Kiddon_et+al_2015_a",
            "entry": "Chloe Kiddon, Ganesa Thandavam Ponnuraj, Luke Zettlemoyer, and Yejin Choi. Mise en place: Unsupervised interpretation of instructional recipes. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiddon%2C%20Chloe%20Ponnuraj%2C%20Ganesa%20Thandavam%20Zettlemoyer%2C%20Luke%20Choi%2C%20Yejin%20Mise%20en%20place%3A%20Unsupervised%20interpretation%20of%20instructional%20recipes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiddon%2C%20Chloe%20Ponnuraj%2C%20Ganesa%20Thandavam%20Zettlemoyer%2C%20Luke%20Choi%2C%20Yejin%20Mise%20en%20place%3A%20Unsupervised%20interpretation%20of%20instructional%20recipes%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kipf_2017_a",
            "entry": "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "Levy_et+al_2017_a",
            "entry": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke S. Zettlemoyer. Zero-shot relation extraction via reading comprehension. In CoNLL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Seo%2C%20Minjoon%20Choi%2C%20Eunsol%20Zettlemoyer%2C%20Luke%20S.%20Zero-shot%20relation%20extraction%20via%20reading%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Seo%2C%20Minjoon%20Choi%2C%20Eunsol%20Zettlemoyer%2C%20Luke%20S.%20Zero-shot%20relation%20extraction%20via%20reading%20comprehension%202017"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Roth_et+al_2018_a",
            "entry": "Benjamin Roth, Costanza Conforti, Nina Poerner, Sanjeev Karn, and Hinrich Schutze. Neural architectures for open-type relation argument extraction. arXiv preprint arXiv:1803.01707, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01707"
        },
        {
            "id": "Seo_et+al_2017_a",
            "entry": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. In ICLR, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Minjoon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Minjoon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202017"
        },
        {
            "id": "Seo_et+al_2017_b",
            "entry": "Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Query-reduction networks for question answering. In ICLR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Query-reduction%20networks%20for%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Query-reduction%20networks%20for%20question%20answering%202017"
        },
        {
            "id": "Tandon_et+al_2018_a",
            "entry": "Niket Tandon, Bhavana Dalvi Mishra, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. Reasoning about actions and state changes by injecting commonsense knowledge. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tandon%2C%20Niket%20Mishra%2C%20Bhavana%20Dalvi%20Grus%2C%20Joel%20Yih%2C%20Wen-tau%20Reasoning%20about%20actions%20and%20state%20changes%20by%20injecting%20commonsense%20knowledge%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tandon%2C%20Niket%20Mishra%2C%20Bhavana%20Dalvi%20Grus%2C%20Joel%20Yih%2C%20Wen-tau%20Reasoning%20about%20actions%20and%20state%20changes%20by%20injecting%20commonsense%20knowledge%202018"
        },
        {
            "id": "Weston_et+al_2015_a",
            "entry": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05698"
        },
        {
            "id": "Xiong_et+al_2017_a",
            "entry": "Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiong%2C%20Caiming%20Zhong%2C%20Victor%20Socher%2C%20Richard%20Dynamic%20coattention%20networks%20for%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiong%2C%20Caiming%20Zhong%2C%20Victor%20Socher%2C%20Richard%20Dynamic%20coattention%20networks%20for%20question%20answering%202017"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Adams%20Wei%20Dohan%2C%20David%20Luong%2C%20Minh-Thang%20Zhao%2C%20Rui%20Qanet%3A%20Combining%20local%20convolution%20with%20global%20self-attention%20for%20reading%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Adams%20Wei%20Dohan%2C%20David%20Luong%2C%20Minh-Thang%20Zhao%2C%20Rui%20Qanet%3A%20Combining%20local%20convolution%20with%20global%20self-attention%20for%20reading%20comprehension%202018"
        },
        {
            "id": "Implementation_2014_a",
            "entry": "Implementation details of KG-MRC are as follows. In all experiments, the word embeddings are initialized with FastText embeddings (Joulin et al., 2016); we use a document LSTM with two layers, the number of hidden units in each layer is 64. We apply dropout rate of 0.4 in all recurrent layers, and 0.3 in all other layers. The number of recurrent graph layers were set to (L = 2). The hidden unit size for the recurrent graph component was set to 64. During training, the mini-batch size is 8. We use adam (Kingma & Ba, 2014) as the step rule for optimization, The learning rate is set to 0.002. The model is implemented using PyTorch (Paszke et al., 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Implementation%20details%20of%20KGMRC%20are%20as%20follows%20In%20all%20experiments%20the%20word%20embeddings%20are%20initialized%20with%20FastText%20embeddings%20Joulin%20et%20al%202016%20we%20use%20a%20document%20LSTM%20with%20two%20layers%20the%20number%20of%20hidden%20units%20in%20each%20layer%20is%2064%20We%20apply%20dropout%20rate%20of%2004%20in%20all%20recurrent%20layers%20and%2003%20in%20all%20other%20layers%20The%20number%20of%20recurrent%20graph%20layers%20were%20set%20to%20L%20%202%20The%20hidden%20unit%20size%20for%20the%20recurrent%20graph%20component%20was%20set%20to%2064%20During%20training%20the%20minibatch%20size%20is%208%20We%20use%20adam%20Kingma%20%20Ba%202014%20as%20the%20step%20rule%20for%20optimization%20The%20learning%20rate%20is%20set%20to%200002%20The%20model%20is%20implemented%20using%20PyTorch%20Paszke%20et%20al%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Implementation%20details%20of%20KGMRC%20are%20as%20follows%20In%20all%20experiments%20the%20word%20embeddings%20are%20initialized%20with%20FastText%20embeddings%20Joulin%20et%20al%202016%20we%20use%20a%20document%20LSTM%20with%20two%20layers%20the%20number%20of%20hidden%20units%20in%20each%20layer%20is%2064%20We%20apply%20dropout%20rate%20of%2004%20in%20all%20recurrent%20layers%20and%2003%20in%20all%20other%20layers%20The%20number%20of%20recurrent%20graph%20layers%20were%20set%20to%20L%20%202%20The%20hidden%20unit%20size%20for%20the%20recurrent%20graph%20component%20was%20set%20to%2064%20During%20training%20the%20minibatch%20size%20is%208%20We%20use%20adam%20Kingma%20%20Ba%202014%20as%20the%20step%20rule%20for%20optimization%20The%20learning%20rate%20is%20set%20to%200002%20The%20model%20is%20implemented%20using%20PyTorch%20Paszke%20et%20al%202017"
        }
    ]
}
