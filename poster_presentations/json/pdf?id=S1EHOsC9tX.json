{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs",
        "author": "Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel L\u00e1zaro-Gredilla, Christopher Laan, Bhaskara Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, Alex Lavin, D. Scott Phoenix",
        "date": 2017,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1EHOsC9tX",
            "doi": "10.1126/science.aag2612"
        },
        "journal": "Science",
        "volume": "358",
        "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L\u221e defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L\u221e perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.",
        "pages": "eaag2612"
    },
    "keywords": [
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "lp norm",
            "url": "https://en.wikipedia.org/wiki/lp_norm"
        },
        {
            "term": "Intelligence Advanced Research Projects Activity",
            "url": "https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "Deep neural networks",
            "url": "https://en.wikipedia.org/wiki/Deep_neural_networks"
        }
    ],
    "abbreviations": {
        "DNNs": "Deep neural networks",
        "VAEs": "variational autoencoders",
        "ELBO": "evidence lower bound",
        "ABS": "Analysis by Synthesis model",
        "MIM": "Momentum Iterative Method",
        "BIM": "Basic Iterative Method",
        "FGSM": "Fast Gradient Sign Method",
        "FGM": "Fast Gradient Method",
        "NN": "Nearest Neighbour",
        "IARPA": "Intelligence Advanced Research Projects Activity"
    },
    "highlights": [
        "Deep neural networks (DNNs) are strikingly susceptible to minimal adversarial perturbations (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>), perturbations that are imperceptible to humans but which can switch the class prediction of Deep neural networks to basically any desired target class.<br/><br/>One key problem in finding successful defenses is the difficulty of reliably evaluating model robustness",
        "We show that MNIST is unsolved from the point of adversarial robustness: the SOTA defense of <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al (2018</a>) is still highly vulnerable to tiny perturbations that are meaningless to humans",
        "In this paper we demonstrated that, despite years of work, we as a community failed to create neural networks that can be considered robust on MNIST from the point of human perception",
        "We showed that even today\u2019s best defense is susceptible to small adversarial perturbations that make little to no semantic sense to humans",
        "We presented a new approach based on analysis by synthesis that seeks to explain its inference by means of the actual image features",
        "We performed an extensive analysis to show that minimal adversarial perturbations in this model are large across all tested Lp norms and semantically meaningful to humans"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) are strikingly susceptible to minimal adversarial perturbations (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>), perturbations that are imperceptible to humans but which can switch the class prediction of Deep neural networks to basically any desired target class.<br/><br/>One key problem in finding successful defenses is the difficulty of reliably evaluating model robustness",
        "We show that MNIST is unsolved from the point of adversarial robustness: the SOTA defense of <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al (2018</a>) is still highly vulnerable to tiny perturbations that are meaningless to humans",
        "For each model and Lp norm, we show how the accuracy of the models decreases with increasing adversarial perturbation size (Figure 2) and report two metrics: the median adversarial distance (Table 1, left values) and the model\u2019s accuracy against bounded adversarial perturbations (Table 1, right values)",
        "Minimal Adversarials Our robustness evaluation results of all models are reported in Table 1 and Figure 2",
        "For L2 our Analysis by Synthesis model model outperforms all other models by a large margin. For L\u221e, our Binary Analysis by Synthesis model model is state-of-the-art in terms of median perturbation size",
        "In this paper we demonstrated that, despite years of work, we as a community failed to create neural networks that can be considered robust on MNIST from the point of human perception",
        "We showed that even today\u2019s best defense is susceptible to small adversarial perturbations that make little to no semantic sense to humans",
        "We presented a new approach based on analysis by synthesis that seeks to explain its inference by means of the actual image features",
        "We performed an extensive analysis to show that minimal adversarial perturbations in this model are large across all tested Lp norms and semantically meaningful to humans",
        "We demonstrated that MNIST is still not solved from the point of adversarial robustness and showed that our novel approach based on analysis by synthesis has great potential to reduce the vulnerability against adversarial attacks and to align machine perception with human perception"
    ],
    "summary": [
        "Deep neural networks (DNNs) are strikingly susceptible to minimal adversarial perturbations (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a></a>), perturbations that are imperceptible to humans but which can switch the class prediction of DNNs to basically any desired target class.<br/><br/>One key problem in finding successful defenses is the difficulty of reliably evaluating model robustness.",
        "We show that MNIST is unsolved from the point of adversarial robustness: the SOTA defense of <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al (2018</a>) is still highly vulnerable to tiny perturbations that are meaningless to humans.",
        "As we see in the results section, this defense is limited to the metric it is trained on (L\u221e) and it is straight-forward to generate small adversarial perturbations that carry little semantic meaning for humans.",
        "This includes in particular DefenseGAN (<a class=\"ref-link\" id=\"cSamangouei_et+al_2018_a\" href=\"#rSamangouei_et+al_2018_a\">Samangouei et al, 2018</a>), Adversarial Perturbation Elimination GAN (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a>) and Robust Manifold Defense (<a class=\"ref-link\" id=\"cIlyas_et+al_2017_a\" href=\"#rIlyas_et+al_2017_a\">Ilyas et al, 2017</a>), all of which project an image onto the manifold defined by a generator network G.",
        "We here follow a different approach by modeling the input distribution within each class, and by classifying a new sample according to the class under which it has the highest likelihood.",
        "Because we chose this posterior to be Gaussian, the class-conditional likelihoods can only change gracefully with changes in x, a property which allows us to derive lower bounds on the model robustness.",
        "Evaluating model robustness is difficult because each attack only provides an upper bound on the size of the adversarial perturbations (<a class=\"ref-link\" id=\"cUesato_et+al_2018_a\" href=\"#rUesato_et+al_2018_a\">Uesato et al, 2018</a>).",
        "To make this bound as tight as possible we apply many different attacks and choose the best one for each sample and model combination which often perform internal hyperparameter optimization).",
        "The commonly reported model accuracy on bounded adversarial perturbations, on the other hand, requires a metric-specific threshold that can bias the results.",
        "Distal Adversarials We probe the behavior of CNN, Madry et al and our ABS model outside the data distribution.",
        "We performed an extensive analysis to show that minimal adversarial perturbations in this model are large across all tested Lp norms and semantically meaningful to humans.",
        "We put a lot of effort into an extensive evaluation of adversarial robustness using a large collection of powerful attacks, including one designed to be effective against the ABS model, and we will release the model architecture and trained weights as a friendly invitation to fellow researchers to evaluate our model.",
        "We demonstrated that MNIST is still not solved from the point of adversarial robustness and showed that our novel approach based on analysis by synthesis has great potential to reduce the vulnerability against adversarial attacks and to align machine perception with human perception."
    ],
    "headline": "We show that even the widely recognized and by far most successful L\u221e defense by Madry et al. has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, classifies unrecognizable images with high certainty, performs not much better than simple input binarization and features adversarial perturbations that make little sense to humans",
    "reference_links": [
        {
            "id": "Athalye_2018_a",
            "entry": "Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial example defenses. arXiv preprint arXiv:1804.03286, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03286"
        },
        {
            "id": "Athalye_et+al_2018_b",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Brendel_2017_a",
            "entry": "W. Brendel and M. Bethge. Comment on \u201cbiologically inspired protection of deep networks from adversarial attacks\u201d. arXiv preprint arXiv:1704.01547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01547"
        },
        {
            "id": "Brendel_et+al_2018_a",
            "entry": "Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SyZI0GWCZ.",
            "url": "https://openreview.net/forum?id=SyZI0GWCZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brendel%2C%20Wieland%20Rauber%2C%20Jonas%20Bethge%2C%20Matthias%20Decision-based%20adversarial%20attacks%3A%20Reliable%20attacks%20against%20black-box%20machine%20learning%20models%202018"
        },
        {
            "id": "Bubeck_2018_a",
            "entry": "S\u00e9bastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints. arXiv preprint arXiv:1805.10204, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10204"
        },
        {
            "id": "Buckman_et+al_2018_a",
            "entry": "Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=S18Su--CW.",
            "url": "https://openreview.net/forum?id=S18Su--CW",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Magnet and\" efficient defenses against adversarial attacks\" are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08478"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Dhillon_et+al_2018_a",
            "entry": "Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C. Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust adversarial defense. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1uR4GZRZ.",
            "url": "https://openreview.net/forum?id=H1uR4GZRZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhillon%2C%20Guneet%20S.%20Azizzadenesheli%2C%20Kamyar%20Bernstein%2C%20Jeremy%20D.%20Kossaifi%2C%20Jean%20Stochastic%20activation%20pruning%20for%20robust%20adversarial%20defense%202018"
        },
        {
            "id": "Dong_et+al_2017_a",
            "entry": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, and Jun Zhu. Boosting adversarial attacks with momentum. arxiv preprint. arXiv preprint arXiv:1710.06081, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06081"
        },
        {
            "id": "George_et+al_2017_a",
            "entry": "Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel L\u00e1zaro-Gredilla, Christopher Laan, Bhaskara Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, Alex Lavin, and D. Scott Phoenix. A generative vision model that trains with high data efficiency and breaks text-based captchas. Science, 358(6368), 2017. ISSN 0036-8075. doi: 10.1126/science.aag2612. URL http://science.sciencemag.org/content/358/6368/eaag2612.",
            "crossref": "https://dx.doi.org/10.1126/science.aag2612",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1126/science.aag2612"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Guo_et+al_2018_a",
            "entry": "Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SyJ7ClWCb.",
            "url": "https://openreview.net/forum?id=SyJ7ClWCb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Chuan%20Rana%2C%20Mayank%20Cisse%2C%20Moustapha%20van%20der%20Maaten%2C%20Laurens%20Countering%20adversarial%20images%20using%20input%20transformations%202018"
        },
        {
            "id": "Hein_2017_a",
            "entry": "Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems 30, pp. 2266\u20132276. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "Ilyas_et+al_2017_a",
            "entry": "Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold defense: Adversarial training using generative models. arXiv preprint arXiv:1712.09196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09196"
        },
        {
            "id": "Ilyas_et+al_2018_a",
            "entry": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. arXiv preprint arXiv:1804.08598, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08598"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Kabilan_et+al_2018_a",
            "entry": "Vishaal Munusamy Kabilan, Brandon Morris, and Anh Nguyen. Vectordefense: Vectorization as a defense to adversarial examples. arXiv preprint arXiv:1804.08529, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08529"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "Liao_et+al_2017_a",
            "entry": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against adversarial attacks using high-level representation guided denoiser. arXiv preprint arXiv:1712.02976, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02976"
        },
        {
            "id": "Madry_et+al_2018_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.",
            "url": "https://openreview.net/forum?id=rJzIBfZAb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "Meng_2017_a",
            "entry": "Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017"
        },
        {
            "id": "Nguyen_et+al_2015_a",
            "entry": "Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015-06"
        },
        {
            "id": "Prakash_et+al_2018_a",
            "entry": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting adversarial attacks with pixel deflection. arXiv preprint arXiv:1801.08926, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08926"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Bys4ob-Rb.",
            "url": "https://openreview.net/forum?id=Bys4ob-Rb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018"
        },
        {
            "id": "Rauber_et+al_2017_a",
            "entry": "Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017. URL http://arxiv.org/abs/1707.04131.",
            "url": "http://arxiv.org/abs/1707.04131",
            "arxiv_url": "https://arxiv.org/pdf/1707.04131"
        },
        {
            "id": "Samangouei_et+al_2018_a",
            "entry": "Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkJ3ibb0-.",
            "url": "https://openreview.net/forum?id=BkJ3ibb0-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samangouei%2C%20Pouya%20Kabkab%2C%20Maya%20Chellappa%2C%20Rama%20Defense-GAN%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018"
        },
        {
            "id": "Schmidt_et+al_2018_a",
            "entry": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. CoRR, abs/1804.11285, 2018. URL http://arxiv.org/abs/1804.11285.",
            "url": "http://arxiv.org/abs/1804.11285",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "Learning_2017_a",
            "entry": "Bernhard Sch\u00f6lkopf. Causal learning, 2017. URL https://icml.cc/Conferences/2017/ Schedule?showEvent=931. Thirty-fourth International Conference on Machine Learning.",
            "url": "https://icml.cc/Conferences/2017/Schedule?showEvent=931",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bernhard%20Sch%C3%B6lkopf%20Causal%20learning%202017%20URL%20httpsicmlccConferences2017%20ScheduleshowEvent931%20Thirtyfourth%20International%20Conference%20on%20Machine%20Learning"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Shiwei Shen, Guoqing Jin, Ke Gao, and Yongdong Zhang. Ape-gan: Adversarial perturbation elimination with gan. arXiv preprint arXiv:1707.05474, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05474"
        },
        {
            "id": "Song_et+al_2018_a",
            "entry": "Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJUYGxbCW.",
            "url": "https://openreview.net/forum?id=rJUYGxbCW",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Yang%20Kim%2C%20Taesup%20Nowozin%2C%20Sebastian%20Ermon%2C%20Stefano%20Pixeldefend%3A%20Leveraging%20generative%20models%20to%20understand%20and%20defend%20against%20adversarial%20examples%202018"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tram_et+al_2017_a",
            "entry": "Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07204"
        },
        {
            "id": "Tsipras_et+al_2018_a",
            "entry": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12152"
        },
        {
            "id": "Uesato_et+al_2018_a",
            "entry": "Jonathan Uesato, Brendan O\u2019Donoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial risk and the dangers of evaluating against weak attacks. In Proceedings of the 35th International Conference on Machine Learning, 2018. URL http://proceedings.mlr.press/v80/uesato18a.html.",
            "url": "http://proceedings.mlr.press/v80/uesato18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uesato%2C%20Jonathan%20O%E2%80%99Donoghue%2C%20Brendan%20Kohli%2C%20Pushmeet%20van%20den%20Oord%2C%20Aaron%20Adversarial%20risk%20and%20the%20dangers%20of%20evaluating%20against%20weak%20attacks%202018"
        },
        {
            "id": "Xie_et+al_2018_a",
            "entry": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Sk9yuql0Z.",
            "url": "https://openreview.net/forum?id=Sk9yuql0Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Ren%2C%20Zhou%20Mitigating%20adversarial%20effects%20through%20randomization%202018"
        },
        {
            "id": "Zheng_et+al_2018_a",
            "entry": "Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. arXiv preprint arXiv:1808.05537, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05537"
        }
    ]
}
