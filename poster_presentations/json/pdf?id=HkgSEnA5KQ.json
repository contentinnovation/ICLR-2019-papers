{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GUIDING POLICIES WITH LANGUAGE VIA META-LEARNING",
        "author": "John D. Co-Reyes, Abhishek Gupta Suvansh Sanjeev Nick Altieri Jacob Andreas John DeNero Pieter Abbeel Sergey Levine University of California, Berkeley",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkgSEnA5KQ"
        },
        "abstract": "Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following."
    },
    "keywords": [
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "autonomous agent",
            "url": "https://en.wikipedia.org/wiki/autonomous_agent"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "policy learning",
            "url": "https://en.wikipedia.org/wiki/policy_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "GPL": "guided policies with language",
        "RL": "reinforcement learning"
    },
    "highlights": [
        "Behavioral skills or policies for autonomous agents are typically specified in terms of reward functions or demonstrations",
        "Reward functions must be engineered manually, which can be challenging in real-world environments, especially when the learned policies operate directly on raw sensory perception",
        "An autonomous agent might be unable to deduce how to perform a task from a single instruction, even if it is very precise",
        "By iteratively incorporating language corrections, such a model in effect implements a learning algorithm, analogously to meta-reinforcement learning recurrent models proposed in prior work that read in previous trajectories and rewards (<a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>; Wang et al, 2016a)",
        "We provide the policy with the previous corrections as well but omit in the notation for clarity. This procedure is summarized in Algorithm 2. This procedure is similar to meta-reinforcement learning (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>), but uses grounded natural language corrections in order to guide learning of new tasks with feedback provided in the loop",
        "We presented meta-learning for guided policies with language (GPL), a framework for interactive learning of tasks with in-the-loop language corrections"
    ],
    "key_statements": [
        "Behavioral skills or policies for autonomous agents are typically specified in terms of reward functions or demonstrations",
        "Reward functions must be engineered manually, which can be challenging in real-world environments, especially when the learned policies operate directly on raw sensory perception",
        "An autonomous agent might be unable to deduce how to perform a task from a single instruction, even if it is very precise",
        "By iteratively incorporating language corrections, such a model in effect implements a learning algorithm, analogously to meta-reinforcement learning recurrent models proposed in prior work that read in previous trajectories and rewards (<a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>; Wang et al, 2016a)",
        "Using a guided policies with language model meta-trained as described in the previous section, we can solve new \u201cmetatesting\u201d tasks Ttest \u223c p(T ) drawn from the same distribution of tasks with interactive language corrections",
        "We provide the policy with the previous corrections as well but omit in the notation for clarity. This procedure is summarized in Algorithm 2. This procedure is similar to meta-reinforcement learning (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>), but uses grounded natural language corrections in order to guide learning of new tasks with feedback provided in the loop",
        "We evaluate our method comparatively, in order to understand whether iterative corrections provide an improvement over standard instruction-following methods, and compare guided policies with language to an oracle model that receives a much more detailed instruction, but without the iterative structure of interactive corrections",
        "We presented meta-learning for guided policies with language (GPL), a framework for interactive learning of tasks with in-the-loop language corrections"
    ],
    "summary": [
        "Behavioral skills or policies for autonomous agents are typically specified in terms of reward functions or demonstrations.",
        "At meta-test time, this model can generalize to new tasks, and learn those tasks quickly through iterative language corrections.",
        "As described in Section 3, our model for guiding policies with language (GPL) must take in an initial language instruction, and iteratively incorporate corrections after each attempt at the task.",
        "By iteratively incorporating language corrections, such a model in effect implements a learning algorithm, analogously to meta-reinforcement learning recurrent models proposed in prior work that read in previous trajectories and rewards (<a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\"><a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a></a>; Wang et al, 2016a).",
        "We will describe a meta-learning algorithm that can train this model such that it is able to adapt to iterative corrections effectively at meta-test time.",
        "In order for the GPL model to be able to learn behaviors from corrections, it must be meta-trained to understand both instructions and corrections properly, put them in the context of its own previous trajectories, and associate them with objects and events in the world.",
        "Starting from an initialization where the previous trajectory \u03c40 and correction c0 are set to be empty sequences, we repeat the following process: first, we run the policy corresponding to the current learned model \u03c0(a|s, LT , \u03c40, c0) to generate a new trajectory \u03c41 for the task T .",
        "Using a GPL model meta-trained as described in the previous section, we can solve new \u201cmetatesting\u201d tasks Ttest \u223c p(T ) drawn from the same distribution of tasks with interactive language corrections.",
        "This procedure is similar to meta-reinforcement learning (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\"><a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a></a>), but uses grounded natural language corrections in order to guide learning of new tasks with feedback provided in the loop.",
        "Our model must learn to map corrections to changes in behavior which may be more modular, disentangled, and easier to generalize compared to mapping a long list of instructions to a single policy that can solve the task.",
        "The RL baseline is able to achieve better final performance but takes orders of magnitude more training examples and has access to the test task reward.",
        "The GPL model is trained via meta-learning, using a dataset of other tasks to learn how to ground language corrections in terms of behaviors and objects.",
        "To scale corrections to real-world tasks, it is vital to handle new concepts, in terms of actions or objects, not seen at training time."
    ],
    "headline": "We propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill",
    "reference_links": [
        {
            "id": "Abbeel_2004_a",
            "entry": "Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20Pieter%20Ng%2C%20Andrew%20Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20Pieter%20Ng%2C%20Andrew%20Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Akrour_et+al_2011_a",
            "entry": "Riad Akrour, Marc Schoenauer, and Mich\u00e8le Sebag. Preference-based policy learning. In ECML/PKDD, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akrour%2C%20Riad%20Schoenauer%2C%20Marc%20Sebag%2C%20Mich%C3%A8le%20Preference-based%20policy%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akrour%2C%20Riad%20Schoenauer%2C%20Marc%20Sebag%2C%20Mich%C3%A8le%20Preference-based%20policy%20learning%202011"
        },
        {
            "id": "Akrour_et+al_2012_a",
            "entry": "Riad Akrour, Marc Schoenauer, and Mich\u00e8le Sebag. APRIL: active preference-learning based reinforcement learning. CoRR, abs/1208.0984, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1208.0984"
        },
        {
            "id": "Andreas_2015_a",
            "entry": "Jacob Andreas and Dan Klein. Alignment-based compositional semantics for instruction following. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Alignment-based%20compositional%20semantics%20for%20instruction%20following%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Alignment-based%20compositional%20semantics%20for%20instruction%20following%202015"
        },
        {
            "id": "Andreas_et+al_2018_a",
            "entry": "Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Learning%20with%20latent%20language%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Learning%20with%20latent%20language%202018"
        },
        {
            "id": "Argall_et+al_2009_a",
            "entry": "Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robot. Auton. Syst., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Artzi_2013_a",
            "entry": "Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. In TACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013"
        },
        {
            "id": "Branavan_et+al_2009_a",
            "entry": "SRK Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In ACL, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Branavan%2C%20S.R.K.%20Chen%2C%20Harr%20Zettlemoyer%2C%20Luke%20Barzilay%2C%20Regina%20Reinforcement%20learning%20for%20mapping%20instructions%20to%20actions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Branavan%2C%20S.R.K.%20Chen%2C%20Harr%20Zettlemoyer%2C%20Luke%20Barzilay%2C%20Regina%20Reinforcement%20learning%20for%20mapping%20instructions%20to%20actions%202009"
        },
        {
            "id": "Chen_2011_a",
            "entry": "David Chen and Raymond Mooney. Learning to interpret natural language navigation instructions from observations. In AAAI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20David%20Mooney%2C%20Raymond%20Learning%20to%20interpret%20natural%20language%20navigation%20instructions%20from%20observations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20David%20Mooney%2C%20Raymond%20Learning%20to%20interpret%20natural%20language%20navigation%20instructions%20from%20observations%202011"
        },
        {
            "id": "Chevalier-Boisvert_2018_a",
            "entry": "Maxime Chevalier-Boisvert and Lucas Willems. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.",
            "url": "https://github.com/maximecb/gym-minigrid"
        },
        {
            "id": "Christiano_et+al_2017_a",
            "entry": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20B.%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20B.%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017"
        },
        {
            "id": "Clavera_et+al_2018_a",
            "entry": "Ignasi Clavera, Anusha Nagabandi, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt: Meta-learning for model-based control. CoRR, abs/1803.11347, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11347"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl$\u02c62$: Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Asri_et+al_2016_a",
            "entry": "Layla El Asri, Jing He, and Kaheer Suleman. A sequence-to-sequence model for user simulation in spoken dialogue systems. In arXiv preprint arXiv:1607.00070, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.00070"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. CoRR, abs/1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. CoRR, abs/1805.11686, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11686"
        },
        {
            "id": "Janner_et+al_2018_a",
            "entry": "Michael Janner, Karthik Narasimhan, and Regina Barzilay. Representation learning for grounded spatial reasoning. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janner%2C%20Michael%20Narasimhan%2C%20Karthik%20Barzilay%2C%20Regina%20Representation%20learning%20for%20grounded%20spatial%20reasoning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janner%2C%20Michael%20Narasimhan%2C%20Karthik%20Barzilay%2C%20Regina%20Representation%20learning%20for%20grounded%20spatial%20reasoning%202018"
        },
        {
            "id": "Kim_2013_a",
            "entry": "Joohyun Kim and Raymond Mooney. Adapting discriminative reranking to grounded language learning. In ACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Joohyun%20Mooney%2C%20Raymond%20Adapting%20discriminative%20reranking%20to%20grounded%20language%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Joohyun%20Mooney%2C%20Raymond%20Adapting%20discriminative%20reranking%20to%20grounded%20language%20learning%202013"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. CoRR, abs/1707.03374, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03374"
        },
        {
            "id": "Macmahon_et+al_2006_a",
            "entry": "Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacMahon%2C%20Matt%20Stankiewicz%2C%20Brian%20Kuipers%2C%20Benjamin%20Walk%20the%20talk%3A%20Connecting%20language%2C%20knowledge%2C%20and%20action%20in%20route%20instructions%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacMahon%2C%20Matt%20Stankiewicz%2C%20Brian%20Kuipers%2C%20Benjamin%20Walk%20the%20talk%3A%20Connecting%20language%2C%20knowledge%2C%20and%20action%20in%20route%20instructions%202006"
        },
        {
            "id": "Mishra_et+al_2017_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolutions. CoRR, abs/1707.03141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03141"
        },
        {
            "id": "Misra_et+al_2017_a",
            "entry": "Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Dipendra%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Dipendra%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017"
        },
        {
            "id": "Misra_et+al_2017_b",
            "entry": "Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Dipendra%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Dipendra%20Langford%2C%20John%20Artzi%2C%20Yoav%20Mapping%20instructions%20and%20visual%20observations%20to%20actions%20with%20reinforcement%20learning%202017"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder P. Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. CoRR, abs/1706.05064, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05064"
        },
        {
            "id": "Pilarski_et+al_2011_a",
            "entry": "Patrick Pilarski, Michael Dawson, Thomas Degris, Farbod Fahimi, Jason Carey, and Richard Sutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. In IEEE ICORR, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilarski%2C%20Patrick%20Dawson%2C%20Michael%20Degris%2C%20Thomas%20Fahimi%2C%20Farbod%20Online%20human%20training%20of%20a%20myoelectric%20prosthesis%20controller%20via%20actor-critic%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilarski%2C%20Patrick%20Dawson%2C%20Michael%20Degris%2C%20Thomas%20Fahimi%2C%20Farbod%20Online%20human%20training%20of%20a%20myoelectric%20prosthesis%20controller%20via%20actor-critic%20reinforcement%20learning%202011"
        },
        {
            "id": "Reddy_et+al_2018_a",
            "entry": "Siddharth Reddy, Sergey Levine, and Anca D. Dragan. Shared autonomy via deep reinforcement learning. CoRR, abs/1802.01744, 2018. URL http://arxiv.org/abs/1802.01744.",
            "url": "http://arxiv.org/abs/1802.01744",
            "arxiv_url": "https://arxiv.org/pdf/1802.01744"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "St\u00e9phane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20J.%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20J.%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. Meta-learning with memory-augmented neural networks. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Meta-learning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Meta-learning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Evolutionary%20principles%20in%20self-referential%20learning%2C%20or%20on%20learning%20how%20to%20learn%3A%20the%20meta-meta-%201987"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. CoRR, abs/1703.05175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.05175"
        },
        {
            "id": "Sung_et+al_2017_a",
            "entry": "Flood Sung, Li Zhang, Tao Xiang, Timothy M. Hospedales, and Yongxin Yang. Learning to learn: Meta-critic networks for sample efficient learning. CoRR, abs/1706.09529, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09529"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard Sutton and Andrew Barto. Introduction to Reinforcement Learning, volume 1. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20Barto%2C%20Andrew%20Introduction%20to%20Reinforcement%20Learning%2C%20volume%201%201998"
        },
        {
            "id": "Tellex_et+al_2011_a",
            "entry": "Stephanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Steph Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tellex%2C%20Stephanie%20Kollar%2C%20Thomas%20Dickerson%2C%20Steven%20Walter%2C%20Matthew%20Understanding%20natural%20language%20commands%20for%20robotic%20navigation%20and%20mobile%20manipulation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tellex%2C%20Stephanie%20Kollar%2C%20Thomas%20Dickerson%2C%20Steven%20Walter%2C%20Matthew%20Understanding%20natural%20language%20commands%20for%20robotic%20navigation%20and%20mobile%20manipulation%202011"
        },
        {
            "id": "Thomaz_et+al_2006_a",
            "entry": "Andrea Lockerd Thomaz, Guy Hoffman, and Cynthia Breazeal. Reinforcement learning with human teachers: Understanding how people want to teach robots. In RO-MAN, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomaz%2C%20Andrea%20Lockerd%20Hoffman%2C%20Guy%20Breazeal%2C%20Cynthia%20Reinforcement%20learning%20with%20human%20teachers%3A%20Understanding%20how%20people%20want%20to%20teach%20robots%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomaz%2C%20Andrea%20Lockerd%20Hoffman%2C%20Guy%20Breazeal%2C%20Cynthia%20Reinforcement%20learning%20with%20human%20teachers%3A%20Understanding%20how%20people%20want%20to%20teach%20robots%202006"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IROS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Kavukcuoglu%2C%20Koray%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Vogel_2010_a",
            "entry": "Adam Vogel and Dan Jurafsky. Learning to follow navigational directions. In ACL, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vogel%2C%20Adam%20Jurafsky%2C%20Dan%20Learning%20to%20follow%20navigational%20directions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vogel%2C%20Adam%20Jurafsky%2C%20Dan%20Learning%20to%20follow%20navigational%20directions%202010"
        },
        {
            "id": "Wang_et+al_0000_a",
            "entry": "Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R\u00e9mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement learn. CoRR, abs/1611.05763, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "Warnell_et+al_2017_a",
            "entry": "Published as a conference paper at ICLR 2019 Sida Wang, Percy Liang, and Chris Manning. Learning language games through interaction. In ACL, 2016b. Garrett Warnell, Nicholas R. Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER: interactive agent shaping in high-dimensional state spaces. CoRR, abs/1709.10163, 2017. Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor learning and planning. In CoRL, 2018. Kelvin Xu, Ellis Ratner, Anca D. Dragan, Sergey Levine, and Chelsea Finn. Learning a prior over intent via meta-inverse reinforcement learning. CoRR, abs/1805.12573, 2018. Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, 2008.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10163"
        }
    ]
}
