{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HIERARCHICAL INTERPRETATIONS FOR NEURAL NET-",
        "author": "WORK PREDICTIONS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkEqro0ctQ"
        },
        "abstract": "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN\u2019s outputs. We also find that ACD\u2019s hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise."
    },
    "keywords": [
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "Deep neural networks",
            "url": "https://en.wikipedia.org/wiki/Deep_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        }
    ],
    "abbreviations": {
        "DNNs": "Deep neural networks",
        "ACD": "agglomerative contextual decomposition",
        "CD": "CONTEXTUAL DECOMPOSITION",
        "CNNs": "convolutional neural networks",
        "LSTMs": "long short term memory networks",
        "SST": "Stanford Sentiment Treebank",
        "IG": "Integrated Gradients"
    },
    "highlights": [
        "Deep neural networks (DNNs) have recently demonstrated impressive predictive performance due to their ability to learn complex, non-linear, relationships between variables",
        "We demonstrate the utility of agglomerative contextual decomposition on both long short term memory networks (LSTMs) (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>) trained on the Stanford Sentiment Treebank (SST) (<a class=\"ref-link\" id=\"cSocher_et+al_2013_a\" href=\"#rSocher_et+al_2013_a\">Socher et al, 2013</a>) and convolutional neural networks trained on MNIST (<a class=\"ref-link\" id=\"cLecun_1998_a\" href=\"#rLecun_1998_a\">LeCun, 1998</a>) and ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>)",
        "We present empirical validation of agglomerative contextual decomposition on both long short term memory networks trained on Stanford Sentiment Treebank and convolutional neural networks trained on MNIST and ImageNet",
        "We introduce agglomerative contextual decomposition (ACD), a novel hierarchical interpretation algorithm",
        "The benefits of capturing the non-linearities inherent in Deep neural networks are demonstrated through human experiments and examples of diagnosing incorrect predictions and dataset bias",
        "We demonstrate that agglomerative contextual decomposition\u2019s hierarchy is robust to adversarial perturbations in convolutional neural networks, implying that it captures fundamental aspects of the input and ignores spurious noise"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) have recently demonstrated impressive predictive performance due to their ability to learn complex, non-linear, relationships between variables",
        "The inability to effectively visualize these relationships has led Deep neural networks to be characterized as black boxes",
        "We introduce the use of hierarchical interpretations to explain Deep neural networks predictions",
        "Given a prediction from a trained Deep neural networks, agglomerative contextual decomposition produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction",
        "While we focus on Deep neural networks and use CONTEXTUAL DECOMPOSITION as our importance measure, this concept is general, and could be readily applied to any model with a suitable measure for computing importances of groups of variables",
        "We demonstrate the utility of agglomerative contextual decomposition on both long short term memory networks (LSTMs) (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a>) trained on the Stanford Sentiment Treebank (SST) (<a class=\"ref-link\" id=\"cSocher_et+al_2013_a\" href=\"#rSocher_et+al_2013_a\">Socher et al, 2013</a>) and convolutional neural networks trained on MNIST (<a class=\"ref-link\" id=\"cLecun_1998_a\" href=\"#rLecun_1998_a\">LeCun, 1998</a>) and ImageNet (<a class=\"ref-link\" id=\"cRussakovsky_et+al_2015_a\" href=\"#rRussakovsky_et+al_2015_a\">Russakovsky et al, 2015</a>)",
        "We show that agglomerative contextual decomposition produces intuitive visualizations that enable users to better reason about and trust Deep neural networks",
        "Given two Deep neural networks models, we show that users can use the output of agglomerative contextual decomposition to select the model with higher predictive accuracy, and that overall they rank agglomerative contextual decomposition as more trustworthy than prior interpretation methods",
        "Our work focuses on local interpretations, where the task is to interpret individual predictions made by a Deep neural networks",
        "This section introduces agglomerative contextual decomposition through two contributions: Sec 3.1 proposes a generalization of CONTEXTUAL DECOMPOSITION from long short term memory networks to arbitrary Deep neural networks, and Sec 3.2 explains the main contribution: how to combine these CONTEXTUAL DECOMPOSITION scores with hierarchical clustering to produce agglomerative contextual decomposition.\n3.1",
        "Given the generalized CONTEXTUAL DECOMPOSITION scores introduced above, we introduce the clustering procedure used to produce agglomerative contextual decomposition interpretations",
        "We use CONTEXTUAL DECOMPOSITION scores to arrive at the agglomerative contextual decomposition algorithm, which makes the method specific to Deep neural networks, but given a feature group scoring function, Algorithm 1 can yield interpretations for any predictive model",
        "We present empirical validation of agglomerative contextual decomposition on both long short term memory networks trained on Stanford Sentiment Treebank and convolutional neural networks trained on MNIST and ImageNet",
        "We demonstrate the use of agglomerative contextual decomposition to diagnose incorrect predictions in Stanford Sentiment Treebank and identify dataset bias in ImageNet",
        "Text example - diagnosing incorrect predictions In the first example, we show the result of running agglomerative contextual decomposition for our Stanford Sentiment Treebank long short term memory networks model in Figure 2",
        "In Table 1, we use agglomerative contextual decomposition to show the top-scoring phrases of different lengths for our long short term memory networks trained on Stanford Sentiment Treebank",
        "QUANTITATIVE EXPERIMENTS Having introduced our visualization and provided qualitative evidence of its uses, we provide quantitative evidence of the benefits of agglomerative contextual decomposition. 4.3.1",
        "HUMAN EXPERIMENTS We demonstrate through human experiments that agglomerative contextual decomposition allows users to better trust and reason about the accuracy of Deep neural networks",
        "We provide evidence that, on MNIST, the hierarchical clustering produced by agglomerative contextual decomposition is largely robust to adversarial perturbations",
        "This suggests that agglomerative contextual decomposition\u2019s hierarchy captures fundamental features of an image, and is largely immune to the spurious noise favored by adversarial examples",
        "To measure the robustness of agglomerative contextual decomposition\u2019s hierarchy, we first qualitatively compare the interpretations produced by agglomerative contextual decomposition on both an unaltered image and an adversarially perturbed version of that image",
        "We introduce a metric to quantify the similarity between two agglomerative contextual decomposition hierarchies",
        "The resulting correlations are substantially lower, indicating that features detected by agglomerative contextual decomposition are more stable to adversarial attacks than comparable methods. These results provide evidence that agglomerative contextual decomposition\u2019s hierarchy captures fundamental features of an image, and is largely immune to the spurious noise favored by adversarial examples",
        "We introduce agglomerative contextual decomposition (ACD), a novel hierarchical interpretation algorithm",
        "The benefits of capturing the non-linearities inherent in Deep neural networks are demonstrated through human experiments and examples of diagnosing incorrect predictions and dataset bias",
        "We demonstrate that agglomerative contextual decomposition\u2019s hierarchy is robust to adversarial perturbations in convolutional neural networks, implying that it captures fundamental aspects of the input and ignores spurious noise"
    ],
    "summary": [
        "Deep neural networks (DNNs) have recently demonstrated impressive predictive performance due to their ability to learn complex, non-linear, relationships between variables.",
        "We show that ACD produces intuitive visualizations that enable users to better reason about and trust DNNs. In particular, given two DNN models, we show that users can use the output of ACD to select the model with higher predictive accuracy, and that overall they rank ACD as more trustworthy than prior interpretation methods.",
        "We demonstrate that ACD\u2019s hierarchy is robust to adversarial perturbations (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>) in CNNs. DNN Prediction negative",
        "In the case of LSTMs, Murdoch et al (2018) demonstrated the limitations of prior work on interpretation using word-level scores, and introduced contextual decomposition (CD), an algorithm for producing phrase-level importance scores from LSTMs. Another simple baseline is occlusion, where a group of features is set to some reference value, such as zero, and the importance of the group is defined to be the resulting decrease in the prediction value (Zeiler & Fergus, 2014; <a class=\"ref-link\" id=\"cLi_et+al_2016_a\" href=\"#rLi_et+al_2016_a\">Li et al, 2016</a>).",
        "Algorithm 1 is not specific to DNNs; it requires only a method to obtain importance scores for groups of input features.",
        "We use CD scores to arrive at the ACD algorithm, which makes the method specific to DNNs, but given a feature group scoring function, Algorithm 1 can yield interpretations for any predictive model.",
        "We present empirical validation of ACD on both LSTMs trained on SST and CNNs trained on MNIST and ImageNet. First, we introduce the reader to our visualization in Sec 4.2, and how it can be used to understand models in settings such as diagnosing incorrect predictions, identifying dataset bias, and identifying representative phrases of differing lengths.",
        "In Table 1, we use ACD to show the top-scoring phrases of different lengths for our LSTM trained on SST.",
        "To measure the robustness of ACD\u2019s hierarchy, we first qualitatively compare the interpretations produced by ACD on both an unaltered image and an adversarially perturbed version of that image.",
        "These results provide evidence that ACD\u2019s hierarchy captures fundamental features of an image, and is largely immune to the spurious noise favored by adversarial examples.",
        "ACD is the first method to use a hierarchy to interpret individual neural network predictions.",
        "Doing so enables ACD to automatically detect and display non-linear contributions to individual DNN predictions, something prior interpretation methods are unable to do.",
        "The benefits of capturing the non-linearities inherent in DNNs are demonstrated through human experiments and examples of diagnosing incorrect predictions and dataset bias.",
        "We demonstrate that ACD\u2019s hierarchy is robust to adversarial perturbations in CNNs, implying that it captures fundamental aspects of the input and ignores spurious noise"
    ],
    "headline": "We introduce the use of hierarchical interpretations to explain Deep neural networks predictions through our proposed method: agglomerative contextual decomposition",
    "reference_links": [
        {
            "id": "Ancona_et+al_2018_a",
            "entry": "Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. In 6th International Conference on Learning Representations (ICLR 2018), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ancona%2C%20Marco%20Ceolini%2C%20Enea%20Oztireli%2C%20Cengiz%20Gross%2C%20Markus%20Towards%20better%20understanding%20of%20gradient-based%20attribution%20methods%20for%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ancona%2C%20Marco%20Ceolini%2C%20Enea%20Oztireli%2C%20Cengiz%20Gross%2C%20Markus%20Towards%20better%20understanding%20of%20gradient-based%20attribution%20methods%20for%20deep%20neural%20networks%202018"
        },
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016"
        },
        {
            "id": "Angermueller_et+al_2016_a",
            "entry": "Christof Angermueller, Tanel Parnamaa, Leopold Parts, and Oliver Stegle. Deep learning for computational biology. Molecular systems biology, 12(7):878, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angermueller%2C%20Christof%20Parnamaa%2C%20Tanel%20Parts%2C%20Leopold%20Stegle%2C%20Oliver%20Deep%20learning%20for%20computational%20biology%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angermueller%2C%20Christof%20Parnamaa%2C%20Tanel%20Parts%2C%20Leopold%20Stegle%2C%20Oliver%20Deep%20learning%20for%20computational%20biology%202016"
        },
        {
            "id": "Bach_et+al_2015_a",
            "entry": "Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gregoire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gregoire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "Baehrens_et+al_2010_a",
            "entry": "David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert MAzller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803\u20131831, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20How%20to%20explain%20individual%20classification%20decisions%202010"
        },
        {
            "id": "Brendel_et+al_2017_a",
            "entry": "Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04248"
        },
        {
            "id": "Brennan_2013_a",
            "entry": "Tim Brennan and William L Oliver. The emergence of machine learning techniques in criminology. Criminology & Public Policy, 12(3):551\u2013562, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brennan%2C%20Tim%20Oliver%2C%20William%20L.%20The%20emergence%20of%20machine%20learning%20techniques%20in%20criminology%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brennan%2C%20Tim%20Oliver%2C%20William%20L.%20The%20emergence%20of%20machine%20learning%20techniques%20in%20criminology%202013"
        },
        {
            "id": "Dabkowski_2017_a",
            "entry": "Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint arXiv:1705.07857, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07857"
        },
        {
            "id": "Dwork_et+al_2012_a",
            "entry": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214\u2013226. ACM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "Fong_2017_a",
            "entry": "Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. arXiv preprint arXiv:1704.03296, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03296"
        },
        {
            "id": "Frosst_2017_a",
            "entry": "Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09784"
        },
        {
            "id": "Godin_et+al_2018_a",
            "entry": "Frederic Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, and Thomas Demeester. Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3275\u20133284, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Godin%2C%20Frederic%20Demuynck%2C%20Kris%20Dambre%2C%20Joni%20Neve%2C%20Wesley%20De%20Explaining%20character-aware%20neural%20networks%20for%20word-level%20prediction%3A%20Do%20they%20discover%20linguistic%20rules%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Godin%2C%20Frederic%20Demuynck%2C%20Kris%20Dambre%2C%20Joni%20Neve%2C%20Wesley%20De%20Explaining%20character-aware%20neural%20networks%20for%20word-level%20prediction%3A%20Do%20they%20discover%20linguistic%20rules%3F%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Goodman_2016_a",
            "entry": "Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a\u201d right to explanation\u201d. arXiv preprint arXiv:1606.08813, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.08813"
        },
        {
            "id": "Harman_1965_a",
            "entry": "Gilbert H Harman. The inference to the best explanation. The philosophical review, 74(1):88\u201395, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harman%2C%20Gilbert%20H.%20The%20inference%20to%20the%20best%20explanation%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harman%2C%20Gilbert%20H.%20The%20inference%20to%20the%20best%20explanation%201965"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Keil_2006_a",
            "entry": "Frank C Keil. Explanation and understanding. Annu. Rev. Psychol., 57:227\u2013254, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keil%2C%20Frank%20C.%20Explanation%20and%20understanding%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keil%2C%20Frank%20C.%20Explanation%20and%20understanding%202006"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08220"
        },
        {
            "id": "Litjens_et+al_2017_a",
            "entry": "Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen AWM van der Laak, Bram van Ginneken, and Clara I Sanchez. A survey on deep learning in medical image analysis. Medical image analysis, 42: 60\u201388, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Litjens%2C%20Geert%20Kooi%2C%20Thijs%20Bejnordi%2C%20Babak%20Ehteshami%20Setio%2C%20Arnaud%20Arindra%20Adiyoso%20and%20Clara%20I%20Sanchez.%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Litjens%2C%20Geert%20Kooi%2C%20Thijs%20Bejnordi%2C%20Babak%20Ehteshami%20Setio%2C%20Arnaud%20Arindra%20Adiyoso%20and%20Clara%20I%20Sanchez.%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20analysis%202017"
        },
        {
            "id": "Lundberg_2017_a",
            "entry": "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pp. 4768\u20134777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "Dezfooli_et+al_2016_a",
            "entry": "Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Murdoch_2017_a",
            "entry": "W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. arXiv preprint arXiv:1702.02540, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02540"
        },
        {
            "id": "Murdoch_et+al_2018_a",
            "entry": "W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition to extract interactions from lstms. arXiv preprint arXiv:1801.05453, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05453"
        },
        {
            "id": "Murdoch_et+al_2019_a",
            "entry": "W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Interpretable machine learning: definitions, methods, and applications. arXiv preprint arXiv:1901.04592, 2019.",
            "arxiv_url": "https://arxiv.org/pdf/1901.04592"
        },
        {
            "id": "Olah_et+al_2017_a",
            "entry": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olah%2C%20Chris%20Mordvintsev%2C%20Alexander%20Schubert%2C%20Ludwig%20Feature%20visualization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olah%2C%20Chris%20Mordvintsev%2C%20Alexander%20Schubert%2C%20Ludwig%20Feature%20visualization%202017"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "Rauber_et+al_2017_a",
            "entry": "Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0. 8.0: A python toolbox to benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04131"
        },
        {
            "id": "Read_1993_a",
            "entry": "Stephen J Read and Amy Marcus-Newhall. Explanatory coherence in social explanations: A parallel distributed processing account. Journal of Personality and Social Psychology, 65(3):429, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Read%2C%20Stephen%20J.%20Marcus-Newhall%2C%20Amy%20Explanatory%20coherence%20in%20social%20explanations%3A%20A%20parallel%20distributed%20processing%20account%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Read%2C%20Stephen%20J.%20Marcus-Newhall%2C%20Amy%20Explanatory%20coherence%20in%20social%20explanations%3A%20A%20parallel%20distributed%20processing%20account%201993"
        },
        {
            "id": "Ribeiro_et+al_2016_a",
            "entry": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135\u20131144. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Selvaraju_et+al_2016_a",
            "entry": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. See https://arxiv.org/abs/1610.02391 v3, 7(8), 2016.",
            "url": "https://arxiv.org/abs/1610.02391",
            "arxiv_url": "https://arxiv.org/pdf/1610.02391"
        },
        {
            "id": "Shrikumar_et+al_2016_a",
            "entry": "Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.01713"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "Springenberg_et+al_2014_a",
            "entry": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6806"
        },
        {
            "id": "Sundararajan_et+al_2017_a",
            "entry": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Axiomatic%20attribution%20for%20deep%20networks%202017"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tsang_et+al_2017_a",
            "entry": "Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network weights. arXiv preprint arXiv:1705.04977, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04977"
        },
        {
            "id": "Yosinski_et+al_2015_a",
            "entry": "Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.06579"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnn knowledge via an explanatory graph. arXiv preprint arXiv:1708.01785, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.01785"
        },
        {
            "id": "Zintgraf_et+al_2017_a",
            "entry": "Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04595"
        }
    ]
}
