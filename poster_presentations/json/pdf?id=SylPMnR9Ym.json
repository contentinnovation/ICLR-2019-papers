{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING WHAT YOU CAN DO",
        "author": "BEFORE DOING ANYTHING",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SylPMnR9Ym"
        },
        "abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent\u2019s action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene\u2019s dynamics while being minimally sensitive to the scene\u2019s static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.1"
    },
    "keywords": [
        {
            "term": "Principal Component Analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_Component_Analysis"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        }
    ],
    "abbreviations": {
        "CLASP": "Composable Learned Action Space Predictor",
        "RL": "Reinforcement learning",
        "IB": "Information Bottleneck",
        "CNNs": "convolutional neural networks",
        "BAIR": "Berkeley AI Research",
        "PCA": "Principal Component Analysis"
    },
    "highlights": [
        "Agents behaving in real-world environments rely on perception to judge what actions they can take and what effect these actions will have",
        "Infants need to relate the set of motor primitives they can generate to the action spaces exploited by adults (<a class=\"ref-link\" id=\"cDominici_et+al_2011_a\" href=\"#rDominici_et+al_2011_a\">Dominici et al (2011</a>)), and a representation acquired by observation may allow an infant to more efficiently learn to produce natural, goal-directed walking behavior",
        "We show that after Composable Learned Action Space Predictor is trained, it can be used for both action-conditioned video prediction and planning, and provide a procedure to plan in the learned representation",
        "Our results, combined with the data efficiency result, suggest that our method is robust to visual changes and can be used for passive learning from videos that are obtained under different visual conditions, or even videos of different agents, such as videos obtained from the Internet, as long as the action space of the observed agents coincides with the target agent",
        "We have shown a way of learning the structure of an agent\u2019s action space from visual observations alone by imposing the properties of minimality and composability on a latent variable for stochastic video prediction",
        "The resulting representation can be used for a range of tasks, such as action-conditioned video prediction and planning in the learned latent action space"
    ],
    "key_statements": [
        "Agents behaving in real-world environments rely on perception to judge what actions they can take and what effect these actions will have",
        "We focus on the problem of learning an agent\u2019s action space from unlabeled visual observations",
        "Infants need to relate the set of motor primitives they can generate to the action spaces exploited by adults (<a class=\"ref-link\" id=\"cDominici_et+al_2011_a\" href=\"#rDominici_et+al_2011_a\">Dominici et al (2011</a>)), and a representation acquired by observation may allow an infant to more efficiently learn to produce natural, goal-directed walking behavior",
        "We evaluate the proposal that learning what you can do before doing anything can lead to action space representations that make subsequent learning more efficient",
        "In contrast to most approaches to unsupervised learning of dynamics, which focus on learning the statistical structure of the environment, we focus on disentangling action information from the instantaneous state of the environment (Fig. 1)",
        "We show that our method learns a representation of actions that is independent of scene content and visual characteristics on (i) a simulated robot with one degree of freedom and the Berkeley AI Research robot pushing dataset (<a class=\"ref-link\" id=\"cEbert_et+al_2017_a\" href=\"#rEbert_et+al_2017_a\">Ebert et al (2017</a>))",
        "We demonstrate that the learned representation can be used for action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action-labeled videos than extant supervised methods.\n2 RELATED WORK",
        "In Sec. 3.1, we describe a variational video prediction model similar to that of <a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\">Denton & Fergus (2018</a>) that provides us with a framework for learning a latent representation zt at time t of the change between the past and the current frames",
        "We describe how the learned bijective mapping can be used for tasks such as action-conditioned video prediction (Sec. 4.2) and planning in the learned action space (Sec. 4.3).\n3.1",
        "To optimize the log-likelihood of this generative model, we introduce an additional network approximating the posterior of the latent variable zt \u223c q\u03c6 = N, \u03c3\u03c6)",
        "We show that after Composable Learned Action Space Predictor is trained, it can be used for both action-conditioned video prediction and planning, and provide a procedure to plan in the learned representation",
        "While the baseline without composability training fails to learn a representation disentangled from the static content, our method correctly recovers the structure of possible actions of the robot.\n4.2",
        "Our results, combined with the data efficiency result, suggest that our method is robust to visual changes and can be used for passive learning from videos that are obtained under different visual conditions, or even videos of different agents, such as videos obtained from the Internet, as long as the action space of the observed agents coincides with the target agent",
        "We have shown a way of learning the structure of an agent\u2019s action space from visual observations alone by imposing the properties of minimality and composability on a latent variable for stochastic video prediction",
        "The resulting representation can be used for a range of tasks, such as action-conditioned video prediction and planning in the learned latent action space"
    ],
    "summary": [
        "Agents behaving in real-world environments rely on perception to judge what actions they can take and what effect these actions will have.",
        "We develop a model that learns to represent an agent\u2019s action space given only unlabeled videos of the agent.",
        "Given a small number of action-labeled sequences we can execute the plan by learning a simple mapping from latent action representations to the agent\u2019s controls.",
        "We base our work on recent stochastic video prediction methods (<a class=\"ref-link\" id=\"cBabaeizadeh_et+al_2018_a\" href=\"#rBabaeizadeh_et+al_2018_a\">Babaeizadeh et al (2018</a>); <a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\"><a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\">Denton & Fergus (2018</a></a>); <a class=\"ref-link\" id=\"cLee_et+al_2018_a\" href=\"#rLee_et+al_2018_a\">Lee et al (2018</a>)) and impose two properties on the latent representation.",
        "We introduce a method for unsupervised learning of an agent\u2019s action space by training the latent representation of a stochastic video prediction model for the desiderata of minimality and composability.",
        "In Sec. 3.1, we describe a variational video prediction model similar to that of <a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\"><a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\">Denton & Fergus (2018</a></a>) that provides us with a framework for learning a latent representation zt at time t of the change between the past and the current frames.",
        "We learn a simple bijective mapping from a small number of action-annotated frame sequences from the training data.",
        "We show that after CLASP is trained, it can be used for both action-conditioned video prediction and planning, and provide a procedure to plan in the learned representation.",
        "While the baseline without composability training fails to learn a representation disentangled from the static content, our method correctly recovers the structure of possible actions of the robot.",
        "Our model only needs a small number of action-labeled training sequences to achieve good performance, as it learns the structure of actions from passive observations.",
        "This is evidence that the learned representation captures the dynamics of the environment and is not sensitive to changes in visual characteristics that do not affect the agent\u2019s action space.",
        "Our results, combined with the data efficiency result, suggest that our method is robust to visual changes and can be used for passive learning from videos that are obtained under different visual conditions, or even videos of different agents, such as videos obtained from the Internet, as long as the action space of the observed agents coincides with the target agent.",
        "We have shown a way of learning the structure of an agent\u2019s action space from visual observations alone by imposing the properties of minimality and composability on a latent variable for stochastic video prediction.",
        "The resulting representation can be used for a range of tasks, such as action-conditioned video prediction and planning in the learned latent action space.",
        "It captures meaningful structure in synthetic settings and achieves promising results in realistic visual settings"
    ],
    "headline": "We address the problem of learning an agent\u2019s action space purely from visual observation",
    "reference_links": [
        {
            "id": "Adolph_et+al_2012_a",
            "entry": "Karen Adolph, Whitney Cole, Meghana Komati, Jessie Garciaguirre, Daryaneh Badaly, Jesse Lingeman, Gladys Chan, and Rachel Sotsky. How do you learn to walk? thousands of steps and dozens of falls per day. Psychological Science, 23(11), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adolph%2C%20Karen%20Cole%2C%20Whitney%20Komati%2C%20Meghana%20Garciaguirre%2C%20Jessie%20How%20do%20you%20learn%20to%20walk%3F%20thousands%20of%20steps%20and%20dozens%20of%20falls%20per%20day%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adolph%2C%20Karen%20Cole%2C%20Whitney%20Komati%2C%20Meghana%20Garciaguirre%2C%20Jessie%20How%20do%20you%20learn%20to%20walk%3F%20thousands%20of%20steps%20and%20dozens%20of%20falls%20per%20day%202012"
        },
        {
            "id": "Agrawal_et+al_2016_a",
            "entry": "Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Proceedings of Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20V.%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20V.%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016"
        },
        {
            "id": "Alemi_et+al_2018_a",
            "entry": "Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a broken ELBO. In Proceedings of International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20Alexander%20Poole%2C%20Ben%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20Fixing%20a%20broken%20ELBO%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20Alexander%20Poole%2C%20Ben%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20Fixing%20a%20broken%20ELBO%202018"
        },
        {
            "id": "Alemi_et+al_2017_a",
            "entry": "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In Proceedings of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017"
        },
        {
            "id": "Babaeizadeh_et+al_2018_a",
            "entry": "Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational video prediction. In Proceedings of International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Babaeizadeh%2C%20Mohammad%20Finn%2C%20Chelsea%20Erhan%2C%20Dumitru%20Campbell%2C%20Roy%20H.%20Stochastic%20variational%20video%20prediction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Babaeizadeh%2C%20Mohammad%20Finn%2C%20Chelsea%20Erhan%2C%20Dumitru%20Campbell%2C%20Roy%20H.%20Stochastic%20variational%20video%20prediction%202018"
        },
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Burgess_et+al_2018_a",
            "entry": "Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in \u03b2-VAE. arXiv:1804.03599, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03599"
        },
        {
            "id": "Chiappa_et+al_2017_a",
            "entry": "Silvia Chiappa, S\u00e9bastien Racani\u00e8re, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In Proceedings of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiappa%2C%20Silvia%20Racani%C3%A8re%2C%20S%C3%A9bastien%20Wierstra%2C%20Daan%20Mohamed%2C%20Shakir%20Recurrent%20environment%20simulators%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiappa%2C%20Silvia%20Racani%C3%A8re%2C%20S%C3%A9bastien%20Wierstra%2C%20Daan%20Mohamed%2C%20Shakir%20Recurrent%20environment%20simulators%202017"
        },
        {
            "id": "Chung_et+al_2015_a",
            "entry": "Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Proceedings of Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015"
        },
        {
            "id": "Ignasi_2017_a",
            "entry": "Ignasi Clavera and P Abbeel. Policy transfer via modularity. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ignasi%20Clavera%20and%20P%20Abbeel%20Policy%20transfer%20via%20modularity%20In%20Proceedings%20of%20IEEERSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ignasi%20Clavera%20and%20P%20Abbeel%20Policy%20transfer%20via%20modularity%20In%20Proceedings%20of%20IEEERSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%202017"
        },
        {
            "id": "Denton_2017_a",
            "entry": "Emily Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from video. In Proceedings of Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20Birodkar%2C%20Vighnesh%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20Birodkar%2C%20Vighnesh%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "Denton_2018_a",
            "entry": "Emily Denton and Rob Fergus. Stochastic Video Generation with a Learned Prior. In Proceedings of International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20Fergus%2C%20Rob%20Stochastic%20Video%20Generation%20with%20a%20Learned%20Prior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20Fergus%2C%20Rob%20Stochastic%20Video%20Generation%20with%20a%20Learned%20Prior%202018"
        },
        {
            "id": "Devin_et+al_2017_a",
            "entry": "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In Proceedings of IEEE International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devin%2C%20Coline%20Gupta%2C%20Abhishek%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devin%2C%20Coline%20Gupta%2C%20Abhishek%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017"
        },
        {
            "id": "Dominici_et+al_2011_a",
            "entry": "Nadia Dominici, Yuri Ivanenko, Germana Cappellini, Andrea d\u2019Avella, Vito Mond\u00ec, Marika Cicchese, Adele Fabiano, Tiziana Silei, Ambrogio Di Paolo, Carlo Giannini, Richard Poppele, and Francesco Lacquaniti. Locomotor primitives in newborn babies and their development. Science, 334(6058), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dominici%2C%20Nadia%20Ivanenko%2C%20Yuri%20Cappellini%2C%20Germana%20d%E2%80%99Avella%2C%20Andrea%20Locomotor%20primitives%20in%20newborn%20babies%20and%20their%20development%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dominici%2C%20Nadia%20Ivanenko%2C%20Yuri%20Cappellini%2C%20Germana%20d%E2%80%99Avella%2C%20Andrea%20Locomotor%20primitives%20in%20newborn%20babies%20and%20their%20development%202011"
        },
        {
            "id": "Ebert_et+al_2017_a",
            "entry": "Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In Conference on Robotic Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebert%2C%20Frederik%20Finn%2C%20Chelsea%20Lee%2C%20Alex%20X.%20Levine%2C%20Sergey%20Self-supervised%20visual%20planning%20with%20temporal%20skip%20connections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ebert%2C%20Frederik%20Finn%2C%20Chelsea%20Lee%2C%20Alex%20X.%20Levine%2C%20Sergey%20Self-supervised%20visual%20planning%20with%20temporal%20skip%20connections%202017"
        },
        {
            "id": "Finn_2017_a",
            "entry": "Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Proceedings of IEEE International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Deep%20visual%20foresight%20for%20planning%20robot%20motion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Deep%20visual%20foresight%20for%20planning%20robot%20motion%202017"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Proceedings of Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Goodfellow%2C%20Ian%20Levine%2C%20Sergey%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Goodfellow%2C%20Ian%20Levine%2C%20Sergey%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016"
        },
        {
            "id": "Finn_et+al_2017_b",
            "entry": "Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. In Conference on Robotic Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017"
        },
        {
            "id": "Goroshin_et+al_2015_a",
            "entry": "Ross Goroshin, Michael F Mathieu, and Yann LeCun. Learning to linearize under uncertainty. In Proceedings of Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goroshin%2C%20Ross%20Mathieu%2C%20Michael%20F.%20LeCun%2C%20Yann%20Learning%20to%20linearize%20under%20uncertainty%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goroshin%2C%20Ross%20Mathieu%2C%20Michael%20F.%20LeCun%2C%20Yann%20Learning%20to%20linearize%20under%20uncertainty%202015"
        },
        {
            "id": "Ha_2018_a",
            "entry": "David Ha and Jurgen Schmidhuber. World models. arXiv:1803.10122, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10122"
        },
        {
            "id": "Henaff_et+al_2017_a",
            "entry": "Mikael Henaff, Junbo Zhao, and Yann LeCun. Prediction under uncertainty with error-encoding networks. arXiv:1711.04994, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04994"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In Proceedings of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20beta-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8), 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jaegle_et+al_2018_a",
            "entry": "Andrew Jaegle, Stephen Phillips, Daphne Ippolito, and Kostas Daniilidis. Understanding image motion with group representations. In Proceedings of International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaegle%2C%20Andrew%20Phillips%2C%20Stephen%20Ippolito%2C%20Daphne%20Daniilidis%2C%20Kostas%20Understanding%20image%20motion%20with%20group%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaegle%2C%20Andrew%20Phillips%2C%20Stephen%20Ippolito%2C%20Daphne%20Daniilidis%2C%20Kostas%20Understanding%20image%20motion%20with%20group%20representations%202018"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proceedings of European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016"
        },
        {
            "id": "Kandel_et+al_2012_a",
            "entry": "Eric R. Kandel, James H. Schwartz, Thomas M. Jessell, Steven A. Siegelbaum, and A. J. Hudspeth (eds.). Principles of Neural Science. McGraw-Hill Education, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eric%20R%20Kandel%20James%20H%20Schwartz%20Thomas%20M%20Jessell%20Steven%20A%20Siegelbaum%20and%20A%20J%20Hudspeth%20eds%20Principles%20of%20Neural%20Science%20McGrawHill%20Education%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eric%20R%20Kandel%20James%20H%20Schwartz%20Thomas%20M%20Jessell%20Steven%20A%20Siegelbaum%20and%20A%20J%20Hudspeth%20eds%20Principles%20of%20Neural%20Science%20McGrawHill%20Education%202012"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Klimov_2018_a",
            "entry": "Oleg Klimov and John Schulman. Roboschool: Open-source software for robot simulation, 2018. URL https://blog.openai.com/roboschool.",
            "url": "https://blog.openai.com/roboschool"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. arXiv:1804.01523, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01523"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Marblestone_et+al_2016_a",
            "entry": "Adam H Marblestone, Greg Wayne, and Konrad P Kording. Toward an integration of deep learning and neuroscience. Frontiers in computational neuroscience, 10, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marblestone%2C%20Adam%20H.%20Wayne%2C%20Greg%20Kording%2C%20Konrad%20P.%20Toward%20an%20integration%20of%20deep%20learning%20and%20neuroscience%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marblestone%2C%20Adam%20H.%20Wayne%2C%20Greg%20Kording%2C%20Konrad%20P.%20Toward%20an%20integration%20of%20deep%20learning%20and%20neuroscience%202016"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518, 02 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mueller_et+al_2018_a",
            "entry": "Matthias M\u00fcller, Alexey Dosovitskiy, Bernard Ghanem, and Vladen Koltun. Driving policy transfer via modularity and abstraction. Conference on Robotic Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20Matthias%20Dosovitskiy%2C%20Alexey%20Ghanem%2C%20Bernard%20Koltun%2C%20Vladen%20Driving%20policy%20transfer%20via%20modularity%20and%20abstraction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%BCller%2C%20Matthias%20Dosovitskiy%2C%20Alexey%20Ghanem%2C%20Bernard%20Koltun%2C%20Vladen%20Driving%20policy%20transfer%20via%20modularity%20and%20abstraction%202018"
        },
        {
            "id": "Niekum_et+al_2015_a",
            "entry": "Scott Niekum, Sarah Osentoski, George Konidaris, Sachin Chitta, Bhaskara Marthi, and Andrew G Barto. Learning grounded finite-state representations from unstructured demonstrations. International Journal of Robotics Research, 34(2), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niekum%2C%20Scott%20Osentoski%2C%20Sarah%20Konidaris%2C%20George%20Chitta%2C%20Sachin%20Learning%20grounded%20finite-state%20representations%20from%20unstructured%20demonstrations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niekum%2C%20Scott%20Osentoski%2C%20Sarah%20Konidaris%2C%20George%20Chitta%2C%20Sachin%20Learning%20grounded%20finite-state%20representations%20from%20unstructured%20demonstrations%202015"
        },
        {
            "id": "Oh_et+al_2015_a",
            "entry": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Proceedings of Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In Proceedings of International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Alexei%20A%20Efros%2C%20and%20Trevor%20Darrell.%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Alexei%20A%20Efros%2C%20and%20Trevor%20Darrell.%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Rizzolatti_et+al_1996_a",
            "entry": "Giacomo Rizzolatti, Luciano Fadiga, Vittorio Gallese, and Leonardo Fogassi. Premotor cortex and the recognition of motor actions. Cognitive Brain Research, 3(2), 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rizzolatti%2C%20Giacomo%20Fadiga%2C%20Luciano%20Gallese%2C%20Vittorio%20Fogassi%2C%20Leonardo%20Premotor%20cortex%20and%20the%20recognition%20of%20motor%20actions%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rizzolatti%2C%20Giacomo%20Fadiga%2C%20Luciano%20Gallese%2C%20Vittorio%20Fogassi%2C%20Leonardo%20Premotor%20cortex%20and%20the%20recognition%20of%20motor%20actions%201996"
        },
        {
            "id": "Romo_et+al_2004_a",
            "entry": "Ranulfo Romo, Adri\u00e1n Hern\u00e1ndez, and Antonio Zainos. Neuronal correlates of a perceptual decision in ventral premotor cortex. Neuron, 41(1), 2018/05/18 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romo%2C%20Ranulfo%20Hern%C3%A1ndez%2C%20Adri%C3%A1n%20Zainos%2C%20Antonio%20Neuronal%20correlates%20of%20a%20perceptual%20decision%20in%20ventral%20premotor%20cortex%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romo%2C%20Ranulfo%20Hern%C3%A1ndez%2C%20Adri%C3%A1n%20Zainos%2C%20Antonio%20Neuronal%20correlates%20of%20a%20perceptual%20decision%20in%20ventral%20premotor%20cortex%202004"
        },
        {
            "id": "Rubinstein_2004_a",
            "entry": "Reuven Y. Rubinstein and Dirk P. Kroese. The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning. Springer-Verlag New York, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Reuven%20Y.%20Kroese%2C%20Dirk%20P.%20The%20Cross-Entropy%20Method%3A%20A%20Unified%20Approach%20to%20Combinatorial%20Optimization%2C%20Monte-Carlo%20Simulation%20and%20Machine%20Learning%202004"
        },
        {
            "id": "Schaal_2006_a",
            "entry": "Stefan Schaal. Dynamic Movement Primitives - A Framework for Motor Control in Humans and Humanoid Robotics. Springer Tokyo, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaal%2C%20Stefan%20Dynamic%20Movement%20Primitives%20-%20A%20Framework%20for%20Motor%20Control%20in%20Humans%20and%20Humanoid%20Robotics%202006"
        },
        {
            "id": "Schaal_et+al_2005_a",
            "entry": "Stefan Schaal, Jan Peters, Jun Nakanishi, and Auke Ijspeert. Learning movement primitives. In Paolo Dario and Raja Chatila (eds.), Robotics Research. Springer Berlin Heidelberg, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaal%2C%20Stefan%20Peters%2C%20Jan%20Nakanishi%2C%20Jun%20Ijspeert%2C%20Auke%20Learning%20movement%20primitives%202005"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Srivastava_et+al_2015_a",
            "entry": "Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using LSTMs. In Proceedings of International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Mansimov%2C%20Elman%20Salakhudinov%2C%20Ruslan%20Unsupervised%20learning%20of%20video%20representations%20using%20LSTMs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Mansimov%2C%20Elman%20Salakhudinov%2C%20Ruslan%20Unsupervised%20learning%20of%20video%20representations%20using%20LSTMs%202015"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "Thomas_et+al_2017_a",
            "entry": "Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features. arXiv:1708.01289, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.01289"
        },
        {
            "id": "Tishby_et+al_1999_a",
            "entry": "N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. Allerton Conference on Communication, Control, and Computing, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Pereira%2C%20F.C.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Pereira%2C%20F.C.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999"
        },
        {
            "id": "Tulyakov_et+al_2018_a",
            "entry": "Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion and content for video generation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tulyakov%2C%20Sergey%20Liu%2C%20Ming-Yu%20Yang%2C%20Xiaodong%20Kautz%2C%20Jan%20MoCoGAN%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tulyakov%2C%20Sergey%20Liu%2C%20Ming-Yu%20Yang%2C%20Xiaodong%20Kautz%2C%20Jan%20MoCoGAN%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018"
        },
        {
            "id": "Ullman_et+al_2012_a",
            "entry": "Shimon Ullman, Daniel Harari, and Nimrod Dorfman. From simple innate biases to complex visual concepts. Proceedings of the National Academy of Sciences, 109(44), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullman%2C%20Shimon%20Harari%2C%20Daniel%20Dorfman%2C%20Nimrod%20From%20simple%20innate%20biases%20to%20complex%20visual%20concepts%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullman%2C%20Shimon%20Harari%2C%20Daniel%20Dorfman%2C%20Nimrod%20From%20simple%20innate%20biases%20to%20complex%20visual%20concepts%202012"
        },
        {
            "id": "Villegas_et+al_2017_a",
            "entry": "Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and content for natural video sequence prediction. In Proceedings of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017"
        },
        {
            "id": "Vondrick_et+al_2016_a",
            "entry": "Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Anticipating%20visual%20representations%20from%20unlabeled%20video%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Anticipating%20visual%20representations%20from%20unlabeled%20video%202016"
        },
        {
            "id": "Wayne_et+al_2018_a",
            "entry": "Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka GrabskaBarwinska, Jack W. Rae, Piotr Mirowski, Joel Z. Leibo, Adam Santoro, Mevlana Gemici, Malcolm Reynolds, Tim Harley, Josh Abramson, Shakir Mohamed, Danilo Jimenez Rezende, David Saxton, Adam Cain, Chloe Hillier, David Silver, Koray Kavukcuoglu, Matthew Botvinick, Demis Hassabis, and Timothy P. Lillicrap. Unsupervised predictive memory in a goal-directed agent. arXiv:1803.10760, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10760"
        },
        {
            "id": "Weber_et+al_2017_a",
            "entry": "Published as a conference paper at ICLR 2019 Theophane Weber, S\u00e9bastien Racani\u00e8re, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. In Proceedings of Neural Information Processing Systems, 2017. Shi Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Proceedings of Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weber%2C%20S%C3%A9bastien%20Racani%C3%A8re%20Reichert%2C%20David%20P.%20Buesing%2C%20Lars%20Guez%2C%20Arthur%20Published%20as%20a%20conference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weber%2C%20S%C3%A9bastien%20Racani%C3%A8re%20Reichert%2C%20David%20P.%20Buesing%2C%20Lars%20Guez%2C%20Arthur%20Published%20as%20a%20conference%202017"
        },
        {
            "id": "We_2018_a",
            "entry": "We use an architecture similar to SVG-FP of Denton & Fergus (2018). Input images xt are encoded using a convolutional neural network CNNe(\u00b7) to produce a low-dimensional representation CNNe(xt); output image encodings can be decoded with a neural network with transposed convolutions CNNd(\u00b7). We use a Long Short-Term Memory network LSTM(\u00b7, \u00b7) for the generative network \u03bc\u03b8(xt\u22121, zt) = CNNd(LSTM(CNNe(xt\u22121), zt)), and a multilayer perceptron MLPinfer for the approximate inference network [\u03bc\u03c6(xt, xt\u22121), \u03c3\u03c6(xt, xt\u22121)] = MLPinfer(CNNe(xt), CNNe(xt\u22121)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20use%20an%20architecture%20similar%20to%20SVGFP%20of%20Denton%20%20Fergus%202018%20Input%20images%20xt%20are%20encoded%20using%20a%20convolutional%20neural%20network%20CNNe%20to%20produce%20a%20lowdimensional%20representation%20CNNext%20output%20image%20encodings%20can%20be%20decoded%20with%20a%20neural%20network%20with%20transposed%20convolutions%20CNNd%20We%20use%20a%20Long%20ShortTerm%20Memory%20network%20LSTM%20%20for%20the%20generative%20network%20%CE%BC%CE%B8xt1%20zt%20%20CNNdLSTMCNNext1%20zt%20and%20a%20multilayer%20perceptron%20MLPinfer%20for%20the%20approximate%20inference%20network%20%CE%BC%CF%86xt%20xt1%20%CF%83%CF%86xt%20xt1%20%20MLPinferCNNext%20CNNext1"
        },
        {
            "id": "Unlike_2018_b",
            "entry": "Unlike in Denton & Fergus (2018), the generating network p\u03b8 does not observe ground truth frames xK+1:T \u22121 in the future during training but autoregressively takes its own predicted frames xK+1:T \u22121 as inputs. This allows the network LSTM to generalize to observing the generated frame encodings LSTM(CNNe(xt\u22121), zt) at test time when no ground truth future frames are available. We use a recurrence relation of the form LSTM(LSTM(xt\u22122, zt\u22121), zt). To overcome the generalization problem, Denton & Fergus (2018) instead re-encode the produced frames with a recurrence relation of the form LSTM(CNNe(CNNd(LSTM(xt\u22122, zt\u22121))), zt). Our approach omits the re-encoding, which saves a considerable amount of computation.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Unlike%20in%20Denton%20%20Fergus%202018%20the%20generating%20network%20p%CE%B8%20does%20not%20observe%20ground%20truth%20frames%20xK1T%201%20in%20the%20future%20during%20training%20but%20autoregressively%20takes%20its%20own%20predicted%20frames%20xK1T%201%20as%20inputs%20This%20allows%20the%20network%20LSTM%20to%20generalize%20to%20observing%20the%20generated%20frame%20encodings%20LSTMCNNext1%20zt%20at%20test%20time%20when%20no%20ground%20truth%20future%20frames%20are%20available%20We%20use%20a%20recurrence%20relation%20of%20the%20form%20LSTMLSTMxt2%20zt1%20zt%20To%20overcome%20the%20generalization%20problem%20Denton%20%20Fergus%202018%20instead%20reencode%20the%20produced%20frames%20with%20a%20recurrence%20relation%20of%20the%20form%20LSTMCNNeCNNdLSTMxt2%20zt1%20zt%20Our%20approach%20omits%20the%20reencoding%20which%20saves%20a%20considerable%20amount%20of%20computation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Unlike%20in%20Denton%20%20Fergus%202018%20the%20generating%20network%20p%CE%B8%20does%20not%20observe%20ground%20truth%20frames%20xK1T%201%20in%20the%20future%20during%20training%20but%20autoregressively%20takes%20its%20own%20predicted%20frames%20xK1T%201%20as%20inputs%20This%20allows%20the%20network%20LSTM%20to%20generalize%20to%20observing%20the%20generated%20frame%20encodings%20LSTMCNNext1%20zt%20at%20test%20time%20when%20no%20ground%20truth%20future%20frames%20are%20available%20We%20use%20a%20recurrence%20relation%20of%20the%20form%20LSTMLSTMxt2%20zt1%20zt%20To%20overcome%20the%20generalization%20problem%20Denton%20%20Fergus%202018%20instead%20reencode%20the%20produced%20frames%20with%20a%20recurrence%20relation%20of%20the%20form%20LSTMCNNeCNNdLSTMxt2%20zt1%20zt%20Our%20approach%20omits%20the%20reencoding%20which%20saves%20a%20considerable%20amount%20of%20computation"
        },
        {
            "id": "We_2015_a",
            "entry": "We use Algorithm 1 for visual servoing. At each time step, we initially sample M latent sequences z0 from the prior N (0, I) and use the video prediction model to retrieve M corresponding image sequences \u03c4, each with K frames. We define the cost of an image trajectory as the cosine distance between the VGG16 (Simonyan & Zisserman (2015)) feature representations of the target image and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20use%20Algorithm%201%20for%20visual%20servoing%20At%20each%20time%20step%20we%20initially%20sample%20M%20latent%20sequences%20z0%20from%20the%20prior%20N%200%20I%20and%20use%20the%20video%20prediction%20model%20to%20retrieve%20M%20corresponding%20image%20sequences%20%CF%84%20each%20with%20K%20frames%20We%20define%20the%20cost%20of%20an%20image%20trajectory%20as%20the%20cosine%20distance%20between%20the%20VGG16%20Simonyan%20%20Zisserman%202015%20feature%20representations%20of%20the%20target%20image%20and",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20use%20Algorithm%201%20for%20visual%20servoing%20At%20each%20time%20step%20we%20initially%20sample%20M%20latent%20sequences%20z0%20from%20the%20prior%20N%200%20I%20and%20use%20the%20video%20prediction%20model%20to%20retrieve%20M%20corresponding%20image%20sequences%20%CF%84%20each%20with%20K%20frames%20We%20define%20the%20cost%20of%20an%20image%20trajectory%20as%20the%20cosine%20distance%20between%20the%20VGG16%20Simonyan%20%20Zisserman%202015%20feature%20representations%20of%20the%20target%20image%20and"
        }
    ]
}
