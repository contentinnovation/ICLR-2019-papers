{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?",
        "author": "Dzmitry Bahdanau, Mila, Universitede Montreal AdeptMind Scholar Element AI",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkezXnA9YX"
        },
        "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "visual reasoning",
            "url": "https://en.wikipedia.org/wiki/visual_reasoning"
        },
        {
            "term": "language understanding",
            "url": "https://en.wikipedia.org/wiki/language_understanding"
        },
        {
            "term": "Connectionism",
            "url": "https://en.wikipedia.org/wiki/Connectionism"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reading comprehension",
            "url": "https://en.wikipedia.org/wiki/reading_comprehension"
        }
    ],
    "abbreviations": {
        "VQA": "visual question answering",
        "SQOOP": "Spatial Queries On Object Pairs",
        "RelNet": "Relation Network",
        "NMN": "Neural Module Networks"
    },
    "highlights": [
        "Neural network based models have become the workhorse of natural language understanding and generation",
        "We focus on the task of visual question answering (VQA), in particular its simplest binary form, when the answer is either \u201cyes\u201d or \u201cno\u201d",
        "We find that Neural Module Networks-Chain has little difficulty learning to answer Spatial Queries On Object Pairs questions when it sees all of them at training time, even though it previously shows poor generalization when testing on unseen examples",
        "We have conducted a rigorous investigation of an important form of systematic generalization required for grounded language understanding: the ability to reason about all possible pairs of objects despite being trained on a small subset of such pairs",
        "The intuitive appeal of modularity and structure in designing neural architectures for language understanding is supported by our results, which show how a modular model consisting of general purpose residual blocks generalizes much better than a number of baselines, including architectures such as MAC, FiLM and Relation Network that were designed for visual reasoning",
        "We have shown how sensitive the high performance of the modular models is to the layout of modules, and how a tree-like structure generalizes much stronger than a typical chain of layers"
    ],
    "key_statements": [
        "Neural network based models have become the workhorse of natural language understanding and generation",
        "A growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world",
        "We focus on the task of visual question answering (VQA), in particular its simplest binary form, when the answer is either \u201cyes\u201d or \u201cno\u201d",
        "We find that Neural Module Networks-Chain has little difficulty learning to answer Spatial Queries On Object Pairs questions when it sees all of them at training time, even though it previously shows poor generalization when testing on unseen examples",
        "Besides the theoretical appeal of systematicity, our study is inspired by highly related prior evidence that when trained on downstream language understanding tasks, neural networks often generalize poorly and latch on to dataset-specific regularities",
        "<a class=\"ref-link\" id=\"cAndreas_et+al_2016_a\" href=\"#rAndreas_et+al_2016_a\">Andreas et al (2016</a>) introduced Neural Module Networks as a modular, structured visual question answering model where a fixed number of handcrafted neural modules are chosen and composed together in a layout determined by the dependency parse of the question",
        "We have conducted a rigorous investigation of an important form of systematic generalization required for grounded language understanding: the ability to reason about all possible pairs of objects despite being trained on a small subset of such pairs",
        "The intuitive appeal of modularity and structure in designing neural architectures for language understanding is supported by our results, which show how a modular model consisting of general purpose residual blocks generalizes much better than a number of baselines, including architectures such as MAC, FiLM and Relation Network that were designed for visual reasoning",
        "We have shown how sensitive the high performance of the modular models is to the layout of modules, and how a tree-like structure generalizes much stronger than a typical chain of layers"
    ],
    "summary": [
        "Neural network based models have become the workhorse of natural language understanding and generation.",
        "We experiment with the End-to-End NMN (N2NMN) (<a class=\"ref-link\" id=\"cHu_et+al_2017_a\" href=\"#rHu_et+al_2017_a\">Hu et al, 2017</a>) paradigm from this family, which predicts the layout with a seq2seq model (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a>) and computes the parametrization of the modules using a soft attention mechanism.",
        "Based on the generic NMN model described above, we experiment with several specific architectures that differ in the way the modules are connected and parametrized.",
        "The NMN-Tree model differs from the generic models in that it does not use a language encoder and is instead built from modules that are parametrized by question words directly.",
        "The strong generalization of the NMN-Tree is impressive, but a significant amount of prior knowledge about the task was required to come up with the successful layout and parametrization used in this model.",
        "The scatterplot of the test error rate versus \u03ba (Figure 5) shows that for #rhs/lhs=1 high generalization is strongly associated with higher \u03ba, meaning that it is necessary to have different modules strongly focusing on different object words in order to generalize in this most challenging setting.",
        "Besides the theoretical appeal of systematicity, our study is inspired by highly related prior evidence that when trained on downstream language understanding tasks, neural networks often generalize poorly and latch on to dataset-specific regularities.",
        "<a class=\"ref-link\" id=\"cAndreas_et+al_2016_a\" href=\"#rAndreas_et+al_2016_a\">Andreas et al (2016</a>) introduced NMNs as a modular, structured VQA model where a fixed number of handcrafted neural modules are chosen and composed together in a layout determined by the dependency parse of the question.",
        "The intuitive appeal of modularity and structure in designing neural architectures for language understanding is supported by our results, which show how a modular model consisting of general purpose residual blocks generalizes much better than a number of baselines, including architectures such as MAC, FiLM and RelNet that were designed for visual reasoning.",
        "We have shown how sensitive the high performance of the modular models is to the layout of modules, and how a tree-like structure generalizes much stronger than a typical chain of layers.",
        "Our parametrization induction results on the #rhs/lhs=2 split are encouraging, as they show that compared to generic models, a weaker nudge towards systematicity may suffice for end-to-end NMNs. While our investigation has been performed on a synthetic dataset, we believe that it is the realworld language understanding where our findings may be most relevant."
    ],
    "headline": "Using a synthetic visual question answering test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them",
    "reference_links": [
        {
            "id": "Agrawal_et+al_2016_a",
            "entry": "Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Analyzing the Behavior of Visual Question Answering Models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, January 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Aishwarya%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Analyzing%20the%20Behavior%20of%20Visual%20Question%20Answering%20Models%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Aishwarya%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Analyzing%20the%20Behavior%20of%20Visual%20Question%20Answering%20Models%202016-01"
        },
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural Module Networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL http://arxiv.org/abs/1511.02799.",
            "url": "http://arxiv.org/abs/1511.02799",
            "arxiv_url": "https://arxiv.org/pdf/1511.02799"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the 2015 International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015"
        },
        {
            "id": "Bastings_et+al_2018_a",
            "entry": "Joost Bastings, Marco Baroni, Jason Weston, Kyunghyun Cho, and Douwe Kiela. Jump to better conclusions: SCAN both left and right. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 47\u201355, Brussels, Belgium, November 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-5407.",
            "url": "https://www.aclweb.org/anthology/W18-5407",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastings%2C%20Joost%20Baroni%2C%20Marco%20Weston%2C%20Jason%20Cho%2C%20Kyunghyun%20Jump%20to%20better%20conclusions%3A%20SCAN%20both%20left%20and%20right%202018-11"
        },
        {
            "id": "Bingham_et+al_2017_a",
            "entry": "Eli Bingham, Piero Molino, Paul Szerlip, Obermeyer Fritz, and Goodman Noah. Characterizing how Visual Question Answering scales with the world. In NIPS 2017 Visually-Grounded Interaction and Language Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bingham%2C%20Eli%20Molino%2C%20Piero%20Szerlip%2C%20Paul%20Fritz%2C%20Obermeyer%20Characterizing%20how%20Visual%20Question%20Answering%20scales%20with%20the%20world%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bingham%2C%20Eli%20Molino%2C%20Piero%20Szerlip%2C%20Paul%20Fritz%2C%20Obermeyer%20Characterizing%20how%20Visual%20Question%20Answering%20scales%20with%20the%20world%202017"
        },
        {
            "id": "Calvo_2003_a",
            "entry": "Francisco Calvo and Eliana Colunga. The statistical brain: Reply to Marcus The algebraic mind. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 25, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calvo%2C%20Francisco%20Colunga%2C%20Eliana%20The%20statistical%20brain%3A%20Reply%20to%20Marcus%20The%20algebraic%20mind%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calvo%2C%20Francisco%20Colunga%2C%20Eliana%20The%20statistical%20brain%3A%20Reply%20to%20Marcus%20The%20algebraic%20mind%202003"
        },
        {
            "id": "Fodor_1988_a",
            "entry": "Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1):3\u201371, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988"
        },
        {
            "id": "Gaunt_et+al_2016_a",
            "entry": "Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Differentiable Programs with Neural Libraries. In Proceedings of the 34th International Conference on Machine Learning, November 2016. URL http://arxiv.org/abs/1611.02109.arXiv:1611.02109.",
            "url": "http://arxiv.org/abs/1611.02109.arXiv:1611.02109",
            "arxiv_url": "https://arxiv.org/pdf/1611.02109"
        },
        {
            "id": "Gong_et+al_2017_a",
            "entry": "Yichen Gong, Heng Luo, and Jian Zhang. Natural Language Inference over Interaction Space. In Proceedings of the 2018 International Conference on Learning Representations, 2017. URL http://arxiv.org/abs/1709.04348.arXiv:1709.04348.",
            "url": "http://arxiv.org/abs/1709.04348.arXiv:1709.04348",
            "arxiv_url": "https://arxiv.org/pdf/1709.04348"
        },
        {
            "id": "Gupta_2018_a",
            "entry": "Nitish Gupta and Mike Lewis. Neural Compositional Denotational Semantics for Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/D18-1239.",
            "url": "http://aclweb.org/anthology/D18-1239",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Nitish%20Lewis%2C%20Mike%20Neural%20Compositional%20Denotational%20Semantics%20for%20Question%20Answering%202018"
        },
        {
            "id": "Gururangan_et+al_2018_a",
            "entry": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. Annotation Artifacts in Natural Language Inference Data. In Proceedings of NAACL-HLT 2018, March 2018. URL http://arxiv.org/abs/1803.02324.arXiv:1803.02324.",
            "url": "http://arxiv.org/abs/1803.02324.arXiv:1803.02324",
            "arxiv_url": "https://arxiv.org/pdf/1803.02324"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to Reason: End-to-End Module Networks for Visual Question Answering. In Proceedings of 2017 IEEE International Conference on Computer Vision, April 2017. URL http://arxiv.org/abs/1704.05526.arXiv:1704.05526.",
            "url": "http://arxiv.org/abs/1704.05526.arXiv:1704.05526",
            "arxiv_url": "https://arxiv.org/pdf/1704.05526"
        },
        {
            "id": "Hu_et+al_2018_a",
            "entry": "Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable Neural Computation via Stack Neural Module Networks. In Proceedings of 2018 European Conference on Computer Vision, July 2018. URL http://arxiv.org/abs/1807.08556.arXiv:1807.08556.",
            "url": "http://arxiv.org/abs/1807.08556.arXiv:1807.08556",
            "arxiv_url": "https://arxiv.org/pdf/1807.08556"
        },
        {
            "id": "Hudson_2018_a",
            "entry": "Drew A. Hudson and Christopher D. Manning. Compositional Attention Networks for Machine Reasoning. In Proceedings of the 2018 International Conference on Learning Representations, February 2018. URL https://openreview.net/forum?id=S1Euwz-Rb.",
            "url": "https://openreview.net/forum?id=S1Euwz-Rb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hudson%2C%20Drew%20A.%20Manning%2C%20Christopher%20D.%20Compositional%20Attention%20Networks%20for%20Machine%20Reasoning%202018-02"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448\u2013456, 2015. URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.",
            "url": "http://jmlr.org/proceedings/papers/v37/ioffe15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20Normalization%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20ICML%202015%20Lille%20France%20611%20July%202015%20pp%20448456%202015%20URL%20httpjmlrorgproceedingspapersv37ioffe15html"
        },
        {
            "id": "Jia_2017_a",
            "entry": "Robin Jia and Percy Liang. Adversarial Examples for Evaluating Reading Comprehension Systems. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021\u20132031, 2017. doi: 10.18653/v1/D17-1215. URL https://aclanthology.coli.uni-saarland.de/papers/D17-1215/d17-1215.",
            "crossref": "https://dx.doi.org/10.18653/v1/D17-1215",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/D17-1215"
        },
        {
            "id": "Jiang_et+al_2018_a",
            "entry": "Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Pythia v0.1: The winning entry to the vqa challenge 2018. https://github.com/facebookresearch/pythia, 2018.",
            "url": "https://github.com/facebookresearch/pythia"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), December 2016. URL http://arxiv.org/abs/1612.06890.arXiv:1612.06890.",
            "url": "http://arxiv.org/abs/1612.06890.arXiv:1612.06890",
            "arxiv_url": "https://arxiv.org/pdf/1612.06890"
        },
        {
            "id": "Johnson_et+al_2017_a",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Inferring and Executing Programs for Visual Reasoning. In Proceedings of 2017 IEEE International Conference on Computer Vision, 2017. URL http://arxiv.org/abs/1705.03633.",
            "url": "http://arxiv.org/abs/1705.03633",
            "arxiv_url": "https://arxiv.org/pdf/1705.03633"
        },
        {
            "id": "Kannan_et+al_2016_a",
            "entry": "Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos, Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, and Vivek Ramavajjala. Smart Reply: Automated Response Suggestion for Email. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pp. 955\u2013964, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939801. URL http://doi.acm.org/10.1145/2939672.2939801.",
            "crossref": "https://dx.doi.org/10.1145/2939672.2939801",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2939672.2939801"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 2015 International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6980.arXiv:1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980.arXiv:1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kuhnle_1704_a",
            "entry": "Alexander Kuhnle and Ann Copestake. ShapeWorld - A new test methodology for multimodal language understanding. arXiv:1704.04517 [cs], April 2017. URL http://arxiv.org/abs/1704.04517.arXiv:1704.04517.",
            "url": "http://arxiv.org/abs/1704.04517.arXiv:1704.04517",
            "arxiv_url": "https://arxiv.org/pdf/1704.04517"
        },
        {
            "id": "Lake_2018_a",
            "entry": "Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 36th International Conference on Machine Learning, 2018. URL http://arxiv.org/abs/1711.00350.arXiv:1711.00350.",
            "url": "http://arxiv.org/abs/1711.00350.arXiv:1711.00350",
            "arxiv_url": "https://arxiv.org/pdf/1711.00350"
        },
        {
            "id": "Loula_et+al_2018_a",
            "entry": "Joao Loula, Marco Baroni, and Brenden M. Lake. Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. In Proceedings of the 2018 BlackboxNLP EMNLP Workshop, July 2018. URL https://arxiv.org/abs/1807.07545.",
            "url": "https://arxiv.org/abs/1807.07545",
            "arxiv_url": "https://arxiv.org/pdf/1807.07545"
        },
        {
            "id": "Malinowski_2014_a",
            "entry": "Mateusz Malinowski and Mario Fritz. A Multi-world Approach to Question Answering About Realworld Scenes Based on Uncertain Input. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS\u201914, pp. 1682\u20131690, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2968826.2969014.",
            "url": "http://dl.acm.org/citation.cfm?id=2968826.2969014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malinowski%2C%20Mateusz%20Fritz%2C%20Mario%20A%20Multi-world%20Approach%20to%20Question%20Answering%20About%20Realworld%20Scenes%20Based%20on%20Uncertain%20Input%202014"
        },
        {
            "id": "Marcus_1998_a",
            "entry": "Gary F. Marcus. Rethinking Eliminative Connectionism. Cognitive Psychology, 37(3):243\u2013282, December 1998. ISSN 0010-0285. doi: 10.1006/cogp.1998.0694. URL http://www.sciencedirect.com/science/article/pii/S0010028598906946.",
            "crossref": "https://dx.doi.org/10.1006/cogp.1998.0694",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1006/cogp.1998.0694"
        },
        {
            "id": "Marcus_2003_a",
            "entry": "Gary F. Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Gary%20F.%20The%20algebraic%20mind%3A%20Integrating%20connectionism%20and%20cognitive%20science%202003"
        },
        {
            "id": "Perez_et+al_2017_a",
            "entry": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer. In In Proceedings of the 2017 AAAI Conference on Artificial Intelligence, 2017. URL http://arxiv.org/abs/1709.07871.",
            "url": "http://arxiv.org/abs/1709.07871",
            "arxiv_url": "https://arxiv.org/pdf/1709.07871"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems 31, June 2017. URL http://arxiv.org/abs/1706.01427.arXiv:1706.01427.",
            "url": "http://arxiv.org/abs/1706.01427.arXiv:1706.01427",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Smolensky_1987_a",
            "entry": "Paul Smolensky. The constituent structure of connectionist mental states: A reply to Fodor and Pylyshyn. Southern Journal of Philosophy, 26(Supplement):137\u2013161, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20The%20constituent%20structure%20of%20connectionist%20mental%20states%3A%20A%20reply%20to%20Fodor%20and%20Pylyshyn%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20Paul%20The%20constituent%20structure%20of%20connectionist%20mental%20states%3A%20A%20reply%20to%20Fodor%20and%20Pylyshyn%201987"
        },
        {
            "id": "Suarez_et+al_2018_a",
            "entry": "Joseph Suarez, Justin Johnson, and Fei-Fei Li. DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer. arXiv:1803.11361 [cs], March 2018. URL http://arxiv.org/abs/1803.11361.arXiv:1803.11361.",
            "url": "http://arxiv.org/abs/1803.11361.arXiv:1803.11361",
            "arxiv_url": "https://arxiv.org/pdf/1803.11361"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems 27, pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%202014"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Wei Wang, Ming Yan, and Chen Wu. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1705\u20131714, Melbourne, Australia, 2018. Association for Computational Linguistics. URL http://aclweb.org/anthology/P18-1158.",
            "url": "http://aclweb.org/anthology/P18-1158",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wei%20Yan%2C%20Ming%20Wu%2C%20Chen%20Multi-Granularity%20Hierarchical%20Attention%20Fusion%20Networks%20for%20Reading%20Comprehension%20and%20Question%20Answering%202018"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, and others. Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "We_2015_a",
            "entry": "We trained all models by minimizing the cross entropy loss log p(y|x, q) on the training set, where y \u2208 {yes, no} is the correct answer, x is the image, q is the question. In all our experiments we used the Adam optimizer (Kingma & Ba, 2015) with hyperparameters \u03b1 = 0.0001, \u03b21 = 0.9, \u03b22 = 0.999, = 10\u221210. We continuously monitored validation set performance of all models during training, selected the best one and reported its performance on the test set. The number of training iterations for each model was selected in preliminary investigations based on our observations of how long it takes for different models to converge. This information, as well as other training details, can be found in Table 3.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20trained%20all%20models%20by%20minimizing%20the%20cross%20entropy%20loss%20log%20pyx%20q%20on%20the%20training%20set%20where%20y%20%20yes%20no%20is%20the%20correct%20answer%20x%20is%20the%20image%20q%20is%20the%20question%20In%20all%20our%20experiments%20we%20used%20the%20Adam%20optimizer%20Kingma%20%20Ba%202015%20with%20hyperparameters%20%CE%B1%20%2000001%20%CE%B21%20%2009%20%CE%B22%20%200999%20%201010%20We%20continuously%20monitored%20validation%20set%20performance%20of%20all%20models%20during%20training%20selected%20the%20best%20one%20and%20reported%20its%20performance%20on%20the%20test%20set%20The%20number%20of%20training%20iterations%20for%20each%20model%20was%20selected%20in%20preliminary%20investigations%20based%20on%20our%20observations%20of%20how%20long%20it%20takes%20for%20different%20models%20to%20converge%20This%20information%20as%20well%20as%20other%20training%20details%20can%20be%20found%20in%20Table%203"
        }
    ]
}
