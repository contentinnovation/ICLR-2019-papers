{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ENVIRONMENT PROBING INTERACTION POLICIES",
        "author": "Wenxuan Zhou, Lerrel Pinto, Abhinav Gupta, 1The Robotics Institute, Carnegie Mellon University 2Facebook AI Research",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryl8-3AcFX"
        },
        "abstract": "A key challenge in reinforcement learning (RL) is environment generalization: a policy trained to solve a task in one environment often fails to solve the same task in a slightly different test environment. A common approach to improve interenvironment transfer is to learn policies that are invariant to the distribution of testing environments. However, we argue that instead of being invariant, the policy should identify the specific nuances of an environment and exploit them to achieve better performance. In this work, we propose the \u201cEnvironment-Probing\u201d Interaction (EPI) policy, a policy that probes a new environment to extract an implicit understanding of that environment\u2019s behavior. Once this environmentspecific information is obtained, it is used as an additional input to a task-specific policy that can now perform environment-conditioned actions to solve a task. To learn these EPI-policies, we present a reward function based on transition predictability. Specifically, a higher reward is given if the trajectory generated by the EPI-policy can be used to better predict transitions. We experimentally show that EPI-conditioned task-specific policies significantly outperform commonly used policy generalization methods on novel testing environments."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "test environment",
            "url": "https://en.wikipedia.org/wiki/test_environment"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "system identification",
            "url": "https://en.wikipedia.org/wiki/system_identification"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "EPI": "ENVIRONMENT-PROBING INTERACTIONS"
    },
    "highlights": [
        "Over the last few years deep reinforcement learning (RL) has shown tremendous progress",
        "We argue that instead of learning a policy that is invariant to environment dynamics we need a policy that adapts to its environment",
        "We model our problem as a continuous space Markov Decision Process represented as the tuple (S, A, P, r, \u03b3, S), where S is a set of continuous states of the agent, A is a set of continuous actions the agent can take at a given state, P : S \u00d7 A \u00d7 S \u2192 R is the transition probability function, r : S \u00d7 A \u2192 R is the reward function, \u03b3 is the discount factor, and S is the initial state distribution",
        "We present experimental results to demonstrate how the ENVIRONMENT-PROBING INTERACTIONS-policy helps the agent understand the environment, and allows the task-specific policy to generalize better to unseen test environments",
        "We have presented an approach to extract environment behavior information by probing it using an ENVIRONMENT-PROBING INTERACTIONS-policy",
        "Given unseen test environments on Hopper and Striker, we show that our embedding conditioned task policies improve generalization to unseen test environments"
    ],
    "key_statements": [
        "Over the last few years deep reinforcement learning (RL) has shown tremendous progress",
        "We argue that instead of learning a policy that is invariant to environment dynamics we need a policy that adapts to its environment",
        "We model our problem as a continuous space Markov Decision Process represented as the tuple (S, A, P, r, \u03b3, S), where S is a set of continuous states of the agent, A is a set of continuous actions the agent can take at a given state, P : S \u00d7 A \u00d7 S \u2192 R is the transition probability function, r : S \u00d7 A \u2192 R is the reward function, \u03b3 is the discount factor, and S is the initial state distribution",
        "We present experimental results to demonstrate how the ENVIRONMENT-PROBING INTERACTIONS-policy helps the agent understand the environment, and allows the task-specific policy to generalize better to unseen test environments",
        "Code is available at https://github.com/Wenxuan-Zhou/ENVIRONMENT-PROBING INTERACTIONS",
        "Environments: To evaluate the ENVIRONMENT-PROBING INTERACTIONS-policy, we need appropriate simulation environments that are sensitive to environment parameters",
        "We have presented an approach to extract environment behavior information by probing it using an ENVIRONMENT-PROBING INTERACTIONS-policy",
        "Given unseen test environments on Hopper and Striker, we show that our embedding conditioned task policies improve generalization to unseen test environments"
    ],
    "summary": [
        "Over the last few years deep reinforcement learning (RL) has shown tremendous progress.",
        "A popular approach for this is to first perform explicit system identification of the environment, and use the estimated parameters to generate an environment specific policy (<a class=\"ref-link\" id=\"cYu_et+al_2017_a\" href=\"#rYu_et+al_2017_a\">Yu et al, 2017</a>).",
        "We present a framework in which the agent first performs \u201cenvironment-probing\u201d interactions that extract information from an environment, leverages this information to achieve the goal with a task-specific policy.",
        "We separately optimize an \u201cEnvironment-Probing\u201d Interaction policy (EPI-policy) to extract environment information.",
        "This compact embedding vector is passed to a prediction model that provides higher rewards to the trajectories that are useful in estimating environment-dependent transitions.",
        "Once we have learned the EPI-policy, we can use the generated environment embedding as an input to the task-specific policy.",
        "To learn the EPI-policy, we introduce a reward formulation based on the prediction models (Fig. 1).",
        "The difference in performance between a simple prediction model f and the embedding-conditioned prediction model fepi is used to train the EPI-policy.",
        "For a given training environment, the trajectory \u03c4epi generated by EPI-policy \u03c0epi is fed into fepi to calculate the prediction loss Lepi pred.",
        "The EPI-policy is optimized using TRPO for a few iterations with rewards from the learned prediction model.",
        "After the EPI-policy and the embedding network are optimized, they are fixed and used for training the task-specific policy.",
        "The agent follows the same procedure: it first executes the EPI-policy to get the environment embedding, followed by the task-specific policy to achieve the goal.",
        "We present experimental results to demonstrate how the EPI-policy helps the agent understand the environment, and allows the task-specific policy to generalize better to unseen test environments.",
        "It performs explicit system identification that estimates environment parameters using a history of states and actions from the Oracle Policy.",
        "Direct Reward Policy: this baseline evaluates the interactions by directly taking the final reward of the following task-specific policy trajectory instead of the prediction reward.",
        "We observe that for both the Hopper and the Striker environments, the EPI Policy achieves better performance than the baselines and is comparable to the performance of the Oracle Policy.",
        "To learn the EPI-policy we use a reward function which prefers trajectories that improve environment transition predictability.",
        "Given unseen test environments on Hopper and Striker, we show that our embedding conditioned task policies improve generalization to unseen test environments."
    ],
    "headline": "We argue that instead of being invariant, the policy should identify the specific nuances of an environment and exploit them to achieve better performance",
    "reference_links": [
        {
            "id": "Armstrong_1987_a",
            "entry": "B. Armstrong. On finding \u2019exciting\u2019 trajectories for identification experiments involving systems with non-linear dynamics. In Proceedings. 1987 IEEE International Conference on Robotics and Automation, volume 4, pp. 1131\u20131139, March 1987. doi: 10.1109/ROBOT.1987.1087968.",
            "crossref": "https://dx.doi.org/10.1109/ROBOT.1987.1087968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ROBOT.1987.1087968"
        },
        {
            "id": "Bongard_2005_a",
            "entry": "Josh C Bongard and Hod Lipson. Nonlinear system identification using coevolution of models and tests. IEEE Transactions on Evolutionary Computation, 9(4):361\u2013384, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bongard%2C%20Josh%20C.%20Lipson%2C%20Hod%20Nonlinear%20system%20identification%20using%20coevolution%20of%20models%20and%20tests%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bongard%2C%20Josh%20C.%20Lipson%2C%20Hod%20Nonlinear%20system%20identification%20using%20coevolution%20of%20models%20and%20tests%202005"
        },
        {
            "id": "Braun_et+al_2009_a",
            "entry": "Daniel A Braun, Ad Aertsen, Daniel M Wolpert, and Carsten Mehring. Learning optimal adaptation strategies in unpredictable motor tasks. Journal of Neuroscience, 29(20):6472\u20136478, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Braun%2C%20Daniel%20A.%20Aertsen%2C%20Ad%20Wolpert%2C%20Daniel%20M.%20Mehring%2C%20Carsten%20Learning%20optimal%20adaptation%20strategies%20in%20unpredictable%20motor%20tasks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Braun%2C%20Daniel%20A.%20Aertsen%2C%20Ad%20Wolpert%2C%20Daniel%20M.%20Mehring%2C%20Carsten%20Learning%20optimal%20adaptation%20strategies%20in%20unpredictable%20motor%20tasks%202009"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Chentanez_et+al_2005_a",
            "entry": "Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 1281\u20131288, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "Clavera_et+al_2018_a",
            "entry": "Ignasi Clavera, Anusha Nagabandi, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt: Meta-learning for model-based control. arXiv preprint arXiv:1803.11347, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11347"
        },
        {
            "id": "Denil_et+al_2016_a",
            "entry": "Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, and Nando de Freitas. Learning to perform physics experiments via deep reinforcement learning. arXiv preprint arXiv:1611.01843, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01843"
        },
        {
            "id": "Duan_et+al_2012_a",
            "entry": "Lixin Duan, Dong Xu, and Ivor Tsang. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.4660"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329\u20131338, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Duan_et+al_0000_a",
            "entry": "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "Gevers_2006_a",
            "entry": "Michel Gevers et al. System identification without lennart ljung: what would have been different? Forever Ljung in System Identification, Studentlitteratur AB, Norrtalje, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gevers%2C%20Michel%20System%20identification%20without%20lennart%20ljung%3A%20what%20would%20have%20been%20different%3F%20Forever%20Ljung%20in%20System%20Identification%202006"
        },
        {
            "id": "Gupta_et+al_2017_a",
            "entry": "Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces to transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02949"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "Hoffman_et+al_2014_a",
            "entry": "Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, and Kate Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems, pp. 3536\u20133544, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Judy%20Guadarrama%2C%20Sergio%20Tzeng%2C%20Eric%20S.%20Hu%2C%20Ronghang%20Lsda%3A%20Large%20scale%20detection%20through%20adaptation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Judy%20Guadarrama%2C%20Sergio%20Tzeng%2C%20Eric%20S.%20Hu%2C%20Ronghang%20Lsda%3A%20Large%20scale%20detection%20through%20adaptation%202014"
        },
        {
            "id": "James_et+al_2017_a",
            "entry": "Stephen James, Andrew J Davison, and Edward Johns. Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. arXiv preprint arXiv:1707.02267, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02267"
        },
        {
            "id": "Kaelbling_et+al_1996_a",
            "entry": "Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Moore%2C%20Andrew%20W.%20Reinforcement%20learning%3A%20A%20survey%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Moore%2C%20Andrew%20W.%20Reinforcement%20learning%3A%20A%20survey%201996"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kulis_et+al_2011_a",
            "entry": "Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1785\u20131792. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulis%2C%20Brian%20Saenko%2C%20Kate%20Darrell%2C%20Trevor%20What%20you%20saw%20is%20not%20what%20you%20get%3A%20Domain%20adaptation%20using%20asymmetric%20kernel%20transforms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulis%2C%20Brian%20Saenko%2C%20Kate%20Darrell%2C%20Trevor%20What%20you%20saw%20is%20not%20what%20you%20get%3A%20Domain%20adaptation%20using%20asymmetric%20kernel%20transforms%202011"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04779"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Long_et+al_2015_a",
            "entry": "Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02791"
        },
        {
            "id": "Milanese_2013_a",
            "entry": "Mario Milanese, John Norton, Helene Piet-Lahanier, and Eric Walter. Bounding approaches to system identification. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Milanese%2C%20Mario%20Norton%2C%20John%20Helene%20Piet-Lahanier%2C%20and%20Eric%20Walter.%20Bounding%20approaches%20to%20system%20identification%202013"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Pathak_et+al_0000_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. arXiv preprint arXiv:1804.08606, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08606"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. arXiv preprint arXiv:1710.06537, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06537"
        },
        {
            "id": "Peng_et+al_2018_a",
            "entry": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Pinto_et+al_0000_a",
            "entry": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06542"
        },
        {
            "id": "Pinto_et+al_2017_a",
            "entry": "Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. ICML, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20Lerrel%20Davidson%2C%20James%20Sukthankar%2C%20Rahul%20Gupta%2C%20Abhinav%20Robust%20adversarial%20reinforcement%20learning%202017"
        },
        {
            "id": "Rajeswaran_et+al_2017_a",
            "entry": "Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epopt: Learning robust neural network policies using model ensembles. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajeswaran%2C%20Aravind%20Ghotra%2C%20Sarvjeet%20Levine%2C%20Sergey%20Ravindran%2C%20Balaraman%20Epopt%3A%20Learning%20robust%20neural%20network%20policies%20using%20model%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajeswaran%2C%20Aravind%20Ghotra%2C%20Sarvjeet%20Levine%2C%20Sergey%20Ravindran%2C%20Balaraman%20Epopt%3A%20Learning%20robust%20neural%20network%20policies%20using%20model%20ensembles%202017"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A Rusu, Matej Vecerik, Thomas Rothorl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04286"
        },
        {
            "id": "Sadeghi_2016_a",
            "entry": "Fereshteh Sadeghi and Sergey Levine. (cad)2rl: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04201"
        },
        {
            "id": "S_et+al_2018_a",
            "entry": "Steindor S\u00e6mundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement learning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07551"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "Tobin_et+al_2017_a",
            "entry": "Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. IROS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tobin%2C%20Josh%20Fong%2C%20Rachel%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Domain%20randomization%20for%20transferring%20deep%20neural%20networks%20from%20simulation%20to%20the%20real%20world%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tobin%2C%20Josh%20Fong%2C%20Rachel%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Domain%20randomization%20for%20transferring%20deep%20neural%20networks%20from%20simulation%20to%20the%20real%20world%202017"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Tzeng_et+al_2014_a",
            "entry": "Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3474"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "Yosinski_et+al_2014_a",
            "entry": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320\u20133328, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01557"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. arXiv preprint arXiv:1702.02453, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02453"
        }
    ]
}
