{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADVERSARIAL AUDIO SYNTHESIS",
        "author": "Chris Donahue Department of Music UC San Diego cdonahue@ucsd.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByMVTsR5KQ"
        },
        "abstract": "Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that\u2014without labels\u2014WaveGAN learns to produce intelligible words when trained on a smallvocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising."
    },
    "keywords": [
        {
            "term": "audio signal",
            "url": "https://en.wikipedia.org/wiki/audio_signal"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "speech enhancement",
            "url": "https://en.wikipedia.org/wiki/speech_enhancement"
        },
        {
            "term": "audio synthesis",
            "url": "https://en.wikipedia.org/wiki/audio_synthesis"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "sound effect",
            "url": "https://en.wikipedia.org/wiki/sound_effect"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "DCGAN": "deep convolutional GAN"
    },
    "highlights": [
        "Synthesizing audio for specific domains has many practical applications in creative sound design for music and film",
        "With WaveGAN, we provide both a starting point for practical audio synthesis with Generative Adversarial Networks and a recipe for modifying other image generation methods to operate on waveforms",
        "With SpecGAN, our frequency-domain audio generation model, we design a spectrogram representation that is both well-suited to Generative Adversarial Networks designed for image generation and can be approximately inverted",
        "We present WaveGAN, the first application of Generative Adversarial Networks to unsupervised audio generation",
        "In its current form, WaveGAN can be used for creative sound design in multimedia production",
        "By providing a template for modifying image generation models to operate on audio, we hope that this work catalyzes future investigation of Generative Adversarial Networks for audio synthesis"
    ],
    "key_statements": [
        "Synthesizing audio for specific domains has many practical applications in creative sound design for music and film",
        "With WaveGAN, we provide both a starting point for practical audio synthesis with Generative Adversarial Networks and a recipe for modifying other image generation methods to operate on waveforms",
        "With SpecGAN, our frequency-domain audio generation model, we design a spectrogram representation that is both well-suited to Generative Adversarial Networks designed for image generation and can be approximately inverted",
        "We evaluate our metrics on the real training data, the real test data, and a version of SC09 generated by a parametric speech synthesizer (<a class=\"ref-link\" id=\"cBuchner_2017_a\" href=\"#rBuchner_2017_a\">Buchner, 2017</a>)",
        "We present WaveGAN, the first application of Generative Adversarial Networks to unsupervised audio generation",
        "In its current form, WaveGAN can be used for creative sound design in multimedia production",
        "In our future work we plan to extend WaveGAN to operate on variable-length audio and explore a variety of label conditioning strategies",
        "By providing a template for modifying image generation models to operate on audio, we hope that this work catalyzes future investigation of Generative Adversarial Networks for audio synthesis"
    ],
    "summary": [
        "Synthesizing audio for specific domains has many practical applications in creative sound design for music and film.",
        "Unlike with GANs, the autoregressive setting results in slow generation as output audio samples must be fed back into the model one at a time.",
        "We investigate both waveform and spectrogram strategies for generating one-second slices of audio with GANs.1 For our spectrogram approach (SpecGAN), we first design a spectrogram representation that allows for approximate inversion, and bootstrap the two-dimensional deep convolutional GAN (DCGAN) method (<a class=\"ref-link\" id=\"cRadford_et+al_2016_a\" href=\"#rRadford_et+al_2016_a\">Radford et al, 2016</a>) to operate on these spectrograms.",
        "With WaveGAN, we provide both a starting point for practical audio synthesis with GANs and a recipe for modifying other image generation methods to operate on waveforms.",
        "On criteria of sound quality and speaker diversity, human judges indicate a preference for the audio generated by WaveGAN compared to that from SpecGAN.",
        "With SpecGAN, our frequency-domain audio generation model, we design a spectrogram representation that is both well-suited to GANs designed for image generation and can be approximately inverted.",
        "During our quantitative evaluation of SC09, our WaveGAN networks converge by their early stopping criteria within four days (200k iterations, around 3500 epochs), and produce speech-like audio within the first hour of training.",
        "<a class=\"ref-link\" id=\"cSalimans_et+al_2016_a\" href=\"#rSalimans_et+al_2016_a\">Salimans et al (2016</a>) propose the inception score, which uses a pre-trained Inception classifier (<a class=\"ref-link\" id=\"cSzegedy_et+al_2016_a\" href=\"#rSzegedy_et+al_2016_a\">Szegedy et al, 2016</a>) to measure both the diversity and semantic discriminability of generated images, finding that the measure correlates well with human judgement.",
        "A generative model that overfits the training data will achieve a high score by outputting examples on which the classifier was trained.",
        "If the generative model produces examples from the training set, this measure will be 0.",
        "Using our best WaveGAN and SpecGAN models as measured by inception score, we generate random examples until we have 300 for each digit\u20143000 total.",
        "While these measures indicate that our generative models produce examples with statistics that deviate from those of the real data, neither metric indicates that the models achieve high inception scores by the trivial solutions outlined in Section 6.2.",
        "It appears that SpecGAN might better capture the variance in the underlying data compared to WaveGAN, but its success is compromised by sound quality issues when its spectrograms are inverted to audio.",
        "By providing a template for modifying image generation models to operate on audio, we hope that this work catalyzes future investigation of GANs for audio synthesis."
    ],
    "headline": "In this paper we introduce WaveGAN, a first attempt at applying Generative Adversarial Networks to unsupervised synthesis of raw-waveform audio",
    "reference_links": [
        {
            "id": "Arik_et+al_2017_a",
            "entry": "Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sercan%20Arik%20Gregory%20Diamos%20Andrew%20Gibiansky%20John%20Miller%20Kainan%20Peng%20Wei%20Ping%20Jonathan%20Raiman%20and%20Yanqi%20Zhou%20Deep%20Voice%202%20Multispeaker%20neural%20texttospeech%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sercan%20Arik%20Gregory%20Diamos%20Andrew%20Gibiansky%20John%20Miller%20Kainan%20Peng%20Wei%20Ping%20Jonathan%20Raiman%20and%20Yanqi%20Zhou%20Deep%20Voice%202%20Multispeaker%20neural%20texttospeech%20In%20NIPS%202017"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%20Soumith%20Chintala%20and%20Leon%20Bottou%20Wasserstein%20GAN%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%20Soumith%20Chintala%20and%20Leon%20Bottou%20Wasserstein%20GAN%20In%20ICML%202017"
        },
        {
            "id": "Berthelot_et+al_2017_a",
            "entry": "David Berthelot, Tom Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "Boesman_2018_a",
            "entry": "Peter Boesman. Bird recordings. https://www.xeno-canto.org/contributor/ OOECIWCSWV, 2018. Accessed:2018-01-08.",
            "url": "https://www.xeno-canto.org/contributor/OOECIWCSWV"
        },
        {
            "id": "Buchner_2017_a",
            "entry": "Johannes Buchner. Synthetic speech commands dataset. https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset, 2017. Accessed:2017-01-15.",
            "url": "https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and Chenliang Xu. Deep cross-modal audio-visual generation. In Proceedings of the on Thematic Workshops of ACM Multimedia, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Lele%20Srivastava%2C%20Sudhanshu%20Duan%2C%20Zhiyao%20Xu%2C%20Chenliang%20Deep%20cross-modal%20audio-visual%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Lele%20Srivastava%2C%20Sudhanshu%20Duan%2C%20Zhiyao%20Xu%2C%20Chenliang%20Deep%20cross-modal%20audio-visual%20generation%202017"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS Deep Learning and Representation Learning Workshop, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Junyoung%20Gulcehre%2C%20Caglar%20Cho%2C%20KyungHyun%20Bengio%2C%20Yoshua%20Empirical%20evaluation%20of%20gated%20recurrent%20neural%20networks%20on%20sequence%20modeling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Junyoung%20Gulcehre%2C%20Caglar%20Cho%2C%20KyungHyun%20Bengio%2C%20Yoshua%20Empirical%20evaluation%20of%20gated%20recurrent%20neural%20networks%20on%20sequence%20modeling%202014"
        },
        {
            "id": "Donahue_et+al_2018_a",
            "entry": "Chris Donahue, Bo Li, and Rohit Prabhavalkar. Exploring speech enhancement with generative adversarial networks for robust speech recognition. In ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Chris%20Li%2C%20Bo%20Prabhavalkar%2C%20Rohit%20Exploring%20speech%20enhancement%20with%20generative%20adversarial%20networks%20for%20robust%20speech%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Chris%20Li%2C%20Bo%20Prabhavalkar%2C%20Rohit%20Exploring%20speech%20enhancement%20with%20generative%20adversarial%20networks%20for%20robust%20speech%20recognition%202018"
        },
        {
            "id": "Dudley_1939_a",
            "entry": "Homer Dudley. Remaking speech. The Journal of the Acoustical Society of America, 1939.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dudley%2C%20Homer%20Remaking%20speech%201939",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dudley%2C%20Homer%20Remaking%20speech%201939"
        },
        {
            "id": "Engel_et+al_2017_a",
            "entry": "Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with WaveNet autoencoders. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Jesse%20Resnick%2C%20Cinjon%20Roberts%2C%20Adam%20Dieleman%2C%20Sander%20Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders%202017"
        },
        {
            "id": "Engel_et+al_2019_a",
            "entry": "Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. GANSynth: Adversarial neural audio synthesis. In ICLR, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Jesse%20Agrawal%2C%20Kumar%20Krishna%20Chen%2C%20Shuo%20Gulrajani%2C%20Ishaan%20GANSynth%3A%20Adversarial%20neural%20audio%20synthesis%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Jesse%20Agrawal%2C%20Kumar%20Krishna%20Chen%2C%20Shuo%20Gulrajani%2C%20Ishaan%20GANSynth%3A%20Adversarial%20neural%20audio%20synthesis%202019"
        },
        {
            "id": "Fan_et+al_2017_a",
            "entry": "Zhe-Cheng Fan, Yen-Lin Lai, and Jyh-Shing Roger Jang. SVSGAN: Singing voice separation via generative adversarial network. arXiv:1710.11428, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11428"
        },
        {
            "id": "Garofolo_et+al_1993_a",
            "entry": "John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, David S Pallett, Nancy L Dahlgren, and Victor Zue. TIMIT acoustic-phonetic continuous speech corpus. Linguistic data consortium, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garofolo%2C%20John%20S.%20Lamel%2C%20Lori%20F.%20Fisher%2C%20William%20M.%20Fiscus%2C%20Jonathan%20G.%20TIMIT%20acoustic-phonetic%20continuous%20speech%20corpus.%20Linguistic%20data%20consortium%201993"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20networks%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20networks%20In%20NIPS%202014"
        },
        {
            "id": "Griffin_1984_a",
            "entry": "Daniel Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffin%2C%20Daniel%20Lim%2C%20Jae%20Signal%20estimation%20from%20modified%20short-time%20fourier%20transform%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffin%2C%20Daniel%20Lim%2C%20Jae%20Signal%20estimation%20from%20modified%20short-time%20fourier%20transform%201984"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "Hershey_et+al_2017_a",
            "entry": "Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. CNN architectures for large-scale audio classification. In ICASSP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hershey%2C%20Shawn%20Chaudhuri%2C%20Sourish%20Ellis%2C%20Daniel%20P.W.%20Gemmeke%2C%20Jort%20F.%20CNN%20architectures%20for%20large-scale%20audio%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hershey%2C%20Shawn%20Chaudhuri%2C%20Sourish%20Ellis%2C%20Daniel%20P.W.%20Gemmeke%2C%20Jort%20F.%20CNN%20architectures%20for%20large-scale%20audio%20classification%202017"
        },
        {
            "id": "Hunt_1996_a",
            "entry": "Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In ICASSP, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hunt%2C%20Andrew%20J.%20Black%2C%20Alan%20W.%20Unit%20selection%20in%20a%20concatenative%20speech%20synthesis%20system%20using%20a%20large%20speech%20database%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hunt%2C%20Andrew%20J.%20Black%2C%20Alan%20W.%20Unit%20selection%20in%20a%20concatenative%20speech%20synthesis%20system%20using%20a%20large%20speech%20database%201996"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, and Juhan Nam. Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms. In Sound and Music Computing Conference, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jongpil%20Park%2C%20Jiyoung%20Kim%2C%20Keunhyoung%20Luke%20Nam%2C%20Juhan%20Sample-level%20deep%20convolutional%20neural%20networks%20for%20music%20auto-tagging%20using%20raw%20waveforms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jongpil%20Park%2C%20Jiyoung%20Kim%2C%20Keunhyoung%20Luke%20Nam%2C%20Juhan%20Sample-level%20deep%20convolutional%20neural%20networks%20for%20music%20auto-tagging%20using%20raw%20waveforms%202017"
        },
        {
            "id": "Ling_et+al_2015_a",
            "entry": "Zhen-Hua Ling, Shi-Yin Kang, Heiga Zen, Andrew Senior, Mike Schuster, Xiao-Jun Qian, Helen M Meng, and Li Deng. Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends. IEEE Signal Processing Magazine, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ling%2C%20Zhen-Hua%20Kang%2C%20Shi-Yin%20Zen%2C%20Heiga%20Senior%2C%20Andrew%20Deep%20learning%20for%20acoustic%20modeling%20in%20parametric%20speech%20generation%3A%20A%20systematic%20review%20of%20existing%20techniques%20and%20future%20trends%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ling%2C%20Zhen-Hua%20Kang%2C%20Shi-Yin%20Zen%2C%20Heiga%20Senior%2C%20Andrew%20Deep%20learning%20for%20acoustic%20modeling%20in%20parametric%20speech%20generation%3A%20A%20systematic%20review%20of%20existing%20techniques%20and%20future%20trends%202015"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Mehri_et+al_2017_a",
            "entry": "Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. SampleRNN: An unconditional end-to-end neural audio generation model. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Gulrajani%2C%20Ishaan%20Kumar%2C%20Rithesh%20SampleRNN%3A%20An%20unconditional%20end-to-end%20neural%20audio%20generation%20model%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Gulrajani%2C%20Ishaan%20Kumar%2C%20Rithesh%20SampleRNN%3A%20An%20unconditional%20end-to-end%20neural%20audio%20generation%20model%202017"
        },
        {
            "id": "Michelsanti_2017_a",
            "entry": "Daniel Michelsanti and Zheng-Hua Tan. Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification. In INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michelsanti%2C%20Daniel%20Tan%2C%20Zheng-Hua%20Conditional%20generative%20adversarial%20networks%20for%20speech%20enhancement%20and%20noise-robust%20speaker%20verification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Michelsanti%2C%20Daniel%20Tan%2C%20Zheng-Hua%20Conditional%20generative%20adversarial%20networks%20for%20speech%20enhancement%20and%20noise-robust%20speaker%20verification%202017"
        },
        {
            "id": "Morise_et+al_2016_a",
            "entry": "Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. WORLD: a vocoder-based high-quality speech synthesis system for real-time applications. IEICE Transactions on Information and Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morise%2C%20Masanori%20Yokomori%2C%20Fumiya%20Ozawa%2C%20Kenji%20WORLD%3A%20a%20vocoder-based%20high-quality%20speech%20synthesis%20system%20for%20real-time%20applications%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morise%2C%20Masanori%20Yokomori%2C%20Fumiya%20Ozawa%2C%20Kenji%20WORLD%3A%20a%20vocoder-based%20high-quality%20speech%20synthesis%20system%20for%20real-time%20applications%202016"
        },
        {
            "id": "Eric_1990_a",
            "entry": "Eric Moulines and Francis Charpentier. Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech communication, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eric%20Moulines%20and%20Francis%20Charpentier%20Pitchsynchronous%20waveform%20processing%20techniques%20for%20texttospeech%20synthesis%20using%20diphones%20Speech%20communication%201990"
        },
        {
            "id": "Odena_et+al_2016_a",
            "entry": "Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016"
        },
        {
            "id": "Pascual_et+al_2017_a",
            "entry": "Santiago Pascual, Antonio Bonafonte, and Joan Serra. SEGAN: Speech enhancement generative adversarial network. In INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascual%2C%20Santiago%20Bonafonte%2C%20Antonio%20Serra%2C%20Joan%20SEGAN%3A%20Speech%20enhancement%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascual%2C%20Santiago%20Bonafonte%2C%20Antonio%20Serra%2C%20Joan%20SEGAN%3A%20Speech%20enhancement%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Ping_et+al_2018_a",
            "entry": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20ICLR%202018"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Sainath_et+al_2015_a",
            "entry": "Tara N Sainath, Ron J Weiss, Andrew Senior, Kevin W Wilson, and Oriol Vinyals. Learning the speech front-end with raw waveform CLDNNs. In INTERSPEECH, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20Tara%20N.%20Weiss%2C%20Ron%20J.%20Senior%2C%20Andrew%20Wilson%2C%20Kevin%20W.%20Learning%20the%20speech%20front-end%20with%20raw%20waveform%20CLDNNs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20Tara%20N.%20Weiss%2C%20Ron%20J.%20Senior%2C%20Andrew%20Wilson%2C%20Kevin%20W.%20Learning%20the%20speech%20front-end%20with%20raw%20waveform%20CLDNNs%202015"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20WaveNet%20on%20Mel%20spectrogram%20predictions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20WaveNet%20on%20Mel%20spectrogram%20predictions%202018"
        },
        {
            "id": "Shrivastava_et+al_2017_a",
            "entry": "Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb. Learning from simulated and unsupervised images through adversarial training. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrivastava%2C%20Ashish%20Pfister%2C%20Tomas%20Tuzel%2C%20Oncel%20Susskind%2C%20Joshua%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrivastava%2C%20Ashish%20Pfister%2C%20Tomas%20Tuzel%2C%20Oncel%20Susskind%2C%20Joshua%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017"
        },
        {
            "id": "Sotelo_et+al_2017_a",
            "entry": "Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio. Char2Wav: End-to-end speech synthesis. In ICLR Workshops, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sotelo%2C%20Jose%20Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Santos%2C%20Joao%20Felipe%20Char2Wav%3A%20End-to-end%20speech%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sotelo%2C%20Jose%20Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Santos%2C%20Joao%20Felipe%20Char2Wav%3A%20End-to-end%20speech%20synthesis%202017"
        },
        {
            "id": "Stevens_et+al_1937_a",
            "entry": "Stanley Smith Stevens, John Volkmann, and Edwin B Newman. A scale for the measurement of the psychological magnitude pitch. The Journal of the Acoustical Society of America, 1937.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stevens%2C%20Stanley%20Smith%20Volkmann%2C%20John%20Newman%2C%20Edwin%20B.%20A%20scale%20for%20the%20measurement%20of%20the%20psychological%20magnitude%20pitch%201937",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stevens%2C%20Stanley%20Smith%20Volkmann%2C%20John%20Newman%2C%20Edwin%20B.%20A%20scale%20for%20the%20measurement%20of%20the%20psychological%20magnitude%20pitch%201937"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Theis_et+al_2016_a",
            "entry": "Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20Lucas%20van%20den%20Oord%2C%20Aaron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20Lucas%20van%20den%20Oord%2C%20Aaron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016"
        },
        {
            "id": "Tokuda_et+al_2013_a",
            "entry": "Keiichi Tokuda, Yoshihiko Nankaku, Tomoki Toda, Heiga Zen, Junichi Yamagishi, and Keiichiro Oura. Speech synthesis based on hidden Markov models. Proceedings of the IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tokuda%2C%20Keiichi%20Nankaku%2C%20Yoshihiko%20Toda%2C%20Tomoki%20Zen%2C%20Heiga%20Speech%20synthesis%20based%20on%20hidden%20Markov%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tokuda%2C%20Keiichi%20Nankaku%2C%20Yoshihiko%20Toda%2C%20Tomoki%20Zen%2C%20Heiga%20Speech%20synthesis%20based%20on%20hidden%20Markov%20models%202013"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv:1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Yuxuan_et+al_2017_a",
            "entry": "Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. arXiv:1703.10135, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10135"
        },
        {
            "id": "Warden_2018_a",
            "entry": "Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv:1804.03209, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03209"
        },
        {
            "id": "Yoshimura_2002_a",
            "entry": "Takayoshi Yoshimura. Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for hmm-based text-to-speech systems. 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoshimura%2C%20Takayoshi%20Simultaneous%20modeling%20of%20phonetic%20and%20prosodic%20parameters%2C%20and%20characteristic%20conversion%20for%20hmm-based%20text-to-speech%20systems%202002"
        },
        {
            "id": "Zen_et+al_2009_a",
            "entry": "Heiga Zen, Keiichi Tokuda, and Alan W Black. Statistical parametric speech synthesis. Speech Communication, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zen%2C%20Heiga%20Tokuda%2C%20Keiichi%20Black%2C%20Alan%20W.%20Statistical%20parametric%20speech%20synthesis%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zen%2C%20Heiga%20Tokuda%2C%20Keiichi%20Black%2C%20Alan%20W.%20Statistical%20parametric%20speech%20synthesis%202009"
        }
    ]
}
