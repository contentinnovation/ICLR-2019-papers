{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "ACCELERATING NONCONVEX LEARNING VIA REPLICA EXCHANGE LANGEVIN DIFFUSION",
        "author": "Yi Chen Department of Industrial Engineering & Management Science Northwestern University Evanston, IL 60201, USA yichen2016@u.northwestern.edu",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJfPFjA9Fm"
        },
        "abstract": "Langevin diffusion is a powerful method for nonconvex optimization, which enables the escape from local minima by injecting noise into the gradient. In particular, the temperature parameter controlling the noise level gives rise to a tradeoff between \u201cglobal exploration\u201d and \u201clocal exploitation\u201d, which correspond to high and low temperatures. To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures. We theoretically analyze the acceleration effect of replica exchange from two perspectives: (i) the convergence in \u03c72-divergence, and (ii) the large deviation principle. Such an acceleration effect allows us to faster approach the global minima. Furthermore, by discretizing the replica exchange Langevin diffusion, we obtain a discrete-time algorithm. For such an algorithm, we quantify its discretization error in theory and demonstrate its acceleration effect in practice."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "global optimization",
            "url": "https://en.wikipedia.org/wiki/global_optimization"
        },
        {
            "term": "probability measure",
            "url": "https://en.wikipedia.org/wiki/probability_measure"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "diffusion process",
            "url": "https://en.wikipedia.org/wiki/diffusion_process"
        }
    ],
    "abbreviations": {
        "LDP": "LARGE DEVIATION PRINCIPLE"
    },
    "highlights": [
        "We consider the problem of minimizing a nonconvex objective function, which arises from numerous machine learning problems such as training neural networks",
        "One idea is to construct a diffusion process whose invariant distribution concentrates around the global minima (<a class=\"ref-link\" id=\"cChiang_et+al_1987_a\" href=\"#rChiang_et+al_1987_a\">Chiang et al, 1987</a>)",
        "The two Langevin diffusions exchange their positions following a specific swapping scheme, which ensures that the invariant distribution concentrates around the global minima",
        "We show that essentially the acceleration effects are the consequences of an additional term in the infinitesimal generator, which is introduced by replica exchange",
        "Our contribution is twofold: (i) We quantify the acceleration effect of the replica exchange Langevin diffusion from two perspectives, that is, the convergence of the \u03c72-divergence and the LARGE DEVIATION PRINCIPLE for empirical measures. We propose a nonconvex optimization algorithm based on the discretization of the replica exchange Langevin diffusion and establish an upper bound for the discretization error.\n2 BASIC IDEA",
        "We lay out the theoretical analysis of replica exchange Langevin diffusion introduced in \u00a72, which demonstrates the acceleration effect of swapping"
    ],
    "key_statements": [
        "We consider the problem of minimizing a nonconvex objective function, which arises from numerous machine learning problems such as training neural networks",
        "One idea is to construct a diffusion process whose invariant distribution concentrates around the global minima (<a class=\"ref-link\" id=\"cChiang_et+al_1987_a\" href=\"#rChiang_et+al_1987_a\">Chiang et al, 1987</a>)",
        "If we run such a diffusion process for a sufficiently long time, we expect to draw from its invariant distribution a point that is close to a global minimum with high probability",
        "The two Langevin diffusions exchange their positions following a specific swapping scheme, which ensures that the invariant distribution concentrates around the global minima",
        "Compared with the standard Langevin diffusion, replica exchange accelerates the rate of convergence to the invariant distribution",
        "It is worth mentioning that, in this paper we focuses on the case of swapping two Langevin diffusions, our theory and method naturally extend to multiple Langevin diffusions",
        "We show that essentially the acceleration effects are the consequences of an additional term in the infinitesimal generator, which is introduced by replica exchange",
        "Our contribution is twofold: (i) We quantify the acceleration effect of the replica exchange Langevin diffusion from two perspectives, that is, the convergence of the \u03c72-divergence and the LARGE DEVIATION PRINCIPLE for empirical measures. We propose a nonconvex optimization algorithm based on the discretization of the replica exchange Langevin diffusion and establish an upper bound for the discretization error.\n2 BASIC IDEA",
        "To bridge the gap between high and low temperatures, in this paper we study an adaptive algorithm called replica exchange Langevin diffusion",
        "We lay out the theoretical analysis of replica exchange Langevin diffusion introduced in \u00a72, which demonstrates the acceleration effect of swapping",
        "For the replica exchange Langevin diffusion, the following theorem shows that Poincareinequality holds under the smoothness and dissipativity conditions in Assumption 3.1, which implies (3.11)",
        "We focus on the empirical measure of the replica exchange Langevin diffusion {Zt}t\u22650 defined in",
        "Note that the two particles in the replica exchange Langevin diffusion swap their positions with a specific rate, which is equivalent in distribution to swapping their temperatures with the same rate (<a class=\"ref-link\" id=\"cDupuis_et+al_2012_a\" href=\"#rDupuis_et+al_2012_a\">Dupuis et al, 2012</a>)"
    ],
    "summary": [
        "We consider the problem of minimizing a nonconvex objective function, which arises from numerous machine learning problems such as training neural networks.",
        "The two Langevin diffusions exchange their positions following a specific swapping scheme, which ensures that the invariant distribution concentrates around the global minima.",
        "Compared with the standard Langevin diffusion, replica exchange accelerates the rate of convergence to the invariant distribution.",
        "Further combined with the Poincareinequality, we establish the exponential rate of convergence of the \u03c72-divergence to zero, where the acceleration effect of replica exchange is characterized by the Poincareconstant.",
        "Our contribution is twofold: (i) We quantify the acceleration effect of the replica exchange Langevin diffusion from two perspectives, that is, the convergence of the \u03c72-divergence and the LDP for empirical measures.",
        "As is shown in Lemma 3.2, the specific form of s(x1, x2) in (2.6) ensures that for any a, the invariant distribution of the replica exchange Langevin diffusion {Zt}t\u22650 is the same as (2.5), that is, as if the two particles are independent.",
        "We lay out the theoretical analysis of replica exchange Langevin diffusion introduced in \u00a72, which demonstrates the acceleration effect of swapping.",
        "We consider the replica exchange Langevin diffusion {Zt}t\u22650 defined by (2.4)(2.6), where Zt = (Zt(1), Zt(2)) denotes the positions of the two particles at time t, and a \u2265 0 is the swapping intensity that controls the frequency of swapping.",
        "The replica exchange Langevin diffusion {Zt}t\u22650 defined in (2.4)-(2.6) is ergodic, which means that, with any initialization, the distribution of Zt converges to the invariant distribution \u03c0.",
        "For the replica exchange Langevin diffusion, the following theorem shows that Poincareinequality holds under the smoothness and dissipativity conditions in Assumption 3.1, which implies (3.11).",
        "To see how swapping accelerates the convergence of the empirical measure towards its invariant distribution, note that the LDP rate function in (3.13) decomposes into two terms.",
        "The first term corresponds to the LDP rate function of replica exchange Langevin diffusion without swapping, that is, a = 0.",
        "Note that the two particles in the replica exchange Langevin diffusion swap their positions with a specific rate, which is equivalent in distribution to swapping their temperatures with the same rate (<a class=\"ref-link\" id=\"cDupuis_et+al_2012_a\" href=\"#rDupuis_et+al_2012_a\">Dupuis et al, 2012</a>).",
        "Note that the temperature swapping Langevin diffusion in (3.14) is equivalent to the replica exchange Langevin diffusion defined in (2.4)-(2.6) in distribution.",
        "We characterize the discretization error by the mean squared error between the continuous-time temperature swapping Langevin diffusion in (3.14) and the continuous-time interpolation in (3.17) of the discrete-time algorithm in (3.15) and (3.16)."
    ],
    "headline": "To attain the advantages of both regimes, we propose to use replica exchange, which swaps between two Langevin diffusions with different temperatures",
    "reference_links": [
        {
            "id": "Bakry_et+al_2008_a",
            "entry": "Dominique Bakry, Franck Barthe, Patrick Cattiaux, and Arnaud Guillin. A simple proof of the Poincareinequality for a large class of probability measures. Electronic Communications in Probability, 13:60\u201366, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bakry%2C%20Dominique%20Barthe%2C%20Franck%20Cattiaux%2C%20Patrick%20Guillin%2C%20Arnaud%20A%20simple%20proof%20of%20the%20Poincareinequality%20for%20a%20large%20class%20of%20probability%20measures%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bakry%2C%20Dominique%20Barthe%2C%20Franck%20Cattiaux%2C%20Patrick%20Guillin%2C%20Arnaud%20A%20simple%20proof%20of%20the%20Poincareinequality%20for%20a%20large%20class%20of%20probability%20measures%202008"
        },
        {
            "id": "Belloni_et+al_2015_a",
            "entry": "Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Conference on Learning Theory, pp. 240\u2013265, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belloni%2C%20Alexandre%20Liang%2C%20Tengyuan%20Narayanan%2C%20Hariharan%20Rakhlin%2C%20Alexander%20Escaping%20the%20local%20minima%20via%20simulated%20annealing%3A%20Optimization%20of%20approximately%20convex%20functions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belloni%2C%20Alexandre%20Liang%2C%20Tengyuan%20Narayanan%2C%20Hariharan%20Rakhlin%2C%20Alexander%20Escaping%20the%20local%20minima%20via%20simulated%20annealing%3A%20Optimization%20of%20approximately%20convex%20functions%202015"
        },
        {
            "id": "Billingsley_2013_a",
            "entry": "Patrick Billingsley. Convergence of probability measures. Wiley, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Billingsley%2C%20Patrick%20Convergence%20of%20probability%20measures%202013"
        },
        {
            "id": "Bubeck_et+al_2015_a",
            "entry": "Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.02564"
        },
        {
            "id": "Cerny_1985_a",
            "entry": "Vladim\u0131r Cerny. Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm. Journal of optimization theory and applications, 45(1):41\u201351, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cerny%2C%20Vladim%C4%B1r%20Thermodynamical%20approach%20to%20the%20traveling%20salesman%20problem%3A%20An%20efficient%20simulation%20algorithm%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cerny%2C%20Vladim%C4%B1r%20Thermodynamical%20approach%20to%20the%20traveling%20salesman%20problem%3A%20An%20efficient%20simulation%20algorithm%201985"
        },
        {
            "id": "Cheng_et+al_2017_a",
            "entry": "Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03663"
        },
        {
            "id": "Chiang_et+al_1987_a",
            "entry": "Tzuu-Shuh Chiang, Chii-Ruey Hwang, and Shuenn Jyi Sheu. Diffusion for global optimization in Rn. SIAM Journal on Control and Optimization, 25(3):737\u2013753, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiang%2C%20Tzuu-Shuh%20Hwang%2C%20Chii-Ruey%20Sheu%2C%20Shuenn%20Jyi%20Diffusion%20for%20global%20optimization%20in%20Rn%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiang%2C%20Tzuu-Shuh%20Hwang%2C%20Chii-Ruey%20Sheu%2C%20Shuenn%20Jyi%20Diffusion%20for%20global%20optimization%20in%20Rn%201987"
        },
        {
            "id": "Dalalyan_2009_a",
            "entry": "Arnak Dalalyan and Alexandre B Tsybakov. Sparse regression learning by aggregation and Langevin Monte Carlo. arXiv preprint arXiv:0903.1223, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/0903.1223"
        },
        {
            "id": "Dalalyan_2017_a",
            "entry": "Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651\u2013676, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalalyan%2C%20Arnak%20S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalalyan%2C%20Arnak%20S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017"
        },
        {
            "id": "Monroe_1975_a",
            "entry": "Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. Communications on Pure and Applied Mathematics, 28(1):1\u201347, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donsker%2C%20Monroe%20D%20Varadhan%2C%20SR%20Srinivasa%20Asymptotic%20evaluation%20of%20certain%20Markov%20process%20expectations%20for%20large%20time%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donsker%2C%20Monroe%20D%20Varadhan%2C%20SR%20Srinivasa%20Asymptotic%20evaluation%20of%20certain%20Markov%20process%20expectations%20for%20large%20time%201975"
        },
        {
            "id": "Dragomir_2003_a",
            "entry": "Sever Silvestru Dragomir. Some Gronwall-type inequalities and applications. Nova Science Publishers, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dragomir%2C%20Sever%20Silvestru%20Some%20Gronwall-type%20inequalities%20and%20applications%202003"
        },
        {
            "id": "Dupuis_et+al_2012_a",
            "entry": "Paul Dupuis, Yufei Liu, Nuria Plattner, and Jimmie D Doll. On the infinite swapping limit for parallel tempering. Multiscale Modeling & Simulation, 10(3):986\u20131022, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dupuis%2C%20Paul%20Liu%2C%20Yufei%20Plattner%2C%20Nuria%20Doll%2C%20Jimmie%20D.%20On%20the%20infinite%20swapping%20limit%20for%20parallel%20tempering%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dupuis%2C%20Paul%20Liu%2C%20Yufei%20Plattner%2C%20Nuria%20Doll%2C%20Jimmie%20D.%20On%20the%20infinite%20swapping%20limit%20for%20parallel%20tempering%202012"
        },
        {
            "id": "Durmus_2017_a",
            "entry": "Alain Durmus, Eric Moulines, et al. Nonasymptotic convergence analysis for the unadjusted Langevin algorithm. The Annals of Applied Probability, 27(3):1551\u20131587, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durmus%2C%20Alain%20Moulines%2C%20Eric%20Nonasymptotic%20convergence%20analysis%20for%20the%20unadjusted%20Langevin%20algorithm%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durmus%2C%20Alain%20Moulines%2C%20Eric%20Nonasymptotic%20convergence%20analysis%20for%20the%20unadjusted%20Langevin%20algorithm%202017"
        },
        {
            "id": "Earl_2005_a",
            "entry": "David J Earl and Michael W Deem. Parallel tempering: Theory, applications, and new perspectives. Physical Chemistry Chemical Physics, 7(23):3910\u20133916, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Earl%2C%20David%20J.%20Deem%2C%20Michael%20W.%20Parallel%20tempering%3A%20Theory%2C%20applications%2C%20and%20new%20perspectives%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Earl%2C%20David%20J.%20Deem%2C%20Michael%20W.%20Parallel%20tempering%3A%20Theory%2C%20applications%2C%20and%20new%20perspectives%202005"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u008aonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%C2%8Aonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%C2%8Aonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Holden Lee, and Andrej Risteski. Beyond log-concavity: Provable guarantees for sampling multimodal distributions using simulated tempering Langevin Monte Carlo. arXiv preprint arXiv:1710.02736, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.02736"
        },
        {
            "id": "Geman_1986_a",
            "entry": "Stuart Geman and Chii-Ruey Hwang. Diffusions for global optimization. SIAM Journal on Control and Optimization, 24(5):1031\u20131043, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geman%2C%20Stuart%20Hwang%2C%20Chii-Ruey%20Diffusions%20for%20global%20optimization%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geman%2C%20Stuart%20Hwang%2C%20Chii-Ruey%20Diffusions%20for%20global%20optimization%201986"
        },
        {
            "id": "Geyer_1995_a",
            "entry": "Charles J Geyer and Elizabeth A Thompson. Annealing Markov chain Monte Carlo with applications to ancestral inference. Journal of the American Statistical Association, 90(431):909\u2013920, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geyer%2C%20Charles%20J.%20Thompson%2C%20Elizabeth%20A.%20Annealing%20Markov%20chain%20Monte%20Carlo%20with%20applications%20to%20ancestral%20inference%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geyer%2C%20Charles%20J.%20Thompson%2C%20Elizabeth%20A.%20Annealing%20Markov%20chain%20Monte%20Carlo%20with%20applications%20to%20ancestral%20inference%201995"
        },
        {
            "id": "Gidas_1985_a",
            "entry": "Basilis Gidas. Global optimization via the Langevin equation. In 24th IEEE Conference on Decision and Control, 1985, volume 24, pp. 774\u2013778. IEEE, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidas%2C%20Basilis%20Global%20optimization%20via%20the%20Langevin%20equation%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidas%2C%20Basilis%20Global%20optimization%20via%20the%20Langevin%20equation%201985"
        },
        {
            "id": "Hale_2010_a",
            "entry": "Jack K Hale. Asymptotic behavior of dissipative systems. Number 25. American Mathematical Society, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hale%2C%20Jack%20K.%20Asymptotic%20behavior%20of%20dissipative%20systems.%20Number%2025%202010"
        },
        {
            "id": "Hazan_et+al_2016_a",
            "entry": "Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-convex problems. In International Conference on Machine Learning, pp. 1833\u20131841, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016"
        },
        {
            "id": "Hwang_1980_a",
            "entry": "Chii-Ruey Hwang. Laplace\u2019s method revisited: Weak convergence of probability measures. The Annals of Probability, pp. 1177\u20131182, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hwang%2C%20Chii-Ruey%20Laplace%E2%80%99s%20method%20revisited%3A%20Weak%20convergence%20of%20probability%20measures%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hwang%2C%20Chii-Ruey%20Laplace%E2%80%99s%20method%20revisited%3A%20Weak%20convergence%20of%20probability%20measures%201980"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Marinari_1992_a",
            "entry": "Enzo Marinari and Giorgio Parisi. Simulated tempering: a new Monte Carlo scheme. EPL (Europhysics Letters), 19(6):451, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marinari%2C%20Enzo%20Parisi%2C%20Giorgio%20Simulated%20tempering%3A%20a%20new%20Monte%20Carlo%20scheme%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marinari%2C%20Enzo%20Parisi%2C%20Giorgio%20Simulated%20tempering%3A%20a%20new%20Monte%20Carlo%20scheme%201992"
        },
        {
            "id": "Phillips_1957_a",
            "entry": "Ralph Saul Phillips and Einar Hille. Functional analysis and semi-groups. RI, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Phillips%2C%20Ralph%20Saul%20Hille%2C%20Einar%20Functional%20analysis%20and%20semi-groups%201957"
        },
        {
            "id": "Polyak_1992_a",
            "entry": "Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20T.%20Juditsky%2C%20Anatoli%20B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20T.%20Juditsky%2C%20Anatoli%20B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992"
        },
        {
            "id": "Raginsky_et+al_2017_a",
            "entry": "Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: A nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03849"
        },
        {
            "id": "Sindhikara_et+al_2008_a",
            "entry": "Daniel Sindhikara, Yilin Meng, and Adrian E Roitberg. Exchange frequency in replica exchange molecular dynamics. The Journal of chemical physics, 128(2):01B609, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sindhikara%2C%20Daniel%20Meng%2C%20Yilin%20Roitberg%2C%20Adrian%20E.%20Exchange%20frequency%20in%20replica%20exchange%20molecular%20dynamics%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sindhikara%2C%20Daniel%20Meng%2C%20Yilin%20Roitberg%2C%20Adrian%20E.%20Exchange%20frequency%20in%20replica%20exchange%20molecular%20dynamics%202008"
        },
        {
            "id": "Woodard_et+al_2009_a",
            "entry": "Dawn B Woodard, Scott C Schmidler, Mark Huber, et al. Conditions for rapid mixing of parallel and simulated tempering on multimodal distributions. The Annals of Applied Probability, 19(2): 617\u2013640, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodard%2C%20Dawn%20B.%20Schmidler%2C%20Scott%20C.%20Huber%2C%20Mark%20Conditions%20for%20rapid%20mixing%20of%20parallel%20and%20simulated%20tempering%20on%20multimodal%20distributions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodard%2C%20Dawn%20B.%20Schmidler%2C%20Scott%20C.%20Huber%2C%20Mark%20Conditions%20for%20rapid%20mixing%20of%20parallel%20and%20simulated%20tempering%20on%20multimodal%20distributions%202009"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 3126\u20133137, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Pan%20Chen%2C%20Jinghui%20Zou%2C%20Difan%20Gu%2C%20Quanquan%20Global%20convergence%20of%20langevin%20dynamics%20based%20algorithms%20for%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Pan%20Chen%2C%20Jinghui%20Zou%2C%20Difan%20Gu%2C%20Quanquan%20Global%20convergence%20of%20langevin%20dynamics%20based%20algorithms%20for%20nonconvex%20optimization%202018"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Brando Miranda, Noah Golowich, and Tomaso Poggio. Theory of deep learning iib: Optimization properties of sgd. arXiv preprint arXiv:1801.02254, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02254"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. arXiv preprint arXiv:1702.05575, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05575"
        },
        {
            "id": "Zheng_2003_a",
            "entry": "Zhongrong Zheng. On swapping and simulated tempering algorithms. Stochastic Processes and their Applications, 104(1):131\u2013154, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Zhongrong%20On%20swapping%20and%20simulated%20tempering%20algorithms.%20Stochastic%20Processes%20and%20their%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Zhongrong%20On%20swapping%20and%20simulated%20tempering%20algorithms.%20Stochastic%20Processes%20and%20their%202003"
        },
        {
            "id": "Zou_et+al_0000_a",
            "entry": "Difan Zou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced hamilton monte carlo methods. arXiv preprint arXiv:1802.04791, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04791"
        },
        {
            "id": "Zou_et+al_2018_a",
            "entry": "Difan Zou, Pan Xu, and Quanquan Gu. Subsampled stochastic variance-reduced gradient langevin dynamics. In International Conference on Uncertainty in Artificial Intelligence, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Difan%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Subsampled%20stochastic%20variance-reduced%20gradient%20langevin%20dynamics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Difan%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Subsampled%20stochastic%20variance-reduced%20gradient%20langevin%20dynamics%202018"
        },
        {
            "id": "Zou_et+al_2019_a",
            "entry": "Difan Zou, Pan Xu, and Quanquan Gu. Sampling from non-log-concave distributions via variancereduced gradient langevin dynamics. In International Conference on Artificial Intelligence and Statistics, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Difan%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Sampling%20from%20non-log-concave%20distributions%20via%20variancereduced%20gradient%20langevin%20dynamics%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Difan%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Sampling%20from%20non-log-concave%20distributions%20via%20variancereduced%20gradient%20langevin%20dynamics%202019"
        },
        {
            "id": "Flat_2016_a",
            "entry": "Flat and sharp minima: The Langevin diffusion yields an invariant distribution that concentrates around the global minima. In particular, as illustrated in Keskar et al. (2016), flat minima generally achieves better generalization than sharp ones. Meanwhile, as observed in Zhang et al. (2018), the invariant distribution of the Langevin diffusion biases more towards the flat minima than the sharp minima. Intuitively, this observation follows from locally integrating the density of the invariant distribution, which is proportional to exp{\u2212U (x)/\u03c4 }. As specified in (2.3), the invariant distribution of the replica exchange Langevin diffusion takes the same form, and hence processes the same desired property of biasing towards the flat minima.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flat%20and%20sharp%20minima%20The%20Langevin%20diffusion%20yields%20an%20invariant%20distribution%20that%20concentrates%20around%20the%20global%20minima%20In%20particular%20as%20illustrated%20in%20Keskar%20et%20al%202016%20flat%20minima%20generally%20achieves%20better%20generalization%20than%20sharp%20ones%20Meanwhile%20as%20observed%20in%20Zhang%20et%20al%202018%20the%20invariant%20distribution%20of%20the%20Langevin%20diffusion%20biases%20more%20towards%20the%20flat%20minima%20than%20the%20sharp%20minima%20Intuitively%20this%20observation%20follows%20from%20locally%20integrating%20the%20density%20of%20the%20invariant%20distribution%20which%20is%20proportional%20to%20expU%20x%CF%84%20%20As%20specified%20in%2023%20the%20invariant%20distribution%20of%20the%20replica%20exchange%20Langevin%20diffusion%20takes%20the%20same%20form%20and%20hence%20processes%20the%20same%20desired%20property%20of%20biasing%20towards%20the%20flat%20minima",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flat%20and%20sharp%20minima%20The%20Langevin%20diffusion%20yields%20an%20invariant%20distribution%20that%20concentrates%20around%20the%20global%20minima%20In%20particular%20as%20illustrated%20in%20Keskar%20et%20al%202016%20flat%20minima%20generally%20achieves%20better%20generalization%20than%20sharp%20ones%20Meanwhile%20as%20observed%20in%20Zhang%20et%20al%202018%20the%20invariant%20distribution%20of%20the%20Langevin%20diffusion%20biases%20more%20towards%20the%20flat%20minima%20than%20the%20sharp%20minima%20Intuitively%20this%20observation%20follows%20from%20locally%20integrating%20the%20density%20of%20the%20invariant%20distribution%20which%20is%20proportional%20to%20expU%20x%CF%84%20%20As%20specified%20in%2023%20the%20invariant%20distribution%20of%20the%20replica%20exchange%20Langevin%20diffusion%20takes%20the%20same%20form%20and%20hence%20processes%20the%20same%20desired%20property%20of%20biasing%20towards%20the%20flat%20minima"
        },
        {
            "id": "2",
            "entry": "(2) The map t \u2192 Pt is continuous in the sense that, for all f \u2208 C(Rd), t \u2192 Pt(f ) is a continuous map from R+ to C(Rd).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20map%20t%20%20Pt%20is%20continuous%20in%20the%20sense%20that%20for%20all%20f%20%20CRd%20t%20%20Ptf%20%20is%20a%20continuous%20map%20from%20R%20to%20CRd"
        },
        {
            "id": "3",
            "entry": "(3) For all f \u2208 C(Rd) and s, t > 0, we have Ps+t(f ) = Ps(Pt(f )).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20all%20f%20%20CRd%20and%20s%20t%20%200%20we%20have%20Pstf%20%20%20PsPtf"
        },
        {
            "id": "5",
            "entry": "(5) For all f \u2265 0, we have Pt(f ) \u2265 0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20all%20f%20%200%20we%20have%20Ptf%20%20%200"
        },
        {
            "id": "Given_2013_a",
            "entry": "Given a Markov semigroup, we can define its infinitesimal generator, a linear operator that describes the Markov semigroup\u2019s behavior for an infinitesimal t. Formally, we have the following definition. See, e.g., Revuz & Yor (2013) for more details.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Given%20a%20Markov%20semigroup%20we%20can%20define%20its%20infinitesimal%20generator%20a%20linear%20operator%20that%20describes%20the%20Markov%20semigroups%20behavior%20for%20an%20infinitesimal%20t%20Formally%20we%20have%20the%20following%20definition%20See%20eg%20Revuz%20%20Yor%202013%20for%20more%20details",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Given%20a%20Markov%20semigroup%20we%20can%20define%20its%20infinitesimal%20generator%20a%20linear%20operator%20that%20describes%20the%20Markov%20semigroups%20behavior%20for%20an%20infinitesimal%20t%20Formally%20we%20have%20the%20following%20definition%20See%20eg%20Revuz%20%20Yor%202013%20for%20more%20details"
        }
    ]
}
