{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DATA-DEPENDENT CORESETS FOR COMPRESSING NEURAL NETWORKS WITH APPLICATIONS TO GENERALIZATION BOUNDS",
        "author": "Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, Daniela Rus",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJfwJ2A5KX"
        },
        "abstract": "We present an efficient coresets-based neural network compression algorithm that sparsifies the parameters of a trained fully-connected neural network in a manner that provably approximates the network\u2019s output. Our approach is based on an importance sampling scheme that judiciously defines a sampling distribution over the neural network parameters, and as a result, retains parameters of high importance while discarding redundant ones. We leverage a novel, empirical notion of sensitivity and extend traditional coreset constructions to the application of compressing parameters. Our theoretical analysis establishes guarantees on the size and accuracy of the resulting compressed network and gives rise to generalization bounds that may provide new insights into the generalization properties of neural networks. We demonstrate the practical effectiveness of our algorithm on a variety of neural network configurations and real-world data sets."
    },
    "keywords": [
        {
            "term": "Cumulative Distribution Function",
            "url": "https://en.wikipedia.org/wiki/Cumulative_Distribution_Function"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Singular Value Decomposition",
            "url": "https://en.wikipedia.org/wiki/Singular_Value_Decomposition"
        },
        {
            "term": "Convolutional Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "compression algorithm",
            "url": "https://en.wikipedia.org/wiki/compression_algorithm"
        }
    ],
    "abbreviations": {
        "SVD": "Singular Value Decomposition",
        "ReLU": "Rectified Linear Unit",
        "CDF": "Cumulative Distribution Function",
        "CNNs": "Convolutional Neural Networks"
    },
    "highlights": [
        "Large-scale neural networks have demonstrated unprecedented empirical success in high-impact applications such as object classification, speech recognition, computer vision, and natural language processing",
        "We evaluate the practical effectiveness of our compression algorithm on popular benchmark data sets (MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>), FashionMNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al, 2017</a>), and CIFAR10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton, 2009</a>)) and varying fully-connected trained neural network configurations: 2 to 5 hidden layers, 100 to 1000 hidden units, either fixed hidden sizes or decreasing hidden size denoted by pyramid in the figures",
        "Reducing the number of non-zero parameters of a network, i.e., in sparsifying the weight matrices, to that of uniform sampling, Singular Value Decomposition (SVD), and current state-of-the-art sampling schemes for matrix sparsification (<a class=\"ref-link\" id=\"cDrineas_2011_a\" href=\"#rDrineas_2011_a\">Drineas & Zouzias, 2011</a>; <a class=\"ref-link\" id=\"cAchlioptas_et+al_2013_a\" href=\"#rAchlioptas_et+al_2013_a\">Achlioptas et al, 2013</a>; <a class=\"ref-link\" id=\"cKundu_2014_a\" href=\"#rKundu_2014_a\">Kundu & Drineas, 2014</a>), which are based on matrix norms \u2013 1 and 2 (Frobenius)",
        "We presented a coresets-based neural network compression algorithm for compressing the parameters of a trained fully-connected neural network in a manner that approximately preserves the network\u2019s output",
        "Our method and analysis extend traditional coreset constructions to the application of compressing parameters, which may be of independent interest",
        "We empirically demonstrated the practical effectiveness of our compression algorithm on a variety of neural network configurations and real-world data sets"
    ],
    "key_statements": [
        "Large-scale neural networks have demonstrated unprecedented empirical success in high-impact applications such as object classification, speech recognition, computer vision, and natural language processing",
        "Our work aims to simultaneously introduce a practical algorithm for compressing neural network parameters with provable guarantees and close the research gap in prior coresets work, which has predominantly focused on compressing input data points",
        "An efficient neural network compression algorithm, CoreNet, based on our extended coreset approach that sparsifies the parameters via importance sampling of weighted edges.\n3",
        "We present our first assumption that pertains to the Cumulative Distribution Function (CDF) of the relative importance random variable",
        "We evaluate the practical effectiveness of our compression algorithm on popular benchmark data sets (MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>), FashionMNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al, 2017</a>), and CIFAR10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton, 2009</a>)) and varying fully-connected trained neural network configurations: 2 to 5 hidden layers, 100 to 1000 hidden units, either fixed hidden sizes or decreasing hidden size denoted by pyramid in the figures",
        "Reducing the number of non-zero parameters of a network, i.e., in sparsifying the weight matrices, to that of uniform sampling, Singular Value Decomposition (SVD), and current state-of-the-art sampling schemes for matrix sparsification (<a class=\"ref-link\" id=\"cDrineas_2011_a\" href=\"#rDrineas_2011_a\">Drineas & Zouzias, 2011</a>; <a class=\"ref-link\" id=\"cAchlioptas_et+al_2013_a\" href=\"#rAchlioptas_et+al_2013_a\">Achlioptas et al, 2013</a>; <a class=\"ref-link\" id=\"cKundu_2014_a\" href=\"#rKundu_2014_a\">Kundu & Drineas, 2014</a>), which are based on matrix norms \u2013 1 and 2 (Frobenius)",
        "Experiment Setup We compare against three variations of our compression algorithm: (i) sole edge sampling (CoreNet), edge sampling with neuron pruning (CoreNet+), and edge sampling with neuron pruning and amplification (CoreNet++)",
        "We presented a coresets-based neural network compression algorithm for compressing the parameters of a trained fully-connected neural network in a manner that approximately preserves the network\u2019s output",
        "Our method and analysis extend traditional coreset constructions to the application of compressing parameters, which may be of independent interest",
        "We empirically demonstrated the practical effectiveness of our compression algorithm on a variety of neural network configurations and real-world data sets"
    ],
    "summary": [
        "Large-scale neural networks have demonstrated unprecedented empirical success in high-impact applications such as object classification, speech recognition, computer vision, and natural language processing.",
        "We introduce a neural network compression approach based on identifying and removing weighted edges with low relative importance via coresets, small weighted subsets of the original set that approximate the pertinent cost function.",
        "2. An efficient neural network compression algorithm, CoreNet, based on our extended coreset approach that sparsifies the parameters via importance sampling of weighted edges.",
        "4. Analytical results establishing guarantees on the approximation accuracy, size, and generalization of the compressed neural network.",
        "5. Evaluations on real-world data sets that demonstrate the practical effectiveness of our algorithm in compressing neural network parameters and validate our theoretical results.",
        "Building upon the work of <a class=\"ref-link\" id=\"cArora_et+al_2018_a\" href=\"#rArora_et+al_2018_a\">Arora et al (2018</a>), we extend our theoretical compression results to establish novel generalization bounds for fully-connected neural networks.",
        "Our method (Alg. 1) hinges on the insight that a validation set of data points P i.\u223ci.d. Dn can be used to approximate the relative importance, i.e., sensitivity, of each weighted edge with respect to the input data distribution D.",
        "11: return w; Subsequently, we apply our core sampling scheme to sparsify the set of incoming weighted edges to each neuron in all layers (Lines 7-13).",
        "Assumption 1 is a technical assumption on the ratio of the weighted activations that will enable us to rule out pathological problem instances where the relative importance of each edge cannot be well-approximated using a small number of data points S \u2286 P.",
        "We evaluate the practical effectiveness of our compression algorithm on popular benchmark data sets (MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>), FashionMNIST (<a class=\"ref-link\" id=\"cXiao_et+al_2017_a\" href=\"#rXiao_et+al_2017_a\">Xiao et al, 2017</a>), and CIFAR10 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky & Hinton, 2009</a>)) and varying fully-connected trained neural network configurations: 2 to 5 hidden layers, 100 to 1000 hidden units, either fixed hidden sizes or decreasing hidden size denoted by pyramid in the figures.",
        "Reducing the number of non-zero parameters of a network, i.e., in sparsifying the weight matrices, to that of uniform sampling, Singular Value Decomposition (SVD), and current state-of-the-art sampling schemes for matrix sparsification (<a class=\"ref-link\" id=\"cDrineas_2011_a\" href=\"#rDrineas_2011_a\">Drineas & Zouzias, 2011</a>; <a class=\"ref-link\" id=\"cAchlioptas_et+al_2013_a\" href=\"#rAchlioptas_et+al_2013_a\">Achlioptas et al, 2013</a>; <a class=\"ref-link\" id=\"cKundu_2014_a\" href=\"#rKundu_2014_a\">Kundu & Drineas, 2014</a>), which are based on matrix norms \u2013 1 and 2 (Frobenius).",
        "The results presented further suggest that empirical sensitivity can effectively capture the relative importance of neural network parameters, leading to a more informed importance sampling scheme.",
        "Our work distinguishes itself from prior approaches in that it establishes theoretical guarantees on the approximation accuracy and size of the generated compressed network.",
        "We conjecture that our compression algorithm can be used to reduce storage requirements of neural network models and enable fast inference in practical settings"
    ],
    "headline": "We present an efficient coresets-based neural network compression algorithm that sparsifies the parameters of a trained fully-connected neural network in a manner that provably approximates the network\u2019s output",
    "reference_links": [
        {
            "id": "Achlioptas_et+al_2013_a",
            "entry": "Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Matrix entry-wise sampling: Simple is best. Submitted to KDD, 2013(1.1):1\u20134, 2013. 8, 27",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achlioptas%2C%20Dimitris%20Karnin%2C%20Zohar%20Liberty%2C%20Edo%20Matrix%20entry-wise%20sampling%3A%20Simple%20is%20best%202013"
        },
        {
            "id": "Agarwal_et+al_2005_a",
            "entry": "Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via coresets. Combinatorial and computational geometry, 52:1\u201330, 2005. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Pankaj%20K.%20Har-Peled%2C%20Sariel%20Varadarajan%2C%20Kasturi%20R.%20Geometric%20approximation%20via%20coresets%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Pankaj%20K.%20Har-Peled%2C%20Sariel%20Varadarajan%2C%20Kasturi%20R.%20Geometric%20approximation%20via%20coresets%202005"
        },
        {
            "id": "Aghasi_et+al_2017_a",
            "entry": "Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. In Advances in Neural Information Processing Systems, pp. 3180\u20133189, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aghasi%2C%20Alireza%20Abdi%2C%20Afshin%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20Convex%20pruning%20of%20deep%20neural%20networks%20with%20performance%20guarantee%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aghasi%2C%20Alireza%20Abdi%2C%20Afshin%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20Convex%20pruning%20of%20deep%20neural%20networks%20with%20performance%20guarantee%202017"
        },
        {
            "id": "Alvarez_2017_a",
            "entry": "Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In Advances in Neural Information Processing Systems, pp. 856\u2013867, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez%2C%20Jose%20M.%20Salzmann%2C%20Mathieu%20Compression-aware%20training%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvarez%2C%20Jose%20M.%20Salzmann%2C%20Mathieu%20Compression-aware%20training%20of%20deep%20networks%202017"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018. 2, 7",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Bachem_et+al_2017_a",
            "entry": "Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017. 2",
            "arxiv_url": "https://arxiv.org/pdf/1703.06476"
        },
        {
            "id": "Badrinarayanan_et+al_2015_a",
            "entry": "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. CoRR, abs/1511.00561, 2015. URL http://arxiv.org/abs/1511.00561.1",
            "url": "http://arxiv.org/abs/1511.00561.1",
            "arxiv_url": "https://arxiv.org/pdf/1511.00561"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Braverman_et+al_2016_a",
            "entry": "Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016. 1, 2",
            "arxiv_url": "https://arxiv.org/pdf/1612.00889"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing convolutional neural networks. CoRR, abs/1506.04449, 2015a. URL http://arxiv.org/abs/1506.04449.2",
            "url": "http://arxiv.org/abs/1506.04449.2",
            "arxiv_url": "https://arxiv.org/pdf/1506.04449"
        },
        {
            "id": "Chen_et+al_2015_b",
            "entry": "Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. CoRR, abs/1504.04788, 2015b. URL http://arxiv.org/abs/1504.04788.2",
            "url": "http://arxiv.org/abs/1504.04788.2",
            "arxiv_url": "https://arxiv.org/pdf/1504.04788"
        },
        {
            "id": "Cheng_et+al_2015_a",
            "entry": "Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2857\u20132865, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Yu%20Yu%2C%20Felix%20X.%20Feris%2C%20Rogerio%20S.%20Kumar%2C%20Sanjiv%20and%20Shi-Fu%20Chang.%20An%20exploration%20of%20parameter%20redundancy%20in%20deep%20networks%20with%20circulant%20projections%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Yu%20Yu%2C%20Felix%20X.%20Feris%2C%20Rogerio%20S.%20Kumar%2C%20Sanjiv%20and%20Shi-Fu%20Chang.%20An%20exploration%20of%20parameter%20redundancy%20in%20deep%20networks%20with%20circulant%20projections%202015"
        },
        {
            "id": "Choromanska_et+al_2016_a",
            "entry": "Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, and Yann LeCun. Binary embeddings with structured hashed projections. In International Conference on Machine Learning, pp. 344\u2013353, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Choromanski%2C%20Krzysztof%20Bojarski%2C%20Mariusz%20Jebara%2C%20Tony%20Binary%20embeddings%20with%20structured%20hashed%20projections%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Choromanski%2C%20Krzysztof%20Bojarski%2C%20Mariusz%20Jebara%2C%20Tony%20Binary%20embeddings%20with%20structured%20hashed%20projections%202016"
        },
        {
            "id": "Denil_et+al_2013_a",
            "entry": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2148\u20132156. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf.2",
            "url": "http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf.2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Ranzato%2C%20Marc%20Aurelio%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "Denton_et+al_2014_a",
            "entry": "Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. CoRR, abs/1404.0736, 2014. URL http://arxiv.org/abs/1404.0736.2",
            "url": "http://arxiv.org/abs/1404.0736.2",
            "arxiv_url": "https://arxiv.org/pdf/1404.0736"
        },
        {
            "id": "Doerr_2018_a",
            "entry": "Benjamin Doerr. Probabilistic tools for the analysis of randomized optimization heuristics. arXiv preprint arXiv:1801.06733, 2018. 23",
            "arxiv_url": "https://arxiv.org/pdf/1801.06733"
        },
        {
            "id": "Dong_et+al_2017_a",
            "entry": "Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4860\u20134874, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layer-wise%20optimal%20brain%20surgeon%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layer-wise%20optimal%20brain%20surgeon%202017"
        },
        {
            "id": "Drineas_2011_a",
            "entry": "Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrixvalued bernstein inequality. Information Processing Letters, 111(8):385\u2013389, 2011. 8, 28",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Zouzias%2C%20Anastasios%20A%20note%20on%20element-wise%20matrix%20sparsification%20via%20a%20matrixvalued%20bernstein%20inequality%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Zouzias%2C%20Anastasios%20A%20note%20on%20element-wise%20matrix%20sparsification%20via%20a%20matrixvalued%20bernstein%20inequality%202011"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. 2",
            "arxiv_url": "https://arxiv.org/pdf/1703.11008"
        },
        {
            "id": "Feldman_2011_a",
            "entry": "Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 569\u2013578. ACM, 2011. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Dan%20Langberg%2C%20Michael%20A%20unified%20framework%20for%20approximating%20and%20clustering%20data%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Dan%20Langberg%2C%20Michael%20A%20unified%20framework%20for%20approximating%20and%20clustering%20data%202011"
        },
        {
            "id": "Gong_et+al_2014_a",
            "entry": "Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing Deep Convolutional Networks using Vector Quantization. arXiv preprint arXiv:1412.6115, 2014. 2",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL http://arxiv.org/abs/1510.00149.2, 29",
            "url": "http://arxiv.org/abs/1510.00149.2",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "Huggins_et+al_2016_a",
            "entry": "Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic regression. arXiv preprint arXiv:1605.06423, 2016. 2",
            "arxiv_url": "https://arxiv.org/pdf/1605.06423"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. 2",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Ioannou_et+al_2015_a",
            "entry": "Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training cnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744, 2015. 2",
            "arxiv_url": "https://arxiv.org/pdf/1511.06744"
        },
        {
            "id": "Jaderberg_et+al_2014_a",
            "entry": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. 2",
            "arxiv_url": "https://arxiv.org/pdf/1405.3866"
        },
        {
            "id": "Johnson_1984_a",
            "entry": "William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984"
        },
        {
            "id": "Kim_et+al_2015_a",
            "entry": "Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530, 2015. 2",
            "arxiv_url": "https://arxiv.org/pdf/1511.06530"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 7, 27",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097\u2013 1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.1",
            "url": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.1",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kundu_2014_a",
            "entry": "Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix sparsification. arXiv preprint arXiv:1404.0320, 2014. 8, 28",
            "arxiv_url": "https://arxiv.org/pdf/1404.0320"
        },
        {
            "id": "Langberg_2010_a",
            "entry": "Michael Langberg and Leonard J Schulman. Universal \u03b5-approximators for integrals. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 598\u2013607. SIAM, 2010. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langberg%2C%20Michael%20Schulman%2C%20Leonard%20J.%20Universal%20%CE%B5-approximators%20for%20integrals%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langberg%2C%20Michael%20Schulman%2C%20Leonard%20J.%20Universal%20%CE%B5-approximators%20for%20integrals%202010"
        },
        {
            "id": "Lebedev_2016_a",
            "entry": "Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 2554\u20132564. IEEE, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lebedev%2C%20Vadim%20Lempitsky%2C%20Victor%20Fast%20convnets%20using%20group-wise%20brain%20damage%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lebedev%2C%20Vadim%20Lempitsky%2C%20Victor%20Fast%20convnets%20using%20group-wise%20brain%20damage%202016"
        },
        {
            "id": "Lecun_et+al_1990_a",
            "entry": "Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pp. 598\u2013605, 1990. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. 7, 27",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Advances in Neural Information Processing Systems, pp. 2178\u20132188, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Ji%20Rao%2C%20Yongming%20Lu%2C%20Jiwen%20Zhou%2C%20Jie%20Runtime%20neural%20pruning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Ji%20Rao%2C%20Yongming%20Lu%2C%20Jiwen%20Zhou%2C%20Jie%20Runtime%20neural%20pruning%202017"
        },
        {
            "id": "Long_et+al_2015_a",
            "entry": "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015-06-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015-06-01"
        },
        {
            "id": "Molina_et+al_2018_a",
            "entry": "Alejandro Molina, Alexander Munteanu, and Kristian Kersting. Core dependency networks. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI). AAAI Press Google Scholar, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molina%2C%20Alejandro%20Munteanu%2C%20Alexander%20Kersting%2C%20Kristian%20Core%20dependency%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molina%2C%20Alejandro%20Munteanu%2C%20Alexander%20Kersting%2C%20Kristian%20Core%20dependency%20networks%202018"
        },
        {
            "id": "Munteanu_2018_a",
            "entry": "Alexander Munteanu and Chris Schwiegelshohn. Coresets-methods and history: A theoreticians design pattern for approximation and streaming algorithms. KI-Kunstliche Intelligenz, 32(1):37\u201353, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munteanu%2C%20Alexander%20Schwiegelshohn%2C%20Chris%20Coresets-methods%20and%20history%3A%20A%20theoreticians%20design%20pattern%20for%20approximation%20and%20streaming%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munteanu%2C%20Alexander%20Schwiegelshohn%2C%20Chris%20Coresets-methods%20and%20history%3A%20A%20theoreticians%20design%20pattern%20for%20approximation%20and%20streaming%20algorithms%202018"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a. 2",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Neyshabur_et+al_2017_b",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949\u20135958, 2017b. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017. 27",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017%2027",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017%2027"
        },
        {
            "id": "Shi_et+al_2009_a",
            "entry": "Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and SVN Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research, 10(Nov):2615\u20132637, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Qinfeng%20Petterson%2C%20James%20Dror%2C%20Gideon%20Langford%2C%20John%20Hash%20kernels%20for%20structured%20data%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Qinfeng%20Petterson%2C%20James%20Dror%2C%20Gideon%20Langford%2C%20John%20Hash%20kernels%20for%20structured%20data%202009"
        },
        {
            "id": "Sindhwani_et+al_2015_a",
            "entry": "Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pp. 3088\u20133096, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20Kumar%2C%20Sanjiv%20Structured%20transforms%20for%20small-footprint%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20Kumar%2C%20Sanjiv%20Structured%20transforms%20for%20small-footprint%20deep%20learning%202015"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015. 2",
            "arxiv_url": "https://arxiv.org/pdf/1511.06067"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017. 2",
            "arxiv_url": "https://arxiv.org/pdf/1702.04008"
        },
        {
            "id": "Vershynin_2016_a",
            "entry": "Roman Vershynin. High-dimensional probability. An Introduction with Applications, 2016. 6, 18",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20High-dimensional%20probability.%20An%20Introduction%20with%20Applications%202016"
        },
        {
            "id": "Weinberger_et+al_2009_a",
            "entry": "Kilian Q. Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alexander J. Smola. Feature hashing for large scale multitask learning. CoRR, abs/0902.2206, 2009. URL http://arxiv.org/abs/0902.2206.2",
            "url": "http://arxiv.org/abs/0902.2206.2",
            "arxiv_url": "https://arxiv.org/pdf/0902.2206"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074\u20132082, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized Convolutional Neural Networks for Mobile Devices. In Proceedings of the Inernational Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20Convolutional%20Neural%20Networks%20for%20Mobile%20Devices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20Convolutional%20Neural%20Networks%20for%20Mobile%20Devices%202016"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. 7, 27",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Han%20Rasul%2C%20Kashif%20Vollgraf%2C%20Roland%20Fashion-mnist%3A%20a%20novel%20image%20dataset%20for%20benchmarking%20machine%20learning%20algorithms%202017"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7370\u20137379, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Xiyu%20Liu%2C%20Tongliang%20Wang%2C%20Xinchao%20and%20Dacheng%20Tao.%20On%20compressing%20deep%20models%20by%20low%20rank%20and%20sparse%20decomposition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Xiyu%20Liu%2C%20Tongliang%20Wang%2C%20Xinchao%20and%20Dacheng%20Tao.%20On%20compressing%20deep%20models%20by%20low%20rank%20and%20sparse%20decomposition%202017"
        },
        {
            "id": "Zhao_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, and Bo Yuan. Theoretical properties for neural networks with weight matrices of low displacement rank. CoRR, abs/1703.00144, 2017. URL http://arxiv.org/abs/1703.00144.",
            "url": "http://arxiv.org/abs/1703.00144",
            "arxiv_url": "https://arxiv.org/pdf/1703.00144"
        },
        {
            "id": "2",
            "entry": "2 Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. In Proceedings of the International Conference on Learning Representations (ICLR) 2017, feb 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Aojun%20Yao%2C%20Anbang%20Guo%2C%20Yiwen%20Xu%2C%20Lin%20Incremental%20Network%20Quantization%3A%20Towards%20Lossless%20CNNs%20with%20Low-Precision%20Weights%202017-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Aojun%20Yao%2C%20Anbang%20Guo%2C%20Yiwen%20Xu%2C%20Lin%20Incremental%20Network%20Quantization%3A%20Towards%20Lossless%20CNNs%20with%20Low-Precision%20Weights%202017-02"
        },
        {
            "id": "2",
            "entry": "2 Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Compressibility and generalization in large-scale deep learning. arXiv preprint arXiv:1804.05862, 2018. 2",
            "arxiv_url": "https://arxiv.org/pdf/1804.05862"
        },
        {
            "id": "We_2016_a",
            "entry": "We know that each random variable Yx satisfies E [Yx ] = 0 and by Assumption 2, is subexponential with parameter \u03bb \u2264 \u03bb\u2217. Thus, Y is a sum of |S| independent, zero-mean \u03bb\u2217-subexponential random variables, which implies that E [Y] = 0 and that we can readily apply Bernstein\u2019s inequality for subexponential random variables (Vershynin, 2016) to obtain for t \u2265 0",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20know%20that%20each%20random%20variable%20Yx%20satisfies%20E%20Yx%20%20%200%20and%20by%20Assumption%202%20is%20subexponential%20with%20parameter%20%CE%BB%20%20%CE%BB%20Thus%20Y%20is%20a%20sum%20of%20S%20independent%20zeromean%20%CE%BBsubexponential%20random%20variables%20which%20implies%20that%20E%20Y%20%200%20and%20that%20we%20can%20readily%20apply%20Bernsteins%20inequality%20for%20subexponential%20random%20variables%20Vershynin%202016%20to%20obtain%20for%20t%20%200"
        },
        {
            "id": "Moreover_2016_a",
            "entry": "Moreover, for a single Yx, we have by the equivalent definition of a subexponential random variable (Vershynin, 2016) that for u \u2265 0",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreover%2C%20for%20a%20single%20Yx%20we%20have%20by%20the%20equivalent%20definition%20of%20a%20subexponential%20random%20variable%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreover%2C%20for%20a%20single%20Yx%20we%20have%20by%20the%20equivalent%20definition%20of%20a%20subexponential%20random%20variable%202016"
        }
    ]
}
