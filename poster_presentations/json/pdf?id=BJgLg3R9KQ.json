{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING WHAT AND WHERE TO ATTEND",
        "author": "Drew Linsley, Dan Shiebler, Sven Eberhardt and Thomas Serre Department of Cognitive Linguistic & Psychological Sciences Carney Institute for Brain Science Brown University Providence, RI 02912 {drew_linsley,thomas_serre}@brown.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJgLg3R9KQ"
        },
        "abstract": "Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs). Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels. Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived \u201ctop-down\u201d attention maps. Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than \u201cbottom-up\u201d saliency features for rapid image categorization. As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers."
    },
    "keywords": [
        {
            "term": "human visual system",
            "url": "https://en.wikipedia.org/wiki/human_visual_system"
        },
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "multilayer perceptron",
            "url": "https://en.wikipedia.org/wiki/multilayer_perceptron"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "psychophysics",
            "url": "https://en.wikipedia.org/wiki/psychophysics"
        }
    ],
    "abbreviations": {
        "DCNs": "deep convolutional networks",
        "GALA": "global-and-local attention",
        "MLP": "multilayer perceptron",
        "ReLU": "rectified linear function",
        "TFRC": "TensorFlow Research Cloud"
    },
    "highlights": [
        "Attention has become the subject of intensive research within the deep learning community",
        "While biology is sometimes mentioned as a source of inspiration (<a class=\"ref-link\" id=\"cStollenga_et+al_2014_a\" href=\"#rStollenga_et+al_2014_a\">Stollenga et al, 2014</a>; <a class=\"ref-link\" id=\"cMnih_et+al_2014_a\" href=\"#rMnih_et+al_2014_a\">Mnih et al, 2014</a>; <a class=\"ref-link\" id=\"cCao_et+al_2015_a\" href=\"#rCao_et+al_2015_a\">Cao et al, 2015</a>; You et al, 2016; <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>; <a class=\"ref-link\" id=\"cWang_et+al_2017_a\" href=\"#rWang_et+al_2017_a\">Wang et al, 2017</a>; <a class=\"ref-link\" id=\"cBiparva_2017_a\" href=\"#rBiparva_2017_a\">Biparva and Tsotsos, 2017</a>), the attentional mechanisms that have been considered remain limited in comparison to the rich and diverse array of processes used by the human visual system",
        "Whereas human attention is controlled by varying task demands, attention networks used in computer vision are solely optimized for object recognition",
        "This means that, unlike infants who can rely on a myriad of visual cues and supervision to learn to focus their attention (<a class=\"ref-link\" id=\"cItti_et+al_2005_a\" href=\"#rItti_et+al_2005_a\">Itti et al, 2005</a>), deep convolutional networks must solve this challenging problem with weak supervisory signals derived from statistical associations between image pixels and class labels",
        "We have described the ClickMe dataset, which is aimed at supplementing ImageNet with nearly a half-million human-derived attention maps",
        "The approach was validated with human psychophysics, which indicated the sufficiency of ClickMe features for rapid visual categorization"
    ],
    "key_statements": [
        "Attention has become the subject of intensive research within the deep learning community",
        "While biology is sometimes mentioned as a source of inspiration (<a class=\"ref-link\" id=\"cStollenga_et+al_2014_a\" href=\"#rStollenga_et+al_2014_a\">Stollenga et al, 2014</a>; <a class=\"ref-link\" id=\"cMnih_et+al_2014_a\" href=\"#rMnih_et+al_2014_a\">Mnih et al, 2014</a>; <a class=\"ref-link\" id=\"cCao_et+al_2015_a\" href=\"#rCao_et+al_2015_a\">Cao et al, 2015</a>; You et al, 2016; <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>; <a class=\"ref-link\" id=\"cWang_et+al_2017_a\" href=\"#rWang_et+al_2017_a\">Wang et al, 2017</a>; <a class=\"ref-link\" id=\"cBiparva_2017_a\" href=\"#rBiparva_2017_a\">Biparva and Tsotsos, 2017</a>), the attentional mechanisms that have been considered remain limited in comparison to the rich and diverse array of processes used by the human visual system",
        "Whereas human attention is controlled by varying task demands, attention networks used in computer vision are solely optimized for object recognition",
        "This means that, unlike infants who can rely on a myriad of visual cues and supervision to learn to focus their attention (<a class=\"ref-link\" id=\"cItti_et+al_2005_a\" href=\"#rItti_et+al_2005_a\">Itti et al, 2005</a>), deep convolutional networks must solve this challenging problem with weak supervisory signals derived from statistical associations between image pixels and class labels",
        "As we demonstrate through psychophysics, our top-down attention maps better reflect human recognition strategies than Salicon, as they are collected while human observers search for visual features that are informative for object categorization.\n1.2",
        "Our contributions are three-fold: (i) We describe a large-scale online experiment ClickMe.ai used to supplement ImageNet with nearly a half-million top-down attention maps derived from human participants",
        "These maps are validated using human psychophysics and found to be more diagnostic than bottom-up attention maps for rapid visual categorization. As a proof of concept, we describe an extension of the leading squeeze-and-excitation (SE) module: the global-and-local attention (GALA) module, which combines global contextual guidance with local saliency and substantially improves accuracy on ILSVRC12. We find that incorporating ClickMe supervision into global-and-local attention training leads to an even larger gain in accuracy while encouraging visual representations that are more interpretable and more similar to those derived from human observers",
        "Rapid vision experiments have classically been used in visual neuroscience to probe visual responses while controlling for feedback processes (Thorpe et al, 1996; <a class=\"ref-link\" id=\"cSerre_et+al_2007_a\" href=\"#rSerre_et+al_2007_a\">Serre et al, 2007</a>; <a class=\"ref-link\" id=\"cEberhardt_et+al_2016_a\" href=\"#rEberhardt_et+al_2016_a\"><a class=\"ref-link\" id=\"cEberhardt_et+al_2016_a\" href=\"#rEberhardt_et+al_2016_a\">Eberhardt et al, 2016</a></a>). We devised such a rapid vision categorization experiment to compare the contribution of top-down ClickMe features for object recognition with features derived from bottom-up image saliency (Fig. 2B), closely following the approach of (<a class=\"ref-link\" id=\"cEberhardt_et+al_2016_a\" href=\"#rEberhardt_et+al_2016_a\"><a class=\"ref-link\" id=\"cEberhardt_et+al_2016_a\" href=\"#rEberhardt_et+al_2016_a\">Eberhardt et al, 2016</a></a>)",
        "Images were presented to human participants either intact or with a phase-scrambled perceptual mask which selectively exposed their most important visual features according to attention maps derived from either ClickMe or Salicon",
        "We identified 6 mid- to high-level feature layers in ResNet-50 to use with global-and-local attention, since these will in principle encode visual features that are qualitatively similar to the object-parts highlighted in ClickMe maps",
        "We have introduced the global-and-local attention module for learning local- and global-attention during object recognition",
        "We evaluated our approach for supervising global-and-local attention with ClickMe maps by partitioning the ClickMe dataset into separate folds for training, validation, and testing",
        "We found that the global-and-local attention-ResNet-50 was more accurate at object classification than either the ResNet-50 or the SE-ResNet-50",
        "We found that the global-and-local attention-ResNet-50 trained with ClickMe maps outperformed the controls (Table 3 in Appendix)",
        "Some object features can be made out in the attention maps of a global-and-local attention-ResNet-50 trained without ClickMe maps, but localization is weak and the maps are far more difficult to interpret",
        "The interpretability of the attention used by these global-and-local attention modules is reported in Table 1: global-and-local attention-ResNet-50 trained with ClickMe selects more similar features to human observers than the global-and-local attention-ResNet-50 trained without ClickMe",
        "We have described the ClickMe dataset, which is aimed at supplementing ImageNet with nearly a half-million human-derived attention maps",
        "The approach was validated with human psychophysics, which indicated the sufficiency of ClickMe features for rapid visual categorization",
        "While a detailed analysis of the ClickMe features falls outside the scope of the present study, we expect a more systematic analysis of this data, including the timecourse of feature selection (<a class=\"ref-link\" id=\"cCichy_et+al_2016_a\" href=\"#rCichy_et+al_2016_a\">Cichy et al, 2016</a>; <a class=\"ref-link\" id=\"cHa_2017_a\" href=\"#rHa_2017_a\">Ha and Eck, 2017</a>), will aid our understanding of the different attention mechanisms responsible for the selection of diagnostic image features",
        "We described a novel global-and-local attention (GALA) module and found that the proposed global-and-local attention-ResNet-50, significantly increases accuracy in this regime and cuts down top-5 error by \u223c 25% over both ResNet-50 and SE-ResNet-50",
        "We described an approach to co-train global-and-local attention using ClickMe supervision and cue the network to attend to image regions that are diagnostic to humans for object recognition"
    ],
    "summary": [
        "Attention has become the subject of intensive research within the deep learning community.",
        "We find that incorporating ClickMe supervision into GALA training leads to an even larger gain in accuracy while encouraging visual representations that are more interpretable and more similar to those derived from human observers.",
        "We devised such a rapid vision categorization experiment to compare the contribution of top-down ClickMe features for object recognition with features derived from bottom-up image saliency (Fig. 2B), closely following the approach of (<a class=\"ref-link\" id=\"cEberhardt_et+al_2016_a\" href=\"#rEberhardt_et+al_2016_a\">Eberhardt et al, 2016</a>).",
        "Images were presented to human participants either intact or with a phase-scrambled perceptual mask which selectively exposed their most important visual features according to attention maps derived from either ClickMe or Salicon.",
        "We performed a systematic analysis over different values of the hyperparameter \u03bb, which scaled the magnitude of the ClickMe map loss (Eq 1), while recording object classification accuracy and the similarity between ground-truth ClickMe maps and model attention maps (Fig. 7 in Appendix).",
        "The second control tested if ClickMe maps could directly improve feature learning in ResNet-50 architectures, without the aid of the GALA module.",
        "Because GALA-ResNet-50 networks were trained \u201cfrom scratch\u201d on the ClickMe dataset, we were able to visualize the features selected by each for object recognition.",
        "Including ClickMe supervision in GALAResNet-50 training yielded gradient images which highlighted features that were qualitatively more local and consistent with those identified by human observers (Fig. 8 in Appendix), emphasizing object parts such as facial features in animals, and the tires and headlights of cars.",
        "We can visualize these attention maps to see the image locations GALA modules learn to select (Fig. 8 in Appendix).",
        "Attention in the GALA-ResNet-50 trained with ClickMe maps, virtually without exception, focuses either on a single important visual feature of the target object class, or segments the figural object from the background.",
        "Some object features can be made out in the attention maps of a GALA-ResNet-50 trained without ClickMe maps, but localization is weak and the maps are far more difficult to interpret.",
        "The interpretability of the attention used by these GALA modules is reported in Table 1: GALA-ResNet-50 trained with ClickMe selects more similar features to human observers than the GALA-ResNet-50 trained without ClickMe. Attention in the GALA-ResNet-50 trained with ClickMe supervision generalizes to object images not found in ILSVRC12.",
        "We described an approach to co-train GALA using ClickMe supervision and cue the network to attend to image regions that are diagnostic to humans for object recognition."
    ],
    "headline": "We demonstrate the benefit of using stronger supervisory signals by teaching deep convolutional networks to attend to image regions that humans deem important for object recognition",
    "reference_links": [
        {
            "id": "Bell_et+al_2016_a",
            "entry": "S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2874\u20132883, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bell%2C%20S.%20Zitnick%2C%20C.Lawrence%20Bala%2C%20K.%20Girshick%2C%20R.%20Inside-outside%20net%3A%20Detecting%20objects%20in%20context%20with%20skip%20pooling%20and%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bell%2C%20S.%20Zitnick%2C%20C.Lawrence%20Bala%2C%20K.%20Girshick%2C%20R.%20Inside-outside%20net%3A%20Detecting%20objects%20in%20context%20with%20skip%20pooling%20and%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Biparva_2017_a",
            "entry": "M. Biparva and J. Tsotsos. STNet: Selective tuning of convolutional networks for object localization. In The IEEE International Conference on Computer Vision (ICCV), volume 2, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biparva%2C%20M.%20J.%20Tsotsos.%20STNet%3A%20Selective%20tuning%20of%20convolutional%20networks%20for%20object%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biparva%2C%20M.%20J.%20Tsotsos.%20STNet%3A%20Selective%20tuning%20of%20convolutional%20networks%20for%20object%20localization%202017"
        },
        {
            "id": "Cao_et+al_2015_a",
            "entry": "C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang, L. Wang, C. Huang, W. Xu, D. Ramanan, and T. S. Huang. Look and think twice: Capturing Top-Down visual attention with feedback convolutional neural networks. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 2956\u20132964, Dec. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20C.%20Liu%2C%20X.%20Yang%2C%20Y.%20Yu%2C%20Y.%20Look%20and%20think%20twice%3A%20Capturing%20Top-Down%20visual%20attention%20with%20feedback%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20C.%20Liu%2C%20X.%20Yang%2C%20Y.%20Yu%2C%20Y.%20Look%20and%20think%20twice%3A%20Capturing%20Top-Down%20visual%20attention%20with%20feedback%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T. S. Chua. SCA-CNN: Spatial and Channel-Wise attention in convolutional networks for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6298\u20136306, July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20L.%20Zhang%2C%20H.%20Xiao%2C%20J.%20Nie%2C%20L.%20SCA-CNN%3A%20Spatial%20and%20Channel-Wise%20attention%20in%20convolutional%20networks%20for%20image%20captioning%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20L.%20Zhang%2C%20H.%20Xiao%2C%20J.%20Nie%2C%20L.%20SCA-CNN%3A%20Spatial%20and%20Channel-Wise%20attention%20in%20convolutional%20networks%20for%20image%20captioning%202017-07"
        },
        {
            "id": "Cichy_et+al_2016_a",
            "entry": "R. M. Cichy, A. Khosla, D. Pantazis, A. Torralba, and A. Oliva. Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. Sci. Rep., 6:27755, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cichy%2C%20R.M.%20Khosla%2C%20A.%20Pantazis%2C%20D.%20Torralba%2C%20A.%20Comparison%20of%20deep%20neural%20networks%20to%20spatio-temporal%20cortical%20dynamics%20of%20human%20visual%20object%20recognition%20reveals%20hierarchical%20correspondence%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cichy%2C%20R.M.%20Khosla%2C%20A.%20Pantazis%2C%20D.%20Torralba%2C%20A.%20Comparison%20of%20deep%20neural%20networks%20to%20spatio-temporal%20cortical%20dynamics%20of%20human%20visual%20object%20recognition%20reveals%20hierarchical%20correspondence%202016-06"
        },
        {
            "id": "Das_et+al_2016_a",
            "entry": "A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 932\u2013937, Stroudsburg, PA, USA, 2016. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20A.%20Agrawal%2C%20H.%20Zitnick%2C%20L.%20Parikh%2C%20D.%20Human%20attention%20in%20visual%20question%20answering%3A%20Do%20humans%20and%20deep%20networks%20look%20at%20the%20same%20regions%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20A.%20Agrawal%2C%20H.%20Zitnick%2C%20L.%20Parikh%2C%20D.%20Human%20attention%20in%20visual%20question%20answering%3A%20Do%20humans%20and%20deep%20networks%20look%20at%20the%20same%20regions%3F%202016"
        },
        {
            "id": "Deng_et+al_2013_a",
            "entry": "J. Deng, J. Krause, and L. Fei-Fei. Fine-Grained crowdsourcing for Fine-Grained recognition. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 580\u2013587, June 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Krause%2C%20J.%20Fei-Fei%2C%20L.%20Fine-Grained%20crowdsourcing%20for%20Fine-Grained%20recognition%202013-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Krause%2C%20J.%20Fei-Fei%2C%20L.%20Fine-Grained%20crowdsourcing%20for%20Fine-Grained%20recognition%202013-06"
        },
        {
            "id": "Deng_et+al_2016_a",
            "entry": "J. Deng, J. Krause, M. Stark, and L. Fei-Fei. Leveraging the wisdom of the crowd for Fine-Grained recognition. IEEE Trans. Pattern Anal. Mach. Intell., 38(4):666\u2013676, Apr. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Krause%2C%20J.%20Stark%2C%20M.%20Fei-Fei%2C%20L.%20Leveraging%20the%20wisdom%20of%20the%20crowd%20for%20Fine-Grained%20recognition%202016-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Krause%2C%20J.%20Stark%2C%20M.%20Fei-Fei%2C%20L.%20Leveraging%20the%20wisdom%20of%20the%20crowd%20for%20Fine-Grained%20recognition%202016-04"
        },
        {
            "id": "Eberhardt_et+al_2016_a",
            "entry": "S. Eberhardt, J. Cader, and T. Serre. How deep is the feature analysis underlying rapid visual categorization? In Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eberhardt%2C%20S.%20Cader%2C%20J.%20Serre%2C%20T.%20How%20deep%20is%20the%20feature%20analysis%20underlying%20rapid%20visual%20categorization%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eberhardt%2C%20S.%20Cader%2C%20J.%20Serre%2C%20T.%20How%20deep%20is%20the%20feature%20analysis%20underlying%20rapid%20visual%20categorization%3F%202016"
        },
        {
            "id": "Edgington_1964_a",
            "entry": "E. S. Edgington. Randomization tests. The Journal of psychology, 57:445\u2013449, Apr. 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edgington%2C%20E.S.%20Randomization%20tests%201964-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edgington%2C%20E.S.%20Randomization%20tests%201964-04"
        },
        {
            "id": "Gureckis_et+al_2016_a",
            "entry": "T. M. Gureckis, J. Martin, J. McDonnell, A. S. Rich, D. Markant, A. Coenen, D. Halpern, J. B. Hamrick, and P. Chan. psiturk: An open-source framework for conducting replicable behavioral experiments online. Behav. Res. Methods, 48(3):829\u2013842, Sept. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gureckis%2C%20T.M.%20Martin%2C%20J.%20McDonnell%2C%20J.%20Rich%2C%20A.S.%20psiturk%3A%20An%20open-source%20framework%20for%20conducting%20replicable%20behavioral%20experiments%20online%202016-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gureckis%2C%20T.M.%20Martin%2C%20J.%20McDonnell%2C%20J.%20Rich%2C%20A.S.%20psiturk%3A%20An%20open-source%20framework%20for%20conducting%20replicable%20behavioral%20experiments%20online%202016-09"
        },
        {
            "id": "Ha_2017_a",
            "entry": "D. Ha and D. Eck. A neural representation of sketch drawings. Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ha%2C%20D.%20Eck%2C%20D.%20A%20neural%20representation%20of%20sketch%20drawings%202017-04"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "J. Hu, L. Shen, and G. Sun. Squeeze-and-Excitation networks. Sept. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Shen%2C%20L.%20Sun%2C%20G.%20Squeeze-and-Excitation%20networks%202017-09"
        },
        {
            "id": "Itti_2001_a",
            "entry": "L. Itti and C. Koch. Computational modelling of visual attention. Nat. Rev. Neurosci., 2(3):194\u2013203, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Itti%2C%20L.%20Koch%2C%20C.%20Computational%20modelling%20of%20visual%20attention%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Itti%2C%20L.%20Koch%2C%20C.%20Computational%20modelling%20of%20visual%20attention%202001"
        },
        {
            "id": "Itti_et+al_2005_a",
            "entry": "L. Itti, G. Rees, and J. K. Tsotsos. Neurobiology of attention. Academic Press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Itti%2C%20L.%20Rees%2C%20G.%20Tsotsos%2C%20J.K.%20Neurobiology%20of%20attention%202005"
        },
        {
            "id": "Jetley_et+al_2018_a",
            "entry": "S. Jetley, N. A. Lord, N. Lee, and P. H. S. Torr. Learn to pay attention. Apr. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jetley%2C%20S.%20Lord%2C%20N.A.%20Lee%2C%20N.%20Torr%2C%20P.H.S.%20Learn%20to%20pay%20attention%202018-04"
        },
        {
            "id": "Jiang_et+al_2015_a",
            "entry": "M. Jiang, S. Huang, J. Duan, and Q. Zhao. SALICON: Saliency in context. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1072\u20131080, June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Jiang%20S%20Huang%20J%20Duan%20and%20Q%20Zhao%20SALICON%20Saliency%20in%20context%20In%202015%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20pages%2010721080%20June%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Jiang%20S%20Huang%20J%20Duan%20and%20Q%20Zhao%20SALICON%20Saliency%20in%20context%20In%202015%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20pages%2010721080%20June%202015"
        },
        {
            "id": "Johnson_2001_a",
            "entry": "S. P. Johnson. Visual development in human infants: Binding features, surfaces, and objects. Vis. cogn., 8(3-5):565\u2013578, June 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20S.P.%20Visual%20development%20in%20human%20infants%3A%20Binding%20features%2C%20surfaces%2C%20and%20objects.%20Vis%202001-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20S.P.%20Visual%20development%20in%20human%20infants%3A%20Binding%20features%2C%20surfaces%2C%20and%20objects.%20Vis%202001-06"
        },
        {
            "id": "Judd_et+al_2009_a",
            "entry": "T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Judd%2C%20T.%20Ehinger%2C%20K.%20Durand%2C%20F.%20Torralba%2C%20A.%20Learning%20to%20predict%20where%20humans%20look%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Judd%2C%20T.%20Ehinger%2C%20K.%20Durand%2C%20F.%20Torralba%2C%20A.%20Learning%20to%20predict%20where%20humans%20look%202009"
        },
        {
            "id": "Kovashka_et+al_2016_a",
            "entry": "A. Kovashka, O. Russakovsky, L. Fei-Fei, and K. Grauman. Crowdsourcing in computer vision. Nov. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kovashka%2C%20A.%20Russakovsky%2C%20O.%20Fei-Fei%2C%20L.%20Grauman%2C%20K.%20Crowdsourcing%20in%20computer%20vision%202016-11"
        },
        {
            "id": "Lin_et+al_2014_a",
            "entry": "T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. Lawrence Zitnick, and P. Doll\u00e1r. Microsoft COCO: Common objects in context. May 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20T.-Y.%20Maire%2C%20M.%20Belongie%2C%20S.%20Bourdev%2C%20L.%20Microsoft%20COCO%3A%20Common%20objects%20in%20context%202014-05"
        },
        {
            "id": "Linsley_et+al_2017_a",
            "entry": "D. Linsley, S. Eberhardt, T. Sharma, P. Gupta, and T. Serre. What are the visual features underlying human versus machine vision? Jan. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linsley%2C%20D.%20Eberhardt%2C%20S.%20Sharma%2C%20T.%20Gupta%2C%20P.%20What%20are%20the%20visual%20features%20underlying%20human%20versus%20machine%20vision%3F%202017-01"
        },
        {
            "id": "Mnih_et+al_2014_a",
            "entry": "V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. Advances in Neural Information Processing Systems 27, 27:1\u20139, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20Kavukcuoglu%2C%20K.%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20Kavukcuoglu%2C%20K.%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "Nam_et+al_2017_a",
            "entry": "H. Nam, J. W. Ha, and J. Kim. Dual attention networks for multimodal reasoning and matching. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2156\u20132164, July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nam%2C%20H.%20Ha%2C%20J.W.%20Kim%2C%20J.%20Dual%20attention%20networks%20for%20multimodal%20reasoning%20and%20matching%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nam%2C%20H.%20Ha%2C%20J.W.%20Kim%2C%20J.%20Dual%20attention%20networks%20for%20multimodal%20reasoning%20and%20matching%202017-07"
        },
        {
            "id": "Nguyen_et+al_2018_a",
            "entry": "T. V. Nguyen, Q. Zhao, and S. Yan. Attentive systems: A survey. Int. J. Comput. Vis., 126(1):86\u2013110, Jan. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.V.%20Zhao%2C%20Q.%20Yan%2C%20S.%20Attentive%20systems%3A%20A%20survey%202018-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.V.%20Zhao%2C%20Q.%20Yan%2C%20S.%20Attentive%20systems%3A%20A%20survey%202018-01"
        },
        {
            "id": "Oliva_2007_a",
            "entry": "A. Oliva and A. Torralba. The role of context in object recognition. Trends Cogn. Sci., 11(12): 520\u2013527, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliva%2C%20A.%20Torralba%2C%20A.%20The%20role%20of%20context%20in%20object%20recognition%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oliva%2C%20A.%20Torralba%2C%20A.%20The%20role%20of%20context%20in%20object%20recognition%202007"
        },
        {
            "id": "Ostrovsky_et+al_2009_a",
            "entry": "Y. Ostrovsky, E. Meyers, S. Ganesh, U. Mathur, and P. Sinha. Visual parsing after recovery from blindness. Psychol. Sci., 20(12):1484\u20131491, Dec. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ostrovsky%2C%20Y.%20Meyers%2C%20E.%20Ganesh%2C%20S.%20Mathur%2C%20U.%20Visual%20parsing%20after%20recovery%20from%20blindness%202009-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ostrovsky%2C%20Y.%20Meyers%2C%20E.%20Ganesh%2C%20S.%20Mathur%2C%20U.%20Visual%20parsing%20after%20recovery%20from%20blindness%202009-12"
        },
        {
            "id": "Park_et+al_2018_a",
            "entry": "J. Park, S. Woo, J.-Y. Lee, and I.-S. Kweon. Bam: Bottleneck attention module. CoRR, abs/1807.06514, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06514"
        },
        {
            "id": "Patro_2018_a",
            "entry": "B. Patro and V. P. Namboodiri. Differential attention for visual question answering. Apr. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patro%2C%20B.%20Namboodiri%2C%20V.P.%20Differential%20attention%20for%20visual%20question%20answering%202018-04"
        },
        {
            "id": "Saleh_et+al_2016_a",
            "entry": "B. Saleh, A. Elgammal, and J. Feldman. The role of typicality in object classification: Improving the generalization capacity of convolutional neural networks. Feb. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saleh%2C%20B.%20Elgammal%2C%20A.%20Feldman%2C%20J.%20The%20role%20of%20typicality%20in%20object%20classification%3A%20Improving%20the%20generalization%20capacity%20of%20convolutional%20neural%20networks%202016-02"
        },
        {
            "id": "Serre_et+al_2007_a",
            "entry": "T. Serre, A. Oliva, and T. Poggio. A feedforward architecture accounts for rapid categorization. Proceedings of the National Academy of Sciences of the United States of America, 104(15): 6424\u20136429, Apr. 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serre%2C%20T.%20Oliva%2C%20A.%20Poggio%2C%20T.%20A%20feedforward%20architecture%20accounts%20for%20rapid%20categorization%202007-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serre%2C%20T.%20Oliva%2C%20A.%20Poggio%2C%20T.%20A%20feedforward%20architecture%20accounts%20for%20rapid%20categorization%202007-04"
        },
        {
            "id": "Vadivel_et+al_2015_a",
            "entry": "K. Shanmuga Vadivel, T. Ngo, M. Eckstein, and B. S. Manjunath. Eye tracking assisted extraction of attentionally important objects from videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3241\u20133250, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vadivel%2C%20K.Shanmuga%20Ngo%2C%20T.%20Eckstein%2C%20M.%20Manjunath%2C%20B.S.%20Eye%20tracking%20assisted%20extraction%20of%20attentionally%20important%20objects%20from%20videos%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vadivel%2C%20K.Shanmuga%20Ngo%2C%20T.%20Eckstein%2C%20M.%20Manjunath%2C%20B.S.%20Eye%20tracking%20assisted%20extraction%20of%20attentionally%20important%20objects%20from%20videos%202015"
        },
        {
            "id": "Smilkov_et+al_2017_a",
            "entry": "D. Smilkov, N. Thorat, B. Kim, F. Vi\u00e9gas, and M. Wattenberg. SmoothGrad: removing noise by adding noise. June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smilkov%2C%20D.%20Thorat%2C%20N.%20Kim%2C%20B.%20Vi%C3%A9gas%2C%20F.%20SmoothGrad%3A%20removing%20noise%20by%20adding%20noise%202017-06"
        },
        {
            "id": "Stollenga_et+al_2014_a",
            "entry": "M. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber. Deep networks with internal selective attention through feedback connections. arXiv preprint arXiv:..., page 13, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stollenga%2C%20M.%20Masci%2C%20J.%20Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Deep%20networks%20with%20internal%20selective%20attention%20through%20feedback%20connections%202014"
        },
        {
            "id": "Sutskever_et+al_2013_a",
            "entry": "I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pages 1139\u20131147, Feb. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Martens%2C%20J.%20Dahl%2C%20G.%20Hinton%2C%20G.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20I.%20Martens%2C%20J.%20Dahl%2C%20G.%20Hinton%2C%20G.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013-02"
        },
        {
            "id": "Thorpe_et+al_6582_a",
            "entry": "S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature, 381 (6582):520\u2013522, June 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thorpe%2C%20S.%20Fize%2C%20D.%20Marlot%2C%20C.%20Speed%20of%20processing%20in%20the%20human%20visual%20system%206582-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thorpe%2C%20S.%20Fize%2C%20D.%20Marlot%2C%20C.%20Speed%20of%20processing%20in%20the%20human%20visual%20system%206582-06"
        },
        {
            "id": "Torralba_et+al_2006_a",
            "entry": "A. Torralba, A. Oliva, M. S. Castelhano, and J. M. Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychol. Rev., 113 (4):766\u2013786, Oct. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torralba%2C%20A.%20Oliva%2C%20A.%20Castelhano%2C%20M.S.%20Henderson%2C%20J.M.%20Contextual%20guidance%20of%20eye%20movements%20and%20attention%20in%20real-world%20scenes%3A%20the%20role%20of%20global%20features%20in%20object%20search%202006-10-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Torralba%2C%20A.%20Oliva%2C%20A.%20Castelhano%2C%20M.S.%20Henderson%2C%20J.M.%20Contextual%20guidance%20of%20eye%20movements%20and%20attention%20in%20real-world%20scenes%3A%20the%20role%20of%20global%20features%20in%20object%20search%202006-10-04"
        },
        {
            "id": "Ullman_et+al_2016_a",
            "entry": "S. Ullman, L. Assif, E. Fetaya, and D. Harari. Atoms of recognition in human and computer vision. Proc. Natl. Acad. Sci. U. S. A., 113(10):2744\u20132749, Mar. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullman%2C%20S.%20Assif%2C%20L.%20Fetaya%2C%20E.%20Harari%2C%20D.%20Atoms%20of%20recognition%20in%20human%20and%20computer%20vision%202016-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullman%2C%20S.%20Assif%2C%20L.%20Fetaya%2C%20E.%20Harari%2C%20D.%20Atoms%20of%20recognition%20in%20human%20and%20computer%20vision%202016-03"
        },
        {
            "id": "Von_et+al_2006_a",
            "entry": "L. von Ahn, R. Liu, and M. Blum. Peekaboom: A game for locating objects in images. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI \u201906, pages 55\u201364, New York, NY, USA, 2006. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=von%20Ahn%2C%20L.%20Liu%2C%20R.%20Blum%2C%20M.%20Peekaboom%3A%20A%20game%20for%20locating%20objects%20in%20images%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=von%20Ahn%2C%20L.%20Liu%2C%20R.%20Blum%2C%20M.%20Peekaboom%3A%20A%20game%20for%20locating%20objects%20in%20images%202006"
        },
        {
            "id": "Vondrick_et+al_2015_a",
            "entry": "C. Vondrick, H. Pirsiavash, A. Oliva, and A. Torralba. Learning visual biases from human imagination. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 289\u2013297. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Oliva%2C%20A.%20Torralba%2C%20A.%20Learning%20visual%20biases%20from%20human%20imagination%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Oliva%2C%20A.%20Torralba%2C%20A.%20Learning%20visual%20biases%20from%20human%20imagination%202015"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang. Residual attention network for image classification. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6450\u20136458, July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017-07"
        },
        {
            "id": "You_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning with semantic attention. In 2016",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20Q.%20Jin%2C%20H.%20Wang%2C%20Z.%20Fang%2C%20C.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20Q.%20Jin%2C%20H.%20Wang%2C%20Z.%20Fang%2C%20C.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4651\u20134659, June 2016. S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Dec. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Paying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Paying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%202016-06"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection. Nov. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Bau%2C%20D.%20Oliva%2C%20A.%20Torralba%2C%20A.%20Interpreting%20deep%20visual%20representations%20via%20network%20dissection%202017-11"
        }
    ]
}
