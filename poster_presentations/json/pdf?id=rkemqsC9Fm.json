{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "INFORMATION THEORETIC LOWER BOUNDS ON NEGATIVE LOG LIKELIHOOD",
        "author": "Luis A. Lastras-Montano IBM Research AI Yorktown Heights, NY, 10598, USA lastrasl@us.ibm.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkemqsC9Fm"
        },
        "abstract": "In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing ratedistortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling."
    },
    "keywords": [
        {
            "term": "lossy compression",
            "url": "https://en.wikipedia.org/wiki/lossy_compression"
        },
        {
            "term": "information theory",
            "url": "https://en.wikipedia.org/wiki/information_theory"
        },
        {
            "term": "likelihood function",
            "url": "https://en.wikipedia.org/wiki/likelihood_function"
        },
        {
            "term": "log likelihood",
            "url": "https://en.wikipedia.org/wiki/log_likelihood"
        },
        {
            "term": "Evidence Lower Bound",
            "url": "https://en.wikipedia.org/wiki/Evidence_Lower_Bound"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "rate distortion function",
            "url": "https://en.wikipedia.org/wiki/rate_distortion_function"
        },
        {
            "term": "rate distortion theory",
            "url": "https://en.wikipedia.org/wiki/rate_distortion_theory"
        }
    ],
    "abbreviations": {
        "ELBO": "Evidence Lower Bound",
        "VAE": "Variational Autoencoder",
        "HVAE": "hierarchical Variational Autoencoder"
    },
    "highlights": [
        "It has the advantage that the lower bound is written as a function of the trivial upper bound (3) - if someone proposes a latent variable model p(x) which uses a likelihood function (x|z), the optimal negative log likelihood value when we optimize the prior is known to be within a gap of sup log 1 N",
        "Theorem 1 suggests that the problem of lower bounding negative log likelihood in a latent variable modeling problem may be related to the problem of lower bounding a rate-distortion function",
        "Theorem 2",
        "In the article we showed how some classical results in latent variable modeling can be seen as relatively simple consequences of results in ratedistortion function computation, and further argued that these results help in understanding whether prior or likelihood functions can be improved further, demonstrating this with some experimental results in an image modeling problem",
        "While rate-distortion function computation is an important subject in information theory, the true crown jewel is Shannon\u2019s famous source coding theorem; to-date we are not aware of this important result being connected directly to the problem of latent variable modeling"
    ],
    "key_statements": [
        "It has the advantage that the lower bound is written as a function of the trivial upper bound (3) - if someone proposes a latent variable model p(x) which uses a likelihood function (x|z), the optimal negative log likelihood value when we optimize the prior is known to be within a gap of sup log 1 N",
        "Theorem 1 suggests that the problem of lower bounding negative log likelihood in a latent variable modeling problem may be related to the problem of lower bounding a rate-distortion function",
        "Theorem 2",
        "In the article we showed how some classical results in latent variable modeling can be seen as relatively simple consequences of results in ratedistortion function computation, and further argued that these results help in understanding whether prior or likelihood functions can be improved further, demonstrating this with some experimental results in an image modeling problem",
        "While rate-distortion function computation is an important subject in information theory, the true crown jewel is Shannon\u2019s famous source coding theorem; to-date we are not aware of this important result being connected directly to the problem of latent variable modeling"
    ],
    "summary": [
        "It has the advantage that the lower bound is written as a function of the trivial upper bound (3) - if someone proposes a latent variable model p(x) which uses a likelihood function (x|z), the optimal negative log likelihood value when we optimize the prior is known to be within a gap of sup log 1 N",
        "The fundamental connection between the log likelihood optimization in latent variable modeling and the computation of rate-distortion functions became more clear.",
        "Theorem 1 suggests that the problem of lower bounding negative log likelihood in a latent variable modeling problem may be related to the problem of lower bounding a rate-distortion function.",
        "In order to explore a variety of prior and likelihood function combinations we took advantage of the publicly available source code that the authors of the VampPrior article (<a class=\"ref-link\" id=\"cTomczak_2018_a\" href=\"#rTomczak_2018_a\">Tomczak & Welling, 2018</a>) used in their work, and extended their code to create sample from Z using the strategy described above, and implemented the computation of the c(z) quantities, both of which are straightforward given that the ELBO optimization gives us Q(z|x) and an easy means to evaluate c(z) for any desired value of z.",
        "We want the reader to focus on this prototypical setting: A modeling expert has chosen a Gaussian unit variance, zero mean prior p(z), and has decided on a simple likelihood function class.",
        "We claim that the information theoretically motivated quantity log c(z) and its associated glossy statistics (16) do provide useful guidance in the modeling task, and given how easy they are to estimate, could be regularly checked by a modeling expert to gauge whether their model can be further improved by either improving the prior or the likelihood function.",
        "In the article we showed how some classical results in latent variable modeling can be seen as relatively simple consequences of results in ratedistortion function computation, and further argued that these results help in understanding whether prior or likelihood functions can be improved further, demonstrating this with some experimental results in an image modeling problem.",
        "While rate-distortion function computation is an important subject in information theory, the true crown jewel is Shannon\u2019s famous source coding theorem; to-date we are not aware of this important result being connected directly to the problem of latent variable modeling."
    ],
    "headline": "Given data {x1, \u00b7 \u00b7 \u00b7 , xN } the first problem we study is the following optimization problem: for a fixed, 1N",
    "reference_links": [
        {
            "id": "Alemi_et+al_2018_a",
            "entry": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a broken elbo. In Proceedings of the 35th International Conference on Machine Learning. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20Alexander%20A.%20Poole%2C%20Ben%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Fixing%20a%20broken%20elbo%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20Alexander%20A.%20Poole%2C%20Ben%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Fixing%20a%20broken%20elbo%202018"
        },
        {
            "id": "Banerjee_et+al_2004_a",
            "entry": "Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, and Srujana Merugu. An information theoretic analysis of maximum likelihood mixture estimation for exponential families. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pp. 8\u2013, New York, NY, USA, 2004. ACM. ISBN 1-58113-838-5.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Arindam%20Dhillon%2C%20Inderjit%20Ghosh%2C%20Joydeep%20Merugu%2C%20Srujana%20An%20information%20theoretic%20analysis%20of%20maximum%20likelihood%20mixture%20estimation%20for%20exponential%20families%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Arindam%20Dhillon%2C%20Inderjit%20Ghosh%2C%20Joydeep%20Merugu%2C%20Srujana%20An%20information%20theoretic%20analysis%20of%20maximum%20likelihood%20mixture%20estimation%20for%20exponential%20families%202004"
        },
        {
            "id": "Berger_1971_a",
            "entry": "T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice-Hall electrical engineering series. Prentice-Hall, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berger%2C%20T.%20Rate%20Distortion%20Theory%3A%20A%20Mathematical%20Basis%20for%20Data%20Compression.%20Prentice-Hall%20electrical%20engineering%20series%201971"
        },
        {
            "id": "Blahut_1972_a",
            "entry": "R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on Information Theory, 18(4):460\u2013473, Jul 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blahut%2C%20R.%20Computation%20of%20channel%20capacity%20and%20rate-distortion%20functions%201972-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blahut%2C%20R.%20Computation%20of%20channel%20capacity%20and%20rate-distortion%20functions%201972-07"
        },
        {
            "id": "Bowman_et+al_2016_a",
            "entry": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In The SIGNLL Conference on Computational Natural Language Learning. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowman%2C%20Samuel%20R.%20Vilnis%2C%20Luke%20Vinyals%2C%20Oriol%20Dai%2C%20Andrew%20M.%20Generating%20sentences%20from%20a%20continuous%20space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20Samuel%20R.%20Vilnis%2C%20Luke%20Vinyals%2C%20Oriol%20Dai%2C%20Andrew%20M.%20Generating%20sentences%20from%20a%20continuous%20space%202016"
        },
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In The International Conference on Learning Representations (ICLR). 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Yuri%20Grosse%2C%20Roger%20B.%20Salakhutdinov%2C%20Ruslan%20Importance%20Weighted%20Autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Grosse%2C%20Roger%20B.%20Salakhutdinov%2C%20Ruslan%20Importance%20Weighted%20Autoencoders%202016"
        },
        {
            "id": "Freyfaces_0000_a",
            "entry": "FreyFaces. Frey Faces Data Set. URL https://cs.nyu.edu/\u0303roweis/data/frey_rawface.mat.",
            "url": "https://cs.nyu.edu/\u0303roweis/data/frey_rawface.mat"
        },
        {
            "id": "Giraldo_2013_a",
            "entry": "Luis Gonzalo Sanchez Giraldo and Jose C. Pr\u0131ncipe. Rate-distortion auto-encoders. CoRR, abs/1312.7381, 2013. URL http://arxiv.org/abs/1312.7381.",
            "url": "http://arxiv.org/abs/1312.7381",
            "arxiv_url": "https://arxiv.org/pdf/1312.7381"
        },
        {
            "id": "Higgins_et+al_2017_a",
            "entry": "I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In The International Conference on Learning Representations (ICLR), Toulon. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20I.%20Matthey%2C%20L.%20Pal%2C%20A.%20Burgess%2C%20C.%20Beta-VAE%3A%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20I.%20Matthey%2C%20L.%20Pal%2C%20A.%20Burgess%2C%20C.%20Beta-VAE%3A%20Learning%20Basic%20Visual%20Concepts%20with%20a%20Constrained%20Variational%20Framework%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D.P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In The International Conference on Learning Representations (ICLR), Banff. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. In Science, volume 350, pp. 1332?1338. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Larochelle_2011_a",
            "entry": "Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS). 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011"
        },
        {
            "id": "Lindsay_1983_a",
            "entry": "Bruce G. Lindsay. The geometry of mixture likelihoods: A general theory. Ann. Statist., 11(1): 86\u201394, 03 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lindsay%2C%20Bruce%20G.%20The%20geometry%20of%20mixture%20likelihoods%3A%20A%20general%20theory%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lindsay%2C%20Bruce%20G.%20The%20geometry%20of%20mixture%20likelihoods%3A%20A%20general%20theory%201983"
        },
        {
            "id": "Marlin_et+al_2010_a",
            "entry": "Benjamin Marlin, Kevin Swersky, Bo Chen, and Nando Freitas. Inductive principles for restricted boltzmann machine learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9, pp. 509\u2013516, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marlin%2C%20Benjamin%20Swersky%2C%20Kevin%20Chen%2C%20Bo%20Freitas%2C%20Nando%20Inductive%20principles%20for%20restricted%20boltzmann%20machine%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marlin%2C%20Benjamin%20Swersky%2C%20Kevin%20Chen%2C%20Bo%20Freitas%2C%20Nando%20Inductive%20principles%20for%20restricted%20boltzmann%20machine%20learning%202010"
        },
        {
            "id": "Neal_1998_a",
            "entry": "Radford M. Neal and Geoffrey E. Hinton. A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants, pp. 355\u2013368. Springer Netherlands, Dordrecht, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Hinton%2C%20Geoffrey%20E.%20A%20View%20of%20the%20Em%20Algorithm%20that%20Justifies%20Incremental%2C%20Sparse%2C%20and%20other%20Variants%201998"
        },
        {
            "id": "Rolfe_2017_a",
            "entry": "Jason Tyler Rolfe. Discrete variational autoencoders. In The International Conference on Learning Representations (ICLR), Toulon. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rolfe%2C%20Jason%20Tyler%20Discrete%20variational%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rolfe%2C%20Jason%20Tyler%20Discrete%20variational%20autoencoders%202017"
        },
        {
            "id": "Rose_1998_a",
            "entry": "Kenneth Rose. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proceedings of the IEEE, (11):2210\u20132239, November 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rose%2C%20Kenneth%20Deterministic%20annealing%20for%20clustering%2C%20compression%2C%20classification%2C%20regression%2C%20and%20related%20optimization%20problems%201998-11-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rose%2C%20Kenneth%20Deterministic%20annealing%20for%20clustering%2C%20compression%2C%20classification%2C%20regression%2C%20and%20related%20optimization%20problems%201998-11-11"
        },
        {
            "id": "Shannon_1959_a",
            "entry": "C. E. Shannon. Coding theorems for a discrete source with a fidelity criterion. In IRE Nat. Conv. Rec., Pt. 4, pp. 142\u2013163. 1959.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.E.%20Coding%20theorems%20for%20a%20discrete%20source%20with%20a%20fidelity%20criterion%201959",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.E.%20Coding%20theorems%20for%20a%20discrete%20source%20with%20a%20fidelity%20criterion%201959"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR, abs/1703.00810, 2017. URL http://arxiv.org/abs/1703.00810.",
            "url": "http://arxiv.org/abs/1703.00810",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Slonim_2002_a",
            "entry": "Noam Slonim and Yair Weiss. Maximum likelihood and the information bottleneck. In Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS\u201902, pp. 351\u2013358, Cambridge, MA, USA, 2002. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Slonim%2C%20Noam%20Weiss%2C%20Yair%20Maximum%20likelihood%20and%20the%20information%20bottleneck%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Slonim%2C%20Noam%20Weiss%2C%20Yair%20Maximum%20likelihood%20and%20the%20information%20bottleneck%202002"
        },
        {
            "id": "Tishby_et+al_1999_a",
            "entry": "N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp. 368\u2013377. 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Pereira%2C%20F.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Pereira%2C%20F.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%201999"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "Tomczak_2016_a",
            "entry": "Jakub M. Tomczak and Max Welling. Improving variational auto-encoders using householder flow. Neural Information Processing Systems Workshop on Bayesian Deep Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomczak%2C%20Jakub%20M.%20Welling%2C%20Max%20Improving%20variational%20auto-encoders%20using%20householder%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomczak%2C%20Jakub%20M.%20Welling%2C%20Max%20Improving%20variational%20auto-encoders%20using%20householder%20flow%202016"
        },
        {
            "id": "Tomczak_2018_a",
            "entry": "Jakub M. Tomczak and Max Welling. VAE with a VampPrior. In The 21nd International Conference on Artificial Intelligence and Statistics. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomczak%2C%20Jakub%20M.%20Welling%2C%20Max%20VAE%20with%20a%20VampPrior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomczak%2C%20Jakub%20M.%20Welling%2C%20Max%20VAE%20with%20a%20VampPrior%202018"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aaron%20van%20den%20Oord%20Nal%20Kalchbrenner%20and%20Koray%20Kavukcuoglu%20Pixel%20Recurrent%20Neural%20Networks%20In%20Proceedings%20of%20the%2033rd%20International%20Conference%20on%20Machine%20Learning%20New%20York%20NY%20USA%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aaron%20van%20den%20Oord%20Nal%20Kalchbrenner%20and%20Koray%20Kavukcuoglu%20Pixel%20Recurrent%20Neural%20Networks%20In%20Proceedings%20of%20the%2033rd%20International%20Conference%20on%20Machine%20Learning%20New%20York%20NY%20USA%202016"
        },
        {
            "id": "Watanabe_2015_a",
            "entry": "Kazuho Watanabe and Shiro Ikeda. Entropic risk minimization for nonparametric estimation of mixing distributions. Machine Learning, 99(1):119\u2013136, Apr 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watanabe%2C%20Kazuho%20Ikeda%2C%20Shiro%20Entropic%20risk%20minimization%20for%20nonparametric%20estimation%20of%20mixing%20distributions%202015-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watanabe%2C%20Kazuho%20Ikeda%2C%20Shiro%20Entropic%20risk%20minimization%20for%20nonparametric%20estimation%20of%20mixing%20distributions%202015-04"
        }
    ]
}
