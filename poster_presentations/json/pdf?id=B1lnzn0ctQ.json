{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "ALISTA: ANALYTIC WEIGHTS ARE AS GOOD AS LEARNED WEIGHTS IN LISTA",
        "author": "Jialin Liu, Department of Mathematics University of California, Los Angeles liujl,@math.ucla.edu",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1lnzn0ctQ"
        },
        "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This significantly simplifies the training. Specifically, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al., 2018</a>) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model."
    },
    "keywords": [
        {
            "term": "sparse coding",
            "url": "https://en.wikipedia.org/wiki/sparse_coding"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "compressed sensing",
            "url": "https://en.wikipedia.org/wiki/compressed_sensing"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "restricted isometry property",
            "url": "https://en.wikipedia.org/wiki/restricted_isometry_property"
        },
        {
            "term": "analytic",
            "url": "https://en.wikipedia.org/wiki/analytic"
        }
    ],
    "abbreviations": {
        "ALISTA": "analytic LISTA",
        "ISTA": "iterative shrinkage thresholding algorithm",
        "LISTA": "Learned ISTA",
        "RNN": "recurrent neural network",
        "CSC": "convolutional sparse coding",
        "CNNs": "convolutional neural networks",
        "PGD": "projected gradient descent descent",
        "IHT": "iterative hard thresholding",
        "RIP": "restricted isometry property",
        "AMP": "approximated message passing"
    },
    "highlights": [
        "Sparse coding, is a classical problem in source coding, signal reconstruction, pattern recognition and feature selection",
        "We extend the analytic Learned ISTA to the convolutional case starting from discussing the convolutional sparse coding (CSC)",
        "Similar conclusions can be drawn for convolutional analytic Learned ISTA",
        "The robust analytic Learned ISTA further shows remarkable robustness in sparse code prediction, given that D is randomly perturbed within some extent",
        "If the model is for convolutional case, we add \u201cConv\u201d as the prefix for model name, such as \u201cConv analytic Learned ISTA\u201d that represents the convolutional analytic Learned ISTA.\n5.1",
        "We no longer train any matrix for Learned ISTA but directly use the solution to an analytic minimization problem to solve for its layer-wise weights"
    ],
    "key_statements": [
        "Sparse coding, is a classical problem in source coding, signal reconstruction, pattern recognition and feature selection",
        "We introduce a lower bound of the recovery error of Learned ISTA, which illustrates that the parameters analytically given by (9) are optimal in the convergence order",
        "We propose the analytic Learned ISTA (ALISTA) that decomposes tied-Learned ISTA into two stages: x(k+1) = \u03b7\u03b8(k) x(k) \u2212 \u03b3(k)W T (Dx(k) \u2212 b) , (15)",
        "We extend the analytic Learned ISTA to the convolutional case starting from discussing the convolutional sparse coding (CSC)",
        "Similar conclusions can be drawn for convolutional analytic Learned ISTA",
        "The robust analytic Learned ISTA further shows remarkable robustness in sparse code prediction, given that D is randomly perturbed within some extent",
        "If the model is for convolutional case, we add \u201cConv\u201d as the prefix for model name, such as \u201cConv analytic Learned ISTA\u201d that represents the convolutional analytic Learned ISTA.\n5.1",
        "Besides validating Theorem 3, we present a real image denoising experiment to verify the effectiveness of Conv analytic Learned ISTA",
        "We no longer train any matrix for Learned ISTA but directly use the solution to an analytic minimization problem to solve for its layer-wise weights"
    ],
    "summary": [
        "Sparse coding, is a classical problem in source coding, signal reconstruction, pattern recognition and feature selection.",
        "That is based on decoupling LISTA training into a data-free analytic optimization stage followed by a lighter-weight data-driven learning stage without compromising the optimal linear convergence rate proved in (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a></a></a></a></a>).",
        "Experiments shows ALISTA to perform comparably with previous LISTA models (Gregor & LeCun, 2010; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a></a></a></a></a>) with much lighter-weight training.",
        "We introduce a lower bound of the recovery error of LISTA, which illustrates that the parameters analytically given by (9) are optimal in the convergence order.",
        "We extend the analytic LISTA to the convolutional case starting from discussing the convolutional sparse coding (CSC).",
        "Classical LISTA entangles the learning of all its parameters, and the trained model is tied to one static D.",
        "We experimentally validate Theorems 1 and 2, and show that ALISTA is as effective as classical LISTA (Gregor & LeCun, 2010; <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a></a></a></a></a>)but is much easier to train.",
        "Notation For brevity, we let LISTA denote the vanilla LISTA model (4) in (Gregor & LeCun, 2010); LISTA-CPSS refers to the lately-proposed fast LISTA variant (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a></a></a></a></a>) with weight coupling and support selection; TiLISTA is the tied LISTA (14); and ALISTA is our proposed Analytic LISTA (15).",
        "In Figure 1 (a) noise-less case, all four learned models apparently converge much faster than two iterative solvers (ISTA/FISTA curves almost overlap in this y-scale, at the small number of iterations).",
        "Figure 1(a) supports Theorem 2, that all networks have at most linear convergence, regardless of how freely their parameters can be end-to-end learned.",
        "Table 3 shows wcNonv \u2192 w\u2217, i.e., the solution of the problem (22) converges to that of (24) as N increases, validating the second conclusion of Theorem 3.",
        "We empirically verify the effectiveness of Robust ALISTA, by sampling the dictionary perturbation \u03b5D entry-wise i.i.d. from another Gaussian distribution N (0, \u03c3m2 ax).",
        "Figure 4 plots the results when the trained models are applied on the testing data, generated with the same dictionary and perturbed by N (0, \u03c3t).",
        "We no longer train any matrix for LISTA but directly use the solution to an analytic minimization problem to solve for its layer-wise weights.",
        "The resulting method, Analytic LISTA or ALISTA, is not only faster to train but performs as well as the state-of-the-art variant of LISTA by (<a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\"><a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al, 2018</a></a></a></a></a>).",
        "Our future work will investigate the theoretical sensitivity of ALISTA to noisy measurements"
    ],
    "headline": "We propose Analytic Learned ISTA, where the weight matrix in Learned ISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning",
    "reference_links": [
        {
            "id": "Blumensath_2009_a",
            "entry": "Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265 \u2013 274, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009"
        },
        {
            "id": "Borgerding_et+al_2017_a",
            "entry": "Mark Borgerding, Philip Schniter, and Sundeep Rangan. AMP-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing, 65(16):4293\u20134308, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20AMP-inspired%20deep%20networks%20for%20sparse%20linear%20inverse%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20AMP-inspired%20deep%20networks%20for%20sparse%20linear%20inverse%20problems%202017"
        },
        {
            "id": "Bristow_et+al_2013_a",
            "entry": "Hilton Bristow, Anders P. Eriksson, and Simon Lucey. Fast convolutional sparse coding. 2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 391\u2013398, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bristow%2C%20Hilton%20Eriksson%2C%20Anders%20P.%20Lucey%2C%20Simon%20Fast%20convolutional%20sparse%20coding%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bristow%2C%20Hilton%20Eriksson%2C%20Anders%20P.%20Lucey%2C%20Simon%20Fast%20convolutional%20sparse%20coding%202013"
        },
        {
            "id": "Chalasani_et+al_2013_a",
            "entry": "Rakesh Chalasani, Jose C Principe, and Naveen Ramakrishnan. A fast proximal method for convolutional sparse coding. In Neural Networks (IJCNN), The 2013 International Joint Conference on, pp. 1\u20135. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chalasani%2C%20Rakesh%20Principe%2C%20Jose%20C.%20Ramakrishnan%2C%20Naveen%20A%20fast%20proximal%20method%20for%20convolutional%20sparse%20coding%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chalasani%2C%20Rakesh%20Principe%2C%20Jose%20C.%20Ramakrishnan%2C%20Naveen%20A%20fast%20proximal%20method%20for%20convolutional%20sparse%20coding%202013"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ista and its practical weights and thresholds. arXiv preprint arXiv:1808.10038, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.10038"
        },
        {
            "id": "Donoho_2003_a",
            "entry": "David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197\u2013 2202, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Elad%2C%20Michael%20Optimally%20sparse%20representation%20in%20general%20%28nonorthogonal%29%20dictionaries%20via%20l1%20minimization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Elad%2C%20Michael%20Optimally%20sparse%20representation%20in%20general%20%28nonorthogonal%29%20dictionaries%20via%20l1%20minimization%202003"
        },
        {
            "id": "Elad_2007_a",
            "entry": "Michael Elad. Optimized projections for compressed sensing. IEEE Transactions on Signal Processing, 55(12):5695\u20135702, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elad%2C%20Michael%20Optimized%20projections%20for%20compressed%20sensing%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elad%2C%20Michael%20Optimized%20projections%20for%20compressed%20sensing%202007"
        },
        {
            "id": "Elad_2006_a",
            "entry": "Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image processing, 15(12):3736\u20133745, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elad%2C%20Michael%20Aharon%2C%20Michal%20Image%20denoising%20via%20sparse%20and%20redundant%20representations%20over%20learned%20dictionaries%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elad%2C%20Michael%20Aharon%2C%20Michal%20Image%20denoising%20via%20sparse%20and%20redundant%20representations%20over%20learned%20dictionaries%202006"
        },
        {
            "id": "Garcia-Cardona_2018_a",
            "entry": "Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative review and new algorithms. IEEE Transactions on Computational Imaging, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Convolutional%20dictionary%20learning%3A%20A%20comparative%20review%20and%20new%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Convolutional%20dictionary%20learning%3A%20A%20comparative%20review%20and%20new%20algorithms%202018"
        },
        {
            "id": "Giryes_et+al_2018_a",
            "entry": "Raja Giryes, Yonina C Eldar, Alex Bronstein, and Guillermo Sapiro. Tradeoffs between convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giryes%2C%20Raja%20Eldar%2C%20Yonina%20C.%20Bronstein%2C%20Alex%20Sapiro%2C%20Guillermo%20Tradeoffs%20between%20convergence%20speed%20and%20reconstruction%20accuracy%20in%20inverse%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giryes%2C%20Raja%20Eldar%2C%20Yonina%20C.%20Bronstein%2C%20Alex%20Sapiro%2C%20Guillermo%20Tradeoffs%20between%20convergence%20speed%20and%20reconstruction%20accuracy%20in%20inverse%20problems%202018"
        },
        {
            "id": "Gregor_0000_a",
            "entry": "Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 399\u2013406.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding"
        },
        {
            "id": "Omnipress_2010_a",
            "entry": "Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omnipress%202010"
        },
        {
            "id": "Han_et+al_2013_a",
            "entry": "Sheng Han, Ruiqing Fu, Suzhen Wang, and Xinyu Wu. Online adaptive dictionary learning and weighted sparse coding for abnormality detection. In Image Processing (ICIP), 2013 20th IEEE International Conference on, pp. 151\u2013155. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Sheng%20Fu%2C%20Ruiqing%20Wang%2C%20Suzhen%20Wu%2C%20Xinyu%20Online%20adaptive%20dictionary%20learning%20and%20weighted%20sparse%20coding%20for%20abnormality%20detection%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Sheng%20Fu%2C%20Ruiqing%20Wang%2C%20Suzhen%20Wu%2C%20Xinyu%20Online%20adaptive%20dictionary%20learning%20and%20weighted%20sparse%20coding%20for%20abnormality%20detection%202013"
        },
        {
            "id": "Heide_et+al_2015_a",
            "entry": "Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and flexible convolutional sparse coding. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5135\u2013 5143, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heide%2C%20Felix%20Heidrich%2C%20Wolfgang%20Wetzstein%2C%20Gordon%20Fast%20and%20flexible%20convolutional%20sparse%20coding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heide%2C%20Felix%20Heidrich%2C%20Wolfgang%20Wetzstein%2C%20Gordon%20Fast%20and%20flexible%20convolutional%20sparse%20coding%202015"
        },
        {
            "id": "Ito_et+al_2018_a",
            "entry": "Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ista for sparse signal recovery. 2018 IEEE International Conference on Communications Workshops (ICC Workshops), pp. 1\u20136, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ito%2C%20Daisuke%20Takabe%2C%20Satoshi%20Wadayama%2C%20Tadashi%20Trainable%20ista%20for%20sparse%20signal%20recovery%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ito%2C%20Daisuke%20Takabe%2C%20Satoshi%20Wadayama%2C%20Tadashi%20Trainable%20ista%20for%20sparse%20signal%20recovery%202018"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. Online convolutional dictionary learning. 2017 IEEE International Conference on Image Processing (ICIP), pp. 1707\u20131711, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Jialin%20Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Yin%2C%20Wotao%20Online%20convolutional%20dictionary%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Jialin%20Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Yin%2C%20Wotao%20Online%20convolutional%20dictionary%20learning%202017"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. First-and second-order methods for online convolutional dictionary learning. SIAM Journal on Imaging Sciences, 11(2):1589\u2013 1628, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Jialin%20Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Yin%2C%20Wotao%20First-and%20second-order%20methods%20for%20online%20convolutional%20dictionary%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Jialin%20Garcia-Cardona%2C%20Cristina%20Wohlberg%2C%20Brendt%20Yin%2C%20Wotao%20First-and%20second-order%20methods%20for%20online%20convolutional%20dictionary%20learning%202018"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Canyi Lu, Huan Li, and Zhouchen Lin. Optimized projections for compressed sensing via direct mutual coherence minimization. Signal Processing, 151:45\u201355, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Canyi%20Li%2C%20Huan%20Lin%2C%20Zhouchen%20Optimized%20projections%20for%20compressed%20sensing%20via%20direct%20mutual%20coherence%20minimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Canyi%20Li%2C%20Huan%20Lin%2C%20Zhouchen%20Optimized%20projections%20for%20compressed%20sensing%20via%20direct%20mutual%20coherence%20minimization%202018"
        },
        {
            "id": "Metzler_et+al_2017_a",
            "entry": "Christopher A Metzler, Ali Mousavi, and Richard G Baraniuk. Learned D-AMP: Principled neural network based compressive image recovery. In Advances in Neural Information Processing Systems, pp. 1770\u20131781, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzler%2C%20Christopher%20A.%20Mousavi%2C%20Ali%20Baraniuk%2C%20Richard%20G.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzler%2C%20Christopher%20A.%20Mousavi%2C%20Ali%20Baraniuk%2C%20Richard%20G.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017"
        },
        {
            "id": "Moreau_2017_a",
            "entry": "Thomas Moreau and Joan Bruna. Understanding trainable sparse coding with matrix factorization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreau%2C%20Thomas%20Bruna%2C%20Joan%20Understanding%20trainable%20sparse%20coding%20with%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreau%2C%20Thomas%20Bruna%2C%20Joan%20Understanding%20trainable%20sparse%20coding%20with%20matrix%20factorization%202017"
        },
        {
            "id": "Papyan_et+al_2016_a",
            "entry": "Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional sparse coding. arXiv preprint arXiv:1607.08194, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08194"
        },
        {
            "id": "Papyan_et+al_2017_a",
            "entry": "Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Convolutional dictionary learning via local processing. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5306\u20135314, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papyan%2C%20Vardan%20Romano%2C%20Yaniv%20Sulam%2C%20Jeremias%20Elad%2C%20Michael%20Convolutional%20dictionary%20learning%20via%20local%20processing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papyan%2C%20Vardan%20Romano%2C%20Yaniv%20Sulam%2C%20Jeremias%20Elad%2C%20Michael%20Convolutional%20dictionary%20learning%20via%20local%20processing%202017"
        },
        {
            "id": "Rockafellar_2009_a",
            "entry": "R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.Tyrrell%20Wets%2C%20Roger%20J.-B.%20Variational%20analysis%2C%20volume%20317%202009"
        },
        {
            "id": "Rubinstein_2014_a",
            "entry": "Ron Rubinstein and Michael Elad. Dictionary learning for analysis-synthesis thresholding. IEEE Transactions on Signal Processing, 62(22):5962\u20135972, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Ron%20Elad%2C%20Michael%20Dictionary%20learning%20for%20analysis-synthesis%20thresholding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Ron%20Elad%2C%20Michael%20Dictionary%20learning%20for%20analysis-synthesis%20thresholding%202014"
        },
        {
            "id": "Sprechmann_et+al_0000_a",
            "entry": "Pablo Sprechmann, Alexander M Bronstein, and Guillermo Sapiro. Learning efficient sparse and low rank models. IEEE transactions on pattern analysis and machine intelligence, 37(9):1821\u2013 1833, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models"
        },
        {
            "id": "Sreter_2018_a",
            "entry": "Hillel Sreter and Raja Giryes. Learned convolutional sparse coding. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2191\u20132195. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sreter%2C%20Hillel%20Giryes%2C%20Raja%20Learned%20convolutional%20sparse%20coding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sreter%2C%20Hillel%20Giryes%2C%20Raja%20Learned%20convolutional%20sparse%20coding%202018"
        },
        {
            "id": "Sulam_et+al_2017_a",
            "entry": "Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multi-layer convolutional sparse modeling: Pursuit and dictionary learning. arXiv preprint arXiv:1708.08705, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08705"
        },
        {
            "id": "Tolooshams_et+al_2018_a",
            "entry": "Bahareh Tolooshams, Sourav Dey, and Demba Ba. Scalable convolutional dictionary learning with constrained recurrent sparse auto-encoders. arXiv preprint arXiv:1807.04734, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04734"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Scalable online convolutional sparse coding. IEEE Transactions on Image Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yaqing%20Yao%2C%20Quanming%20Kwok%2C%20James%20T.%20Ni%2C%20Lionel%20M.%20Scalable%20online%20convolutional%20sparse%20coding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yaqing%20Yao%2C%20Quanming%20Kwok%2C%20James%20T.%20Ni%2C%20Lionel%20M.%20Scalable%20online%20convolutional%20sparse%20coding%202018"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, and Thomas S Huang. Learning a taskspecific deep architecture for clustering. In Proceedings of the 2016 SIAM International Conference on Data Mining, pp. 369\u2013377. SIAM, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Chang%2C%20Shiyu%20Zhou%2C%20Jiayu%20Wang%2C%20Meng%20Learning%20a%20taskspecific%20deep%20architecture%20for%20clustering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Chang%2C%20Shiyu%20Zhou%2C%20Jiayu%20Wang%2C%20Meng%20Learning%20a%20taskspecific%20deep%20architecture%20for%20clustering%202016"
        },
        {
            "id": "Wang_et+al_2016_b",
            "entry": "Zhangyang Wang, Qing Ling, and Thomas Huang. Learning deep l0 encoders. In AAAI Conference on Artificial Intelligence, pp. 2194\u20132200, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Ling%2C%20Qing%20Huang%2C%20Thomas%20Learning%20deep%20l0%20encoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Ling%2C%20Qing%20Huang%2C%20Thomas%20Learning%20deep%20l0%20encoders%202016"
        },
        {
            "id": "Wang_et+al_2016_c",
            "entry": "Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S Huang. D3: Deep dual-domain based fast restoration of jpeg-compressed images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2764\u20132772, 2016c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Liu%2C%20Ding%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20D3%3A%20Deep%20dual-domain%20based%20fast%20restoration%20of%20jpeg-compressed%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Liu%2C%20Ding%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20D3%3A%20Deep%20dual-domain%20based%20fast%20restoration%20of%20jpeg-compressed%20images%202016"
        },
        {
            "id": "Wang_et+al_2016_d",
            "entry": "Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S. Huang. Learning a deep \u221e encoder for hashing. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pp. 2174\u20132180. AAAI Press, 2016d.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Yang%2C%20Yingzhen%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20Learning%20a%20deep%20%E2%88%9E%20encoder%20for%20hashing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Yang%2C%20Yingzhen%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20Learning%20a%20deep%20%E2%88%9E%20encoder%20for%20hashing%202016"
        },
        {
            "id": "Wohlberg_2014_a",
            "entry": "Brendt Wohlberg. Efficient convolutional sparse coding. 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7173\u20137177, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brendt%20Wohlberg%20Efficient%20convolutional%20sparse%20coding%202014%20IEEE%20International%20Conference%20on%20Acoustics%20Speech%20and%20Signal%20Processing%20ICASSP%20pp%2071737177%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brendt%20Wohlberg%20Efficient%20convolutional%20sparse%20coding%202014%20IEEE%20International%20Conference%20on%20Acoustics%20Speech%20and%20Signal%20Processing%20ICASSP%20pp%2071737177%202014"
        },
        {
            "id": "Wohlberg_2016_a",
            "entry": "Brendt Wohlberg. Efficient algorithms for convolutional sparse representations. IEEE Transactions on Image Processing, 25:301\u2013315, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wohlberg%2C%20Brendt%20Efficient%20algorithms%20for%20convolutional%20sparse%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wohlberg%2C%20Brendt%20Efficient%20algorithms%20for%20convolutional%20sparse%20representations%202016"
        },
        {
            "id": "Wohlberg_2018_a",
            "entry": "Brendt Wohlberg. Convolutional sparse representations with gradient penalties. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6528\u20136532. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wohlberg%2C%20Brendt%20Convolutional%20sparse%20representations%20with%20gradient%20penalties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wohlberg%2C%20Brendt%20Convolutional%20sparse%20representations%20with%20gradient%20penalties%202018"
        },
        {
            "id": "Xin_et+al_2016_a",
            "entry": "Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep networks? In Advances in Neural Information Processing Systems, pp. 4340\u20134348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xin%2C%20Bo%20Wang%2C%20Yizhou%20Gao%2C%20Wen%20Wipf%2C%20David%20Maximal%20sparsity%20with%20deep%20networks%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xin%2C%20Bo%20Wang%2C%20Yizhou%20Gao%2C%20Wen%20Wipf%2C%20David%20Maximal%20sparsity%20with%20deep%20networks%3F%202016"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "Meng Yang, Weiyang Liu, Weixin Luo, and Linlin Shen. Analysis-synthesis dictionary learning for universality-particularity representation based classification. In AAAI, pp. 2251\u20132257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Meng%20Liu%2C%20Weiyang%20Luo%2C%20Weixin%20Shen%2C%20Linlin%20Analysis-synthesis%20dictionary%20learning%20for%20universality-particularity%20representation%20based%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Meng%20Liu%2C%20Weiyang%20Luo%2C%20Weixin%20Shen%2C%20Linlin%20Analysis-synthesis%20dictionary%20learning%20for%20universality-particularity%20representation%20based%20classification%202016"
        },
        {
            "id": "Zhang_2015_a",
            "entry": "Hui Zhang and Lizhi Cheng. Restricted strong convexity and its applications to convergence analysis of gradient-type methods in convex optimization. Optimization Letters, 9(5):961\u2013979, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Hui%20Cheng%2C%20Lizhi%20Restricted%20strong%20convexity%20and%20its%20applications%20to%20convergence%20analysis%20of%20gradient-type%20methods%20in%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Hui%20Cheng%2C%20Lizhi%20Restricted%20strong%20convexity%20and%20its%20applications%20to%20convergence%20analysis%20of%20gradient-type%20methods%20in%20convex%20optimization%202015"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Published as a conference paper at ICLR 2019 Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing. In IEEE CVPR, 2018. Bin Zhao, Li Fei-Fei, and Eric P Xing. Online detection of unusual events in videos via dynamic sparse coding. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 3313\u20133320. IEEE, 2011. Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W Tsang, Yong Liu, Zheng Qin, and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Jian%20Zhang%20and%20Bernard%20Ghanem%20ISTANet%20Interpretable%20optimizationinspired%20deep%20network%20for%20image%20compressive%20sensing%20In%20IEEE%20CVPR%202018%20Bin%20Zhao%20Li%20FeiFei%20and%20Eric%20P%20Xing%20Online%20detection%20of%20unusual%20events%20in%20videos%20via%20dynamic%20sparse%20coding%20In%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202011%20IEEE%20Conference%20on%20pp%2033133320%20IEEE%202011%20Joey%20Tianyi%20Zhou%20Kai%20Di%20Jiawei%20Du%20Xi%20Peng%20Hao%20Yang%20Sinno%20Jialin%20Pan%20Ivor%20W%20Tsang%20Yong%20Liu%20Zheng%20Qin%20and%20Rick%20Siow%20Mong%20Goh%20SC2Net%20Sparse%20LSTMs%20for%20sparse%20coding%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Jian%20Zhang%20and%20Bernard%20Ghanem%20ISTANet%20Interpretable%20optimizationinspired%20deep%20network%20for%20image%20compressive%20sensing%20In%20IEEE%20CVPR%202018%20Bin%20Zhao%20Li%20FeiFei%20and%20Eric%20P%20Xing%20Online%20detection%20of%20unusual%20events%20in%20videos%20via%20dynamic%20sparse%20coding%20In%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202011%20IEEE%20Conference%20on%20pp%2033133320%20IEEE%202011%20Joey%20Tianyi%20Zhou%20Kai%20Di%20Jiawei%20Du%20Xi%20Peng%20Hao%20Yang%20Sinno%20Jialin%20Pan%20Ivor%20W%20Tsang%20Yong%20Liu%20Zheng%20Qin%20and%20Rick%20Siow%20Mong%20Goh%20SC2Net%20Sparse%20LSTMs%20for%20sparse%20coding%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%202018"
        },
        {
            "id": "35",
            "entry": "(35) Similarly, the corresponding circulant matrix WcNir(i, j; k, l, m) of dictionary w is: WcNir(i, j; k, l, m) =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Similarly%20the%20corresponding%20circulant%20matrix",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Similarly%20the%20corresponding%20circulant%20matrix"
        },
        {
            "id": "Epigraphic_2009_a",
            "entry": "Epigraphic convergence is a standard tool to prove the convergence of a sequence of minimization problems. The definition of epigraphic convergence refers to Definition 7.1 and Proposition 7.2 in (Rockafellar & Wets, 2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Epigraphic%20convergence%20is%20a%20standard%20tool%20to%20prove%20the%20convergence%20of%20a%20sequence%20of%20minimization%20problems%20The%20definition%20of%20epigraphic%20convergence%20refers%20to%20Definition%2071%20and%20Proposition%2072%20in%20Rockafellar%20%20Wets%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Epigraphic%20convergence%20is%20a%20standard%20tool%20to%20prove%20the%20convergence%20of%20a%20sequence%20of%20minimization%20problems%20The%20definition%20of%20epigraphic%20convergence%20refers%20to%20Definition%2071%20and%20Proposition%2072%20in%20Rockafellar%20%20Wets%202009"
        },
        {
            "id": "0",
            "entry": "0. Thus, linear operator T Now we check the conditions of Propositions 7.32(c) and 7.33 in (Rockafellar & Wets, 2009) to apply them.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thus%20linear%20operator%20T%20Now%20we%20check%20the%20conditions%20of%20Propositions%20732c%20and%20733%20in%20Rockafellar%20%20Wets%202009%20to%20apply%20them",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thus%20linear%20operator%20T%20Now%20we%20check%20the%20conditions%20of%20Propositions%20732c%20and%20733%20in%20Rockafellar%20%20Wets%202009%20to%20apply%20them"
        },
        {
            "id": "1",
            "entry": "1. FcNonv \u2192\u2212e Fc2iDr \u22121. This is proved in Step 2. 7.33, we have wN \u2192 wcir. By Definition 4.1 in (Rockafellar & Wets, 2009), we obtain the convergence of the sequence of sets",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=FcNonv%20e%20Fc2iDr%201%20This%20is%20proved%20in%20Step%202%20733%20we%20have%20wN%20%20wcir%20By%20Definition%2041%20in%20Rockafellar%20%20Wets%202009%20we%20obtain%20the%20convergence%20of%20the%20sequence%20of%20sets"
        },
        {
            "id": "6",
            "entry": "6. The performances are measured with NMSE in dB, defined in Section 5.1. From the table we can see that encoders do show better robustness when they have more layers, i.e. larger learning capacity. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20performances%20are%20measured%20with%20NMSE%20in%20dB%20defined%20in%20Section%2051%20From%20the%20table%20we%20can%20see%20that%20encoders%20do%20show%20better%20robustness%20when%20they%20have%20more%20layers%20ie%20larger%20learning%20capacity"
        }
    ]
}
