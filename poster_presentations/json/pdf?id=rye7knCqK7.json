{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Cooperative Multi-agent Control Using Deep Reinforcement Learning",
        "author": "Jayesh K. Gupta, Maxim Egorov, Mykel Kochenderfer",
        "date": 2017,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rye7knCqK7",
            "doi": "10.1007/978-3-319-71682-4_5"
        },
        "journal": "Autonomous Agents and Multiagent SystemsLecture Notes in Computer Science",
        "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multiagent scenarios, but have been restricted to fullycooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards for each agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWarsTM explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.",
        "pages": "66-83"
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "real time strategy game",
            "url": "https://en.wikipedia.org/wiki/real_time_strategy_game"
        },
        {
            "term": "multi agent system",
            "url": "https://en.wikipedia.org/wiki/multi_agent_system"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "MARL": "multi-agent reinforcement learning",
        "MADDPG": "Multi-Agent Deep Deterministic Policy Gradient",
        "VAIN": "Vertex Attention Interaction Networks",
        "PP": "predator-prey environment",
        "TJ": "traffic junction environment",
        "SC": "StarCraft BroodWars2",
        "IRIC": "Individual Reward Independent Controller"
    },
    "highlights": [
        "Communication is an essential element of intelligence as it helps in learning from others experience, work better in teams and pass down knowledge",
        "Predator Prey: Table 1 shows average steps taken by the models to complete an episode i.e. find the prey in mixed setting",
        "We introduced IC3Net which aims to solve multi-agent tasks in various cooperation settings by learning when to communicate",
        "Its continuous communication enables efficient training by backpropagation, while the discrete gating trained by reinforcement learning along with individual rewards allows it to be used in all scenarios and on larger scale",
        "We show that IC3Net performs well in cooperative, mixed or competitive settings and learns to communicate only when necessary",
        "We show scalability of our network by further experiments"
    ],
    "key_statements": [
        "Communication is an essential element of intelligence as it helps in learning from others experience, work better in teams and pass down knowledge",
        "There have been a lot of success in the field of reinforcement learning (RL) in playing Atari Games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a>) to playing Go (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a>), most of which have been limited to the single agent domain",
        "We propose Individualized Controlled Continuous Communication Model (IC3Net), in which each agent is trained with its individualized reward and can be applied to any scenario whether cooperative or not.\n2",
        "Predator Prey: Table 1 shows average steps taken by the models to complete an episode i.e. find the prey in mixed setting",
        "In 20\u00d720 version, the gap in average steps is almost 24 steps, which is a substantial improvement over baselines",
        "We introduced IC3Net which aims to solve multi-agent tasks in various cooperation settings by learning when to communicate",
        "Its continuous communication enables efficient training by backpropagation, while the discrete gating trained by reinforcement learning along with individual rewards allows it to be used in all scenarios and on larger scale",
        "We show that IC3Net performs well in cooperative, mixed or competitive settings and learns to communicate only when necessary",
        "We show scalability of our network by further experiments",
        "We would like to explore possibility of having multi-channel communication where agents can decide on which channel they want to put their information similar to communication groups but dynamic"
    ],
    "summary": [
        "Communication is an essential element of intelligence as it helps in learning from others experience, work better in teams and pass down knowledge.",
        "We propose Individualized Controlled Continuous Communication Model (IC3Net), in which each agent is trained with its individualized reward and can be applied to any scenario whether cooperative or not.",
        "Our work can be considered as an all-scenario extension of Sukhbaatar et al (2016)\u2019s CommNet for collaboration among multiple agents using continuous communication; usable only in cooperative settings as stated in their work and shown by our experiments.",
        "We introduce our model Individualized Controlled Continuous Communication Model (IC3Net) as shown in Figure 1 to work in multi-agent cooperative, competitive and mixed settings where agents learn what to communicate as well as when to communicate.",
        "IC3Net extends this independent controller model by allowing agents to communicate their internal state, gated by a discrete action.",
        "This has two benefits: (i) it allows the model to be applied to both cooperative and competitive scenarios, it helps resolve the credit assignment issue faced by many multi-agent (Sukhbaatar et al, 2016; <a class=\"ref-link\" id=\"cFoerster_et+al_2016_a\" href=\"#rFoerster_et+al_2016_a\">Foerster et al, 2016a</a>) algorithms while improving performance with scalability and is coherent with the findings in <a class=\"ref-link\" id=\"cChang_et+al_2003_a\" href=\"#rChang_et+al_2003_a\">Chang et al (2003</a>).",
        "We study our network in multi-agent cooperative, mixed and competitive scenarios to understand its workings.",
        "This task helps in supporting our claim that IC3Net provides good performance and faster convergence in fully-cooperative scenarios similar to mixed ones.",
        "We analyze working of gating action in IC3Net by using cooperative, competitive and mixed settings in Predator-Prey (4.1.1) and StarCraft explore tasks (4.1.3).",
        "With communication and better knowledge of others, the global reward training face a credit assignment issue which is alleviated by IC3Net as evident by its superior performance compared to CommNet. In Sukhbaatar et al (2016), well-performing agents in the medium and hard versions had vision > 0.",
        "This verifies that individualized rewards in IC3Net help achieve a better or similar performance than CommNet in fully-cooperative tasks with communication due to a better credit assignment.",
        "We introduced IC3Net which aims to solve multi-agent tasks in various cooperation settings by learning when to communicate.",
        "Its continuous communication enables efficient training by backpropagation, while the discrete gating trained by reinforcement learning along with individual rewards allows it to be used in all scenarios and on larger scale.",
        "We show that IC3Net performs well in cooperative, mixed or competitive settings and learns to communicate only when necessary."
    ],
    "headline": "We present Individualized Controlled Continuous Communication Model which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. Proceedings of ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pp. 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pp. 41\u201348, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Busoniu_et+al_2008_a",
            "entry": "Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, March 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busoniu%2C%20Lucian%20Babuska%2C%20Robert%20Schutter%2C%20Bart%20De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Busoniu%2C%20Lucian%20Babuska%2C%20Robert%20Schutter%2C%20Bart%20De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008-03"
        },
        {
            "id": "Chang_et+al_2003_a",
            "entry": "Yu-Han Chang, Tracey Ho, and Leslie Pack Kaelbling. All learning is local: Multi-agent learning in global reward games. In Proceedings of the 16th International Conference on Neural Information Processing Systems, NIPS\u201903, pp. 807\u2013814, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Yu-Han%20Ho%2C%20Tracey%20Kaelbling%2C%20Leslie%20Pack%20All%20learning%20is%20local%3A%20Multi-agent%20learning%20in%20global%20reward%20games%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Yu-Han%20Ho%2C%20Tracey%20Kaelbling%2C%20Leslie%20Pack%20All%20learning%20is%20local%3A%20Multi-agent%20learning%20in%20global%20reward%20games%202003"
        },
        {
            "id": "Foerster_et+al_2016_a",
            "entry": "Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 29, pp. 2137\u20132145. 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20Assael%2C%20Ioannis%20Alexandros%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20Assael%2C%20Ioannis%20Alexandros%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016"
        },
        {
            "id": "Foerster_et+al_0000_a",
            "entry": "Jakob N Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv preprint arXiv:1602.02672, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02672"
        },
        {
            "id": "Foerster_et+al_2018_a",
            "entry": "Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018"
        },
        {
            "id": "Gupta_et+al_2017_a",
            "entry": "Jayesh K. Gupta, Maxim Egorov, and Mykel J. Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Adaptive Learning Agents Workshop, 2017. doi: 10.1007/ 978-3-319-71682-4_5.",
            "crossref": "https://dx.doi.org/10.1007/978-3-319-71682-4_5",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/978-3-319-71682-4_5"
        },
        {
            "id": "Havrylov_2017_a",
            "entry": "Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: learning to communicate with sequences of symbols. In Advances in Neural Information Processing Systems, pp. 2149\u20132159, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hoshen_2017_a",
            "entry": "Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In Advances in Neural Information Processing Systems 30, pp. 2701\u20132711. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoshen%2C%20Yedid%20Vain%3A%20Attentional%20multi-agent%20predictive%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoshen%2C%20Yedid%20Vain%3A%20Attentional%20multi-agent%20predictive%20modeling%202017"
        },
        {
            "id": "Kottur_et+al_2017_a",
            "entry": "Satwik Kottur, Jos\u00e9 Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge \u2018naturally\u2019in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2962\u20132967, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20Lee%2C%20Stefan%20Batra%2C%20Dhruv%20Natural%20language%20does%20not%20emerge%20%E2%80%98naturally%E2%80%99in%20multi-agent%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20Lee%2C%20Stefan%20Batra%2C%20Dhruv%20Natural%20language%20does%20not%20emerge%20%E2%80%98naturally%E2%80%99in%20multi-agent%20dialog%202017"
        },
        {
            "id": "Lauer_2000_a",
            "entry": "Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In In Proceedings of the Seventeenth International Conference on Machine Learning, pp. 535\u2013542. Morgan Kaufmann, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lauer%2C%20Martin%20Riedmiller%2C%20Martin%20An%20algorithm%20for%20distributed%20reinforcement%20learning%20in%20cooperative%20multi-agent%20systems%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lauer%2C%20Martin%20Riedmiller%2C%20Martin%20An%20algorithm%20for%20distributed%20reinforcement%20learning%20in%20cooperative%20multi-agent%20systems%202000"
        },
        {
            "id": "Lazaridou_et+al_2017_a",
            "entry": "Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In Proceedings of ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%202017"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Jason Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. Emergent translation in multi-agent communication. In Proceedings of ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20Cho%2C%20Kyunghyun%20Weston%2C%20Jason%20Kiela%2C%20Douwe%20Emergent%20translation%20in%20multi-agent%20communication%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20Cho%2C%20Kyunghyun%20Weston%2C%20Jason%20Kiela%2C%20Douwe%20Emergent%20translation%20in%20multi-agent%20communication%202018"
        },
        {
            "id": "Lehman_et+al_2018_a",
            "entry": "Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Julie Beaulieu, Peter J Bentley, Samuel Bernard, Guillaume Belson, David M Bryson, Nick Cheney, et al. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. arXiv preprint arXiv:1803.03453, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03453"
        },
        {
            "id": "Ryan_et+al_2017_a",
            "entry": "Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 30, pp. 6379\u20136390. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ryan%20Lowe%2C%20Y.I.W.U.%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Abbeel%2C%20OpenAI%20Pieter%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ryan%20Lowe%2C%20Y.I.W.U.%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Abbeel%2C%20OpenAI%20Pieter%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "Matignon_et+al_2007_a",
            "entry": "L. Matignon, G. J. Laurent, and N. L. Fort-Piat. Hysteretic q-learning:an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 64\u201369, Oct 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matignon%2C%20L.%20Laurent%2C%20G.J.%20Fort-Piat%2C%20N.L.%20Hysteretic%20q-learning%3Aan%20algorithm%20for%20decentralized%20reinforcement%20learning%20in%20cooperative%20multi-agent%20teams%202007-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matignon%2C%20L.%20Laurent%2C%20G.J.%20Fort-Piat%2C%20N.L.%20Hysteretic%20q-learning%3Aan%20algorithm%20for%20decentralized%20reinforcement%20learning%20in%20cooperative%20multi-agent%20teams%202007-10"
        },
        {
            "id": "Matignon_et+al_2012_a",
            "entry": "Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems. The Knowledge Engineering Review, 27(1):1\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matignon%2C%20Laetitia%20Laurent%2C%20Guillaume%20J.%20Fort-Piat%2C%20Nadine%20Le%20Independent%20reinforcement%20learners%20in%20cooperative%20markov%20games%3A%20a%20survey%20regarding%20coordination%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matignon%2C%20Laetitia%20Laurent%2C%20Guillaume%20J.%20Fort-Piat%2C%20Nadine%20Le%20Independent%20reinforcement%20learners%20in%20cooperative%20markov%20games%3A%20a%20survey%20regarding%20coordination%20problems%202012"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mordatch_2018_a",
            "entry": "Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018"
        },
        {
            "id": "Omidshafiei_et+al_2017_a",
            "entry": "Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. pp. 2681\u2013 2690, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omidshafiei%2C%20Shayegan%20Pazis%2C%20Jason%20Amato%2C%20Christopher%20How%2C%20Jonathan%20P.%20Deep%20decentralized%20multi-task%20multi-agent%20reinforcement%20learning%20under%20partial%20observability%202017"
        },
        {
            "id": "Ontan_et+al_2013_a",
            "entry": "Santiago Ontan\u00f3n, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game ai research and competition in starcraft. IEEE Transactions on Computational Intelligence and AI in games, 5(4):293\u2013311, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ontan%C3%B3n%2C%20Santiago%20Synnaeve%2C%20Gabriel%20Uriarte%2C%20Alberto%20Richoux%2C%20Florian%20A%20survey%20of%20real-time%20strategy%20game%20ai%20research%20and%20competition%20in%20starcraft%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ontan%C3%B3n%2C%20Santiago%20Synnaeve%2C%20Gabriel%20Uriarte%2C%20Alberto%20Richoux%2C%20Florian%20A%20survey%20of%20real-time%20strategy%20game%20ai%20research%20and%20competition%20in%20starcraft%202013"
        },
        {
            "id": "Panait_2005_a",
            "entry": "Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387\u2013434, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panait%2C%20Liviu%20Luke%2C%20Sean%20Cooperative%20multi-agent%20learning%3A%20The%20state%20of%20the%20art.%20Autonomous%20agents%20and%20multi-agent%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panait%2C%20Liviu%20Luke%2C%20Sean%20Cooperative%20multi-agent%20learning%3A%20The%20state%20of%20the%20art.%20Autonomous%20agents%20and%20multi-agent%202005"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10069"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Sukhbaatar_2016_a",
            "entry": "Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pp. 2244\u20132252, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016"
        },
        {
            "id": "Tan_1993_a",
            "entry": "Ming Tan. Multi-agent reinforcement learning: independent versus cooperative agents. In Proceedings of the Tenth International Conference on International Conference on Machine Learning, pp. 330\u2013 337. Morgan Kaufmann Publishers Inc., 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tan%2C%20Ming%20Multi-agent%20reinforcement%20learning%3A%20independent%20versus%20cooperative%20agents%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tan%2C%20Ming%20Multi-agent%20reinforcement%20learning%3A%20independent%20versus%20cooperative%20agents%201993"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Usunier_et+al_2017_a",
            "entry": "Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. In Proceedings of ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Usunier%2C%20Nicolas%20Synnaeve%2C%20Gabriel%20Lin%2C%20Zeming%20Chintala%2C%20Soumith%20Episodic%20exploration%20for%20deep%20deterministic%20policies%3A%20An%20application%20to%20starcraft%20micromanagement%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Usunier%2C%20Nicolas%20Synnaeve%2C%20Gabriel%20Lin%2C%20Zeming%20Chintala%2C%20Soumith%20Episodic%20exploration%20for%20deep%20deterministic%20policies%3A%20An%20application%20to%20starcraft%20micromanagement%20tasks%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Sida I Wang, Percy Liang, and Christopher D Manning. Learning language games through interaction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 2368\u20132378, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sida%20I%20Wang%20Percy%20Liang%20and%20Christopher%20D%20Manning%20Learning%20language%20games%20through%20interaction%20In%20Proceedings%20of%20the%2054th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20Volume%201%20Long%20Papers%20volume%201%20pp%2023682378%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sida%20I%20Wang%20Percy%20Liang%20and%20Christopher%20D%20Manning%20Learning%20language%20games%20through%20interaction%20In%20Proceedings%20of%20the%2054th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20Volume%201%20Long%20Papers%20volume%201%20pp%2023682378%202016"
        },
        {
            "id": "Wender_2012_a",
            "entry": "Stefan Wender and Ian Watson. Applying reinforcement learning to small scale combat in the real-time strategy game starcraft: Broodwar. In Computational Intelligence and Games (CIG), 2012 IEEE Conference on, pp. 402\u2013408. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wender%2C%20Stefan%20Watson%2C%20Ian%20Applying%20reinforcement%20learning%20to%20small%20scale%20combat%20in%20the%20real-time%20strategy%20game%20starcraft%3A%20Broodwar%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wender%2C%20Stefan%20Watson%2C%20Ian%20Applying%20reinforcement%20learning%20to%20small%20scale%20combat%20in%20the%20real-time%20strategy%20game%20starcraft%3A%20Broodwar%202012"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        }
    ]
}
