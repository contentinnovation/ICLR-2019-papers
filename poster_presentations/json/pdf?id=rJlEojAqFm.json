{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Guest Editorial: A Revolution in the Warehouse: A Retrospective on Kiva Systems and the Grand Challenges Ahead",
        "author": "Raffaello D'Andrea",
        "date": 2012,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJlEojAqFm",
            "doi": "10.1109/tase.2012.2214676"
        },
        "journal": "IEEE Transactions on Automation Science and Engineering",
        "volume": "9",
        "abstract": "The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents\u2019 future behavior in multi-agent environments. Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents\u2019 behavior, and what events mediate the intensity and valence of social interactions. Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary. Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.",
        "pages": "638-639"
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "autonomous system",
            "url": "https://en.wikipedia.org/wiki/autonomous_system"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "intermediate representation",
            "url": "https://en.wikipedia.org/wiki/intermediate_representation"
        },
        {
            "term": "multi agent system",
            "url": "https://en.wikipedia.org/wiki/multi_agent_system"
        },
        {
            "term": "assembly line",
            "url": "https://en.wikipedia.org/wiki/assembly_line"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "social interactions",
            "url": "https://en.wikipedia.org/wiki/social_interactions"
        },
        {
            "term": "artificial agent",
            "url": "https://en.wikipedia.org/wiki/artificial_agent"
        }
    ],
    "abbreviations": {
        "RFM": "Relational Forward Models",
        "MARL": "multi-agent reinforcement learning",
        "GN": "graph networks",
        "RL": "reinforcement learning",
        "RNNs": "recurrent neural networks",
        "GRU": "Gated Recurrent Unit"
    },
    "highlights": [
        "The study of multi-agent systems has received considerable attention in recent years and some of the most advanced autonomous systems in the world today are multi-agent in nature",
        "Our Relational Forward Models is based on graph networks (GN) (<a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a>), and is trained by supervised learning to predict the dynamics of multi-agent systems",
        "We trained our Relational Forward Models modules and baseline models to predict the actions of each agent in each of the three games we considered",
        "The on-board Relational Forward Models modules were trained from scratch alongside the policy networks, while the agents were learning to act, so that no additional game structure was given to the augmented agents",
        "The analysis tools we introduced allow researchers to answer new questions, which are tailored to multi-agent systems, such as what entities, relations and social interactions drive agents\u2019 behaviors, and what environment events or behavior patterns mediate these social and non-social influence signals",
        "We posit that explicit modeling of teammates and opponents is an important research direction in multi-agent reinforcement learning, and one that might alleviate the need for communication, parameter sharing or centralized controllers to achieve coordination"
    ],
    "key_statements": [
        "The study of multi-agent systems has received considerable attention in recent years and some of the most advanced autonomous systems in the world today are multi-agent in nature",
        "Our Relational Forward Models is based on graph networks (GN) (<a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a>), and is trained by supervised learning to predict the dynamics of multi-agent systems",
        "We show that our model performs well at these tasks, and crucially, produces interpretable intermediate representations that are useful both as analysis tools, and as inputs to artificial agents who can exploit these predictions to improve their decision-making",
        "We trained our Relational Forward Models modules and baseline models to predict the actions of each agent in each of the three games we considered",
        "We note that while significant changes in edge norm or the rank order of edge norm can be used to discover events that qualitatively change agents behavior and factors that mediate agents\u2019 social interaction, the raw values have no intrinsic meaning. These findings highlight how the norm of the edge messages, computed by a Relational Forward Models which is trained to predict the future actions in a multi-agent system, contain intepretable and quantifiable information about when and how certain entities and relations influence agents\u2019 behavior, and about which entities and situations mediate the social influence between agents.\n2.2.3",
        "We extended the agent architecture by embedding a Relational Forward Models module in each agent, and augmenting the policy network\u2019s observations with the Relational Forward Models\u2019s output",
        "Incorporating an on-board Relational Forward Models module did not provide the agents with any additional information above and beyond that provided to baseline agents",
        "The on-board Relational Forward Models modules were trained from scratch alongside the policy networks, while the agents were learning to act, so that no additional game structure was given to the augmented agents",
        "The output of the Relational Forward Models module\u2014predicted action logits for fellow agents\u2014was rendered as image planes whose pixel intensity was proportional to the estimated probability that an agent would be at a certain location at the time step2",
        "Our results show that agents that explicitly model each other using an on-board Relational Forward Models learn to coordinate with one another faster than baseline agents (Fig. 5)",
        "We found that augmenting agents with on-board Relational Forward Models modules was more beneficial to agents learning than using MLP + LSTM models (see Sec",
        "The analysis tools we introduced allow researchers to answer new questions, which are tailored to multi-agent systems, such as what entities, relations and social interactions drive agents\u2019 behaviors, and what environment events or behavior patterns mediate these social and non-social influence signals",
        "We posit that explicit modeling of teammates and opponents is an important research direction in multi-agent reinforcement learning, and one that might alleviate the need for communication, parameter sharing or centralized controllers to achieve coordination"
    ],
    "summary": [
        "The study of multi-agent systems has received considerable attention in recent years and some of the most advanced autonomous systems in the world today are multi-agent in nature.",
        "We build on recent advances in neural networks that effectively perform relational reasoning with graph networks (GN) (<a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\"><a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a></a>) to construct models that learn to predict the forward dynamics of multi-agent systems.",
        "Our RFM is based on graph networks (GN) (<a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\"><a class=\"ref-link\" id=\"cBattaglia_et+al_2018_a\" href=\"#rBattaglia_et+al_2018_a\">Battaglia et al, 2018</a></a>), and is trained by supervised learning to predict the dynamics of multi-agent systems.",
        "We trained our RFM modules and baseline models to predict the actions of each agent in each of the three games we considered.",
        "Models were given a graph representation of the state of the environment, and produced an action prediction for each agent.",
        "These results reproduce and advance the conclusion that relational models can be trained to perform action prediction for multi-agent systems, and are superior to non-relational models for this task (<a class=\"ref-link\" id=\"cKipf_et+al_2018_a\" href=\"#rKipf_et+al_2018_a\">Kipf et al, 2018</a>; <a class=\"ref-link\" id=\"cHoshen_2017_a\" href=\"#rHoshen_2017_a\">Hoshen, 2017</a>).",
        "These findings highlight how the norm of the edge messages, computed by a RFM which is trained to predict the future actions in a multi-agent system, contain intepretable and quantifiable information about when and how certain entities and relations influence agents\u2019 behavior, and about which entities and situations mediate the social influence between agents.",
        "The on-board RFM modules were trained from scratch alongside the policy networks, while the agents were learning to act, so that no additional game structure was given to the augmented agents.",
        "Our results show that agents that explicitly model each other using an on-board RFM learn to coordinate with one another faster than baseline agents (Fig. 5).",
        "In the Coin Game environment, the faster learning rate of RFM-augmented agents appears to be due to a superior efficiency in learning to interpret the teammate\u2019s action and infer the negative coin color in each episode.",
        "We showed that our Relational Forward Model can capture the rich social dynamics of multiagent environments, that its intermediate representations contained valuable interpretable information, and that providing this information to learning agents results in faster learning system.",
        "The analysis tools we introduced allow researchers to answer new questions, which are tailored to multi-agent systems, such as what entities, relations and social interactions drive agents\u2019 behaviors, and what environment events or behavior patterns mediate these social and non-social influence signals.",
        "Providing agents with access the output of RFM modules results in agents that learn to coordinate with one another faster than non-augmented baselines.",
        "We posit that explicit modeling of teammates and opponents is an important research direction in multi-agent RL, and one that might alleviate the need for communication, parameter sharing or centralized controllers to achieve coordination"
    ],
    "headline": "We introduce Relational Forward Models for multi-agent learning, networks that can learn to make accurate predictions of agents\u2019 future behavior in multi-agent environments",
    "reference_links": [
        {
            "id": "Bansal_et+al_2017_a",
            "entry": "Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent Complexity via Multi-Agent Competition. 2017. URL http://arxiv.org/abs/1710.03748.",
            "url": "http://arxiv.org/abs/1710.03748",
            "arxiv_url": "https://arxiv.org/pdf/1710.03748"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu. Interaction Networks for Learning about Objects, Relations and Physics. arXiv, 2016. URL http://arxiv.org/abs/1612.00222.",
            "url": "http://arxiv.org/abs/1612.00222",
            "arxiv_url": "https://arxiv.org/pdf/1612.00222"
        },
        {
            "id": "Battaglia_et+al_2018_a",
            "entry": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. 2018. URL http://arxiv.org/abs/1806.01261.",
            "url": "http://arxiv.org/abs/1806.01261",
            "arxiv_url": "https://arxiv.org/pdf/1806.01261"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. 2014. URL http://arxiv.org/abs/1406.1078.",
            "url": "http://arxiv.org/abs/1406.1078",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Engineering_2012_a",
            "entry": "Engineering, 9(4):638\u2013639, 2012. ISSN 1545-5955. doi: 10.1109/TASE.2012.2214676. URL http://ieeexplore.ieee.org/document/6295687/.",
            "crossref": "https://dx.doi.org/10.1109/TASE.2012.2214676"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. arXiv, 2018. URL http://arxiv.org/abs/1802.01561.",
            "url": "http://arxiv.org/abs/1802.01561",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "Foerster_et+al_2016_a",
            "entry": "Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to Communicate with Deep Multi-Agent Reinforcement Learning. 2016. URL http://arxiv.org/abs/1605.06676.",
            "url": "http://arxiv.org/abs/1605.06676",
            "arxiv_url": "https://arxiv.org/pdf/1605.06676"
        },
        {
            "id": "Foerster_et+al_2017_a",
            "entry": "Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with Opponent-Learning Awareness. 2017. URL http://arxiv.org/abs/1709.04326.",
            "url": "http://arxiv.org/abs/1709.04326",
            "arxiv_url": "https://arxiv.org/pdf/1709.04326"
        },
        {
            "id": "Gilmer_et+al_2017_a",
            "entry": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural Message Passing for Quantum Chemistry. 2017. URL http://arxiv.org/abs/1704.01212.",
            "url": "http://arxiv.org/abs/1704.01212",
            "arxiv_url": "https://arxiv.org/pdf/1704.01212"
        },
        {
            "id": "Hamrick_et+al_2017_a",
            "entry": "Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, and Peter W. Battaglia. Metacontrol for Adaptive Imagination-Based Optimization. 2017. URL http://arxiv.org/abs/1705.02670.",
            "url": "http://arxiv.org/abs/1705.02670",
            "arxiv_url": "https://arxiv.org/pdf/1705.02670"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9. Opponent Modeling in Deep Reinforcement Learning. arXiv, 2016a. URL http://arxiv.org/abs/1609.05559.",
            "url": "http://arxiv.org/abs/1609.05559",
            "arxiv_url": "https://arxiv.org/pdf/1609.05559"
        },
        {
            "id": "He_et+al_2016_b",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE International Conference on Computer Vision (ICCV), volume 11-18-Dece, pp. 1026\u20131034, 2016b. ISBN 9781467383912. doi: 10.1109/ICCV.2015.123.",
            "crossref": "https://dx.doi.org/10.1109/ICCV.2015.123",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICCV.2015.123"
        },
        {
            "id": "Hong_et+al_2017_a",
            "entry": "Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. A Deep Policy Inference Q-Network for Multi-Agent Systems. 2017. URL https://arxiv.org/abs/1712.07893.",
            "url": "https://arxiv.org/abs/1712.07893",
            "arxiv_url": "https://arxiv.org/pdf/1712.07893"
        },
        {
            "id": "Hoshen_2017_a",
            "entry": "Yedid Hoshen. VAIN: Attentional Multi-agent Predictive Modeling. arXiv, 2017. URL http://arxiv.org/abs/1706.06122.",
            "url": "http://arxiv.org/abs/1706.06122",
            "arxiv_url": "https://arxiv.org/pdf/1706.06122"
        },
        {
            "id": "Hughes_et+al_2018_a",
            "entry": "Edward Hughes, Joel Z Leibo, Matthew G Philips, Karl Tuyls, Edgar A Du Nez-Guzm\u00e1n, Antonio Garc\u00eda, Cast\u00e3 Neda, Iain Dunning, Tina Zhu, Kevin R Mckee, Raphael Koster, Heather Roff, and Thore Graepel. Inequity aversion resolves intertemporal social dilemmas. arXiv, 2018. URL https://arxiv.org/pdf/1803.08884.pdf.",
            "url": "https://arxiv.org/pdf/1803.08884.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1803.08884"
        },
        {
            "id": "Jaderberg_et+al_2018_a",
            "entry": "Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. 2018. URL http://arxiv.org/abs/1807.01281.",
            "url": "http://arxiv.org/abs/1807.01281",
            "arxiv_url": "https://arxiv.org/pdf/1807.01281"
        },
        {
            "id": "Kipf_et+al_2018_a",
            "entry": "Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural Relational Inference for Interacting Systems. arXiv, 2018. URL http://arxiv.org/abs/1802.04687.",
            "url": "http://arxiv.org/abs/1802.04687",
            "arxiv_url": "https://arxiv.org/pdf/1802.04687"
        },
        {
            "id": "Lanctot_et+al_2017_a",
            "entry": "Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. Advances in Neural Information Processing Systems 30, (Nips), 2017. ISSN 10495258. URL http://arxiv.org/abs/1711.00832.",
            "url": "http://arxiv.org/abs/1711.00832",
            "arxiv_url": "https://arxiv.org/pdf/1711.00832"
        },
        {
            "id": "Leibo_et+al_2017_a",
            "entry": "Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent Reinforcement Learning in Sequential Social Dilemmas. Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 464\u2013473, 2017. URL https://dl.acm.org/citation.cfm?id=3091194.",
            "url": "https://dl.acm.org/citation.cfm?id=3091194",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leibo%2C%20Joel%20Z.%20Zambaldi%2C%20Vinicius%20Lanctot%2C%20Marc%20Marecki%2C%20Janusz%20Multi-agent%20Reinforcement%20Learning%20in%20Sequential%20Social%20Dilemmas%202017"
        },
        {
            "id": "Lerer_2017_a",
            "entry": "Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilemmas using deep reinforcement learning. CoRR, abs/1707.0, 2017. URL http://arxiv.org/abs/1707.01068{%}0Ahttps://arxiv.org/pdf/1707.01068.pdf.",
            "url": "http://arxiv.org/abs/1707.01068{%}0Ahttps://arxiv.org/pdf/1707.01068.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1707.01068"
        },
        {
            "id": "Lowe_et+al_2017_a",
            "entry": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-Agent ActorCritic for Mixed Cooperative-Competitive Environments. arXiv, 2017. URL http://arxiv.org/abs/1706.02275.",
            "url": "http://arxiv.org/abs/1706.02275",
            "arxiv_url": "https://arxiv.org/pdf/1706.02275"
        },
        {
            "id": "Morcos_et+al_2018_a",
            "entry": "Ari S. Morcos, David G. T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. 2018. URL http://arxiv.org/abs/1803.06959.",
            "url": "http://arxiv.org/abs/1803.06959",
            "arxiv_url": "https://arxiv.org/pdf/1803.06959"
        },
        {
            "id": "Olah_et+al_2017_a",
            "entry": "Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature Visualization. Distill, 2(11):e7, 2017. ISSN 2476-0757. doi: 10.23915/distill.00007. URL https://distill.pub/2017/feature-visualization.",
            "crossref": "https://dx.doi.org/10.23915/distill.00007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.23915/distill.00007"
        },
        {
            "id": "Pachocki_et+al_2018_a",
            "entry": "Jakub Pachocki, Szymon Sidor, Greg Brockman, Filiip Wolski, Jie Tang, Jonathan Raiman, Christy Dennison, Przemyslaw Debiak, Susan Zhang, David Farhi, Brooke Chan, Henrique Ponde, and Petrov Michael. OpenAI Five, 2018. URL https://openai.com/five/.",
            "url": "https://openai.com/five/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jakub%20Pachocki%20Szymon%20Sidor%20Greg%20Brockman%20Filiip%20Wolski%20Jie%20Tang%20Jonathan%20Raiman%20Christy%20Dennison%20Przemyslaw%20Debiak%20Susan%20Zhang%20David%20Farhi%20Brooke%20Chan%20Henrique%20Ponde%20and%20Petrov%20Michael%20OpenAI%20Five%202018%20URL%20httpsopenaicomfive"
        },
        {
            "id": "Pascanu_et+al_2017_a",
            "entry": "Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racani\u00e8re, David Reichert, Th\u00e9ophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. 2017. URL http://arxiv.org/abs/1707.06170.",
            "url": "http://arxiv.org/abs/1707.06170",
            "arxiv_url": "https://arxiv.org/pdf/1707.06170"
        },
        {
            "id": "Perolat_et+al_2017_a",
            "entry": "Julien Perolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. A multi-agent reinforcement learning model of common-pool resource appropriation. 2017. URL http://arxiv.org/abs/1707.06600.",
            "url": "http://arxiv.org/abs/1707.06600",
            "arxiv_url": "https://arxiv.org/pdf/1707.06600"
        },
        {
            "id": "Peysakhovich_2017_a",
            "entry": "Alexander Peysakhovich and Adam Lerer. Consequentialist conditional cooperation in social dilemmas with imperfect information. arXiv, 2017a. URL http://arxiv.org/abs/1710.06975.",
            "url": "http://arxiv.org/abs/1710.06975",
            "arxiv_url": "https://arxiv.org/pdf/1710.06975"
        },
        {
            "id": "Peysakhovich_2017_b",
            "entry": "Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve generalized Stag Hunts better than selfish ones. arXiv, 2017b. URL http://arxiv.org/abs/1709.02865.",
            "url": "http://arxiv.org/abs/1709.02865",
            "arxiv_url": "https://arxiv.org/pdf/1709.02865"
        },
        {
            "id": "Rabinowitz_et+al_2018_a",
            "entry": "Neil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew Botvinick. Machine Theory of Mind. arXiv, 2018. URL http://arxiv.org/abs/1802.07740.",
            "url": "http://arxiv.org/abs/1802.07740",
            "arxiv_url": "https://arxiv.org/pdf/1802.07740"
        },
        {
            "id": "Raileanu_et+al_2018_a",
            "entry": "Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling Others using Oneself in Multi-Agent Reinforcement Learning. arXiv preprint arXiv:1902.09640, 2018. URL http://arxiv.org/abs/1802.09640.",
            "url": "http://arxiv.org/abs/1802.09640",
            "arxiv_url": "https://arxiv.org/pdf/1902.09640"
        },
        {
            "id": "Raposo_et+al_2017_a",
            "entry": "David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, and Peter Battaglia. Discovering objects and their relations from entangled scene representations. arXiv, 2017. URL http://arxiv.org/abs/1702.05068.",
            "url": "http://arxiv.org/abs/1702.05068",
            "arxiv_url": "https://arxiv.org/pdf/1702.05068"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv, 2017. URL http://arxiv.org/abs/1706.01427.",
            "url": "http://arxiv.org/abs/1706.01427",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Sukhbaatar_et+al_2016_a",
            "entry": "Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning Multiagent Communication with Backpropagation. arXiv, 2016. URL http://arxiv.org/abs/1605.07736.",
            "url": "http://arxiv.org/abs/1605.07736",
            "arxiv_url": "https://arxiv.org/pdf/1605.07736"
        },
        {
            "id": "Watters_et+al_2017_a",
            "entry": "Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, and Daniel Zoran. Visual Interaction Networks. arXiv, 2017. URL http://arxiv.org/abs/1706.01433.",
            "url": "http://arxiv.org/abs/1706.01433",
            "arxiv_url": "https://arxiv.org/pdf/1706.01433"
        },
        {
            "id": "Weber_et+al_2017_a",
            "entry": "Th\u00e9ophane Weber, S\u00e9bastien Racani\u00e8re, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. ImaginationAugmented Agents for Deep Reinforcement Learning. 2017. URL http://arxiv.org/abs/1707.06203.",
            "url": "http://arxiv.org/abs/1707.06203",
            "arxiv_url": "https://arxiv.org/pdf/1707.06203"
        },
        {
            "id": "Yosinski_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding Neural Networks Through Deep Visualization. 2015. URL http://arxiv.org/abs/1506.06579. Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. Relational Deep Reinforcement Learning.2018. URL http://arxiv.org/abs/1806.01830. Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks.2013. URL http://arxiv.org/abs/1311.2901. Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. Generative Multi-Agent Behavioral Cloning.2018. URL http://arxiv.org/abs/1803.07612. Stephan Zheng, Yisong Yue, and Patrick Lucey. Generating Long-term Trajectories Using Deep Hierarchical Networks.2017. URL http://arxiv.org/abs/1706.07138.",
            "url": "http://arxiv.org/abs/1506.06579",
            "arxiv_url": "https://arxiv.org/pdf/1506.06579"
        }
    ]
}
