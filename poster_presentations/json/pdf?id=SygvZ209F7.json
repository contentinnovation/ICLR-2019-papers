{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "BIOLOGICALLY-PLAUSIBLE LEARNING ALGORITHMS CAN SCALE TO LARGE DATASETS",
        "author": "Will Xiao Department of Molecular and Cellular Biology Harvard University Cambridge, MA 02138, USA xiaow@fas.harvard.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SygvZ209F7"
        },
        "abstract": "The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this \u201cweight transport problem\u201d (<a class=\"ref-link\" id=\"cGrossberg_1987_a\" href=\"#rGrossberg_1987_a\">Grossberg, 1987</a>), two biologically-plausible algorithms, proposed by Liao et al. (2016b) and Lillicrap et al. (2016), relax BP\u2019s weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algorithm (Liao et al., 2016b), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examined the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet; RetinaNet for MS COCO). Surprisingly, networks trained with signsymmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018) and establish a new benchmark for future biologically-plausible learning algorithms on more difficult datasets and more complex architectures."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "symmetry",
            "url": "https://en.wikipedia.org/wiki/symmetry"
        },
        {
            "term": "COCO",
            "url": "https://en.wikipedia.org/wiki/COCO"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "Semiconductor Research Corporation",
            "url": "https://en.wikipedia.org/wiki/Semiconductor_Research_Corporation"
        },
        {
            "term": "Pharmacology",
            "url": "https://en.wikipedia.org/wiki/Pharmacology"
        },
        {
            "term": "visual cortex",
            "url": "https://en.wikipedia.org/wiki/visual_cortex"
        }
    ],
    "abbreviations": {
        "FA": "feedback alignment",
        "FPN": "Feature Pyramid Network",
        "SRC": "Semiconductor Research Corporation"
    },
    "highlights": [
        "Deep learning models today are highly successful in task performance, learning useful representations, and even matching representations in the brain (<a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\"><a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\">Yamins et al, 2014</a></a>; <a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\"><a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\">Schrimpf et al, 2018</a></a>)",
        "Core to the problem is the fact that backpropagation, the learning algorithm underlying most of today\u2019s deep networks, is difficult to implement in the brain given what we know about the brain\u2019s hardware (<a class=\"ref-link\" id=\"cCrick_1989_a\" href=\"#rCrick_1989_a\">Crick 1989</a>; see <a class=\"ref-link\" id=\"cHinton_2007_a\" href=\"#rHinton_2007_a\">Hinton 2007</a>)",
        "We find that sign-symmetry can train networks on both tasks, achieving similar performance to backpropagation on ImageNet and reasonable performance on MS COCO",
        "In sign-symmetry, at each backward step, feedback weights were taken as the signs of the feedforward weights, scaled by the same scale \u03bb used to initialize that layer.2",
        "Sign-symmetry only slightly underperformed backpropagation in this benchmark large dataset, despite the fact that signsymmetry does not accurately propagate either the magnitude or the sign of the error gradient. This result is not predicted by the performance of signSGD (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a>), where weight updates use the sign of the gradients, but gradients are still calculate accurately; or XNORNet (<a class=\"ref-link\" id=\"cRastegari_et+al_2016_a\" href=\"#rRastegari_et+al_2016_a\">Rastegari et al, 2016</a>), where both feedforward and feedback weights are binary but symmetrical, so error backpropagation is still accurate",
        "We evaluated sign-symmetry and re-evaluated feedback alignment on their effectiveness training ResNet and AlexNet on ImageNet and RetinaNet on MS COCO"
    ],
    "key_statements": [
        "Deep learning models today are highly successful in task performance, learning useful representations, and even matching representations in the brain (<a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\"><a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\">Yamins et al, 2014</a></a>; <a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\"><a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\">Schrimpf et al, 2018</a></a>)",
        "Core to the problem is the fact that backpropagation, the learning algorithm underlying most of today\u2019s deep networks, is difficult to implement in the brain given what we know about the brain\u2019s hardware (<a class=\"ref-link\" id=\"cCrick_1989_a\" href=\"#rCrick_1989_a\">Crick 1989</a>; see <a class=\"ref-link\" id=\"cHinton_2007_a\" href=\"#rHinton_2007_a\">Hinton 2007</a>)",
        "One main reason why backpropagation seems implausible in the brain is that it requires sharing of feedforward and feedback weights",
        "We find that sign-symmetry can train networks on both tasks, achieving similar performance to backpropagation on ImageNet and reasonable performance on MS COCO",
        "In sign-symmetry, at each backward step, feedback weights were taken as the signs of the feedforward weights, scaled by the same scale \u03bb used to initialize that layer.2",
        "Sign-symmetry only slightly underperformed backpropagation in this benchmark large dataset, despite the fact that signsymmetry does not accurately propagate either the magnitude or the sign of the error gradient. This result is not predicted by the performance of signSGD (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a>), where weight updates use the sign of the gradients, but gradients are still calculate accurately; or XNORNet (<a class=\"ref-link\" id=\"cRastegari_et+al_2016_a\" href=\"#rRastegari_et+al_2016_a\">Rastegari et al, 2016</a>), where both feedforward and feedback weights are binary but symmetrical, so error backpropagation is still accurate",
        "Besides the ImageNet classification task, we examined the performance of sign-symmetry on the MS COCO object detection task",
        "<a class=\"ref-link\" id=\"cLillicrap_et+al_2016_a\" href=\"#rLillicrap_et+al_2016_a\">Lillicrap et al (2016</a>) show that with feedback alignment, the alignment angles between feedforward and feedback weights gradually decrease because the feedforward weights learn to align with the feedback weights",
        "We asked whether the same happens in sign-symmetry by computing alignment angles as in <a class=\"ref-link\" id=\"cLillicrap_et+al_2016_a\" href=\"#rLillicrap_et+al_2016_a\">Lillicrap et al (2016</a>): For every pair of feedforward and feedback weight matrices, we flattened the matrices into vectors and computed the angle between the vectors",
        "Since the magnitudes of the feedforward weights were discarded when calculating the error gradients, we looked at how sign-symmetry affected the size of the trained weights",
        "Our results indicate that biologically-plausible learning algorithms, sign-symmetry and feedback alignment, are able to learn on ImageNet",
        "We evaluated sign-symmetry and re-evaluated feedback alignment on their effectiveness training ResNet and AlexNet on ImageNet and RetinaNet on MS COCO"
    ],
    "summary": [
        "Deep learning models today are highly successful in task performance, learning useful representations, and even matching representations in the brain (<a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\"><a class=\"ref-link\" id=\"cYamins_et+al_2014_a\" href=\"#rYamins_et+al_2014_a\">Yamins et al, 2014</a></a>; <a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\"><a class=\"ref-link\" id=\"cSchrimpf_et+al_2018_a\" href=\"#rSchrimpf_et+al_2018_a\">Schrimpf et al, 2018</a></a>).",
        "One main reason why backpropagation seems implausible in the brain is that it requires sharing of feedforward and feedback weights.",
        "When the feedback weights share only the sign but not the magnitude of the feedforward weights (Liao et al, 2016b) or even when the feedback weights are random (<a class=\"ref-link\" id=\"cLillicrap_et+al_2016_a\" href=\"#rLillicrap_et+al_2016_a\">Lillicrap et al, 2016</a>), they can still guide useful learning in the network, with performance comparable to and sometimes even better than performance of backpropagation, on datasets such as MNIST and CIFAR.",
        "We re-examine the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using standard ConvNet architectures (i.e., ResNet-18, AlexNet, and RetinaNet).",
        "We find that sign-symmetry can train networks on both tasks, achieving similar performance to backpropagation on ImageNet and reasonable performance on MS COCO.",
        "Its performance was considerably worse, feedback alignment was still able to guide better learning in the network than reported by <a class=\"ref-link\" id=\"cBartunov_et+al_2018_a\" href=\"#rBartunov_et+al_2018_a\"><a class=\"ref-link\" id=\"cBartunov_et+al_2018_a\" href=\"#rBartunov_et+al_2018_a\">Bartunov et al (2018</a></a>, their Figure 3) if we use backpropagation in the last layer.",
        "We tested using backpropagation exclusively for the last layer in a network otherwise trained with sign-symmetry, but the effect on the performance was minimal.",
        "Standard SGD consistently worked better than BM for sign-symmetry, but BM may improve results for feedback alignment.",
        "The results here represent a lowerbound on the performance of sign-symmetry for training networks on the COCO dataset.",
        "In the backpropagation-trained network was not used in any way), the analogous alignment angle between W and sign(W ) increased for all layers.",
        "Our results indicate that biologically-plausible learning algorithms, sign-symmetry and feedback alignment, are able to learn on ImageNet. This finding seemingly conflicts with the findings by <a class=\"ref-link\" id=\"cBartunov_et+al_2018_a\" href=\"#rBartunov_et+al_2018_a\"><a class=\"ref-link\" id=\"cBartunov_et+al_2018_a\" href=\"#rBartunov_et+al_2018_a\">Bartunov et al (2018</a></a>).",
        "Recent work shows that biologically-plausible learning algorithms do not scale to challenging problems such as ImageNet. We evaluated sign-symmetry and re-evaluated feedback alignment on their effectiveness training ResNet and AlexNet on ImageNet and RetinaNet on MS COCO.",
        "1) sign-symmetry performed nearly as well as backpropagation on ImageNet, 2) slightly modified feedback alignment performed better than previously reported, and 3) both algorithms had reasonable performance on MS COCO with minimal hyperparameter tuning.",
        "These results indicate that biologically-plausible learning algorithms, in particular sign-symmetry, remain promising options for training artificial neural networks and modeling learning in the brain."
    ],
    "headline": "We evaluate the sign-symmetry algorithm, which differs from both BP and feedback alignment in that the feedback and feedforward weights do not share magnitudes but share signs",
    "reference_links": [
        {
            "id": "Bartunov_et+al_2018_a",
            "entry": "Sergey Bartunov, Adam Santoro, Blake A Richards, Geoffrey E Hinton, and Timothy Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures. arXiv Preprint, 2018. URL https://arxiv.org/abs/1807.04587.",
            "url": "https://arxiv.org/abs/1807.04587",
            "arxiv_url": "https://arxiv.org/pdf/1807.04587"
        },
        {
            "id": "Bernstein_et+al_2018_a",
            "entry": "Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD: compressed optimisation for non-convex problems. CoRR, 2018. URL https://arxiv.org/abs/1802.04434.",
            "url": "https://arxiv.org/abs/1802.04434",
            "arxiv_url": "https://arxiv.org/pdf/1802.04434"
        },
        {
            "id": "Crick_1989_a",
            "entry": "Francis Crick. The recent excitement about neural networks. Nature, 337(6203):129\u2013132, 1989. doi: 10.1038/337129a0.",
            "crossref": "https://dx.doi.org/10.1038/337129a0",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/337129a0"
        },
        {
            "id": "Dale_1935_a",
            "entry": "Henry Dale. Pharmacology and Nerve-endings. (Walter Ernest Dixon Memorial Lecture): (Section of Therapeutics and Pharmacology). Proc. R. Soc. Med., 28(3):319\u2013332, Jan 1935.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dale%2C%20Henry%20Pharmacology%20and%20Nerve-endings.%20%28Walter%20Ernest%20Dixon%20Memorial%20Lecture%29%3A%20%28Section%20of%20Therapeutics%20and%20Pharmacology%29%201935-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dale%2C%20Henry%20Pharmacology%20and%20Nerve-endings.%20%28Walter%20Ernest%20Dixon%20Memorial%20Lecture%29%3A%20%28Section%20of%20Therapeutics%20and%20Pharmacology%29%201935-01"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255. IEEE, June 2009. doi: 10.1109/CVPR.2009.5206848.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2009.5206848",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2009.5206848"
        },
        {
            "id": "Grossberg_1987_a",
            "entry": "Stephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cognitive science, 11(1):23\u201363, 1987. doi: 10.1111/j.1551-6708.1987.tb00862.x.",
            "crossref": "https://dx.doi.org/10.1111/j.1551-6708.1987.tb00862.x",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1111/j.1551-6708.1987.tb00862.x"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778. IEEE, June 2016. doi: 10.1109/CVPR.2016.90.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2016.90",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2016.90"
        },
        {
            "id": "Hinton_2007_a",
            "entry": "Geoffrey E. Hinton. How to do backpropagation in a brain. Invited talk at the NIPS\u20192007 Deep Learning Workshop, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20How%20to%20do%20backpropagation%20in%20a%20brain.%20Invited%20talk%20at%20the%20NIPS%E2%80%992007%20Deep%20Learning%20Workshop%202007"
        },
        {
            "id": "Huberman_et+al_2008_a",
            "entry": "Andrew D. Huberman, Marla B. Feller, and Barbara Chapman. Mechanisms underlying development of visual maps and receptive fields. Annu. Rev. Neurosci., 31(1):479\u2013509, July 2008. doi: 10.1146/annurev.neuro.31.060407.125533.",
            "crossref": "https://dx.doi.org/10.1146/annurev.neuro.31.060407.125533",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1146/annurev.neuro.31.060407.125533"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. 32nd ICML - Volume 37, ICML\u201915, pp. 448\u2013456. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045167.",
            "url": "http://dl.acm.org/citation.cfm?id=3045118.3045167",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jain_2018_a",
            "entry": "Viren Jain and Michal Januszewski. Improving connectomics by an order of magnitude. https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html, July 2018. Accessed:2018-11-21.",
            "url": "https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html"
        },
        {
            "id": "Januszewski_et+al_2018_a",
            "entry": "Micha\u0142 Januszewski, Jorgen Kornfeld, Peter H Li, Art Pope, Tim Blakely, Larry Lindsey, Jeremy Maitin-Shepard, Mike Tyka, Winfried Denk, and Viren Jain. High-precision automated reconstruction of neurons with flood-filling networks. Nature methods, 15(8):605, July 2018. doi: 10.1038/s41592-018-0049-4.",
            "crossref": "https://dx.doi.org/10.1038/s41592-018-0049-4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/s41592-018-0049-4"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Max Lapan, Peter G Ericson, Valentin, and Dmitry Ulyanov. CNN benchmarks. https://github.com/jcjohnson/cnn-benchmarks#resnet-cvpr, 2016. Accessed:2018-11-24.",
            "url": "https://github.com/jcjohnson/cnn-benchmarks#resnet-cvpr"
        },
        {
            "id": "Krizhevsky_2014_a",
            "entry": "Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. CoRR, 2014. URL http://arxiv.org/abs/1404.5997.",
            "url": "http://arxiv.org/abs/1404.5997",
            "arxiv_url": "https://arxiv.org/pdf/1404.5997"
        },
        {
            "id": "Liao_2016_a",
            "entry": "Qianli Liao and Tomaso Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:1604.03640, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.03640"
        },
        {
            "id": "Liao_et+al_0000_a",
            "entry": "Qianli Liao, Kenji Kawaguchi, and Tomaso Poggio. Streaming normalization: Towards simpler and more biologically-plausible normalizations for online and recurrent learning. arXiv preprint arXiv:1610.06160, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06160"
        },
        {
            "id": "Liao_et+al_1844_a",
            "entry": "Qianli Liao, Joel Z Leibo, and Tomaso Poggio. How important is weight symmetry in backpropagation? In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 1837\u2013 1844. AAAI Press, 2016b. URL http://dl.acm.org/citation.cfm?id=3016100.3016156.",
            "url": "http://dl.acm.org/citation.cfm?id=3016100.3016156",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liao%2C%20Qianli%20Leibo%2C%20Joel%20Z.%20Poggio%2C%20Tomaso%20How%20important%20is%20weight%20symmetry%20in%20backpropagation%3F%201844"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7, 2016. doi: 10.1038/ncomms13276.",
            "crossref": "https://dx.doi.org/10.1038/ncomms13276",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/ncomms13276"
        },
        {
            "id": "Lin_et+al_2018_a",
            "entry": "Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. IEEE transactions on pattern analysis and machine intelligence, 2018. doi: 10.1109/TPAMI.2018.2858826.",
            "crossref": "https://dx.doi.org/10.1109/TPAMI.2018.2858826"
        },
        {
            "id": "Marshel_et+al_2011_a",
            "entry": "James H. Marshel, Marina E. Garrett, Ian Nauhaus, and Edward M. Callaway. Functional specialization of seven mouse visual cortical areas. Neuron, 72(6):1040\u20131054, 2011. doi: 10.1016/j.neuron.2011.11.013.",
            "crossref": "https://dx.doi.org/10.1016/j.neuron.2011.11.013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.neuron.2011.11.013"
        },
        {
            "id": "Mclaughlin_2005_a",
            "entry": "Todd McLaughlin and Dennis DM O\u2019Leary. Molecular gradients and development of retinotopic maps. Annu. Rev. Neurosci., 28:327\u2013355, 2005. doi: 10.1146/annurev.neuro.28.061604.135714.",
            "crossref": "https://dx.doi.org/10.1146/annurev.neuro.28.061604.135714",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1146/annurev.neuro.28.061604.135714"
        },
        {
            "id": "Poggio_et+al_2017_a",
            "entry": "Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. International Journal of Automation and Computing, 14(5):503\u2013519, October 2017. doi: 10.1007/ s11633-017-1054-2.",
            "crossref": "https://dx.doi.org/10.1007/s11633-017-1054-2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11633-017-1054-2"
        },
        {
            "id": "Rastegari_et+al_2016_a",
            "entry": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet classification using binary convolutional neural networks. CoRR, 2016. URL https://arxiv.org/abs/1603.05279.",
            "url": "https://arxiv.org/abs/1603.05279",
            "arxiv_url": "https://arxiv.org/pdf/1603.05279"
        },
        {
            "id": "Schmidt_et+al_2017_a",
            "entry": "Helene Schmidt, Anjali Gour, Jakob Straehle, Kevin M Boergens, Michael Brecht, and Moritz Helmstaedter. Axonal synapse sorting in medial entorhinal cortex. Nature, 549(7673):469, 2017. doi: 10.1038/nature24005.",
            "crossref": "https://dx.doi.org/10.1038/nature24005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature24005"
        },
        {
            "id": "Schrimpf_et+al_2018_a",
            "entry": "Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, Daniel L. K. Yamins, and James J. DiCarlo. Brain-score: Which artificial neural network for object recognition is most brain-like? bioRxiv, 2018. URL https://www.biorxiv.org/content/early/2018/09/05/407007.",
            "url": "https://www.biorxiv.org/content/early/2018/09/05/407007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schrimpf%2C%20Martin%20Kubilius%2C%20Jonas%20Hong%2C%20Ha%20Majaj%2C%20Najib%20J.%20Brain-score%3A%20Which%20artificial%20neural%20network%20for%20object%20recognition%20is%20most%20brain-like%3F%202018"
        },
        {
            "id": "Strata_1999_a",
            "entry": "Piergiorgio Strata and Robin Harvey. Dale\u2019s principle. Brain Research Bulletin, 50(5):349\u2013350, 1999. ISSN 0361-9230. doi: 10.1016/S0361-9230(99)00100-8.",
            "crossref": "https://dx.doi.org/10.1016/S0361-9230(99)00100-8",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/S0361-9230%2899%2900100-8"
        },
        {
            "id": "Yamins_et+al_2014_a",
            "entry": "Daniel L. K. Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and James J. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl. Acad. Sci. U.S.A, 111(23):8619\u20138624, 2014. ISSN 0027-8424. doi: 10.1073/ pnas.1403112111.",
            "crossref": "https://dx.doi.org/10.1073/pnas.1403112111",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1073/pnas.1403112111"
        }
    ]
}
