{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "KNOWLEDGE FLOW: IMPROVE UPON YOUR TEACH-",
        "author": "Iou-Jen Liu, Jian Peng, Alexander G. Schwing University of Illinois at Urbana-Champaign {iliu, jpeng, aschwing}@illinois.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJeOioA9Y7"
        },
        "abstract": "A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other \u2018knowledge exchange\u2019 methods."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "fine tuning",
            "url": "https://en.wikipedia.org/wiki/fine_tuning"
        },
        {
            "term": "deep net",
            "url": "https://en.wikipedia.org/wiki/deep_net"
        },
        {
            "term": "new model",
            "url": "https://en.wikipedia.org/wiki/New_Model"
        },
        {
            "term": "unsupervised feature learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_feature_learning"
        },
        {
            "term": "knowledge flow",
            "url": "https://en.wikipedia.org/wiki/knowledge_flow"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "PNN": "progressive neural net"
    },
    "highlights": [
        "Research communities have amassed a sizable number of deep net architectures for different tasks, and new ones are added almost daily",
        "Actor-mimic (<a class=\"ref-link\" id=\"cParisotto_et+al_2016_a\" href=\"#rParisotto_et+al_2016_a\">Parisotto et al, 2016</a>) pre-trains a big model on multiple source tasks, the big model is used as a weight initialization for a new model which will be trained on a new target task",
        "Knowledge flow is applicable to a variety of settings from supervised learning to reinforcement learning, which we briefly review to introduce notation",
        "We evaluate knowledge flow on reinforcement learning using Atari games that were used by Rusu et al (2016b); <a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\">Fernando et al (2017</a>)",
        "We developed a general knowledge flow approach that permits to train a deep net from any number of teachers",
        "We showed results for reinforcement learning and supervised learning, demonstrating improvements compared to training from scratch and to fine-tuning"
    ],
    "key_statements": [
        "Research communities have amassed a sizable number of deep net architectures for different tasks, and new ones are added almost daily",
        "Some of those architectures are trained from scratch while others are fine-tuned, i.e., before training, their weights are initialized using a structurally similar deep net which was trained on different data",
        "Actor-mimic (<a class=\"ref-link\" id=\"cParisotto_et+al_2016_a\" href=\"#rParisotto_et+al_2016_a\">Parisotto et al, 2016</a>) pre-trains a big model on multiple source tasks, the big model is used as a weight initialization for a new model which will be trained on a new target task",
        "We evaluate knowledge flow on a variety of tasks from reinforcement learning to fully-supervised learning",
        "Knowledge flow is applicable to a variety of settings from supervised learning to reinforcement learning, which we briefly review to introduce notation",
        "To address their mentioned shortcomings, we propose knowledge flow, a framework that moves \u2018knowledge\u2019 from an arbitrary number of deep nets, referred to as \u2018teachers\u2019 to a deep net under training, called the \u2018student.\u2019",
        "In the following we evaluate knowledge flow on reinforcement and supervised learning tasks",
        "We evaluate knowledge flow on reinforcement learning using Atari games that were used by Rusu et al (2016b); <a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\">Fernando et al (2017</a>)",
        "The results demonstrate that knowledge flow effectively transfers knowledge from teachers to the student",
        "In knowledge flow, the student can avoid the negative impact from insufficiently pretrained teachers, while fine-tuning from an insufficiently pretrained model slows down the training process and may degrade the overall performance",
        "In multi-task learning, information extracted from different tasks are shared to boost performance, while, in knowledge flow, the information of multiple teachers is leveraged to help a student learn better a single, new, previously unseen task",
        "We developed a general knowledge flow approach that permits to train a deep net from any number of teachers",
        "We showed results for reinforcement learning and supervised learning, demonstrating improvements compared to training from scratch and to fine-tuning"
    ],
    "summary": [
        "Research communities have amassed a sizable number of deep net architectures for different tasks, and new ones are added almost daily.",
        "We modify the student net, i.e., f\u03b8 in the supervised setting and \u03c0\u03b8\u03c0 (a|x), V\u03b8v (x) in the reinforcement learning case.",
        "After training, the student model should perform well on the target task without relying on teachers.",
        "Deep Net Transformation: Knowledge flow enhances the student by adding transformed and scaled intermediate representations from teacher models.",
        "Note that while additional trainable parameters Q and w are introduced in our framework, Q and w are not part of the resulting student network since we ensure pjw(l) \u2261 0 \u2200l \u2208 Lj\\l0j at the end of training as discussed .",
        "Decreasing the influence of a teacher too fast may change the output distribution over labels or actions of the student model too much, and lead to performance loss.",
        "Results are reported by using only the student model to avoid even the smallest influence from any teacher nets.",
        "Results: We first compare our framework with PathNet (<a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\"><a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\">Fernando et al, 2017</a></a>) and progressive neural net (PNN) (Rusu et al, 2016b), which are state-of-the-art transfer reinforcement learning frameworks, using their experimental settings.",
        "Compared to PathNet, a student model trained using our transfer framework with one teacher achieves higher scores in 11 out of 14 experiments.",
        "The reasons are twofold: First, a student model in knowledge flow can learn from multiple teachers while the fine-tuning method can only start from one setting.",
        "In knowledge flow, the student can avoid the negative impact from insufficiently pretrained teachers, while fine-tuning from an insufficiently pretrained model slows down the training process and may degrade the overall performance.",
        "Note that in knowledge flow, the student can benefit from the intermediate representations of the teacher, even if input space, output space and objectives differ.",
        "We use Densenet (<a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a>) as a baseline and follow their hyper-parameter settings to train our baseline, teacher and student models.",
        "PathNet (<a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\"><a class=\"ref-link\" id=\"cFernando_et+al_2017_a\" href=\"#rFernando_et+al_2017_a\">Fernando et al, 2017</a></a>) enables multiple agents to train the same deep net while reusing parameters and avoiding catastrophic forgetting.",
        "In multi-task learning, information extracted from different tasks are shared to boost performance, while, in knowledge flow, the information of multiple teachers is leveraged to help a student learn better a single, new, previously unseen task.",
        "We developed a general knowledge flow approach that permits to train a deep net from any number of teachers.",
        "In the future we plan to learn when to use which teacher and how to actively swap teachers during training of a student"
    ],
    "headline": "In this paper, we develop knowledge flow which moves \u2018knowledge\u2019 from multiple deep nets, referred to as teachers, to a new deep net model, called the student",
    "reference_links": [
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proc. ICML, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Pytorch-Playground_2017_a",
            "entry": "pytorch-playground, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=pytorchplayground%202017"
        },
        {
            "id": "_0000_a",
            "entry": "https://github.com/aaron-xichen/",
            "url": "https://github.com/aaron-xichen/"
        },
        {
            "id": "Chen_2016_a",
            "entry": "Z. Chen and B. Liu. Lifelong Machine Learning. Morgan & Claypool Publishers, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Z.%20Liu%2C%20B.%20Lifelong%20Machine%20Learning%202016"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proc. AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. EMNIST: an extension of MNIST to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prafulla%20Dhariwal%20Christopher%20Hesse%20Matthias%20Plappert%20Alec%20Radford%20John%20Schulman%20Szymon%20Sidor%20and%20Yuhuai%20Wu%20Openai%20baselines%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prafulla%20Dhariwal%20Christopher%20Hesse%20Matthias%20Plappert%20Alec%20Radford%20John%20Schulman%20Szymon%20Sidor%20and%20Yuhuai%20Wu%20Openai%20baselines%202017"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "Furlanello_et+al_2016_a",
            "entry": "Tommaso Furlanello, Jiaping Zhao, Andrew M. Saxe, Laurent Itti, and Bosco S. Tjan. Active long term memory networks. arXiv preprint arXiv:1606.02355, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02355"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Jung_et+al_2016_a",
            "entry": "Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim. Less-forgetting learning in deep neural networks. arxiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jung%2C%20Heechul%20Ju%2C%20Jeongwoo%20Jung%2C%20Minju%20Kim%2C%20Junmo%20Less-forgetting%20learning%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Li_2016_a",
            "entry": "Zhizhong Li and Derek Hoiem. Learning without forgetting. In Proc. ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202016"
        },
        {
            "id": "Long_et+al_2015_a",
            "entry": "Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proc. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20I.%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20I.%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015"
        },
        {
            "id": "Mitchell_et+al_2015_a",
            "entry": "T. Mitchell, W. Cohen, E. Hruscha, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohammad, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. Never-ending learning. In Proc. AAAI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitchell%2C%20T.%20Cohen%2C%20W.%20Hruscha%2C%20E.%20Talukdar%2C%20P.%20Never-ending%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mitchell%2C%20T.%20Cohen%2C%20W.%20Hruscha%2C%20E.%20Talukdar%2C%20P.%20Never-ending%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. In Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proc. ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Netzer_et+al_2011_a",
            "entry": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Sinno_2010_a",
            "entry": "Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng., 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinno%20Jialin%20Pan%20and%20Qiang%20Yang%20A%20survey%20on%20transfer%20learning%20IEEE%20Trans%20on%20Knowl%20and%20Data%20Eng%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinno%20Jialin%20Pan%20and%20Qiang%20Yang%20A%20survey%20on%20transfer%20learning%20IEEE%20Trans%20on%20Knowl%20and%20Data%20Eng%202010"
        },
        {
            "id": "Parisotto_et+al_2016_a",
            "entry": "Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. In Proc. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20Ba%2C%20Jimmy%20Lei%20Salakhutdinov%2C%20Ruslan%20Actor-mimic%3A%20Deep%20multitask%20and%20transfer%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20Ba%2C%20Jimmy%20Lei%20Salakhutdinov%2C%20Ruslan%20Actor-mimic%3A%20Deep%20multitask%20and%20transfer%20reinforcement%20learning%202016"
        },
        {
            "id": "Patel_et+al_2015_a",
            "entry": "Vishal M. Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Process. Mag., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patel%2C%20Vishal%20M.%20Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patel%2C%20Vishal%20M.%20Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. In Proc. ICLR, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusu%2C%20Andrei%20A.%20Colmenarejo%2C%20Sergio%20Gomez%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Desjardins%2C%20Guillaume%20Policy%20distillation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusu%2C%20Andrei%20A.%20Colmenarejo%2C%20Sergio%20Gomez%20G%C3%BCl%C3%A7ehre%2C%20%C3%87aglar%20Desjardins%2C%20Guillaume%20Policy%20distillation%202016"
        },
        {
            "id": "Rusu_et+al_0000_a",
            "entry": "Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. In arXiv preprint arXiv:1606.04671, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "Schulman_et+al_2013_a",
            "entry": "Paul Ruvolo and Eric Eaton. Ella: An efficient lifelong learning algorithm. In Proc. ICML, 2013. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Heess_2017_a",
            "entry": "Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Proc. NIPS, 2017. Martin Thoma. Analysis and optimization of convolutional neural network architectures. arXiv preprint arXiv:1707.09725, 2017. Sebastian Thrun. Lifelong learning algorithms. In Learning to Learn. Springer US, 1998. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proc. ICCV, 2015. Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increasing model capacity. In Proc. CVPR, 2017. Yuhuai Wu, Elman Mansimov, Shun Liao, Roger B. Grosse, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Proc. NIPS, 2017. Junbo Jake Zhao, Micha\u00ebl Mathieu, Ross Goroshin, and Yann LeCun. Stacked what-where autoencoders. arXiv preprint arXiv:1506.02351, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09725"
        }
    ]
}
