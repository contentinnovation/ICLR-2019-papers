{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MULTILINGUAL NEURAL MACHINE TRANSLATION WITH SOFT DECOUPLED ENCODING",
        "author": "Xinyi Wang, Hieu Pham, Philip Arthur, and Graham Neubig",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Skeke3C5Fm"
        },
        "journal": "Character",
        "abstract": "Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexicallevel information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs1."
    },
    "keywords": [
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "NMT": "Neural Machine Translation",
        "SDE": "Soft Decoupled Encoding"
    },
    "highlights": [
        "Multilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (<a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\"><a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\">Johnson et al, 2016</a></a>), and in improving translation quality of low-resource languages (<a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\"><a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\">Zoph et al, 2016</a></a>; <a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\"><a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\">Firat et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\"><a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\">Gu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\">Neubig & Hu, 2018</a></a></a>; <a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\"><a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\">Nguyen & Chiang, 2018</a></a>)",
        "Despite the success of multilingual Neural Machine Translation, it remains a research question how to represent the words from multiple languages in a way that is both parameter efficient and conducive to cross-lingual generalization",
        "We propose Soft Decoupled Encoding (SDE), a multilingual lexicon representation framework that obviates the need for segmentation by representing words on a full-word level, but can share parameters intelligently, aiding generalization",
        "Table 3 presents the results of SDEand of other baselines",
        "We show that Soft Decoupled Encoding can intelligently leverage the word similarities between two related languages by softly decoupling the lexical and semantic representations of the words",
        "Acknowledgements: The authors thank David Mortensen for helpful comments, and Amazon for providing GPU credits. This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No HR0011-15-C0114"
    ],
    "key_statements": [
        "Multilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (<a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\"><a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\">Johnson et al, 2016</a></a>), and in improving translation quality of low-resource languages (<a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\"><a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\">Zoph et al, 2016</a></a>; <a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\"><a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\">Firat et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\"><a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\">Gu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\">Neubig & Hu, 2018</a></a></a>; <a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\"><a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\">Nguyen & Chiang, 2018</a></a>)",
        "Despite the success of multilingual Neural Machine Translation, it remains a research question how to represent the words from multiple languages in a way that is both parameter efficient and conducive to cross-lingual generalization",
        "The standard sequence-to-sequence Neural Machine Translation model (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a>) represents each lexical unit by a vector from a look-up table, making it difficult to share across different languages with limited lexicon overlap",
        "We propose Soft Decoupled Encoding (SDE), a multilingual lexicon representation framework that obviates the need for segmentation by representing words on a full-word level, but can share parameters intelligently, aiding generalization",
        "Given the conflict between sharing lexical features and preserving language specific properties, we propose Soft Decoupled Encoding, a general framework to represent lexical units for multilingual Neural Machine Translation",
        "Table 3 presents the results of SDEand of other baselines",
        "Our reimplementation of universal encoder (<a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\">Gu et al, 2018</a>) does not perform well either, probably because the monolingual embedding is not trained on enough data, or the hyperparamters for their method are harder to tune",
        "Soft Decoupled Encoding outperforms the best baselines for all four languages, without using subword units or extra monolingual data",
        "We examine the performance of Soft Decoupled Encoding and the best baseline sub-sep with different vocabulary size and found that Soft Decoupled Encoding is competitive with a small character n-gram vocabulary of size 8K",
        "Figure 4 right shows that the gain peaks on the second bucket for 2 languages and the first bucket for bel and slk, which implies that Soft Decoupled Encoding shows more improvements for words with small but non-zero edit distance from the high-resource language",
        "We show that Soft Decoupled Encoding can intelligently leverage the word similarities between two related languages by softly decoupling the lexical and semantic representations of the words",
        "Acknowledgements: The authors thank David Mortensen for helpful comments, and Amazon for providing GPU credits. This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No HR0011-15-C0114"
    ],
    "summary": [
        "Multilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (<a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\"><a class=\"ref-link\" id=\"cJohnson_et+al_2016_a\" href=\"#rJohnson_et+al_2016_a\">Johnson et al, 2016</a></a>), and in improving translation quality of low-resource languages (<a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\"><a class=\"ref-link\" id=\"cZoph_et+al_2016_a\" href=\"#rZoph_et+al_2016_a\">Zoph et al, 2016</a></a>; <a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\"><a class=\"ref-link\" id=\"cFirat_et+al_2016_a\" href=\"#rFirat_et+al_2016_a\">Firat et al, 2016</a></a>; <a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\"><a class=\"ref-link\" id=\"cGu_et+al_2018_a\" href=\"#rGu_et+al_2018_a\">Gu et al, 2018</a></a>; <a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\">Neubig & Hu, 2018</a></a></a>; <a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\"><a class=\"ref-link\" id=\"cNguyen_2018_a\" href=\"#rNguyen_2018_a\">Nguyen & Chiang, 2018</a></a>).",
        "Subword-based preprocessing can produce sub-optimal segmentations for multilingual data, with semantically identical and spelled languages being split into different granularities (e.g.",
        "In multilingual translation, the subword segmentation can be sub-optimal, as the subwords from high-resource languages, i.e. languages with more training data, might dominate the subword vocabulary, so that the words in low-resource language can be split into extremely small pieces.",
        "Existing methods for lexical unit segmentation lead to difficulties in building an effective embedding look-up strategy for multilingual NMT.",
        "Gu et al (2018)\u2019s method of latent encoding increases lexical level parameter sharing, it still relies on subwords as its fundamental units, and inherits the previously stated problems of sub-word segmentation.",
        "Given the conflict between sharing lexical features and preserving language specific properties, we propose SDE, a general framework to represent lexical units for multilingual NMT.",
        "Lexical similarities of two related languages through the character n-gram embedding while preserving the semantic meaning of lexicons through a shared latent embedding.",
        "LRL Train Dev Test HRL Train settings of prior works on multilingual NMT (<a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\"><a class=\"ref-link\" id=\"cNeubig_2018_a\" href=\"#rNeubig_2018_a\">Neubig & Hu, 2018</a></a>; <a class=\"ref-link\" id=\"cQi_et+al_2018_a\" href=\"#rQi_et+al_2018_a\">Qi et al, 2018</a>), we use three low-resource language datasets: Azerbaijani, Belarusian, Galician to English, aze 5.94k 671 903 tur bel 4.51k 248 664 rus glg 10.0k 682 1007 por slk 61.5k 2271 2445 ces",
        "When using SDE with sub-sep as lexical units, the performance slightly increases for aze, but decreases for the other three language.",
        "When we use words as lexical units but replace character n-gram with subwords, the performance on two of the languages doesn\u2019t change significantly, while the performance decreases for bel and increases for glg.",
        "Figure 4 right shows that the gain peaks on the second bucket for 2 languages and the first bucket for bel and slk, which implies that SDE shows more improvements for words with small but non-zero edit distance from the high-resource language.",
        "Existing methods of lexical representation for multilingual NMT hinder parameter sharing between words that share similar surface forms and/or semantic meanings.",
        "We show that SDE can intelligently leverage the word similarities between two related languages by softly decoupling the lexical and semantic representations of the words.",
        "Our method, used without any subword segmentation, shows significant improvements over the strong multilingual NMT baseline on all languages tested.",
        "The U.S Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on"
    ],
    "headline": "We propose Soft Decoupled Encoding, a multilingual lexicon encoding framework designed to share lexicallevel information intelligently without requiring heuristic preprocessing such as pre-segmenting the data",
    "reference_links": [
        {
            "id": "Ataman_2018_a",
            "entry": "Duygu Ataman and Marcello Federico. Compositional representation of morphologically-rich input for neural machine translation. ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ataman%2C%20Duygu%20Federico%2C%20Marcello%20Compositional%20representation%20of%20morphologically-rich%20input%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ataman%2C%20Duygu%20Federico%2C%20Marcello%20Compositional%20representation%20of%20morphologically-rich%20input%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Chandler_2007_a",
            "entry": "Daniel Chandler. Semiotic: The Basics. 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandler%2C%20Daniel%20Semiotic%3A%20The%20Basics%202007"
        },
        {
            "id": "Cherry_et+al_2018_a",
            "entry": "Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. Revisiting character-based neural machine translation with capacity and compression. CoRR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cherry%2C%20Colin%20Foster%2C%20George%20Bapna%2C%20Ankur%20Firat%2C%20Orhan%20Revisiting%20character-based%20neural%20machine%20translation%20with%20capacity%20and%20compression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cherry%2C%20Colin%20Foster%2C%20George%20Bapna%2C%20Ankur%20Firat%2C%20Orhan%20Revisiting%20character-based%20neural%20machine%20translation%20with%20capacity%20and%20compression%202018"
        },
        {
            "id": "Clark_et+al_2011_a",
            "entry": "Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In ACL, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clark%2C%20Jonathan%20Dyer%2C%20Chris%20Lavie%2C%20Alon%20Smith%2C%20Noah%20Better%20hypothesis%20testing%20for%20statistical%20machine%20translation%3A%20Controlling%20for%20optimizer%20instability%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clark%2C%20Jonathan%20Dyer%2C%20Chris%20Lavie%2C%20Alon%20Smith%2C%20Noah%20Better%20hypothesis%20testing%20for%20statistical%20machine%20translation%3A%20Controlling%20for%20optimizer%20instability%202011"
        },
        {
            "id": "Dyer_et+al_2013_a",
            "entry": "Chris Dyer, Victor Chahuneau, and Noah A. Smith. A simple, fast, and effective reparameterization of IBM model 2. NAACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dyer%2C%20Chris%20Chahuneau%2C%20Victor%20Smith%2C%20Noah%20A.%20A%20simple%2C%20fast%2C%20and%20effective%20reparameterization%20of%20IBM%20model%202%202013"
        },
        {
            "id": "Firat_et+al_2016_a",
            "entry": "Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. NAACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Firat%2C%20Orhan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Multi-way%2C%20multilingual%20neural%20machine%20translation%20with%20a%20shared%20attention%20mechanism%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Firat%2C%20Orhan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Multi-way%2C%20multilingual%20neural%20machine%20translation%20with%20a%20shared%20attention%20mechanism%202016"
        },
        {
            "id": "Greimas_1983_a",
            "entry": "Algirdas Julien Greimas. Structural semantics: An attempt at a method. University of Nebraska Press, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greimas%2C%20Algirdas%20Julien%20Structural%20semantics%3A%20An%20attempt%20at%20a%20method%201983"
        },
        {
            "id": "Gu_et+al_2018_a",
            "entry": "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. Universal neural machine translation for extremely low resource languages. NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Jiatao%20Hassan%2C%20Hany%20Devlin%2C%20Jacob%20Li%2C%20Victor%20O.K.%20Universal%20neural%20machine%20translation%20for%20extremely%20low%20resource%20languages%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Jiatao%20Hassan%2C%20Hany%20Devlin%2C%20Jacob%20Li%2C%20Victor%20O.K.%20Universal%20neural%20machine%20translation%20for%20extremely%20low%20resource%20languages%202018"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. TACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Melvin%20Schuster%2C%20Mike%20Le%2C%20Quoc%20V.%20Krikun%2C%20Maxim%20Google%E2%80%99s%20multilingual%20neural%20machine%20translation%20system%3A%20Enabling%20zero-shot%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Melvin%20Schuster%2C%20Mike%20Le%2C%20Quoc%20V.%20Krikun%2C%20Maxim%20Google%E2%80%99s%20multilingual%20neural%20machine%20translation%20system%3A%20Enabling%20zero-shot%20translation%202016"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. Arxiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jozefowicz%2C%20Rafal%20Vinyals%2C%20Oriol%20Schuster%2C%20Mike%20Shazeer%2C%20Noam%20Exploring%20the%20limits%20of%20language%20modeling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jozefowicz%2C%20Rafal%20Vinyals%2C%20Oriol%20Schuster%2C%20Mike%20Shazeer%2C%20Noam%20Exploring%20the%20limits%20of%20language%20modeling%202016"
        },
        {
            "id": "Kim_et+al_2016_a",
            "entry": "Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Jernite%2C%20Yacine%20Sontag%2C%20David%20Rush%2C%20Alexander%20M.%20Character-aware%20neural%20language%20models%202016"
        },
        {
            "id": "Kudo_2018_a",
            "entry": "Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kudo%2C%20Taku%20Subword%20regularization%3A%20Improving%20neural%20network%20translation%20models%20with%20multiple%20subword%20candidates%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kudo%2C%20Taku%20Subword%20regularization%3A%20Improving%20neural%20network%20translation%20models%20with%20multiple%20subword%20candidates%202018"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation without explicit segmentation. TACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20Cho%2C%20Kyunghyun%20Hofmann%2C%20Thomas%20Fully%20character-level%20neural%20machine%20translation%20without%20explicit%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20Cho%2C%20Kyunghyun%20Hofmann%2C%20Thomas%20Fully%20character-level%20neural%20machine%20translation%20without%20explicit%20segmentation%202017"
        },
        {
            "id": "Luong_et+al_2015_a",
            "entry": "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Minh-Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attentionbased%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Minh-Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attentionbased%20neural%20machine%20translation%202015"
        },
        {
            "id": "Neubig_2018_a",
            "entry": "Graham Neubig and Junjie Hu. Rapid adaptation of neural machine translation to new languages. EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neubig%2C%20Graham%20Hu%2C%20Junjie%20Rapid%20adaptation%20of%20neural%20machine%20translation%20to%20new%20languages%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neubig%2C%20Graham%20Hu%2C%20Junjie%20Rapid%20adaptation%20of%20neural%20machine%20translation%20to%20new%20languages%202018"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Toan Q. Nguyen and David Chiang. Transfer learning across low-resource, related languages for neural machine translation. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Toan%20Q.%20Chiang%2C%20David%20Transfer%20learning%20across%20low-resource%2C%20related%20languages%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Toan%20Q.%20Chiang%2C%20David%20Transfer%20learning%20across%20low-resource%2C%20related%20languages%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Qi_et+al_2018_a",
            "entry": "Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. When and why are pre-trained word embeddings useful for neural machine translation? NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20Ye%20Sachan%2C%20Devendra%20Singh%20Felix%2C%20Matthieu%20Padmanabhan%2C%20Sarguna%20When%20and%20why%20are%20pre-trained%20word%20embeddings%20useful%20for%20neural%20machine%20translation%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20Ye%20Sachan%2C%20Devendra%20Singh%20Felix%2C%20Matthieu%20Padmanabhan%2C%20Sarguna%20When%20and%20why%20are%20pre-trained%20word%20embeddings%20useful%20for%20neural%20machine%20translation%3F%202018"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Van_2008_a",
            "entry": "L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-SNE. Journal of Machine Learning Research, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.J.P.%20Hinton%2C%20G.E.%20Visualizing%20high-dimensional%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20L.J.P.%20Hinton%2C%20G.E.%20Visualizing%20high-dimensional%20data%20using%20t-SNE%202008"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "Wieting_et+al_2016_a",
            "entry": "John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Charagram: Embedding words and sentences via character n-grams. EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wieting%2C%20John%20Bansal%2C%20Mohit%20Gimpel%2C%20Kevin%20Livescu%2C%20Karen%20Charagram%3A%20Embedding%20words%20and%20sentences%20via%20character%20n-grams%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wieting%2C%20John%20Bansal%2C%20Mohit%20Gimpel%2C%20Kevin%20Livescu%2C%20Karen%20Charagram%3A%20Embedding%20words%20and%20sentences%20via%20character%20n-grams%202016"
        },
        {
            "id": "Yih_et+al_2014_a",
            "entry": "Wen-tau Yih, Xiaodong He, and Christopher Meek. Semantic parsing for single-relation question answering. ACL, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yih%2C%20Wen-tau%20He%2C%20Xiaodong%20Meek%2C%20Christopher%20Semantic%20parsing%20for%20single-relation%20question%20answering%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yih%2C%20Wen-tau%20He%2C%20Xiaodong%20Meek%2C%20Christopher%20Semantic%20parsing%20for%20single-relation%20question%20answering%202014"
        },
        {
            "id": "Zoph_et+al_2016_a",
            "entry": "Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low resource neural machine translation. EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20Barret%20Yuret%2C%20Deniz%20May%2C%20Jonathan%20Knight%2C%20Kevin%20Transfer%20learning%20for%20low%20resource%20neural%20machine%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20Barret%20Yuret%2C%20Deniz%20May%2C%20Jonathan%20Knight%2C%20Kevin%20Transfer%20learning%20for%20low%20resource%20neural%20machine%20translation%202016"
        }
    ]
}
