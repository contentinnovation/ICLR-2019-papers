{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "VISUAL EXPLANATION BY INTERPRETATION: IMPROVING VISUAL FEEDBACK CAPABILITIES OF DEEP NEURAL NETWORKS",
        "author": "Jos\u00e9 Oramas M",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1ziPjC5Fm"
        },
        "abstract": "Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNetbased visualizations. Moreover, we introduce an8Flower, a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST, ILSVRC12, Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest."
    },
    "keywords": [
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "IoU": "intersection over union"
    },
    "highlights": [
        "Methods based on deep neural networks (DNNs) have achieved impressive results for several computer vision tasks, such as image classification, object detection and image generation",
        "We aim for more visuallydescriptive predictions and propose means to improve the quality of the visual feedback capabilities of deep neural networks-based methods",
        "We propose a method to enrich the prediction made by deep neural networks by indicating the visual features that contributed to such prediction",
        "Our method identifies features encoded by the network that are relevant for the task addressed by the deep neural networks",
        "We proposed a method to attenuate the artifacts introduced by strided operations in visualizations made by Deconvnet-based methods",
        "We have proposed a novel dataset designed for the objective evaluation of methods for explanation of deep neural networks"
    ],
    "key_statements": [
        "Methods based on deep neural networks (DNNs) have achieved impressive results for several computer vision tasks, such as image classification, object detection and image generation",
        "We aim for more visuallydescriptive predictions and propose means to improve the quality of the visual feedback capabilities of deep neural networks-based methods",
        "We propose a method which, given a trained deep neural networks model, automatically identifies a set of relevant internal filters whose encoded features serve as indicators for the class of interest to be predicted (Fig. 1 left)",
        "<a class=\"ref-link\" id=\"cZeiler_2014_a\" href=\"#rZeiler_2014_a\">Zeiler & Fergus (2014</a>) proposed a deconvolutional network (Deconvnet) which uses activations from a given top layer and reverses the forward pass to reveal which visual patterns from the input image are responsible for the observed activations",
        "In Fig. 6 we show some examples of the visual explanations produced by our method",
        "We propose a method to enrich the prediction made by deep neural networks by indicating the visual features that contributed to such prediction",
        "Our method identifies features encoded by the network that are relevant for the task addressed by the deep neural networks",
        "We proposed a method to attenuate the artifacts introduced by strided operations in visualizations made by Deconvnet-based methods",
        "We have proposed a novel dataset designed for the objective evaluation of methods for explanation of deep neural networks"
    ],
    "summary": [
        "Methods based on deep neural networks (DNNs) have achieved impressive results for several computer vision tasks, such as image classification, object detection and image generation.",
        "We propose a method which, given a trained DNN model, automatically identifies a set of relevant internal filters whose encoded features serve as indicators for the class of interest to be predicted (Fig. 1 left).",
        "We propose an automatic method based on feature selection to identify the network-encoded features that are important for the prediction of a given class.",
        "<a class=\"ref-link\" id=\"cZeiler_2014_a\" href=\"#rZeiler_2014_a\">Zeiler & Fergus (2014</a>) proposed a deconvolutional network (Deconvnet) which uses activations from a given top layer and reverses the forward pass to reveal which visual patterns from the input image are responsible for the observed activations.",
        "We feed this information to the Deconvnet-based method with guided backpropagation from Gr\u00fcn et al (2016) to visualize the important features as defined by the layer/filter pairs (p\u2217, q\u2217).",
        "As mentioned in Sec. 2, methods aiming at interpretation/explanation start from an internal point in the network and go backwards until reaching the input space - producing a heatmap.",
        "Given the set of identified relevant features, for every class, we select images with higher responses.",
        "In Fig. 7, we compare our visualizations with upsampled activation maps from internal layers (<a class=\"ref-link\" id=\"cBau_et+al_2017_a\" href=\"#rBau_et+al_2017_a\">Bau et al (2017</a>); <a class=\"ref-link\" id=\"cZhou_et+al_2016_a\" href=\"#rZhou_et+al_2016_a\">Zhou et al (2016</a>)) and the output of DeconvNet with guided-backpropagation (<a class=\"ref-link\" id=\"cSpringenberg_et+al_2015_a\" href=\"#rSpringenberg_et+al_2015_a\">Springenberg et al (2015</a>)).",
        "For the case of higher layers (Fig. 7), the proposed method provides more precise visualizations when compared to upsampled activation maps.",
        "This suggests that our method is able to maintain focus on relevant class features while producing detailed heatmaps with better visual quality.",
        "We run a similar experiment to that conducted in <a class=\"ref-link\" id=\"cNie_et+al_2018_a\" href=\"#rNie_et+al_2018_a\"><a class=\"ref-link\" id=\"cNie_et+al_2018_a\" href=\"#rNie_et+al_2018_a\">Nie et al (2018</a></a>) where the visual explanation produced for a predicted class of a given model after observing a given image is compared against those when a different class is considered when generating the explanation.",
        "In their work, <a class=\"ref-link\" id=\"cNie_et+al_2018_a\" href=\"#rNie_et+al_2018_a\"><a class=\"ref-link\" id=\"cNie_et+al_2018_a\" href=\"#rNie_et+al_2018_a\">Nie et al (2018</a></a>) and Julius Adebayo (2018) found that explanations from DeconvNet and Guided-Backpropagation methods are not performing well in this respect, yielding visualizations that are not determined by the predicted class, but by the filters of the first layer and the edge-like structures in the input images.",
        "This different result can be understood since, in our method, DeconvNet with Guided-Backpropagation is merely used as a means to highlight the image regions that justify the identified relevant features, not the predicted classes themselves."
    ],
    "headline": "We propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations",
    "reference_links": [
        {
            "id": "Bach_et+al_2015_a",
            "entry": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS ONE, 10(7):e0130140, 07 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "Bau_et+al_2017_a",
            "entry": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017"
        },
        {
            "id": "Chatfield_et+al_2014_a",
            "entry": "K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: delving deep into convolutional nets. In British Machine Vision Conference (BMVC), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chatfield%2C%20K.%20Simonyan%2C%20K.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Return%20of%20the%20devil%20in%20the%20details%3A%20delving%20deep%20into%20convolutional%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chatfield%2C%20K.%20Simonyan%2C%20K.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Return%20of%20the%20devil%20in%20the%20details%3A%20delving%20deep%20into%20convolutional%20nets%202014"
        },
        {
            "id": "Chattopadhyay_et+al_2018_a",
            "entry": "Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, and Vineeth N. Balasubramanian. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks. In Winter Conference on Applications of Computer Vision (WACV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chattopadhyay%2C%20Aditya%20Sarkar%2C%20Anirban%20Howlader%2C%20Prantik%20Balasubramanian%2C%20Vineeth%20N.%20Grad-cam%2B%2B%3A%20Generalized%20gradient-based%20visual%20explanations%20for%20deep%20convolutional%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chattopadhyay%2C%20Aditya%20Sarkar%2C%20Anirban%20Howlader%2C%20Prantik%20Balasubramanian%2C%20Vineeth%20N.%20Grad-cam%2B%2B%3A%20Generalized%20gradient-based%20visual%20explanations%20for%20deep%20convolutional%20networks%202018"
        },
        {
            "id": "Das_et+al_2016_a",
            "entry": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20Abhishek%20Harsh%20Agrawal%2C%20C.Lawrence%20Zitnick%20Parikh%2C%20Devi%20Batra%2C%20Dhruv%20Human%20Attention%20in%20Visual%20Question%20Answering%3A%20Do%20Humans%20and%20Deep%20Networks%20Look%20at%20the%20Same%20Regions%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20Abhishek%20Harsh%20Agrawal%2C%20C.Lawrence%20Zitnick%20Parikh%2C%20Devi%20Batra%2C%20Dhruv%20Human%20Attention%20in%20Visual%20Question%20Answering%3A%20Do%20Humans%20and%20Deep%20Networks%20Look%20at%20the%20Same%20Regions%3F%202016"
        },
        {
            "id": "Doersch_et+al_2015_a",
            "entry": "Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A Efros. What makes paris look like paris? Communications of the ACM, 58(12):103\u2013110, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Sivic%2C%20Josef%20and%20Alexei%20A%20Efros.%20What%20makes%20paris%20look%20like%20paris%3F%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Sivic%2C%20Josef%20and%20Alexei%20A%20Efros.%20What%20makes%20paris%20look%20like%20paris%3F%202015"
        },
        {
            "id": "Escorcia_et+al_2015_a",
            "entry": "V. Escorcia, J. C. Niebles, and B. Ghanem. On the relationship between visual attributes and convolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Escorcia%2C%20V.%20Niebles%2C%20J.C.%20Ghanem%2C%20B.%20On%20the%20relationship%20between%20visual%20attributes%20and%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Escorcia%2C%20V.%20Niebles%2C%20J.C.%20Ghanem%2C%20B.%20On%20the%20relationship%20between%20visual%20attributes%20and%20convolutional%20networks%202015"
        },
        {
            "id": "Fong_2018_a",
            "entry": "R. Fong and A. Vedaldi. Net2Vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fong%2C%20R.%20Vedaldi%2C%20A.%20Net2Vec%3A%20Quantifying%20and%20explaining%20how%20concepts%20are%20encoded%20by%20filters%20in%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fong%2C%20R.%20Vedaldi%2C%20A.%20Net2Vec%3A%20Quantifying%20and%20explaining%20how%20concepts%20are%20encoded%20by%20filters%20in%20deep%20neural%20networks%202018"
        },
        {
            "id": "Gonzalez-Garcia_et+al_2017_a",
            "entry": "Abel Gonzalez-Garcia, Davide Modolo, and Vittorio Ferrari. Do semantic parts emerge in convolutional neural networks? International Journal of Computer Vision (IJCV), pp. 1\u201319, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonzalez-Garcia%2C%20Abel%20Modolo%2C%20Davide%20Ferrari%2C%20Vittorio%20Do%20semantic%20parts%20emerge%20in%20convolutional%20neural%20networks%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonzalez-Garcia%2C%20Abel%20Modolo%2C%20Davide%20Ferrari%2C%20Vittorio%20Do%20semantic%20parts%20emerge%20in%20convolutional%20neural%20networks%3F%202017"
        },
        {
            "id": "Gruen_et+al_2016_a",
            "entry": "F. Gr\u00fcn, C. Rupprecht, N. Navab, and F. Tombari. A taxonomy and library for visualizing learned features in convolutional neural networks. In International Conference on Machine Learning (ICML) Workshops, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCn%2C%20F.%20Rupprecht%2C%20C.%20Navab%2C%20N.%20Tombari%2C%20F.%20A%20taxonomy%20and%20library%20for%20visualizing%20learned%20features%20in%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gr%C3%BCn%2C%20F.%20Rupprecht%2C%20C.%20Navab%2C%20N.%20Tombari%2C%20F.%20A%20taxonomy%20and%20library%20for%20visualizing%20learned%20features%20in%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Hendricks_et+al_2016_a",
            "entry": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. Generating visual explanations. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendricks%2C%20Lisa%20Anne%20Akata%2C%20Zeynep%20Rohrbach%2C%20Marcus%20Donahue%2C%20Jeff%20Generating%20visual%20explanations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendricks%2C%20Lisa%20Anne%20Akata%2C%20Zeynep%20Rohrbach%2C%20Marcus%20Donahue%2C%20Jeff%20Generating%20visual%20explanations%202016"
        },
        {
            "id": "Ian_2018_a",
            "entry": "Ian Goodfellow Moritz Hardt Been Kim Julius Adebayo, Justin Gilmer. Sanity checks for saliency maps. In advances in neural information processing systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Moritz%20Hardt%20Been%20Kim%20Julius%20Adebayo%20Justin%20Gilmer%20Sanity%20checks%20for%20saliency%20maps%20In%20advances%20in%20neural%20information%20processing%20systems%20NIPS%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Moritz%20Hardt%20Been%20Kim%20Julius%20Adebayo%20Justin%20Gilmer%20Sanity%20checks%20for%20saliency%20maps%20In%20advances%20in%20neural%20information%20processing%20systems%20NIPS%202018"
        },
        {
            "id": "Karpathy_2017_a",
            "entry": "A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664\u2013676, April 2017. ISSN 0162-8828. doi: 10.1109/TPAMI.2016.2598339.",
            "crossref": "https://dx.doi.org/10.1109/TPAMI.2016.2598339",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TPAMI.2016.2598339"
        },
        {
            "id": "Kindermans_et+al_2017_a",
            "entry": "P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch\u00fctt, S. D\u00e4hne, D. Erhan, and B. Kim. The (Un)reliability of saliency methods. In NIPS workshop on Explaining and Visualizing Deep Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kindermans%2C%20P.-J.%20Hooker%2C%20S.%20Adebayo%2C%20J.%20Alber%2C%20M.%20The%20%28Un%29reliability%20of%20saliency%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kindermans%2C%20P.-J.%20Hooker%2C%20S.%20Adebayo%2C%20J.%20Alber%2C%20M.%20The%20%28Un%29reliability%20of%20saliency%20methods%202017"
        },
        {
            "id": "Lecun_2010_a",
            "entry": "Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Mairal_et+al_2014_a",
            "entry": "Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. Found. Trends. Comput. Graph. Vis., 8(2-3):85\u2013283, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing.%20Found%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing.%20Found%202014"
        },
        {
            "id": "Nie_et+al_2018_a",
            "entry": "Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation for perplexing behaviors of backpropagationbased visualizations. In Proceedings of the 35th International Conference on Machine Learning, ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nie%2C%20Weili%20Zhang%2C%20Yang%20Patel%2C%20Ankit%20A%20theoretical%20explanation%20for%20perplexing%20behaviors%20of%20backpropagationbased%20visualizations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nie%2C%20Weili%20Zhang%2C%20Yang%20Patel%2C%20Ankit%20A%20theoretical%20explanation%20for%20perplexing%20behaviors%20of%20backpropagationbased%20visualizations%202018"
        },
        {
            "id": "Jos\u00e9_2016_a",
            "entry": "Jos\u00e9 Oramas M. and Tinne Tuytelaars. Modeling visual compatibility through hierarchical mid-level elements. CoRR, abs/1604.00036, 2016. URL http://arxiv.org/abs/1604.00036.",
            "url": "http://arxiv.org/abs/1604.00036",
            "arxiv_url": "https://arxiv.org/pdf/1604.00036"
        },
        {
            "id": "Rematas_et+al_2015_a",
            "entry": "Konstantinos Rematas, Basura Fernando, Frank Dellaert, and Tinne Tuytelaars. Dataset fingerprints: Exploring image collections through data mining. In Computer Vision and Pattern Recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rematas%2C%20Konstantinos%20Fernando%2C%20Basura%20Dellaert%2C%20Frank%20Tuytelaars%2C%20Tinne%20Dataset%20fingerprints%3A%20Exploring%20image%20collections%20through%20data%20mining%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rematas%2C%20Konstantinos%20Fernando%2C%20Basura%20Dellaert%2C%20Frank%20Tuytelaars%2C%20Tinne%20Dataset%20fingerprints%3A%20Exploring%20image%20collections%20through%20data%20mining%202015"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Selvaraju_et+al_2017_a",
            "entry": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In The IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Selvaraju%2C%20Ramprasaath%20R.%20Cogswell%2C%20Michael%20Das%2C%20Abhishek%20Vedantam%2C%20Ramakrishna%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Selvaraju%2C%20Ramprasaath%20R.%20Cogswell%2C%20Michael%20Das%2C%20Abhishek%20Vedantam%2C%20Ramakrishna%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017"
        },
        {
            "id": "Simo-Serra_et+al_2015_a",
            "entry": "E. Simo-Serra, S. Fidler, F. Moreno-Noguer, and R. Urtasun. Neuroaesthetics in fashion: modeling the perception of fashionability. In Computer Vision and Pattern Recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simo-Serra%2C%20E.%20Fidler%2C%20S.%20Moreno-Noguer%2C%20F.%20Urtasun%2C%20R.%20Neuroaesthetics%20in%20fashion%3A%20modeling%20the%20perception%20of%20fashionability%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simo-Serra%2C%20E.%20Fidler%2C%20S.%20Moreno-Noguer%2C%20F.%20Urtasun%2C%20R.%20Neuroaesthetics%20in%20fashion%3A%20modeling%20the%20perception%20of%20fashionability%202015"
        },
        {
            "id": "Simonyan_et+al_2014_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In International Conference on Learning Representations (ICLR) Workshops, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014"
        },
        {
            "id": "Springenberg_et+al_2015_a",
            "entry": "J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller. Striving for simplicity: the all convolutional net. In International Conference on Learning Representations (ICLR) Workshops, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.A.%20Striving%20for%20simplicity%3A%20the%20all%20convolutional%20net%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.A.%20Striving%20for%20simplicity%3A%20the%20all%20convolutional%20net%202015"
        },
        {
            "id": "Van_2008_a",
            "entry": "E. van den Berg and M. P. Friedlander. Probing the pareto frontier for basis pursuit solutions. SIAM J. Sci. Comput., 31(2):890\u2013912, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Berg%2C%20E.%20Friedlander%2C%20M.P.%20Probing%20the%20pareto%20frontier%20for%20basis%20pursuit%20solutions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Berg%2C%20E.%20Friedlander%2C%20M.P.%20Probing%20the%20pareto%20frontier%20for%20basis%20pursuit%20solutions%202008"
        },
        {
            "id": "Vedaldi_2015_a",
            "entry": "A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In ACM international conference on Multimedia (MM), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vedaldi%2C%20A.%20Lenc%2C%20K.%20Matconvnet%3A%20Convolutional%20neural%20networks%20for%20matlab%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vedaldi%2C%20A.%20Lenc%2C%20K.%20Matconvnet%3A%20Convolutional%20neural%20networks%20for%20matlab%202015"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Kaili Wang, Yu-Hui Huang, Jos\u00e9 Oramas M, Luc Van Gool, and Tinne Tuytelaars. An analysis of human-centered geolocation. Winter Conference on Applications of Computer Vision (WACV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Kaili%20Huang%2C%20Yu-Hui%20M%2C%20Jos%C3%A9%20Oramas%20Gool%2C%20Luc%20Van%20An%20analysis%20of%20human-centered%20geolocation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Kaili%20Huang%2C%20Yu-Hui%20M%2C%20Jos%C3%A9%20Oramas%20Gool%2C%20Luc%20Van%20An%20analysis%20of%20human-centered%20geolocation%202018"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Ning Xie, Md. Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, and Michael Raymer. Relating input concepts to convolutional neural network decisions. In Neural Information Processing Systems (NIPS) Workshops, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Ning%20Sarker%2C%20Kamruzzaman%20Doran%2C%20Derek%20Hitzler%2C%20Pascal%20Relating%20input%20concepts%20to%20convolutional%20neural%20network%20decisions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Ning%20Sarker%2C%20Kamruzzaman%20Doran%2C%20Derek%20Hitzler%2C%20Pascal%20Relating%20input%20concepts%20to%20convolutional%20neural%20network%20decisions%202017"
        },
        {
            "id": "Yosinski_et+al_2015_a",
            "entry": "Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. In International Conference on Machine Learning (ICML) Workshops, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Mai%20Fuchs%2C%20Thomas%20J.%20Understanding%20neural%20networks%20through%20deep%20visualization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Mai%20Fuchs%2C%20Thomas%20J.%20Understanding%20neural%20networks%20through%20deep%20visualization%202015"
        },
        {
            "id": "Zeiler_2014_a",
            "entry": "Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Jianming Zhang, Zhe Lin, Shen Xiaohui Brandt, Jonathan, and Stan Sclaroff. Top-down neural attention by excitation backprop. In European Conference on Computer Vision(ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jianming%20Lin%2C%20Zhe%20Brandt%2C%20Shen%20Xiaohui%20Jonathan%20Top-down%20neural%20attention%20by%20excitation%20backprop%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jianming%20Lin%2C%20Zhe%20Brandt%2C%20Shen%20Xiaohui%20Jonathan%20Top-down%20neural%20attention%20by%20excitation%20backprop%202016"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Quanshi%20Wu%2C%20Ying%20Nian%20Zhu%2C%20Song-Chun%20Interpretable%20convolutional%20neural%20networks%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Quanshi%20Wu%2C%20Ying%20Nian%20Zhu%2C%20Song-Chun%20Interpretable%20convolutional%20neural%20networks%202018-06"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "B. Zhou, A. Khosla, Lapedriza. A., A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. In Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Khosla%2C%20A.%20A.%2C%20Lapedriza%20Oliva%2C%20A.%20Learning%20Deep%20Features%20for%20Discriminative%20Localization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Khosla%2C%20A.%20A.%2C%20Lapedriza%20Oliva%2C%20A.%20Learning%20Deep%20Features%20for%20Discriminative%20Localization%202016"
        },
        {
            "id": "Zhou_et+al_2015_a",
            "entry": "Bolei Zhou, Aditya Khosla, \u00c0gata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20%C3%80gata%20Oliva%2C%20Aude%20Object%20detectors%20emerge%20in%20deep%20scene%20cnns%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20%C3%80gata%20Oliva%2C%20Aude%20Object%20detectors%20emerge%20in%20deep%20scene%20cnns%202015"
        }
    ]
}
