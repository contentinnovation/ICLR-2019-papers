{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HINDSIGHT POLICY GRADIENTS",
        "author": "Paulo Rauber IDSIA, USI, SUPSI Lugano, Switzerland paulo@idsia.ch",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bkg2viA5FQ"
        },
        "abstract": "A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency."
    },
    "keywords": [
        {
            "term": "curricula",
            "url": "https://en.wikipedia.org/wiki/curricula"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "robotics",
            "url": "https://en.wikipedia.org/wiki/robotics"
        }
    ],
    "abbreviations": {
        "At": "at \u2208 Val"
    },
    "highlights": [
        "In a traditional reinforcement learning setting, an agent interacts with an environment in a sequence of episodes, observing states and acting according to a policy that ideally maximizes expected cumulative reward",
        "We demonstrate how hindsight can be introduced to policy gradient methods (Williams, 1986; 1992; <a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999a</a>), generalizing this idea to a successful class of reinforcement learning algorithms (<a class=\"ref-link\" id=\"cPeters_2008_a\" href=\"#rPeters_2008_a\">Peters & Schaal, 2008</a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>)",
        "Hindsight refers to the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended",
        "Hindsight has been limited to off-policy reinforcement learning algorithms that rely on experience replay (<a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\">Andrychowicz et al, 2017</a>) and policy search based on Bayesian optimization (<a class=\"ref-link\" id=\"cKarkus_et+al_2016_a\" href=\"#rKarkus_et+al_2016_a\">Karkus et al, 2016</a>)",
        "Besides the straightforward estimator built upon the per-decision hindsight policy gradient, we presented a consistent estimator inspired by weighted importance sampling, together with the corresponding baseline formulation",
        "The inconsistent hindsight policy gradient estimator with a value function baseline employed in our experiments sometimes leads to unstable learning, which is likely related to the difficulty of fitting such a value function without hindsight"
    ],
    "key_statements": [
        "In a traditional reinforcement learning setting, an agent interacts with an environment in a sequence of episodes, observing states and acting according to a policy that ideally maximizes expected cumulative reward",
        "Learning such goal-conditional behavior has received significant attention in machine learning and robotics, especially because a goal-conditional policy may generalize desirable behavior to goals that were never encountered by the agent (<a class=\"ref-link\" id=\"cSchmidhuber_1990_a\" href=\"#rSchmidhuber_1990_a\">Schmidhuber & Huber, 1990</a>; Da Silva et al, 2012; <a class=\"ref-link\" id=\"cKupcsik_et+al_2013_a\" href=\"#rKupcsik_et+al_2013_a\">Kupcsik et al, 2013</a>; <a class=\"ref-link\" id=\"cDeisenroth_et+al_2014_a\" href=\"#rDeisenroth_et+al_2014_a\">Deisenroth et al, 2014</a>; <a class=\"ref-link\" id=\"cSchaul_et+al_2015_a\" href=\"#rSchaul_et+al_2015_a\">Schaul et al, 2015</a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a>; <a class=\"ref-link\" id=\"cKober_et+al_2012_a\" href=\"#rKober_et+al_2012_a\">Kober et al, 2012</a>; <a class=\"ref-link\" id=\"cGhosh_et+al_2018_a\" href=\"#rGhosh_et+al_2018_a\">Ghosh et al, 2018</a>; <a class=\"ref-link\" id=\"cMankowitz_et+al_2018_a\" href=\"#rMankowitz_et+al_2018_a\">Mankowitz et al, 2018</a>; <a class=\"ref-link\" id=\"cPathak_et+al_2018_a\" href=\"#rPathak_et+al_2018_a\">Pathak et al, 2018</a>)",
        "The capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended is called hindsight",
        "We demonstrate how hindsight can be introduced to policy gradient methods (Williams, 1986; 1992; <a class=\"ref-link\" id=\"cSutton_et+al_1999_a\" href=\"#rSutton_et+al_1999_a\">Sutton et al, 1999a</a>), generalizing this idea to a successful class of reinforcement learning algorithms (<a class=\"ref-link\" id=\"cPeters_2008_a\" href=\"#rPeters_2008_a\">Peters & Schaal, 2008</a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a>)",
        "We found that this estimator leads to unstable learning progress, which is probably due to its potential high variance",
        "We introduced techniques that enable learning goal-conditional policies using hindsight",
        "Hindsight refers to the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended",
        "Hindsight has been limited to off-policy reinforcement learning algorithms that rely on experience replay (<a class=\"ref-link\" id=\"cAndrychowicz_et+al_2017_a\" href=\"#rAndrychowicz_et+al_2017_a\">Andrychowicz et al, 2017</a>) and policy search based on Bayesian optimization (<a class=\"ref-link\" id=\"cKarkus_et+al_2016_a\" href=\"#rKarkus_et+al_2016_a\">Karkus et al, 2016</a>)",
        "In addition to the fundamental hindsight policy gradient, our technical results include its baseline and advantage formulations. These results are based on a self-contained goal-conditional policy framework that is introduced in this text",
        "Besides the straightforward estimator built upon the per-decision hindsight policy gradient, we presented a consistent estimator inspired by weighted importance sampling, together with the corresponding baseline formulation",
        "A variant of this estimator leads to remarkable comparative sample efficiency on a diverse selection of sparsereward environments, especially in cases where direct reward signals are extremely difficult to obtain",
        "The main drawback of hindsight policy gradient estimators appears to be their computational cost, which is directly related to the number of active goals in a batch",
        "The inconsistent hindsight policy gradient estimator with a value function baseline employed in our experiments sometimes leads to unstable learning, which is likely related to the difficulty of fitting such a value function without hindsight",
        "There are many possibilities for future work besides integrating hindsight policy gradients into systems that rely on goal-conditional policies: deriving additional estimators; implementing and evaluating hindsight actor-critic methods; assessing whether hindsight policy gradients can successfully circumvent catastrophic forgetting during curriculum learning of goal-conditional policies; approximating the reward function to reduce required supervision; analysing the variance of the proposed estimators; studying the impact of active goal subsampling; and evaluating every technique on continuous action spaces"
    ],
    "summary": [
        "In a traditional reinforcement learning setting, an agent interacts with an environment in a sequence of episodes, observing states and acting according to a policy that ideally maximizes expected cumulative reward.",
        "In many natural sparse-reward environments, active goals will correspond directly to states visited during episodes, which enables computing said expectation exactly when the goal distribution is known.",
        "It is important to note that only likelihood ratios associated to active goals for a given episode will affect the gradient estimate.",
        "In every one of these environments, the agent receives the remaining number of time steps plus one as a reward for reaching the goal state, which ends the episode.",
        "The weighted per-decision hindsight policy gradient estimator used in our experiments (HPG) does not precisely correspond to Expression 12.",
        "When including a baseline that approximates the value function, we again consider only active goals, which by itself generally results in an inconsistent estimator (HPG+B).",
        "Goal-conditional policy gradient estimators with and without an approximate value function baseline (GCPG+B and GCPG, respectively) obtain excellent policies and lead to comparable sample efficiency.",
        "For such a medium batch size, only 3 active goals per episode are subsampled for HPG, showing that subsampling is a viable alternative to reduce the computational cost of hindsight.",
        "In addition to the fundamental hindsight policy gradient, our technical results include its baseline and advantage formulations.",
        "Besides the straightforward estimator built upon the per-decision hindsight policy gradient, we presented a consistent estimator inspired by weighted importance sampling, together with the corresponding baseline formulation.",
        "A variant of this estimator leads to remarkable comparative sample efficiency on a diverse selection of sparsereward environments, especially in cases where direct reward signals are extremely difficult to obtain.",
        "The main drawback of hindsight policy gradient estimators appears to be their computational cost, which is directly related to the number of active goals in a batch.",
        "The inconsistent hindsight policy gradient estimator with a value function baseline employed in our experiments sometimes leads to unstable learning, which is likely related to the difficulty of fitting such a value function without hindsight.",
        "There are many possibilities for future work besides integrating hindsight policy gradients into systems that rely on goal-conditional policies: deriving additional estimators; implementing and evaluating hindsight actor-critic methods; assessing whether hindsight policy gradients can successfully circumvent catastrophic forgetting during curriculum learning of goal-conditional policies; approximating the reward function to reduce required supervision; analysing the variance of the proposed estimators; studying the impact of active goal subsampling; and evaluating every technique on continuous action spaces.",
        "This crucial feature allows natural task formulations that require just trivial reward shaping"
    ],
    "headline": "We demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06"
        },
        {
            "id": "Bishop_2013_a",
            "entry": "C. M. Bishop. Pattern Recognition and Machine Learning. Information science and statistics. Springer, 2013. ISBN 9788132209065.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Pattern%20Recognition%20and%20Machine%20Learning.%20Information%20science%20and%20statistics%202013"
        },
        {
            "id": "Silva_et+al_2012_a",
            "entry": "B. C. Da Silva, G. Konidaris, and A. G. Barto. Learning parameterized skills. In Proceedings of International Conference of Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silva%2C%20B.C.Da%20Konidaris%2C%20G.%20Barto%2C%20A.G.%20Learning%20parameterized%20skills%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silva%2C%20B.C.Da%20Konidaris%2C%20G.%20Barto%2C%20A.G.%20Learning%20parameterized%20skills%202012"
        },
        {
            "id": "Deisenroth_et+al_2014_a",
            "entry": "M. P. Deisenroth, P. Englert, J. Peters, and D. Fox. Multi-task policy search for robotics. In IEEE International Conference on Robotics and Automation, 2014, pp. 3876\u20133881, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.P.%20Englert%2C%20P.%20Peters%2C%20J.%20Fox%2C%20D.%20Multi-task%20policy%20search%20for%20robotics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.P.%20Englert%2C%20P.%20Peters%2C%20J.%20Fox%2C%20D.%20Multi-task%20policy%20search%20for%20robotics%202014"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and P. Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of International Conference on Machine Learning, pp. 1329\u20131338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Fabisch_2014_a",
            "entry": "A. Fabisch and J. H. Metzen. Active contextual policy search. The Journal of Machine Learning Research, 15(1):3371\u20133399, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fabisch%2C%20A.%20Metzen%2C%20J.H.%20Active%20contextual%20policy%20search%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fabisch%2C%20A.%20Metzen%2C%20J.H.%20Active%20contextual%20policy%20search%202014"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel. Reverse curriculum generation for reinforcement learning. In Proceedings of the 1st Annual Conference on Robot Learning, pp. 482\u2013495, 13\u201315 Nov 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Florensa%2C%20C.%20Held%2C%20D.%20Wulfmeier%2C%20M.%20Zhang%2C%20M.%20Reverse%20curriculum%20generation%20for%20reinforcement%20learning%202017-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Florensa%2C%20C.%20Held%2C%20D.%20Wulfmeier%2C%20M.%20Zhang%2C%20M.%20Reverse%20curriculum%20generation%20for%20reinforcement%20learning%202017-11"
        },
        {
            "id": "Ghosh_et+al_2018_a",
            "entry": "D. Ghosh, A. Singh, A. Rajeswaran, V. Kumar, and S. Levine. Divide-and-conquer reinforcement learning. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20D.%20Singh%2C%20A.%20Rajeswaran%2C%20A.%20Kumar%2C%20V.%20Divide-and-conquer%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20D.%20Singh%2C%20A.%20Rajeswaran%2C%20A.%20Kumar%2C%20V.%20Divide-and-conquer%20reinforcement%20learning%202018"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Greensmith_et+al_2004_a",
            "entry": "E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471\u20131530, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greensmith%2C%20E.%20Bartlett%2C%20P.L.%20Baxter%2C%20J.%20Variance%20reduction%20techniques%20for%20gradient%20estimates%20in%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greensmith%2C%20E.%20Bartlett%2C%20P.L.%20Baxter%2C%20J.%20Variance%20reduction%20techniques%20for%20gradient%20estimates%20in%20reinforcement%20learning%202004"
        },
        {
            "id": "Jie_2010_a",
            "entry": "T. Jie and P. Abbeel. On a connection between importance sampling and the likelihood ratio policy gradient. In Advances in Neural Information Processing Systems, pp. 1000\u20131008, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jie%2C%20T.%20Abbeel%2C%20P.%20On%20a%20connection%20between%20importance%20sampling%20and%20the%20likelihood%20ratio%20policy%20gradient%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jie%2C%20T.%20Abbeel%2C%20P.%20On%20a%20connection%20between%20importance%20sampling%20and%20the%20likelihood%20ratio%20policy%20gradient%202010"
        },
        {
            "id": "Karkus_et+al_2016_a",
            "entry": "P. Karkus, A. Kupcsik, D. Hsu, and W. S. Lee. Factored contextual policy search with bayesian optimization. arXiv preprint arXiv:1612.01746, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01746"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Kirkpatrick_et+al_2017_a",
            "entry": "J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "Kober_et+al_2012_a",
            "entry": "J. Kober, A. Wilhelm, E. Oztop, and J. Peters. Reinforcement learning to adjust parametrized motor primitives to new situations. Autonomous Robots, 33(4):361\u2013379, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20J.%20Wilhelm%2C%20A.%20Oztop%2C%20E.%20Peters%2C%20J.%20Reinforcement%20learning%20to%20adjust%20parametrized%20motor%20primitives%20to%20new%20situations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20J.%20Wilhelm%2C%20A.%20Oztop%2C%20E.%20Peters%2C%20J.%20Reinforcement%20learning%20to%20adjust%20parametrized%20motor%20primitives%20to%20new%20situations%202012"
        },
        {
            "id": "Kulkarni_et+al_2016_a",
            "entry": "T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 3675\u20133683, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20T.D.%20Narasimhan%2C%20K.%20Saeedi%2C%20A.%20Tenenbaum%2C%20J.%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20T.D.%20Narasimhan%2C%20K.%20Saeedi%2C%20A.%20Tenenbaum%2C%20J.%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Kupcsik_et+al_2013_a",
            "entry": "A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-efficient generalization of robot skills with contextual policy search. In Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013, pp. 1401\u20131407, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kupcsik%2C%20A.G.%20Deisenroth%2C%20M.P.%20Peters%2C%20J.%20Neumann%2C%20G.%20Data-efficient%20generalization%20of%20robot%20skills%20with%20contextual%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kupcsik%2C%20A.G.%20Deisenroth%2C%20M.P.%20Peters%2C%20J.%20Neumann%2C%20G.%20Data-efficient%20generalization%20of%20robot%20skills%20with%20contextual%20policy%20search%202013"
        },
        {
            "id": "Levy_et+al_2017_a",
            "entry": "A. Levy, R. Platt, and K. Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00948"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Lin_1992_a",
            "entry": "L. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3/4):69\u201397, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20L.%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20L.%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992"
        },
        {
            "id": "Mankowitz_et+al_2018_a",
            "entry": "D. J. Mankowitz, A. \u017d\u00eddek, A. Barreto, D. Horgan, M. Hessel, J. Quan, J. Oh, H. van Hasselt, D. Silver, and T. Schaul. Unicorn: Continual learning with a universal, off-policy agent. arXiv preprint arXiv:1802.08294, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08294"
        },
        {
            "id": "Mccloskey_1989_a",
            "entry": "M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of Learning and Motivation-Advances in Research and Theory, 24 (C):109\u2013165, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCloskey%2C%20M.%20Cohen%2C%20N.J.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20The%20sequential%20learning%20problem%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCloskey%2C%20M.%20Cohen%2C%20N.J.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20The%20sequential%20learning%20problem%201989"
        },
        {
            "id": "Metzen_et+al_2015_a",
            "entry": "J. H. Metzen, A. Fabisch, and J. Hansen. Bayesian optimization for contextual policy search. In Proceedings of the Second Machine Learning in Planning and Control of Robot Motion Workshop., Hamburg, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzen%2C%20J.H.%20Fabisch%2C%20A.%20Hansen%2C%20J.%20Bayesian%20optimization%20for%20contextual%20policy%20search%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzen%2C%20J.H.%20Fabisch%2C%20A.%20Hansen%2C%20J.%20Bayesian%20optimization%20for%20contextual%20policy%20search%202015"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R.%20Stepleton%2C%20T.%20Harutyunyan%2C%20A.%20Bellemare%2C%20M.%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R.%20Stepleton%2C%20T.%20Harutyunyan%2C%20A.%20Bellemare%2C%20M.%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Nair_2010_a",
            "entry": "V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, 06\u201311 Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20J.%20Singh%2C%20S.%20Lee%2C%20H.%20Kohli%2C%20P.%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20J.%20Singh%2C%20S.%20Lee%2C%20H.%20Kohli%2C%20P.%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "D. Pathak, P. Mahmoudieh, M. Luo, P. Agrawal, D. Chen, F. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell. Zero-shot visual imitation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Mahmoudieh%2C%20P.%20Luo%2C%20M.%20Agrawal%2C%20P.%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Mahmoudieh%2C%20P.%20Luo%2C%20M.%20Agrawal%2C%20P.%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Peshkin_2002_a",
            "entry": "L. Peshkin and C. R. Shelton. Learning from scarce experience. In Proceedings of the Nineteenth International Conference on Machine Learning, pp. 498\u2013505, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peshkin%2C%20L.%20Shelton%2C%20C.R.%20Learning%20from%20scarce%20experience%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peshkin%2C%20L.%20Shelton%2C%20C.R.%20Learning%20from%20scarce%20experience%202002"
        },
        {
            "id": "Peters_2008_a",
            "entry": "J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682\u2013697, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20J.%20Schaal%2C%20S.%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20J.%20Schaal%2C%20S.%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008"
        },
        {
            "id": "Plappert_et+al_2018_a",
            "entry": "M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09464"
        },
        {
            "id": "Precup_et+al_2000_a",
            "entry": "D. Precup, R. S. Sutton, and S. P. Singh. Eligibility traces for off-policy policy evaluation. In International Conference on Machine Learning, pp. 759\u2013766, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Precup%2C%20D.%20Sutton%2C%20R.S.%20Singh%2C%20S.P.%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Precup%2C%20D.%20Sutton%2C%20R.S.%20Singh%2C%20S.P.%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In Proceedings of the International Conference on Machine Learning, pp. 1312\u20131320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20value%20function%20approximators%20In%20Proceedings%20of%20the%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20value%20function%20approximators%20In%20Proceedings%20of%20the%20International%20Conference%20on%20Machine%20Learning%20pp%2013121320%202015"
        },
        {
            "id": "Schmidhuber_1990_a",
            "entry": "J. Schmidhuber and R. Huber. Learning to Generate Focus Trajectories for Attentive Vision. Institut f\u00fcr Informatik, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20Generate%20Focus%20Trajectories%20for%20Attentive%20Vision%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20Generate%20Focus%20Trajectories%20for%20Attentive%20Vision%201990"
        },
        {
            "id": "Sen_1994_a",
            "entry": "P. Sen and J. Singer. Large Sample Methods in Statistics: An Introduction with Applications. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis, 1994. ISBN 9780412042218.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sen%2C%20P.%20Singer%2C%20J.%20Large%20Sample%20Methods%20in%20Statistics%3A%20An%20Introduction%20with%20Applications%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sen%2C%20P.%20Singer%2C%20J.%20Large%20Sample%20Methods%20in%20Statistics%3A%20An%20Introduction%20with%20Applications%201994"
        },
        {
            "id": "Srivastava_et+al_2013_a",
            "entry": "R. K. Srivastava, B. R. Steunebrink, and J. Schmidhuber. First experiments with PowerPlay. Neural Networks, 41(0):130 \u2013 136, 2013. Special Issue on Autonomous Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20R.K.%20Steunebrink%2C%20B.R.%20Schmidhuber%2C%20J.%20First%20experiments%20with%20PowerPlay%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20R.K.%20Steunebrink%2C%20B.R.%20Schmidhuber%2C%20J.%20First%20experiments%20with%20PowerPlay%202013"
        },
        {
            "id": "Sukhbaatar_et+al_2018_a",
            "entry": "S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20S.%20Lin%2C%20Z.%20Kostrikov%2C%20I.%20Synnaeve%2C%20G.%20Intrinsic%20motivation%20and%20automatic%20curricula%20via%20asymmetric%20self-play%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20S.%20Lin%2C%20Z.%20Kostrikov%2C%20I.%20Synnaeve%2C%20G.%20Intrinsic%20motivation%20and%20automatic%20curricula%20via%20asymmetric%20self-play%202018"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Bradford Book, 1998. ISBN 9780262193986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, pp. 1057\u20131063, 1999a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%201999"
        },
        {
            "id": "Sutton_et+al_1999_b",
            "entry": "R. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211, 1999b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Precup%2C%20D.%20Singh%2C%20S.%20Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Precup%2C%20D.%20Singh%2C%20S.%20Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "Thomas_2015_a",
            "entry": "P. Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts Amherst, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%2C%20P.%20Safe%20reinforcement%20learning%202015"
        },
        {
            "id": "International_2015_a",
            "entry": "International Conference on Machine Learning, pp. 2380\u20132388, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=International%20Conference%20on%20Machine%20Learning%20pp%2023802388%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=International%20Conference%20on%20Machine%20Learning%20pp%2023802388%202015"
        },
        {
            "id": "Williams_2017_a",
            "entry": "FeUdal networks for hierarchical reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, pp. 3540\u20133549, 06\u201311 Aug 2017. R. J. Williams. Reinforcement-learning in connectionist networks: A mathematical analysis. Technical Report 8605, Institute for Cognitive Science, University of California, San Diego, 1986. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20FeUdal%20networks%20for%20hierarchical%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In IEEE International Conference on Robotics and Automation, pp. 3357\u20133364, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Mottaghi%2C%20R.%20Kolve%2C%20E.%20Lim%2C%20J.J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Y.%20Mottaghi%2C%20R.%20Kolve%2C%20E.%20Lim%2C%20J.J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017"
        }
    ]
}
