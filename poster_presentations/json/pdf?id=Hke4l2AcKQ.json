{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "MAE: MUTUAL POSTERIOR-DIVERGENCE REGULARIZATION FOR VARIATIONAL AUTOENCODERS",
        "author": "Xuezhe Ma, Chunting Zhou & Eduard Hovy Carnegie Mellon University {xuezhem, ctzhou, ehovy}@cs.cmu.edu",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hke4l2AcKQ"
        },
        "abstract": "Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation. Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning."
    },
    "keywords": [
        {
            "term": "geometry",
            "url": "https://en.wikipedia.org/wiki/geometry"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        },
        {
            "term": "latent variable model",
            "url": "https://en.wikipedia.org/wiki/latent_variable_model"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "VAE": "Variational Autoencoder",
        "LVMs": "latent variable models",
        "ELBO": "evidence lower bound",
        "AAEs": "Adversarial Autoencoders",
        "MPD": "mutual posterior diversity",
        "MI": "mutual information",
        "VLAE": "Variational Lossy Autoencoder",
        "AF": "auto-regressive flow",
        "NLL": "negative log-likelihood"
    },
    "highlights": [
        "Representation learning, besides data distributions estimation, is a principle component in generative models",
        "(2) Theoretically, we establish a close relation between MAE and InfoVAE, by showing that the mutual posterior-devergence regularization maximizes a symmetric version of the KL divergence involved in InfoVAE\u2019s mutual information term (\u00a73.3). (3) Experimentally, on three benchmark datasets for images, we demonstrate the effectiveness of MAE as a density estimator by state-of-the-art log-likelihood results on MNIST and OMNIGLOT, and comparable result on CIFAR-10",
        "We evaluate MAE on two binary images that are commonly used for evaluating deep generative models: MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>) and OMNIGLOT (<a class=\"ref-link\" id=\"cLake_et+al_2013_a\" href=\"#rLake_et+al_2013_a\">Lake et al, 2013</a>; <a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\"><a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\">Burda et al, 2015</a></a>), both with dynamically binarized version (<a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\"><a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\">Burda et al, 2015</a></a>)",
        "We proposed a mutual posterior-divergence regularization for Variational Autoencoder, which controls the geometry of the latent space during training",
        "By connecting the mutual posterior diversity with the mutual information, we have formally studied the theoretical properties of the proposed MAEs",
        "Experiments on three benchmark datasets of images show the capability of MAEs on both density estimation and representation learning, with state-of-the-art or comparable likelihood, and superior performance on image reconstruction, unsupervised clustering and semi-supervised classification against previous top-performing models"
    ],
    "key_statements": [
        "Representation learning, besides data distributions estimation, is a principle component in generative models",
        "Variational Autoencoder are usually estimated by maximizing the likelihood of the observed data by marginalizing over the latent variables, typically via optimizing the evidence lower bound (ELBO)",
        "Our contributions are three-fold: (1) Algorithmically, we introduce the mutual posterior-divergence regularization for Variational Autoencoder, named MAEs (\u00a73.2), to control the geometry of the latent space during learning by encouraging the learned variational posteriors to be diverse, to achieve low-redundant, interpretable representation learning",
        "(2) Theoretically, we establish a close relation between MAE and InfoVAE, by showing that the mutual posterior-devergence regularization maximizes a symmetric version of the KL divergence involved in InfoVAE\u2019s mutual information term (\u00a73.3). (3) Experimentally, on three benchmark datasets for images, we demonstrate the effectiveness of MAE as a density estimator by state-of-the-art log-likelihood results on MNIST and OMNIGLOT, and comparable result on CIFAR-10",
        "We provide theoretical justification by connecting the mutual posterior diversity (MPD) in (4) with the mutual information term defined in InfoVAE (<a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\">Zhao et al, 2017</a>)",
        "We evaluate MAE on two binary images that are commonly used for evaluating deep generative models: MNIST (<a class=\"ref-link\" id=\"cLecun_et+al_1998_a\" href=\"#rLecun_et+al_1998_a\">LeCun et al, 1998</a>) and OMNIGLOT (<a class=\"ref-link\" id=\"cLake_et+al_2013_a\" href=\"#rLake_et+al_2013_a\">Lake et al, 2013</a>; <a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\"><a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\">Burda et al, 2015</a></a>), both with dynamically binarized version (<a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\"><a class=\"ref-link\" id=\"cBurda_et+al_2015_a\" href=\"#rBurda_et+al_2015_a\">Burda et al, 2015</a></a>)",
        "Variational Lossy Autoencoder networks used in binary image datasets are similar of that described in <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al (2017a</a>): ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>) encoder same as in ResNet Variational Autoencoder (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>), PixelCNN (Oord et al, 2016) decoder with 6 layers of masked convolution, and 32-dimensional latent code with auto-regressive flow prior implemented with MADE (<a class=\"ref-link\" id=\"cGermain_et+al_2015_a\" href=\"#rGermain_et+al_2015_a\">Germain et al, 2015</a>)",
        "We proposed a mutual posterior-divergence regularization for Variational Autoencoder, which controls the geometry of the latent space during training",
        "By connecting the mutual posterior diversity with the mutual information, we have formally studied the theoretical properties of the proposed MAEs",
        "Experiments on three benchmark datasets of images show the capability of MAEs on both density estimation and representation learning, with state-of-the-art or comparable likelihood, and superior performance on image reconstruction, unsupervised clustering and semi-supervised classification against previous top-performing models"
    ],
    "summary": [
        "Representation learning, besides data distributions estimation, is a principle component in generative models.",
        "VAE (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a>; <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a>) gains popularity for its capability of estimating densities of complex distributions, while automatically learning meaningful representations from raw data.",
        "They proposed a solution by limiting the capacity of the decoder and applied PixelCNN (Oord et al, 2016) with small local receptive fields as the decoder of VAEs to model 2D images, achieving both impressive performance for density estimation and informative latent representations.",
        "By performing image reconstruction, unsupervised and semi-supervised classification, we show that MAE is capable of learning meaningful latent representations, even combined with a sufficiently powerful decoder (\u00a74).",
        "The essential reason of this problem is that, under absolutely unsupervised setting, the marginal likelihood based objective Lelbo incorporates no supervision on the latent space to characterize the latent variable Z with preferred properties w.r.t. representation learning.",
        "VLAE networks used in binary image datasets are similar of that described in <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al (2017a</a>): ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>) encoder same as in ResNet VAE (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>), PixelCNN (Oord et al, 2016) decoder with 6 layers of masked convolution, and 32-dimensional latent code with AF prior implemented with MADE (<a class=\"ref-link\" id=\"cGermain_et+al_2015_a\" href=\"#rGermain_et+al_2015_a\">Germain et al, 2015</a>).",
        "In order to evaluate the quality of the learned latent representations, we conduct three sets of experiments \u2014 image reconstruction and generation, unsupervised clustering, and semi-supervised classification.",
        "The improvements of MAE over ResNet VAE and VLAE are more significant when the number of labeled training data is small, further proving the meaningful representation learned from MAE.",
        "Density estimation performance on CIFAR-10 of MAEs with different hyperparameters is provided in Table 3, compared with the top-performing likelihood-based unconditional generative models and variationally trained latent-variable models.",
        "We proposed a mutual posterior-divergence regularization for VAEs, which controls the geometry of the latent space during training.",
        "By connecting the mutual posterior diversity with the mutual information, we have formally studied the theoretical properties of the proposed MAEs. Experiments on three benchmark datasets of images show the capability of MAEs on both density estimation and representation learning, with state-of-the-art or comparable likelihood, and superior performance on image reconstruction, unsupervised clustering and semi-supervised classification against previous top-performing models.",
        "Another exciting direction is to formally study the properties of the standard deviation of the mutual posterior KL-divergence used to measure smoothness, providing further justification of the proposed regularizer, or even introducing alternatives to further improve performances."
    ],
    "headline": "We introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation",
    "reference_links": [
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "Burda_et+al_2015_a",
            "entry": "Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. In Proceedings of the 5th International Conference on Learning Representations (ICLR-2017), Toulon, France, April 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Duan%2C%20Yan%20Variational%20lossy%20autoencoder%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Duan%2C%20Yan%20Variational%20lossy%20autoencoder%202017-04"
        },
        {
            "id": "Chen_et+al_0000_a",
            "entry": "Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. arXiv preprint arXiv:1712.09763, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09763"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Dziugaite_et+al_2015_a",
            "entry": "Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.03906"
        },
        {
            "id": "Esmaeili_et+al_2018_a",
            "entry": "Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N Siddharth, Brooks Paige, Dana H Brooks, Jennifer Dy, and Jan-Willem van de Meent. Structured disentangled representations. stat, 1050:29, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esmaeili%2C%20Babak%20Wu%2C%20Hao%20Jain%2C%20Sarthak%20Alican%20Bozkurt%2C%20N.Siddharth%20Structured%20disentangled%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esmaeili%2C%20Babak%20Wu%2C%20Hao%20Jain%2C%20Sarthak%20Alican%20Bozkurt%2C%20N.Siddharth%20Structured%20disentangled%20representations%202018"
        },
        {
            "id": "Ganchev_et+al_2010_a",
            "entry": "Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul):2001\u20132049, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pp. 881\u2013889, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Made%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Made%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems (NIPS-2014), pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pp. 3549\u20133557, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016"
        },
        {
            "id": "Gretton_et+al_2007_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Sch\u00f6lkopf, and Alex J Smola. A kernel method for the two-sample-problem. In Advances in neural information processing systems, pp. 513\u2013520, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20method%20for%20the%20two-sample-problem%202007"
        },
        {
            "id": "John_1979_a",
            "entry": "John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):100\u2013108, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%20A%20Hartigan%20and%20Manchek%20A%20Wong%20Algorithm%20as%20136%20A%20kmeans%20clustering%20algorithm%20Journal%20of%20the%20Royal%20Statistical%20Society%20Series%20C%20Applied%20Statistics%20281100108%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%20A%20Hartigan%20and%20Manchek%20A%20Wong%20Algorithm%20as%20136%20A%20kmeans%20clustering%20algorithm%20Journal%20of%20the%20Royal%20Statistical%20Society%20Series%20C%20Applied%20Statistics%20281100108%201979"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hinton_et+al_1995_a",
            "entry": "Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake-sleep\" algorithm for unsupervised neural networks. Science, 268(5214):1158\u20131161, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Dayan%2C%20Peter%20Frey%2C%20Brendan%20J.%20Neal%2C%20Radford%20M.%20The%22%20wake-sleep%22%20algorithm%20for%20unsupervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Dayan%2C%20Peter%20Frey%2C%20Brendan%20J.%20Neal%2C%20Radford%20M.%20The%22%20wake-sleep%22%20algorithm%20for%20unsupervised%20neural%20networks%201995"
        },
        {
            "id": "Hjelm_et+al_2018_a",
            "entry": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.06670"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Jlb_2015_a",
            "entry": "Diederik P Kingma JLB. Adam: A method for stochastic optimization. Proc. of ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JLB%2C%20Diederik%20P.Kingma%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=JLB%2C%20Diederik%20P.Kingma%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02550"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2th International Conference on Learning Representations (ICLR-2014), Banff, Canada, April 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014-04"
        },
        {
            "id": "Kingma_et+al_2014_b",
            "entry": "Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pp. 4743\u20134751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Krause_et+al_2010_a",
            "entry": "Andreas Krause, Pietro Perona, and Ryan G Gomes. Discriminative clustering by regularized information maximization. In Advances in neural information processing systems, pp. 775\u2013783, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Andreas%20Perona%2C%20Pietro%20Gomes%2C%20Ryan%20G.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krause%2C%20Andreas%20Perona%2C%20Pietro%20Gomes%2C%20Ryan%20G.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lake_et+al_2013_a",
            "entry": "Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In Advances in neural information processing systems, pp. 2526\u2013 2534, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20R.%20Tenenbaum%2C%20Josh%20One-shot%20learning%20by%20inverting%20a%20compositional%20causal%20process%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20R.%20Tenenbaum%2C%20Josh%20One-shot%20learning%20by%20inverting%20a%20compositional%20causal%20process%202013"
        },
        {
            "id": "Larochelle_2011_a",
            "entry": "Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2011, pp. 29\u201337, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of International Conference on Machine Learning (ICML-2015), pp. 1718\u20131727, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "Van_2008_a",
            "entry": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Makhzani_et+al_2015_a",
            "entry": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proceedings of International Conference on Machine Learning (ICML-2016), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Polyak_1992_a",
            "entry": "Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20T.%20Juditsky%2C%20Anatoli%20B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20T.%20Juditsky%2C%20Anatoli%20B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05770"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML-2014), pp. 1278\u20131286, Bejing, China, 22\u201324 Jun 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014-06"
        },
        {
            "id": "Rolfe_2016_a",
            "entry": "Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02200"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P Kingma, and Yaroslav Bulatov. Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Karpathy%2C%20Andrej%20Chen%2C%20Xi%20Kingma%2C%20Diederik%20P.%20Pixelcnn%2B%2B%3A%20A%20pixelcnn%20implementation%20with%20discretized%20logistic%20mixture%20likelihood%20and%20other%20modifications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Karpathy%2C%20Andrej%20Chen%2C%20Xi%20Kingma%2C%20Diederik%20P.%20Pixelcnn%2B%2B%3A%20A%20pixelcnn%20implementation%20with%20discretized%20logistic%20mixture%20likelihood%20and%20other%20modifications%202017"
        },
        {
            "id": "Serban_et+al_2017_a",
            "entry": "Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, pp. 3295\u20133301, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Lowe%2C%20Ryan%20Charlin%2C%20Laurent%20and%20Yoshua%20Bengio.%20A%20hierarchical%20latent%20variable%20encoder-decoder%20model%20for%20generating%20dialogues%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Lowe%2C%20Ryan%20Charlin%2C%20Laurent%20and%20Yoshua%20Bengio.%20A%20hierarchical%20latent%20variable%20encoder-decoder%20model%20for%20generating%20dialogues%202017"
        },
        {
            "id": "S_et+al_2016_a",
            "entry": "Casper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Ladder variational autoencoders. In Advances in neural information processing systems, pp. 3738\u20133746, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%B8nderby%2C%20Casper%20Kaae%20Raiko%2C%20Tapani%20Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Ladder%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%B8nderby%2C%20Casper%20Kaae%20Raiko%2C%20Tapani%20Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Ladder%20variational%20autoencoders%202016"
        },
        {
            "id": "S_et+al_0000_a",
            "entry": "Casper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02282"
        },
        {
            "id": "Tomczak_2018_a",
            "entry": "Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial Intelligence and Statistics, pp. 1214\u20131223, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomczak%2C%20Jakub%20Welling%2C%20Max%20Vae%20with%20a%20vampprior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomczak%2C%20Jakub%20Welling%2C%20Max%20Vae%20with%20a%20vampprior%202018"
        },
        {
            "id": "Den_2014_a",
            "entry": "Aaron Van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian mixture models. In Advances in Neural Information Processing Systems, pp. 3518\u20133526, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=den%20Oord%2C%20Aaron%20Van%20Schrauwen%2C%20Benjamin%20Factoring%20variations%20in%20natural%20images%20with%20deep%20gaussian%20mixture%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=den%20Oord%2C%20Aaron%20Van%20Schrauwen%2C%20Benjamin%20Factoring%20variations%20in%20natural%20images%20with%20deep%20gaussian%20mixture%20models%202014"
        },
        {
            "id": "Wainwright_2008_a",
            "entry": "Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1\u20132):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20J%20Wainwright%20Michael%20I%20Jordan%20et%20al%20Graphical%20models%20exponential%20families%20and%20variational%20inference%20Foundations%20and%20Trends%20R%20in%20Machine%20Learning%201121305%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20J%20Wainwright%20Michael%20I%20Jordan%20et%20al%20Graphical%20models%20exponential%20families%20and%20variational%20inference%20Foundations%20and%20Trends%20R%20in%20Machine%20Learning%201121305%202008"
        },
        {
            "id": "Xie_2015_a",
            "entry": "Pengtao Xie, Yuntian Deng, and Eric Xing. Latent variable modeling with diversity-inducing mutual angular regularization. arXiv preprint arXiv:1512.07336, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.07336"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. In Proceedings of International Conference on Machine Learning (ICML-2017), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20Berg-Kirkpatrick%2C%20Taylor%20Improved%20variational%20autoencoders%20for%20text%20modeling%20using%20dilated%20convolutions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20Berg-Kirkpatrick%2C%20Taylor%20Improved%20variational%20autoencoders%20for%20text%20modeling%20using%20dilated%20convolutions%202017"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02262"
        }
    ]
}
