{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SUPERVISED POLICY UPDATE FOR DEEP REINFORCEMENT LEARNING",
        "author": "Quan Vuong University of California, San Diego qvuong@ucsd.edu",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJxTroR9F7"
        },
        "abstract": "We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "trust region",
            "url": "https://en.wikipedia.org/wiki/trust_region"
        },
        {
            "term": "policy gradient method",
            "url": "https://en.wikipedia.org/wiki/policy_gradient_method"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        }
    ],
    "abbreviations": {
        "SPU": "Supervised Policy Update",
        "PPO": "Policy Optimization",
        "DRL": "deep reinforcement learning",
        "MDP": "Markov Decision Process",
        "GAC": "Guided Actor Critic",
        "ACKTR": "Actor-Critic using Kronecker-Factored Trust Region",
        "K-FAC": "Kronecker-factored approximation curvature"
    },
    "highlights": [
        "The policy gradient problem in deep reinforcement learning (DRL) can be defined as seeking a parameterized policy with high expected reward",
        "We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency problem",
        "We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology",
        "We provide ablation studies to show the importance of the different algorithmic components, and a sensitivity analysis to show that Supervised Policy Update\u2019s performance is relatively insensitive to hyper-parameter choices",
        "Since the forward-KL approach is our best performing approach, we focus subsequent analysis on it and hereafter refer to it as Supervised Policy Update",
        "Figure 1 illustrates the performance of Supervised Policy Update versus TRPO, Policy Optimization"
    ],
    "key_statements": [
        "The policy gradient problem in deep reinforcement learning (DRL) can be defined as seeking a parameterized policy with high expected reward",
        "We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency problem",
        "Starting with data generated by the current policy, Supervised Policy Update optimizes over a proximal policy space to find an optimal non-parameterized policy",
        "We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology",
        "Since we clearly outperform TRPO, we argue that Supervised Policy Update\u2019s two-process procedure has significant potentials",
        "We show how a Policy Optimization-like objective can be formulated in the context of Supervised Policy Update",
        "Note that here we are using a variation of the Supervised Policy Update methodology described in Section 4 since here we first create estimates of the expectations in the objective and constraints and solve the optimization problem",
        "We provide ablation studies to show the importance of the different algorithmic components, and a sensitivity analysis to show that Supervised Policy Update\u2019s performance is relatively insensitive to hyper-parameter choices",
        "Since the forward-KL approach is our best performing approach, we focus subsequent analysis on it and hereafter refer to it as Supervised Policy Update",
        "Figure 1 illustrates the performance of Supervised Policy Update versus TRPO, Policy Optimization",
        "We sampled 100 Supervised Policy Update hyper-parameter vectors, and for each one determined the relative performance with respect to TRPO"
    ],
    "summary": [
        "The policy gradient problem in deep reinforcement learning (DRL) can be defined as seeking a parameterized policy with high expected reward.",
        "We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency problem.",
        "NPG/TRPO finds the gradient update by solving the sample efficiency problem (1)-(2) with \u03b7 = DKL, i.e., use the aggregate KL-divergence for the policy proximity constraint (2).",
        "For a given constraint criterion \u03b7(\u03c0, \u03c0\u03b8k ) \u2264 \u03b4, we find the optimal solution to the non-parameterized problem: maximize \u03c0\u2208\u03a0",
        "We will show below the optimal solution \u03c0\u2217 for the non-parameterized problem (9)-(10) can be determined nearly in closed form for many natural constraint criteria \u03b7(\u03c0, \u03c0\u03b8k ) \u2264 \u03b4.",
        "To illustrate the SPU methodology, for three different but natural types of proximity constraints, we solve the corresponding non-parameterized optimization problem and derive the resulting gradient for the SPU supervised learning problem.",
        "The SPU framework allows us to solve the optimization problem with the disaggregated constraints exactly.",
        "We set \u03bbsi = \u03bb for all si in (17) and introduce per-state acceptance to enforce the disaggregated constraints, giving the approximate gradient:",
        "We seek to find the non-parameterized optimal policy by solving: maximize d\u03c0\u03b8k (s) E [A\u03c0\u03b8k (s, a)]",
        "Note that here we are using a variation of the SPU methodology described in Section 4 since here we first create estimates of the expectations in the objective and constraints and solve the optimization problem.",
        "Note that we have included an aggregated constraint (26) in addition to the PPO-like constraint (25), which further ensures that the updated policy is close to \u03c0\u03b8k .",
        "After solving for \u03c0\u2217, we seek a parameterized policy \u03c0\u03b8 that is close to \u03c0\u2217 by minimizing their mean square error over sampled states and actions, i.e. by updating \u03b8 in the negative direction of \u2207\u03b8 i \u2212 \u03c0\u2217)2.",
        "Extensive experimental results demonstrate SPU outperforms recent state-of-the-art methods for environments with continuous or discrete action spaces.",
        "In terms of final performance averaged over all available ten Mujoco environments and ten different seeds in each, SPU with L\u221e constraint (Section 5.3) and SPU with forward KL constraints (Section 5.1) outperform TRPO by 6% and 27% respectively.",
        "To ensure that SPU is not only better than TRPO in terms of performance gain early during training, we further retrain both policies for 3 million timesteps.",
        "We sampled 100 SPU hyper-parameter vectors, and for each one determined the relative performance with respect to TRPO.",
        "We found that for all 100 random hyper-parameter value samples, SPU performed better than TRPO."
    ],
    "headline": "We propose a new sample-efficient methodology, called Supervised Policy Update, for deep reinforcement learning",
    "reference_links": [
        {
            "id": "Yuval_2018_a",
            "entry": "Yuval Tassa Remi Munos Nicolas Heess Martin Riedmiller Abbas Abdolmaleki, Jost Tobias Springenberg. Maximum a posteriori policy optimisation. 2018. URL https://arxiv.org/abs/1806.06920.",
            "url": "https://arxiv.org/abs/1806.06920",
            "arxiv_url": "https://arxiv.org/pdf/1806.06920"
        },
        {
            "id": "Achiam_2017_a",
            "entry": "Joshua Achiam. Advanced policy gradient methods. http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf, 2017.",
            "url": "http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf"
        },
        {
            "id": "Achiam_et+al_2017_b",
            "entry": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pp. 22\u201331, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achiam%2C%20Joshua%20Held%2C%20David%20Tamar%2C%20Aviv%20Abbeel%2C%20Pieter%20Constrained%20policy%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achiam%2C%20Joshua%20Held%2C%20David%20Tamar%2C%20Aviv%20Abbeel%2C%20Pieter%20Constrained%20policy%20optimization%202017"
        },
        {
            "id": "Amari_1998_a",
            "entry": "Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "Bellemare_et+al_2012_a",
            "entry": "Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012. URL http://arxiv.org/abs/1207.4708.",
            "url": "http://arxiv.org/abs/1207.4708",
            "arxiv_url": "https://arxiv.org/pdf/1207.4708"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540.",
            "url": "http://arxiv.org/abs/1606.01540",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016. URL http://arxiv.org/abs/1604.06778.",
            "url": "http://arxiv.org/abs/1604.06778",
            "arxiv_url": "https://arxiv.org/pdf/1604.06778"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290.",
            "url": "http://arxiv.org/abs/1801.01290",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "Peters_2010_a",
            "entry": "Yasemin Altun Jan Peters, Katharina Mulling. Relative entropy policy search. 2010. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264.",
            "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264"
        },
        {
            "id": "Kakade_2003_a",
            "entry": "Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of London London, England, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%202003"
        },
        {
            "id": "Kakade_2002_a",
            "entry": "Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pp. 267\u2013274, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002"
        },
        {
            "id": "Kakade_2002_b",
            "entry": "Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pp. 1531\u20131538, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20A%20natural%20policy%20gradient%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20A%20natural%20policy%20gradient%202002"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Levine_2017_a",
            "entry": "Sergey Levine. UC Berkeley CS294 deep reinforcement learning lecture notes. http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html, 2017.",
            "url": "http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html"
        },
        {
            "id": "Mania_et+al_2018_a",
            "entry": "Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07055"
        },
        {
            "id": "Tan_2018_a",
            "entry": "Ruoming Pang Vijay Vasudevan Quoc V. Le Mingxing Tan, Bo Chen. Mnasnet: Platform-aware neural architecture search for mobile. 2018. URL https://arxiv.org/abs/1807.11626.",
            "url": "https://arxiv.org/abs/1807.11626",
            "arxiv_url": "https://arxiv.org/pdf/1807.11626"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518, 2015. URL http://dx.doi.org/10.1038/nature14236.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Springer_2017_a",
            "entry": "Springer, 1992. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5285\u20135294, 2017. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016. URL http://arxiv.org/abs/1611.01578.",
            "url": "http://arxiv.org/abs/1611.01578",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
