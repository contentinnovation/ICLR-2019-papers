{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR",
        "author": "Pankaj Gupta, Yatin Chaudhary, Florian Buettner, Hinrich Schutze, 1Corporate Technology, Machine-Intelligence (MIC-DE), Siemens AG Munich, Germany 2CIS, University of Munich (LMU) Munich, Germany {pankaj.gupta, yatin.chaudhary, buettner.florian}@siemens.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkgoyn09KQ"
        },
        "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P (word|context) : (1) No language structure in context: Probabilistic topic models ignore word order by summarizing a given context as a \u201cbag-of-word\u201d and consequently the semantics of words in the context is lost. In this work, we incorporate language structure by combining a neural autoregressive topic model (TM) (e.g., DocNADE) with a LSTM based language model (LSTM-LM) in a single probabilistic framework. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns, while the TM simultaneously learns a latent representation from the entire document. In addition, the LSTM-LM models complex characteristics of language (e.g., syntax and semantics), while the TM discovers the underlying thematic structure in a collection of documents. We unite two complementary paradigms of learning the meaning of word occurrences by combining a topic model and a language model in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited context and/or smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the word-topic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe. We present novel neural autoregressive topic model variants coupled with neural language models and embeddings priors that consistently outperform state-of-theart generative topic models in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 7 long-text and 8 short-text datasets from diverse domains."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "topic model",
            "url": "https://en.wikipedia.org/wiki/topic_model"
        },
        {
            "term": "semantics",
            "url": "https://en.wikipedia.org/wiki/semantics"
        },
        {
            "term": "word order",
            "url": "https://en.wikipedia.org/wiki/word_order"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "language structure",
            "url": "https://en.wikipedia.org/wiki/language_structure"
        },
        {
            "term": "Restricted Boltzmann Machine",
            "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_Machine"
        },
        {
            "term": "language model",
            "url": "https://en.wikipedia.org/wiki/language_model"
        },
        {
            "term": "information retrieval",
            "url": "https://en.wikipedia.org/wiki/information_retrieval"
        }
    ],
    "abbreviations": {
        "TM": "topic model",
        "LSTM-LM": "LSTM-based language model",
        "RSM": "Replicated Softmax",
        "DocNADE": "Document Neural Autoregressive Distribution Estimator",
        "NLP": "natural language processing",
        "IR": "information retrieval",
        "LMs": "language models",
        "DMM": "Dirichlet Multinomial Mixture",
        "RBM": "Restricted Boltzmann Machine",
        "RV": "reduced vocabulary",
        "FV": "full text/vocabulary"
    },
    "highlights": [
        "Probabilistic topic models, such as LDA (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al, 2003</a>), Replicated Softmax (RSM) (Salakhutdinov & Hinton, 2009) and Document Neural Autoregressive Distribution Estimator (DocNADE) variants (<a class=\"ref-link\" id=\"cLarochelle_2012_a\" href=\"#rLarochelle_2012_a\">Larochelle & Lauly, 2012</a>; <a class=\"ref-link\" id=\"cZheng_et+al_2016_a\" href=\"#rZheng_et+al_2016_a\">Zheng et al, 2016</a>; <a class=\"ref-link\" id=\"cLauly_et+al_2017_a\" href=\"#rLauly_et+al_2017_a\">Lauly et al, 2017</a>; <a class=\"ref-link\" id=\"cGupta_et+al_2019_a\" href=\"#rGupta_et+al_2019_a\">Gupta et al, 2019</a>) are often used to extract topics from text collections, and predict the probabilities of each word in a given document belonging to each topic",
        "To motivate our first task of extending probabilistic topic models to incorporate word order and language structure, assume that we conduct topic analysis on the following two sentences: Bear falls into market territory and Market falls into bear territory",
        "Contribution 1: We introduce language structure into neural autoregressive topic models via a LSTM-based language model, thereby accounting for word ordering, language concepts and long-range dependencies",
        "Experimental Setup: Document Neural Autoregressive Distribution Estimator is often trained on a reduced vocabulary (RV) after pre-processing; we investigate training it on full text/vocabulary (FV) (Table 2) and compute document representations to perform different evaluation tasks",
        "We have shown that accounting for language concepts such as word ordering, syntactic and semantic information in neural autoregressive topic models helps to better estimate the probability of a word in a given context"
    ],
    "key_statements": [
        "Probabilistic topic models, such as LDA (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al, 2003</a>), Replicated Softmax (RSM) (Salakhutdinov & Hinton, 2009) and Document Neural Autoregressive Distribution Estimator (DocNADE) variants (<a class=\"ref-link\" id=\"cLarochelle_2012_a\" href=\"#rLarochelle_2012_a\">Larochelle & Lauly, 2012</a>; <a class=\"ref-link\" id=\"cZheng_et+al_2016_a\" href=\"#rZheng_et+al_2016_a\">Zheng et al, 2016</a>; <a class=\"ref-link\" id=\"cLauly_et+al_2017_a\" href=\"#rLauly_et+al_2017_a\">Lauly et al, 2017</a>; <a class=\"ref-link\" id=\"cGupta_et+al_2019_a\" href=\"#rGupta_et+al_2019_a\">Gupta et al, 2019</a>) are often used to extract topics from text collections, and predict the probabilities of each word in a given document belonging to each topic",
        "To motivate our first task of extending probabilistic topic models to incorporate word order and language structure, assume that we conduct topic analysis on the following two sentences: Bear falls into market territory and Market falls into bear territory",
        "Contribution 1: We introduce language structure into neural autoregressive topic models via a LSTM-based language model, thereby accounting for word ordering, language concepts and long-range dependencies",
        "We propose two extensions of the Document Neural Autoregressive Distribution Estimator model: (1) ctx-Document Neural Autoregressive Distribution Estimator: introducing language structure via LSTM-based language model and (2) ctx-DocNADEe: incorporating external knowledge via pre-trained word embeddings E, to model short and long texts",
        "Baselines: While, we evaluate our multi-fold contributions on four tasks: generalization, topic coherence, text retrieval and categorization, we compare performance of our proposed models ctx-Document Neural Autoregressive Distribution Estimator and ctx-DocNADEe with related baselines based on: (1) word representation: glove (<a class=\"ref-link\" id=\"cPennington_et+al_2014_a\" href=\"#rPennington_et+al_2014_a\">Pennington et al, 2014</a>), where a document is represented by summing the embedding vectors of it\u2019s words, (2) document representation: doc2vec (<a class=\"ref-link\" id=\"cLe_2014_a\" href=\"#rLe_2014_a\">Le & Mikolov, 2014</a>), (3) LDA based BoW topic model: ProdLDA (<a class=\"ref-link\" id=\"cSrivastava_2017_a\" href=\"#rSrivastava_2017_a\">Srivastava & Sutton, 2017</a>) and SCHOLAR1 (<a class=\"ref-link\" id=\"cCard_et+al_2017_a\" href=\"#rCard_et+al_2017_a\">Card et al, 2017</a>) (4) neural BoW topic model: Document Neural Autoregressive Distribution Estimator and NTM (<a class=\"ref-link\" id=\"cCao_et+al_2015_a\" href=\"#rCao_et+al_2015_a\">Cao et al, 2015</a>) and , (5) topic model, including pre-trained word embeddings: Gauss-LDA (GaussianLDA) (<a class=\"ref-link\" id=\"cDas_et+al_2015_a\" href=\"#rDas_et+al_2015_a\">Das et al, 2015</a>), and glove-Dirichlet Multinomial Mixture, glove-LDA (<a class=\"ref-link\" id=\"cNguyen_et+al_2015_a\" href=\"#rNguyen_et+al_2015_a\">Nguyen et al, 2015</a>). (6) jointly2 trained topic and language models: TDLM (<a class=\"ref-link\" id=\"cLau_et+al_2017_a\" href=\"#rLau_et+al_2017_a\">Lau et al, 2017</a>), Topic-RNN (<a class=\"ref-link\" id=\"cDieng_et+al_2016_a\" href=\"#rDieng_et+al_2016_a\">Dieng et al, 2016</a>) and TCNLM (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>)",
        "Experimental Setup: Document Neural Autoregressive Distribution Estimator is often trained on a reduced vocabulary (RV) after pre-processing; we investigate training it on full text/vocabulary (FV) (Table 2) and compute document representations to perform different evaluation tasks",
        "We have shown that accounting for language concepts such as word ordering, syntactic and semantic information in neural autoregressive topic models helps to better estimate the probability of a word in a given context"
    ],
    "summary": [
        "Probabilistic topic models, such as LDA (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\"><a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al, 2003</a></a>), Replicated Softmax (RSM) (Salakhutdinov & Hinton, 2009) and Document Neural Autoregressive Distribution Estimator (DocNADE) variants (<a class=\"ref-link\" id=\"cLarochelle_2012_a\" href=\"#rLarochelle_2012_a\"><a class=\"ref-link\" id=\"cLarochelle_2012_a\" href=\"#rLarochelle_2012_a\">Larochelle & Lauly, 2012</a></a>; <a class=\"ref-link\" id=\"cZheng_et+al_2016_a\" href=\"#rZheng_et+al_2016_a\"><a class=\"ref-link\" id=\"cZheng_et+al_2016_a\" href=\"#rZheng_et+al_2016_a\">Zheng et al, 2016</a></a>; <a class=\"ref-link\" id=\"cLauly_et+al_2017_a\" href=\"#rLauly_et+al_2017_a\"><a class=\"ref-link\" id=\"cLauly_et+al_2017_a\" href=\"#rLauly_et+al_2017_a\">Lauly et al, 2017</a></a>; <a class=\"ref-link\" id=\"cGupta_et+al_2019_a\" href=\"#rGupta_et+al_2019_a\"><a class=\"ref-link\" id=\"cGupta_et+al_2019_a\" href=\"#rGupta_et+al_2019_a\">Gupta et al, 2019</a></a>) are often used to extract topics from text collections, and predict the probabilities of each word in a given document belonging to each topic.",
        "Contribution 2: We incorporate distributed compositional priors in DocNADE: we use pre-trained word embeddings via LSTM-LM to supplement the multinomial topic model (i.e., DocNADE) in learning latent topic and textual representations on a smaller corpus and/or short texts.",
        "We combine the advantages of complementary learning and external knowledge, and couple topic- and language models with pre-trained word embeddings to model short and long text documents in a unified neural autoregressive framework, named as ctx-DocNADEe. Our approach learns better textual representations, which we quantify via generalizability, interpretability and applicability (e.g., IR and classification).",
        "We propose two extensions of the DocNADE model: (1) ctx-DocNADE: introducing language structure via LSTM-LM and (2) ctx-DocNADEe: incorporating external knowledge via pre-trained word embeddings E, to model short and long texts.",
        "Baselines: While, we evaluate our multi-fold contributions on four tasks: generalization, topic coherence, text retrieval and categorization, we compare performance of our proposed models ctx-DocNADE and ctx-DocNADEe with related baselines based on: (1) word representation: glove (<a class=\"ref-link\" id=\"cPennington_et+al_2014_a\" href=\"#rPennington_et+al_2014_a\">Pennington et al, 2014</a>), where a document is represented by summing the embedding vectors of it\u2019s words, (2) document representation: doc2vec (<a class=\"ref-link\" id=\"cLe_2014_a\" href=\"#rLe_2014_a\">Le & Mikolov, 2014</a>), (3) LDA based BoW TMs: ProdLDA (<a class=\"ref-link\" id=\"cSrivastava_2017_a\" href=\"#rSrivastava_2017_a\">Srivastava & Sutton, 2017</a>) and SCHOLAR1 (<a class=\"ref-link\" id=\"cCard_et+al_2017_a\" href=\"#rCard_et+al_2017_a\">Card et al, 2017</a>) (4) neural BoW TMs: DocNADE and NTM (<a class=\"ref-link\" id=\"cCao_et+al_2015_a\" href=\"#rCao_et+al_2015_a\">Cao et al, 2015</a>) and , (5) TMs, including pre-trained word embeddings: Gauss-LDA (GaussianLDA) (<a class=\"ref-link\" id=\"cDas_et+al_2015_a\" href=\"#rDas_et+al_2015_a\">Das et al, 2015</a>), and glove-DMM, glove-LDA (<a class=\"ref-link\" id=\"cNguyen_et+al_2015_a\" href=\"#rNguyen_et+al_2015_a\">Nguyen et al, 2015</a>).",
        "To evaluate the generative performance of the topic models, we estimate the log-probabilities for the test documents and compute the average held-out perplexity (P P L) per word as, P P L = exp \u2212",
        "Table 8 illustrates an example topic from the 20NSshort text dataset for DocNADE, ctx-DocNADE and ctx-DocNADEe, where the inclusion of embeddings results in a more coherent topic.",
        "For the short texts (Table 3), the glove leads DocNADE in classification performance, suggesting a need for distributional priors in the topic model.",
        "6IR-precision scores at 0.02 retrieval fraction on the short-text datasets by ProdLDA: 20NSshort (.08), TREC6 (.24), R21578title (.31), Subjectivity (.63) and Polarity (.51).",
        "In Figure 4, we quantify the quality of representations learned and demonstrate improvements due to the proposed models, i.e., ctx-DocNADE and ctx-DocNADEe over DocNADE at different fractions of the training data.",
        "We have shown that accounting for language concepts such as word ordering, syntactic and semantic information in neural autoregressive topic models helps to better estimate the probability of a word in a given context.",
        "Our experimental results show that our proposed modeling approaches consistently outperform stateof-the-art generative topic models, quantified by generalization, topic interpretability, and applicability on 15 datasets"
    ],
    "headline": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P : No language structure in context: Probabilistic topic models ignore word order by summarizing a given context as a \u201cbag-of-word\u201d and the semantics of words in the context is lost",
    "reference_links": [
        {
            "id": "Bengio_et+al_2003_a",
            "entry": "Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137\u20131155, 2003. URL http://www.jmlr.org/papers/v3/bengio03a.html.",
            "url": "http://www.jmlr.org/papers/v3/bengio03a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Ducharme%2C%20Rejean%20Vincent%2C%20Pascal%20Janvin%2C%20Christian%20A%20neural%20probabilistic%20language%20model%202003"
        },
        {
            "id": "Blei_et+al_2003_a",
            "entry": "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022, 2003. URL http://www.jmlr.org/papers/v3/blei03a.html.",
            "url": "http://www.jmlr.org/papers/v3/blei03a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "Cao_et+al_2015_a",
            "entry": "Ziqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. A novel neural topic model and its supervised extension. In Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 2210\u20132216. AAAI Press, 2015. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9303.",
            "url": "http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9303",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Ziqiang%20Li%2C%20Sujian%20Liu%2C%20Yang%20Li%2C%20Wenjie%20A%20novel%20neural%20topic%20model%20and%20its%20supervised%20extension%202015"
        },
        {
            "id": "Card_et+al_2017_a",
            "entry": "Dallas Card, Chenhao Tan, and Noah A. Smith. A neural framework for generalized topic models. CoRR, abs/1705.09296, 2017. URL http://arxiv.org/abs/1705.09296.",
            "url": "http://arxiv.org/abs/1705.09296",
            "arxiv_url": "https://arxiv.org/pdf/1705.09296"
        },
        {
            "id": "Chang_et+al_2009_a",
            "entry": "Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems, pp. 288\u2013296. Curran Associates, Inc., 2009. URL http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.",
            "url": "http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Jonathan%20Boyd-Graber%2C%20Jordan%20L.%20Gerrish%2C%20Sean%20Wang%2C%20Chong%20Reading%20tea%20leaves%3A%20How%20humans%20interpret%20topic%20models%202009"
        },
        {
            "id": "Das_et+al_2015_a",
            "entry": "Rajarshi Das, Manzil Zaheer, and Chris Dyer. Gaussian lda for topic models with word embeddings. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 795\u2013804. Association for Computational Linguistics, 2015. doi: 10.3115/v1/P15-1077. URL http://aclweb.org/anthology/P15-1077.",
            "crossref": "https://dx.doi.org/10.3115/v1/P15-1077",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.3115/v1/P15-1077"
        },
        {
            "id": "Dieng_et+al_2016_a",
            "entry": "Adji B. Dieng, Chong Wang, Jianfeng Gao, and John William Paisley. Topicrnn: A recurrent neural network with long-range semantic dependency. CoRR, abs/1611.01702, 2016. URL http://arxiv.org/abs/1611.01702.",
            "url": "http://arxiv.org/abs/1611.01702",
            "arxiv_url": "https://arxiv.org/pdf/1611.01702"
        },
        {
            "id": "Gupta_et+al_2018_a",
            "entry": "Pankaj Gupta, Subburam Rajaram, Hinrich Schutze, and Bernt Andrassy. Deep temporal-recurrentreplicated-softmax for topical trends over time. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1079\u20131089. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-1098. URL http://aclweb.org/anthology/ N18-1098.",
            "crossref": "https://dx.doi.org/10.18653/v1/N18-1098",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/N18-1098"
        },
        {
            "id": "Gupta_et+al_2019_a",
            "entry": "Pankaj Gupta, Yatin Chaudhary, Florian Buettner, and Hinrich Schutze. Document informed neural autoregressive topic models with distributional prior. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, 2019. URL https://arxiv.org/abs/1809.06709.",
            "url": "https://arxiv.org/abs/1809.06709",
            "arxiv_url": "https://arxiv.org/pdf/1809.06709"
        },
        {
            "id": "Hinton_et+al_2006_a",
            "entry": "Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527\u20131554, 2006. doi: 10.1162/neco.2006.18.7.1527. URL https://doi.org/10.1162/neco.2006.18.7.1527.",
            "crossref": "https://dx.doi.org/10.1162/neco.2006.18.7.1527",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/neco.2006.18.7.1527"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.",
            "crossref": "https://dx.doi.org/10.1162/neco.1997.9.8.1735",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1162/neco.1997.9.8.1735"
        },
        {
            "id": "Larochelle_2008_a",
            "entry": "Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted boltzmann machines. In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, volume 307 of ACM International Conference Proceeding Series, pp. 536\u2013543. ACM, 2008. doi: 10.1145/1390156.1390224. URL https://doi.org/10.1145/1390156.1390224.",
            "crossref": "https://dx.doi.org/10.1145/1390156.1390224",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1390156.1390224"
        },
        {
            "id": "Larochelle_2012_a",
            "entry": "Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems, pp. 2717\u20132725, 2012. URL http://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model.",
            "url": "http://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Lauly%2C%20Stanislas%20A%20neural%20autoregressive%20topic%20model%202012"
        },
        {
            "id": "Larochelle_2011_a",
            "entry": "Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS, volume 15 of JMLR Proceedings, pp. 29\u201337. JMLR.org, 2011. URL http://www.jmlr.org/proceedings/papers/v15/larochelle11a/larochelle11a.pdf.",
            "url": "http://www.jmlr.org/proceedings/papers/v15/larochelle11a/larochelle11a.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Murray%2C%20Iain%20The%20neural%20autoregressive%20distribution%20estimator%202011"
        },
        {
            "id": "Lau_et+al_2017_a",
            "entry": "Jey Han Lau, Timothy Baldwin, and Trevor Cohn. Topically driven neural language model. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 355\u2013365. Association for Computational Linguistics, 2017. doi: 10.18653/ v1/P17-1033. URL http://aclweb.org/anthology/P17-1033.",
            "crossref": "https://dx.doi.org/10.18653/v1/P17-1033",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/P17-1033"
        },
        {
            "id": "Lauly_et+al_2017_a",
            "entry": "Stanislas Lauly, Yin Zheng, Alexandre Allauzen, and Hugo Larochelle. Document neural autoregressive distribution estimation. Journal of Machine Learning Research, 18(113):1\u201324, 2017. URL http://jmlr.org/papers/v18/16-017.html.",
            "url": "http://jmlr.org/papers/v18/16-017.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lauly%2C%20Stanislas%20Zheng%2C%20Yin%20Allauzen%2C%20Alexandre%20Larochelle%2C%20Hugo%20Document%20neural%20autoregressive%20distribution%20estimation%202017"
        },
        {
            "id": "Le_2014_a",
            "entry": "Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceedings of the 31th International Conference on Machine Learning, ICML, volume 32 of JMLR Workshop and Conference Proceedings, pp. 1188\u20131196. JMLR.org, 2014. URL http://jmlr.org/proceedings/papers/v32/le14.html.",
            "url": "http://jmlr.org/proceedings/papers/v32/le14.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Mikolov%2C%20Tomas%20Distributed%20representations%20of%20sentences%20and%20documents%202014"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association (INTERSPEECH), pp. 1045\u20131048. ISCA, 2010. URL http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html.",
            "url": "http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Proceeding of the International Conference on Learning Representations Workshop Track. ICLR, 2013. URL http://arxiv.org/abs/1301.3781.",
            "url": "http://arxiv.org/abs/1301.3781",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "Newman_et+al_2009_a",
            "entry": "David Newman, Sarvnaz Karimi, and Lawrence Cavedon. External evaluation of topic models. In Proceedings of the 14th Australasian Document Computing Symposium, 2009. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.529.7854.",
            "url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.529.7854",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newman%2C%20David%20Karimi%2C%20Sarvnaz%20Cavedon%2C%20Lawrence%20External%20evaluation%20of%20topic%20models%202009"
        },
        {
            "id": "Nguyen_et+al_2015_a",
            "entry": "Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. Improving topic models with latent feature word representations. Transactions of the Association for Computational Linguistics, 3:299\u2013313, 2015. URL http://aclweb.org/anthology/Q15-1022.",
            "url": "http://aclweb.org/anthology/Q15-1022",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Dat%20Quoc%20Billingsley%2C%20Richard%20Du%2C%20Lan%20Johnson%2C%20Mark%20Improving%20topic%20models%20with%20latent%20feature%20word%20representations%202015"
        },
        {
            "id": "Nigam_et+al_2000_a",
            "entry": "Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom M. Mitchell. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103\u2013134, 2000. doi: 10.1023/A:1007692713085. URL https://doi.org/10.1023/A:1007692713085.",
            "crossref": "https://dx.doi.org/10.1023/A:1007692713085",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1023/A%3A1007692713085"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1532\u20131543, 2014. URL http://aclweb.org/anthology/D/D14/D14-1162.pdf.",
            "url": "http://aclweb.org/anthology/D/D14/D14-1162.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227\u20132237. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1202.",
            "url": "http://aclweb.org/anthology/N18-1202",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Petterson_et+al_2010_a",
            "entry": "James Petterson, Alexander J. Smola, Tiberio S. Caetano, Wray L. Buntine, and Shravan M. Narayanamurthy. Word features for latent dirichlet allocation. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems, pp. 1921\u20131929. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/4094-word-features-for-latent-dirichlet-allocation.",
            "url": "http://papers.nips.cc/paper/4094-word-features-for-latent-dirichlet-allocation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petterson%2C%20James%20Smola%2C%20Alexander%20J.%20Caetano%2C%20Tiberio%20S.%20Buntine%2C%20Wray%20L.%20Word%20features%20for%20latent%20dirichlet%20allocation%202010"
        },
        {
            "id": "Roder_et+al_2015_a",
            "entry": "Michael Roder, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence measures. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM, pp. 399\u2013408. Association for Computing Machinery, 2015. doi: 10.1145/2684822.2685324. URL https://doi.org/10.1145/2684822.2685324.",
            "crossref": "https://dx.doi.org/10.1145/2684822.2685324",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2684822.2685324"
        },
        {
            "id": "Sahami_2006_a",
            "entry": "Mehran Sahami and Timothy D. Heilman. A web-based kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, WWW, pp. 377\u2013386. Association for Computing Machinery, 2006. doi: 10.1145/1135777. 1135834. URL https://doi.org/10.1145/1135777.1135834.",
            "crossref": "https://dx.doi.org/10.1145/1135777.1135834",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1135777.1135834"
        },
        {
            "id": "Sak_et+al_2014_a",
            "entry": "Hasim Sak, Andrew W. Senior, and Francoise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In 15th Annual Conference of the International Speech Communication Association (INTERSPEECH 2014): Celebrating the Diversity of Spoken Languages, pp. 338\u2013342. International Speech Communication Association, 2014. URL http://www.isca-speech.org/archive/interspeech_2014/i14_0338.html.",
            "url": "http://www.isca-speech.org/archive/interspeech_2014/i14_0338.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sak%2C%20Hasim%20Senior%2C%20Andrew%20W.%20Beaufays%2C%20Francoise%20Long%20short-term%20memory%20recurrent%20neural%20network%20architectures%20for%20large%20scale%20acoustic%20modeling%202014"
        },
        {
            "id": "Curran_2009_a",
            "entry": "Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curran%20Associates%20Inc%202009"
        },
        {
            "id": "URL_0000_a",
            "entry": "URL http://papers.nips.cc/paper/",
            "url": "http://papers.nips.cc/paper/"
        },
        {
            "id": "Srivastava_2017_a",
            "entry": "Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. In 5th International Conference on Learning Representations, ICLR, 2017. URL https://arxiv.org/pdf/1703.01488.pdf.",
            "url": "https://arxiv.org/pdf/1703.01488.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1703.01488"
        },
        {
            "id": "Wallach_2006_a",
            "entry": "Hanna M. Wallach. Topic modeling: beyond bag-of-words. In Proceedings of the Twenty-Third International Conference Machine Learning, (ICML 2006), volume 148 of ACM International Conference Proceeding Series, pp. 977\u2013984. Association for Computing Machinery, 2006. doi: 10.1145/1143844.1143967. URL https://doi.org/10.1145/1143844.1143967.",
            "crossref": "https://dx.doi.org/10.1145/1143844.1143967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1143844.1143967"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and Lawrence Carin. Topic compositional neural language model. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, volume 84, pp. 356\u2013365. PMLR, 2018. URL http://proceedings.mlr.press/v84/wang18a.html.",
            "url": "http://proceedings.mlr.press/v84/wang18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wenlin%20Gan%2C%20Zhe%20Wang%2C%20Wenqi%20Shen%2C%20Dinghan%20Topic%20compositional%20neural%20language%20model%202018"
        },
        {
            "id": "Wang_et+al_2007_a",
            "entry": "Xuerui Wang, Andrew McCallum, and Xing Wei. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007), October 28-31, 2007, Omaha, Nebraska, USA, pp. 697\u2013702. IEEE Computer Society, 2007. doi: 10.1109/ICDM.2007.86. URL https://doi.org/10.1109/ ICDM.2007.86.",
            "crossref": "https://dx.doi.org/10.1109/ICDM.2007.86",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICDM.2007.86"
        },
        {
            "id": "Zheng_et+al_2016_a",
            "entry": "Yin Zheng, Yu-Jin Zhang, and Hugo Larochelle. A deep and autoregressive approach for topic modeling of multimodal data. IEEE transactions on pattern analysis and machine intelligence, 38(6):1056\u20131069, 2016. doi: 10.1109/TPAMI.2015.2476802. URL https://doi.org/10.1109/TPAMI.2015.2476802.",
            "crossref": "https://dx.doi.org/10.1109/TPAMI.2015.2476802"
        },
        {
            "id": "11",
            "entry": "(11) RCV1V2 (Reuters): www.ai.mit.edu/projects/jmlr/papers/ volume5/lewis04a/lyrl2004_rcv1v2_README.htm (12) 20NSsmall: We sample 20 document for training from each class of the 20NS dataset. For validation and test, 10 document for each class.",
            "url": "http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm"
        },
        {
            "id": "14",
            "entry": "(14) Sixxx Requirement OBjects (SiROBs): a collection of paragraphs extracted from industrial tender documents (our industrial corpus).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sixxx%20Requirement%20OBjects%20SiROBs%20a%20collection%20of%20paragraphs%20extracted%20from%20industrial%20tender%20documents%20our%20industrial%20corpus"
        }
    ]
}
