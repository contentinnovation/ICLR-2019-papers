{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "WOULDA, COULDA, SHOULDA: COUNTERFACTUALLY-GUIDED POLICY SEARCH",
        "author": "Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, Nicolas Heess DeepMind lbuesing@google.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJG0voC9YQ"
        },
        "abstract": "Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for modelbased policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods."
    },
    "keywords": [
        {
            "term": "random variables",
            "url": "https://en.wikipedia.org/wiki/random_variables"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "grid world",
            "url": "https://en.wikipedia.org/wiki/grid_world"
        },
        {
            "term": "hard problem",
            "url": "https://en.wikipedia.org/wiki/hard_problem"
        }
    ],
    "abbreviations": {
        "CF-GPS": "Counterfactually-Guided Policy Search",
        "RL": "reinforcement learning",
        "RVs": "random variables",
        "SCM": "structural causal model",
        "CFI": "Counterfactual inference",
        "PE": "policy evaluation",
        "IS": "Importance Sampling",
        "MB-PE": "model-based policy evaluation",
        "CF-PE": "counterfactual reasoning for off-policy evaluation",
        "MB-PS": "model-based policy search",
        "HER": "Hindsight Experience Replay",
        "GPS": "Guided Policy Search",
        "LQR": "linear quadratic regulator",
        "SVG": "Stochastic Value Gradient"
    },
    "highlights": [
        "Imagine that a month ago Alice had two job offers from companies a1 and a2",
        "Motivated by the introductory example, we propose the Counterfactually-Guided Policy Search (CF-Guided Policy Search) algorithm: Instead of relying on data synthesized from scratch by a model, policies are trained on model predictions of alternate outcomes of past experience from the true environment under counterfactual actions, i.e. actions that had not been taken, while everything else remaining the same (<a class=\"ref-link\" id=\"cPearl_2009_a\" href=\"#rPearl_2009_a\">Pearl, 2009</a>)",
        "We evaluate Counterfactually-Guided Policy Search on the PO-SOKOBAN environment, using a modified distributed actor-learner architecture based on <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al (2018</a>): Multiple actors collect real data hT by running the behavior policy \u03bc in the true environment p",
        "Simulating plausible synthetic experience de novo is a hard problem for many environments, often resulting in biases for model-based reinforcement learning algorithms",
        "The first assumption is that off-policy experience is available at all",
        "Future research is needed to investigate if learning a sufficiently strong structural causal model is possible without privileged information for interesting reinforcement learning domains"
    ],
    "key_statements": [
        "Imagine that a month ago Alice had two job offers from companies a1 and a2",
        "Motivated by the introductory example, we propose the Counterfactually-Guided Policy Search (CF-Guided Policy Search) algorithm: Instead of relying on data synthesized from scratch by a model, policies are trained on model predictions of alternate outcomes of past experience from the true environment under counterfactual actions, i.e. actions that had not been taken, while everything else remaining the same (<a class=\"ref-link\" id=\"cPearl_2009_a\" href=\"#rPearl_2009_a\">Pearl, 2009</a>)",
        "We show empirically in a conceptually simple setting, where unknown initial states are inferred in hindsight and re-used to evalute to counterfactual actions, that this can mitigate model mismatch",
        "We show that two previously proposed classes of reinforcement learning algorithms, namely Guided Policy Search (<a class=\"ref-link\" id=\"cLevine_2013_a\" href=\"#rLevine_2013_a\">Levine & Koltun, 2013</a>) and Stochastic Value Gradient methods (<a class=\"ref-link\" id=\"cHeess_et+al_2015_a\" href=\"#rHeess_et+al_2015_a\">Heess et al, 2015</a>), can be interpreted as counterfactual methods, opening up possible generalizations",
        "The policy is parameterized as a deep, recurrent neural network consisting of a 3-layer deep convolutional LSTM (Xingjian et al, 2015) with 32 channels per layer and kernel size of 3",
        "In order to obtain an structural causal model of the environment, for the sake of simplicity, we assume that the ground-truth transition, observation and reward kernels are given",
        "The model is shown in fig. 3 in the appendix and additional details are given in appendix C",
        "The policy evaluation results are shown in fig.\n2",
        "The policy \u03c0 in Counterfactually-Guided Policy Search is optimized on rollouts \u03c4 i that are grounded in data hTi by sampling them from the counterfactual distribution p\u03bb|hTi instead of the prior p\u03bb",
        "We evaluate Counterfactually-Guided Policy Search on the PO-SOKOBAN environment, using a modified distributed actor-learner architecture based on <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al (2018</a>): Multiple actors collect real data hT by running the behavior policy \u03bc in the true environment p",
        "The fact that Guided Policy Search is a successful algorithm in practice shows that the \u2018grounding\u2019 of model-based search / rollouts in real, off-policy data afforded by counterfactual reasoning massively improves the naive, \u2018prior sample\u2019-based model-based policy search algorithm",
        "Simulating plausible synthetic experience de novo is a hard problem for many environments, often resulting in biases for model-based reinforcement learning algorithms",
        "The first assumption is that off-policy experience is available at all",
        "We assumed that there are no additional hidden confounders in the environment and that the main challenge in modelling the environment is capturing the distribution of the noise sources p(U ), whereas we assumed that the transition and reward kernels given the noise is easy to model",
        "Future research is needed to investigate if learning a sufficiently strong structural causal model is possible without privileged information for interesting reinforcement learning domains"
    ],
    "summary": [
        "Imagine that a month ago Alice had two job offers from companies a1 and a2.",
        "Intuition Returning to Alice\u2019s job example from the introduction, we give some intuition for counterfactual inference in SCMs. Given the concrete outcome o, under observed context uc and having joined company a = a1, Alice can try to infer the underlying scenario uo \u223c p that she experiences; this includes factors such as work conditions etc.",
        "Given data D from p\u03bc, our discussion of counterfactual inference in SCMs suggests the following alternative strategy: Assuming no model mismatch, i.e. p\u03bc = p\u03bc, we can regard the task of off-policy evaluation of \u03c0 as a counterfactual query with data hTi , intervention I(\u03bc \u2192 \u03c0) and query variable G.",
        "Based on our discussion of counterfactual policy evaluation, it is straightforward to generalize the MB-PS described above by anchoring the rollouts \u03c4 i under the model p\u03bb in off-policy data D: Instead of sampling \u03c4 i directly from the prior p\u03bb, we draw them from counterfactual distribution p\u03bb|hTi with data hTi \u223c D from the replay buffer, i.e. instead of sampling the scenarios U from the prior we infer them from the given data.",
        "The policy \u03c0 in CF-GPS is optimized on rollouts \u03c4 i that are grounded in data hTi by sampling them from the counterfactual distribution p\u03bb|hTi instead of the prior p\u03bb.",
        "We evaluate CF-GPS on the PO-SOKOBAN environment, using a modified distributed actor-learner architecture based on <a class=\"ref-link\" id=\"cEspeholt_et+al_2018_a\" href=\"#rEspeholt_et+al_2018_a\">Espeholt et al (2018</a>): Multiple actors collect real data hT by running the behavior policy \u03bc in the true environment p.",
        "As expected from the policy evaluation experiments, initial states sampled from the models for GPS and MB-PS are often not solvable, yielding inferior training data for the policy \u03c0.",
        "Consider CF-GPS in the fully-observed MDP setting where Ot = St. assume that the SCM M is structured as follows: Let St+1 = fs(St, At, Ust) be a linear function in (St, At) with coefficients given by Ust. Further, assume an i.i.d. Gaussian mixture model on Ust for all t.",
        "The fact that GPS is a successful algorithm in practice shows that the \u2018grounding\u2019 of model-based search / rollouts in real, off-policy data afforded by counterfactual reasoning massively improves the naive, \u2018prior sample\u2019-based MB-PS algorithm.",
        "We can trust the transition and reward kernels of the model, we can substantially improve model-based RL methods by counterfactual reasoning on off-policy data, as demonstrated in our experiments and by the success of Guided Policy Search and Stochastic Value Gradient methods."
    ],
    "headline": "We propose the Counterfactually-Guided Policy Search algorithm for learning policies in POMDPs from off-policy experience",
    "reference_links": [
        {
            "id": "Abbeel_et+al_2006_a",
            "entry": "Pieter Abbeel, Morgan Quigley, and Andrew Y Ng. Using inaccurate models in reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pp. 1\u20138. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20Pieter%20Quigley%2C%20Morgan%20Ng%2C%20Andrew%20Y.%20Using%20inaccurate%20models%20in%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20Pieter%20Quigley%2C%20Morgan%20Ng%2C%20Andrew%20Y.%20Using%20inaccurate%20models%20in%20reinforcement%20learning%202006"
        },
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Atan_et+al_2016_a",
            "entry": "Onur Atan, William R Zame, Qiaojun Feng, and Mihaela van der Schaar. Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features. arXiv preprint arXiv:1612.08082, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08082"
        },
        {
            "id": "Balke_1994_a",
            "entry": "Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods, bounds and applications. In Proceedings of the Tenth international conference on Uncertainty in artificial intelligence, pp. 46\u201354. Morgan Kaufmann Publishers Inc., 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balke%2C%20Alexander%20Pearl%2C%20Judea%20Counterfactual%20probabilities%3A%20Computational%20methods%2C%20bounds%20and%20applications%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balke%2C%20Alexander%20Pearl%2C%20Judea%20Counterfactual%20probabilities%3A%20Computational%20methods%2C%20bounds%20and%20applications%201994"
        },
        {
            "id": "Bottou_2013_a",
            "entry": "Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. The Journal of Machine Learning Research, 14(1):3207\u20133260, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leon%20Bottou%20Jonas%20Peters%20Joaquin%20QuinoneroCandela%20Denis%20X%20Charles%20D%20Max%20Chickering%20Elon%20Portugaly%20Dipankar%20Ray%20Patrice%20Simard%20and%20Ed%20Snelson%20Counterfactual%20reasoning%20and%20learning%20systems%20The%20example%20of%20computational%20advertising%20The%20Journal%20of%20Machine%20Learning%20Research%2014132073260%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leon%20Bottou%20Jonas%20Peters%20Joaquin%20QuinoneroCandela%20Denis%20X%20Charles%20D%20Max%20Chickering%20Elon%20Portugaly%20Dipankar%20Ray%20Patrice%20Simard%20and%20Ed%20Snelson%20Counterfactual%20reasoning%20and%20learning%20systems%20The%20example%20of%20computational%20advertising%20The%20Journal%20of%20Machine%20Learning%20Research%2014132073260%202013"
        },
        {
            "id": "Browne_et+al_2012_a",
            "entry": "Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of Monte Carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Browne%2C%20Cameron%20B.%20Powley%2C%20Edward%20Whitehouse%2C%20Daniel%20Lucas%2C%20Simon%20M.%20A%20survey%20of%20Monte%20Carlo%20tree%20search%20methods%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Browne%2C%20Cameron%20B.%20Powley%2C%20Edward%20Whitehouse%2C%20Daniel%20Lucas%2C%20Simon%20M.%20A%20survey%20of%20Monte%20Carlo%20tree%20search%20methods%202012"
        },
        {
            "id": "Espeholt_et+al_2018_a",
            "entry": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable distributed Deep-RL with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "Gregor_et+al_2015_a",
            "entry": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.04623"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pp. 3549\u20133557, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems, pp. 2492\u20132501, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Zhaohan%20Thomas%2C%20Philip%20S.%20Brunskill%2C%20Emma%20Using%20options%20and%20covariance%20testing%20for%20long%20horizon%20off-policy%20policy%20evaluation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Zhaohan%20Thomas%2C%20Philip%20S.%20Brunskill%2C%20Emma%20Using%20options%20and%20covariance%20testing%20for%20long%20horizon%20off-policy%20policy%20evaluation%202017"
        },
        {
            "id": "Heess_et+al_2015_a",
            "entry": "Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944\u20132952, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Wayne%2C%20Gregory%20Silver%2C%20David%20Lillicrap%2C%20Tim%20Learning%20continuous%20control%20policies%20by%20stochastic%20value%20gradients%202015"
        },
        {
            "id": "Jiang_2015_a",
            "entry": "Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. arXiv preprint arXiv:1511.03722, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03722"
        },
        {
            "id": "Jiang_et+al_2015_b",
            "entry": "Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 1181\u20131189. International Foundation for Autonomous Agents and Multiagent Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Nan%20Kulesza%2C%20Alex%20Singh%2C%20Satinder%20Lewis%2C%20Richard%20The%20dependence%20of%20effective%20planning%20horizon%20on%20model%20accuracy%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Nan%20Kulesza%2C%20Alex%20Singh%2C%20Satinder%20Lewis%2C%20Richard%20The%20dependence%20of%20effective%20planning%20horizon%20on%20model%20accuracy%202015"
        },
        {
            "id": "Johansson_et+al_2016_a",
            "entry": "Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In International Conference on Machine Learning, pp. 3020\u20133029, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johansson%2C%20Fredrik%20Shalit%2C%20Uri%20Sontag%2C%20David%20Learning%20representations%20for%20counterfactual%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johansson%2C%20Fredrik%20Shalit%2C%20Uri%20Sontag%2C%20David%20Learning%20representations%20for%20counterfactual%20inference%202016"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Levine_2014_a",
            "entry": "Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pp. 1071\u20131079, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Learning%20neural%20network%20policies%20with%20guided%20policy%20search%20under%20unknown%20dynamics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Learning%20neural%20network%20policies%20with%20guided%20policy%20search%20under%20unknown%20dynamics%202014"
        },
        {
            "id": "Levine_2013_a",
            "entry": "Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pp. 1\u20139, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Lihong Li, Shunbao Chen, Jim Kleban, and Ankur Gupta. Counterfactual estimation and optimization of click metrics in search engines: A case study. In Proceedings of the 24th International Conference on World Wide Web, pp. 929\u2013934. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chen%2C%20Shunbao%20Kleban%2C%20Jim%20Gupta%2C%20Ankur%20Counterfactual%20estimation%20and%20optimization%20of%20click%20metrics%20in%20search%20engines%3A%20A%20case%20study%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chen%2C%20Shunbao%20Kleban%2C%20Jim%20Gupta%2C%20Ankur%20Counterfactual%20estimation%20and%20optimization%20of%20click%20metrics%20in%20search%20engines%3A%20A%20case%20study%202015"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo Faisal, Finale DoshiVelez, and Emma Brunskill. Representation balancing mdps for off-policy policy evaluation. arXiv preprint arXiv:1805.09044, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09044"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Nedelec_2017_a",
            "entry": "Thomas Nedelec, Nicolas Le Roux, and Vianney Perchet. A comparative study of counterfactual estimators. arXiv preprint arXiv:1704.00773, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00773"
        },
        {
            "id": "Pearl_2009_a",
            "entry": "Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, New York, NY, USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20Judea%20Causality%3A%20Models%2C%20Reasoning%20and%20Inference%202009"
        },
        {
            "id": "Pearl_2018_a",
            "entry": "Judea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic Books, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20Judea%20Mackenzie%2C%20Dana%20The%20Book%20of%20Why%3A%20The%20New%20Science%20of%20Cause%20and%20Effect%202018"
        },
        {
            "id": "Peters_et+al_2017_a",
            "entry": "J. Peters, D. Janzing, and B. Scholkopf. Elements of Causal Inference - Foundations and Learning Algorithms. Adaptive Computation and Machine Learning Series. The MIT Press, Cambridge, MA, USA, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20J.%20Janzing%2C%20D.%20Scholkopf%2C%20B.%20Elements%20of%20Causal%20Inference%20-%20Foundations%20and%20Learning%20Algorithms.%20Adaptive%20Computation%20and%20Machine%20Learning%20Series%202017"
        },
        {
            "id": "Precup_2000_a",
            "entry": "Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Precup%2C%20Doina%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Precup%2C%20Doina%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation%202000"
        },
        {
            "id": "Racaniere_et+al_2017_a",
            "entry": "Sebastien Racaniere, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5690\u20135701, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racaniere%2C%20Sebastien%20Weber%2C%20Theophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racaniere%2C%20Sebastien%20Weber%2C%20Theophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "Roese_1997_a",
            "entry": "Neal J Roese. Counterfactual thinking. Psychological bulletin, 121(1):133, 1997. Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pp. 814\u2013823, 2015. Erik Talvitie. Model Regularization for Stable Sample Rollouts. In UAI, pp. 780\u2013789, 2014. Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pp. 2139\u20132148, 2016. Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roese%2C%20Neal%20J.%20Counterfactual%20thinking%201997-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roese%2C%20Neal%20J.%20Counterfactual%20thinking%201997-03"
        },
        {
            "id": "26th_2009_a",
            "entry": "26th annual international conference on machine learning, pp. 1049\u20131056. ACM, 2009. Sewall Wright. The relative importance of heredity and environment in determining the piebald pattern of guinea-pigs. Proceedings of the National Academy of Sciences, 6(6):320\u2013332, 1920. Shi Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=th%20annual%20international%20conference%20on%20machine%20learning%20pp%2010491056%20ACM%202009%20Sewall%20Wright%20The%20relative%20importance%20of%20heredity%20and%20environment%20in%20determining%20the%20piebald%20pattern%20of%20guineapigs%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences%2066320332%201920%20Shi%20Xingjian%20Zhourong%20Chen%20Hao%20Wang%20DitYan%20Yeung%20WaiKin%20Wong%20and%20Wangchun%20Woo",
            "oa_query": "https://api.scholarcy.com/oa_version?query=th%20annual%20international%20conference%20on%20machine%20learning%20pp%2010491056%20ACM%202009%20Sewall%20Wright%20The%20relative%20importance%20of%20heredity%20and%20environment%20in%20determining%20the%20piebald%20pattern%20of%20guineapigs%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences%2066320332%201920%20Shi%20Xingjian%20Zhourong%20Chen%20Hao%20Wang%20DitYan%20Yeung%20WaiKin%20Wong%20and%20Wangchun%20Woo"
        },
        {
            "id": "Convolutional_2015_a",
            "entry": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pp. 802\u2013810, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Convolutional%20LSTM%20network%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20802810%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Convolutional%20LSTM%20network%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20802810%202015"
        }
    ]
}
