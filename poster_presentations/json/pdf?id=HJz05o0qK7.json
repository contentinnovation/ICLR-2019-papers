{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MEASURING COMPOSITIONALITY IN REPRESENTATION LEARNING",
        "author": "Jacob Andreas Computer Science Division University of California, Berkeley jda@cs.berkeley.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJz05o0qK7"
        },
        "journal": "Fodor",
        "abstract": "Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs\u2019 learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "semantics",
            "url": "https://en.wikipedia.org/wiki/semantics"
        },
        {
            "term": "representation learning",
            "url": "https://en.wikipedia.org/wiki/representation_learning"
        }
    ],
    "abbreviations": {
        "TRE": "Tree Reconstruction Error"
    },
    "highlights": [
        "The success of modern representation learning techniques has been accompanied by an interest \u27e8\u27e8dark,blue\u27e9,square\u27e9<br/><br/>aaxxx speaker model listener model in understanding the structure of learned repre\u27e8green,square\u27e9<br/><br/>sentations",
        "The success of modern representation learning techniques has been accompanied by an interest \u27e8\u27e8dark,blue\u27e9,square\u27e9",
        "We focus on an oracle setting where the compositional structure of model inputs is known, and where the only question is whether this structure is reflected in model outputs",
        "Our hypothesis is that bigrams whose representations have low Tree Reconstruction Error are those whose meaning is essentially compositional, and well-explained by the constituent words, while bigrams with large reconstruction error will correspond to non-compositional multi-word expressions (<a class=\"ref-link\" id=\"cNattinger_1992_a\" href=\"#rNattinger_1992_a\">Nattinger & DeCarrico, 1992</a>). This task is already well-studied in the natural language processing literature (<a class=\"ref-link\" id=\"cSalehi_et+al_2015_a\" href=\"#rSalehi_et+al_2015_a\">Salehi et al, 2015</a>), and the analysis we present differs only in the use of Tree Reconstruction Error to search for atomic representations rather than taking them to be given by pre-trained word representations",
        "Tree Reconstruction Error is significantly correlated with generalization error",
        "We have introduced a new evaluation method called Tree Reconstruction Error for generating graded judgments about compositional structure in representation learning problems where the structure of the observations is understood"
    ],
    "key_statements": [
        "The success of modern representation learning techniques has been accompanied by an interest \u27e8\u27e8dark,blue\u27e9,square\u27e9<br/><br/>aaxxx speaker model listener model in understanding the structure of learned repre\u27e8green,square\u27e9<br/><br/>sentations",
        "The success of modern representation learning techniques has been accompanied by an interest \u27e8\u27e8dark,blue\u27e9,square\u27e9",
        "We focus on an oracle setting where the compositional structure of model inputs is known, and where the only question is whether this structure is reflected in model outputs",
        "We propose an evaluation metric called Tree Reconstruction Error, which provides graded judgments of compositionality for a given set of pairs",
        "One view of the present work is as a demonstration that we can take existing NLP techniques for compositional representation learning, fit them to representations produced by other models, and view the resulting training loss as a measure of the compositionality of the representation system in question.\n3 EVALUATING COMPOSITIONALITY",
        "Derivations The technique we propose assumes we have prior knowledge about the compositional structure of inputs",
        "We explore the relationship between the information bottleneck and compositionality by comparing Tree Reconstruction Error(X ) to the mutual information I(\u03b8; x) between representations and inputs over the course of training",
        "Our hypothesis is that bigrams whose representations have low Tree Reconstruction Error are those whose meaning is essentially compositional, and well-explained by the constituent words, while bigrams with large reconstruction error will correspond to non-compositional multi-word expressions (<a class=\"ref-link\" id=\"cNattinger_1992_a\" href=\"#rNattinger_1992_a\">Nattinger & DeCarrico, 1992</a>). This task is already well-studied in the natural language processing literature (<a class=\"ref-link\" id=\"cSalehi_et+al_2015_a\" href=\"#rSalehi_et+al_2015_a\">Salehi et al, 2015</a>), and the analysis we present differs only in the use of Tree Reconstruction Error to search for atomic representations rather than taking them to be given by pre-trained word representations",
        "The composition function is again vector addition and distance is cosine distance. (Future work might explore learned composition functions as in e.g. <a class=\"ref-link\" id=\"cGrefenstette_et+al_2013_a\" href=\"#rGrefenstette_et+al_2013_a\">Grefenstette et al, 2013</a>, for future work.) We compare bigram-level judgments of compositionality computed by Tree Reconstruction Error with a dataset of human judgments about noun\u2013noun compounds (<a class=\"ref-link\" id=\"cReddy_et+al_2011_a\" href=\"#rReddy_et+al_2011_a\">Reddy et al, 2011</a>)",
        "Tree Reconstruction Error is significantly correlated with generalization error",
        "We have introduced a new evaluation method called Tree Reconstruction Error for generating graded judgments about compositional structure in representation learning problems where the structure of the observations is understood"
    ],
    "summary": [
        "The success of modern representation learning techniques has been accompanied by an interest \u27e8\u27e8dark,blue\u27e9,square\u27e9<br/><br/>aaxxx speaker model listener model in understanding the structure of learned repre\u27e8green,square\u27e9<br/><br/>sentations.",
        "The first contribution of this paper is a simple formal framework for measuring how well a collection of representations reflects an oracle compositional analysis of model inputs.",
        "A long line of work in natural language processing (<a class=\"ref-link\" id=\"cCoecke_et+al_2010_a\" href=\"#rCoecke_et+al_2010_a\">Coecke et al, 2010</a>; <a class=\"ref-link\" id=\"cBaroni_2010_a\" href=\"#rBaroni_2010_a\">Baroni & Zamparelli, 2010</a>; <a class=\"ref-link\" id=\"cClark_2012_a\" href=\"#rClark_2012_a\">Clark, 2012</a>; <a class=\"ref-link\" id=\"cFyshe_et+al_2015_a\" href=\"#rFyshe_et+al_2015_a\">Fyshe et al, 2015</a>) focuses on learning composition functions to produce distributed representations of phrases and sentences\u2014that is, for purposes of modeling rather than evaluation.",
        "One view of the present work is as a demonstration that we can take existing NLP techniques for compositional representation learning, fit them to representations produced by other models, and view the resulting training loss as a measure of the compositionality of the representation system in question.",
        "This framework proposes that learning in deep models consists of an error minimization phase followed by a compression phase, and that compression is characterized by a decrease in the mutual information between inputs and their computed representations.",
        "This task is already well-studied in the natural language processing literature (<a class=\"ref-link\" id=\"cSalehi_et+al_2015_a\" href=\"#rSalehi_et+al_2015_a\">Salehi et al, 2015</a>), and the analysis we present differs only in the use of TRE to search for atomic representations rather than taking them to be given by pre-trained word representations.",
        "(Future work might explore learned composition functions as in e.g. <a class=\"ref-link\" id=\"cGrefenstette_et+al_2013_a\" href=\"#rGrefenstette_et+al_2013_a\">Grefenstette et al, 2013</a>, for future work.) We compare bigram-level judgments of compositionality computed by TRE with a dataset of human judgments about noun\u2013noun compounds (<a class=\"ref-link\" id=\"cReddy_et+al_2011_a\" href=\"#rReddy_et+al_2011_a\">Reddy et al, 2011</a>).",
        "The section aims at providing a formal, rather than experimental, characterization of the relationship between TRE and another perspective on the analysis of representations with help from oracle derivations.",
        "We have introduced a new evaluation method called TRE for generating graded judgments about compositional structure in representation learning problems where the structure of the observations is understood.",
        "We have applied TRE-based analysis to four different problems in representation learning, relating compositionality to learning dynamics, linguistic compositionality, similarity and generalization.",
        "It is our hope that this line of research opens up two different kinds of new work: better understanding of existing machine learning models, by providing a new set of tools for understanding their representational capacity; and better understanding of problems, by better understanding the kinds of data distributions and loss functions that give rise to compositionalor non-compositional representations of observations."
    ],
    "headline": "We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives",
    "reference_links": [
        {
            "id": "Andreas_2017_a",
            "entry": "Jacob Andreas and Dan Klein. Analogs of linguistic structure in deep representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Analogs%20of%20linguistic%20structure%20in%20deep%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Analogs%20of%20linguistic%20structure%20in%20deep%20representations%202017"
        },
        {
            "id": "Artzi_et+al_2014_a",
            "entry": "Yoav Artzi, Dipanjan Das, and Slav Petrov. Learning compact lexicons for CCG semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1273\u20131283, Doha, Qatar, 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D14-1134.",
            "url": "http://www.aclweb.org/anthology/D14-1134",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Das%2C%20Dipanjan%20Petrov%2C%20Slav%20Learning%20compact%20lexicons%20for%20CCG%20semantic%20parsing%202014"
        },
        {
            "id": "Baroni_2010_a",
            "entry": "Marco Baroni and Roberto Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1183\u20131193, Cambridge, MA, USA, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baroni%2C%20Marco%20Zamparelli%2C%20Roberto%20Nouns%20are%20vectors%2C%20adjectives%20are%20matrices%3A%20Representing%20adjective-noun%20constructions%20in%20semantic%20space%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baroni%2C%20Marco%20Zamparelli%2C%20Roberto%20Nouns%20are%20vectors%2C%20adjectives%20are%20matrices%3A%20Representing%20adjective-noun%20constructions%20in%20semantic%20space%202010"
        },
        {
            "id": "Bille_2005_a",
            "entry": "Philip Bille. A survey on tree edit distance and related problems. Theoretical computer science, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bille%2C%20Philip%20A%20survey%20on%20tree%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bille%2C%20Philip%20A%20survey%20on%20tree%202005"
        },
        {
            "id": "Bogin_et+al_2018_a",
            "entry": "Ben Bogin, Mor Geva, and Jonathan Berant. Emergence of communication in an interactive world with consistent speakers, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogin%2C%20Ben%20Geva%2C%20Mor%20Berant%2C%20Jonathan%20Emergence%20of%20communication%20in%20an%20interactive%20world%20with%20consistent%20speakers%202018"
        },
        {
            "id": "Bojanowski_et+al_2017_a",
            "entry": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20word%20vectors%20with%20subword%20information%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20word%20vectors%20with%20subword%20information%202017"
        },
        {
            "id": "Brighton_2002_a",
            "entry": "Henry Brighton. Compositional syntax from cultural transmission. Artificial Life, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brighton%2C%20Henry%20Compositional%20syntax%20from%20cultural%20transmission%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brighton%2C%20Henry%20Compositional%20syntax%20from%20cultural%20transmission%202002"
        },
        {
            "id": "Brighton_2006_a",
            "entry": "Henry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence of topographic mappings. Artificial life, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brighton%2C%20Henry%20Kirby%2C%20Simon%20Understanding%20linguistic%20evolution%20by%20visualizing%20the%20emergence%20of%20topographic%20mappings%202006"
        },
        {
            "id": "Carnap_1937_a",
            "entry": "Rudolf Carnap. Logical syntax of language. 1937.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carnap%2C%20Rudolf%20Logical%20syntax%20of%20language%201937"
        },
        {
            "id": "Chen_2012_a",
            "entry": "David L Chen. Fast online lexicon learning for grounded language acquisition. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 430\u2013439, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20David%20L.%20Fast%20online%20lexicon%20learning%20for%20grounded%20language%20acquisition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20David%20L.%20Fast%20online%20lexicon%20learning%20for%20grounded%20language%20acquisition%202012"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In Proceedings of the Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20Bahdanau%2C%20Dzmitry%20Bengio%2C%20Yoshua%20On%20the%20properties%20of%20neural%20machine%20translation%3A%20Encoder-decoder%20approaches%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20Bahdanau%2C%20Dzmitry%20Bengio%2C%20Yoshua%20On%20the%20properties%20of%20neural%20machine%20translation%3A%20Encoder-decoder%20approaches%202014"
        },
        {
            "id": "Choi_et+al_2018_a",
            "entry": "Edward Choi, Angeliki Lazaridou, and Nando de Freitas. Compositional obverter communication learning from raw visual input. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Edward%20Lazaridou%2C%20Angeliki%20de%20Freitas%2C%20Nando%20Compositional%20obverter%20communication%20learning%20from%20raw%20visual%20input%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Edward%20Lazaridou%2C%20Angeliki%20de%20Freitas%2C%20Nando%20Compositional%20obverter%20communication%20learning%20from%20raw%20visual%20input%202018"
        },
        {
            "id": "Clark_2012_a",
            "entry": "Stephen Clark. Vector space models of lexical meaning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clark%2C%20Stephen%20Vector%20space%20models%20of%20lexical%20meaning%202012"
        },
        {
            "id": "Coecke_et+al_2010_a",
            "entry": "Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a compositional distributional model of meaning. arXiv preprint arXiv:1003.4394, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1003.4394"
        },
        {
            "id": "Deerwester_et+al_1990_a",
            "entry": "Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391\u2013407, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deerwester%2C%20Scott%20C.%20Dumais%2C%20Susan%20T.%20Landauer%2C%20Thomas%20K.%20Furnas%2C%20George%20W.%20Indexing%20by%20latent%20semantic%20analysis%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deerwester%2C%20Scott%20C.%20Dumais%2C%20Susan%20T.%20Landauer%2C%20Thomas%20K.%20Furnas%2C%20George%20W.%20Indexing%20by%20latent%20semantic%20analysis%201990"
        },
        {
            "id": "Dircks_1999_a",
            "entry": "Christopher Dircks and Scott Stoness. Effective lexicon change in the absence of population flux. Advances in Artificial Life, pp. 720\u2013724, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dircks%2C%20Christopher%20Stoness%2C%20Scott%20Effective%20lexicon%20change%20in%20the%20absence%20of%20population%20flux%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dircks%2C%20Christopher%20Stoness%2C%20Scott%20Effective%20lexicon%20change%20in%20the%20absence%20of%20population%20flux%201999"
        },
        {
            "id": "Dong_2016_a",
            "entry": "Li Dong and Mirella Lapata. Language to logical form with neural attention. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Li%20Lapata%2C%20Mirella%20Language%20to%20logical%20form%20with%20neural%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Li%20Lapata%2C%20Mirella%20Language%20to%20logical%20form%20with%20neural%20attention%202016"
        },
        {
            "id": "Enquist_1994_a",
            "entry": "Magnus Enquist and Anthony Arak. Symmetry, beauty and evolution. Nature, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Enquist%2C%20Magnus%20Arak%2C%20Anthony%20Symmetry%2C%20beauty%20and%20evolution%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Enquist%2C%20Magnus%20Arak%2C%20Anthony%20Symmetry%2C%20beauty%20and%20evolution%201994"
        },
        {
            "id": "Fodor_2002_a",
            "entry": "Jerry A Fodor and Ernest Lepore. The compositionality papers. Oxford University Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Lepore%2C%20Ernest%20The%20compositionality%20papers%202002"
        },
        {
            "id": "Fyshe_et+al_2015_a",
            "entry": "Alona Fyshe, Leila Wehbe, Partha P Talukdar, Brian Murphy, and Tom M Mitchell. A compositional and interpretable semantic space. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pp. 32\u201341, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fyshe%2C%20Alona%20Wehbe%2C%20Leila%20Talukdar%2C%20Partha%20P.%20Murphy%2C%20Brian%20A%20compositional%20and%20interpretable%20semantic%20space%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fyshe%2C%20Alona%20Wehbe%2C%20Leila%20Talukdar%2C%20Partha%20P.%20Murphy%2C%20Brian%20A%20compositional%20and%20interpretable%20semantic%20space%202015"
        },
        {
            "id": "Gatt_et+al_2007_a",
            "entry": "Albert Gatt, Ielka Van Der Sluis, and Kees Van Deemter. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pp. 49\u201356. Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatt%2C%20Albert%20Sluis%2C%20Ielka%20Van%20Der%20Deemter%2C%20Kees%20Van%20Evaluating%20algorithms%20for%20the%20generation%20of%20referring%20expressions%20using%20a%20balanced%20corpus%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatt%2C%20Albert%20Sluis%2C%20Ielka%20Van%20Der%20Deemter%2C%20Kees%20Van%20Evaluating%20algorithms%20for%20the%20generation%20of%20referring%20expressions%20using%20a%20balanced%20corpus%202007"
        },
        {
            "id": "Grefenstette_et+al_2013_a",
            "entry": "Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. Multi-step regression learning for compositional distributional semantics. Proceedings of the International Conference on Computational Semantics, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grefenstette%2C%20Edward%20Dinu%2C%20Georgiana%20Zhang%2C%20Yao-Zhong%20Sadrzadeh%2C%20Mehrnoosh%20Multi-step%20regression%20learning%20for%20compositional%20distributional%20semantics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grefenstette%2C%20Edward%20Dinu%2C%20Georgiana%20Zhang%2C%20Yao-Zhong%20Sadrzadeh%2C%20Mehrnoosh%20Multi-step%20regression%20learning%20for%20compositional%20distributional%20semantics%202013"
        },
        {
            "id": "Havrylov_2017_a",
            "entry": "Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: learning to communicate with sequences of symbols. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Kirby_1998_a",
            "entry": "Simon Kirby. Learning, bottlenecks and the evolution of recursive syntax. In Linguistic Evolution through Language Acquisition: Formal and Computational Models. Cambridge University Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirby%2C%20Simon%20Learning%2C%20bottlenecks%20and%20the%20evolution%20of%20recursive%20syntax.%20In%20Linguistic%20Evolution%20through%20Language%20Acquisition%3A%20Formal%20and%20Computational%20Models%201998"
        },
        {
            "id": "Klein_2004_a",
            "entry": "Dan Klein and Christopher D Manning. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klein%2C%20Dan%20Manning%2C%20Christopher%20D.%20Corpus-based%20induction%20of%20syntactic%20structure%3A%20Models%20of%20dependency%20and%20constituency%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klein%2C%20Dan%20Manning%2C%20Christopher%20D.%20Corpus-based%20induction%20of%20syntactic%20structure%3A%20Models%20of%20dependency%20and%20constituency%202004"
        },
        {
            "id": "Kottur_et+al_2017_a",
            "entry": "Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge\u2019naturally\u2019in multi-agent dialog. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20M.F.%20Lee%2C%20Stefan%20Batra%2C%20Dhruv%20Natural%20language%20does%20not%20emerge%E2%80%99naturally%E2%80%99in%20multi-agent%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20M.F.%20Lee%2C%20Stefan%20Batra%2C%20Dhruv%20Natural%20language%20does%20not%20emerge%E2%80%99naturally%E2%80%99in%20multi-agent%20dialog%202017"
        },
        {
            "id": "Lazaridou_et+al_2016_a",
            "entry": "Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. Towards multi-agent communication-based language learning. arXiv preprint arXiv:1605.07133, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07133"
        },
        {
            "id": "Lazaridou_et+al_2017_a",
            "entry": "Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%20and%20the%20emergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%20and%20the%20emergence%202017"
        },
        {
            "id": "Lazaridou_et+al_2018_a",
            "entry": "Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaridou%2C%20Angeliki%20Hermann%2C%20Karl%20Moritz%20Tuyls%2C%20Karl%20Clark%2C%20Stephen%20Emergence%20of%20linguistic%20communication%20from%20referential%20games%20with%20symbolic%20and%20pixel%20input%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaridou%2C%20Angeliki%20Hermann%2C%20Karl%20Moritz%20Tuyls%2C%20Karl%20Clark%2C%20Stephen%20Emergence%20of%20linguistic%20communication%20from%20referential%20games%20with%20symbolic%20and%20pixel%20input%202018"
        },
        {
            "id": "Lewis_1976_a",
            "entry": "David Lewis. General semantics. In Montague grammar. 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20David%20General%20semantics.%20In%20Montague%20grammar%201976"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mitchell_2008_a",
            "entry": "Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. Proceedings of the Human Language Technology Conference of the Association for Computational Linguistics, pp. 236\u2013244, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitchell%2C%20Jeff%20Lapata%2C%20Mirella%20Vector-based%20models%20of%20semantic%20composition%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mitchell%2C%20Jeff%20Lapata%2C%20Mirella%20Vector-based%20models%20of%20semantic%20composition%202008"
        },
        {
            "id": "Montague_1970_a",
            "entry": "Richard Montague. Universal grammar. 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montague%2C%20Richard%20Universal%20grammar%201970"
        },
        {
            "id": "Mordatch_2017_a",
            "entry": "Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04908"
        },
        {
            "id": "Nattinger_1992_a",
            "entry": "James R Nattinger and Jeanette S DeCarrico. Lexical phrases and language teaching. 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nattinger%2C%20James%20R.%20DeCarrico%2C%20Jeanette%20S.%20Lexical%20phrases%20and%20language%20teaching%201992"
        },
        {
            "id": "Parker_et+al_2011_a",
            "entry": "Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition. Technical report, Linguistic Data Consortium, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parker%2C%20Robert%20Graff%2C%20David%20Kong%2C%20Junbo%20Chen%2C%20Ke%20English%20gigaword%20fifth%20edition%202011"
        },
        {
            "id": "Reddy_et+al_2011_a",
            "entry": "Siva Reddy, Diana McCarthy, and Suresh Manandhar. An empirical study on compositionality in compound nouns. In Proceedings of the International Joint Conference on Natural Language Processing, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddy%2C%20Siva%20McCarthy%2C%20Diana%20Manandhar%2C%20Suresh%20An%20empirical%20study%20on%20compositionality%20in%20compound%20nouns%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddy%2C%20Siva%20McCarthy%2C%20Diana%20Manandhar%2C%20Suresh%20An%20empirical%20study%20on%20compositionality%20in%20compound%20nouns%202011"
        },
        {
            "id": "Salehi_et+al_2015_a",
            "entry": "Bahar Salehi, Paul Cook, and Timothy Baldwin. A word embedding approach to predicting the compositionality of multiword expressions. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salehi%2C%20Bahar%20Cook%2C%20Paul%20Baldwin%2C%20Timothy%20A%20word%20embedding%20approach%20to%20predicting%20the%20compositionality%20of%20multiword%20expressions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salehi%2C%20Bahar%20Cook%2C%20Paul%20Baldwin%2C%20Timothy%20A%20word%20embedding%20approach%20to%20predicting%20the%20compositionality%20of%20multiword%20expressions%202015"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In Proceedings of the International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Saxe_et+al_2018_a",
            "entry": "AM Saxe, Y Bansal, J Dapello, M Advani, A Kolchinsky, BD Tracey, and DD Cox. On the information bottleneck theory of deep learning. In Proceedings of the International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20information%20bottleneck%20theory%20of%20deep%20learning%202018"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning. Diplom Thesis, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Evolutionary%20principles%20in%20self-referential%20learning%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Evolutionary%20principles%20in%20self-referential%20learning%201987"
        },
        {
            "id": "Seo_et+al_2017_a",
            "entry": "Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. Visual reference resolution using attention memory for visual dialog. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Paul%20Hongsuck%20Lehrmann%2C%20Andreas%20Han%2C%20Bohyung%20Sigal%2C%20Leonid%20Visual%20reference%20resolution%20using%20attention%20memory%20for%20visual%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Paul%20Hongsuck%20Lehrmann%2C%20Andreas%20Han%2C%20Bohyung%20Sigal%2C%20Leonid%20Visual%20reference%20resolution%20using%20attention%20memory%20for%20visual%20dialog%202017"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Smolensky_1991_a",
            "entry": "Paul Smolensky. Connectionism, constituency, and the language of thought. In Meaning in Mind: Fodor and His Critics. Blackwell, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20Connectionism%2C%20constituency%2C%20and%20the%20language%20of%20thought.%20In%20Meaning%20in%20Mind%3A%20Fodor%20and%20His%20Critics%201991"
        },
        {
            "id": "Socher_et+al_2012_a",
            "entry": "Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Huval%2C%20Brody%20Manning%2C%20Christopher%20Ng%2C%20Andrew%20Semantic%20compositionality%20through%20recursive%20matrix-vector%20spaces%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Huval%2C%20Brody%20Manning%2C%20Christopher%20Ng%2C%20Andrew%20Semantic%20compositionality%20through%20recursive%20matrix-vector%20spaces%202012"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. Parsing with compositional vector grammars. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Bauer%2C%20John%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Parsing%20with%20compositional%20vector%20grammars%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Bauer%2C%20John%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Parsing%20with%20compositional%20vector%20grammars%202013"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In Information Theory Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20Naftali%20Zaslavsky%2C%20Noga%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "Williams_2010_a",
            "entry": "Published as a conference paper at ICLR 2019 Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 384\u2013394. Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2010. JFAK van Benthem and Alice ter Meulen. Handbook of logic and language. 1996. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. Luke S. Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pp. 658\u2013666, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Joseph%20Turian%20Lev%20Ratinov%20and%20Yoshua%20Bengio%20Word%20representations%20a%20simple%20and%20general%20method%20for%20semisupervised%20learning%20In%20Proceedings%20of%20the%2048th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20pp%20384394%20Proceedings%20of%20the%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%202010%20JFAK%20van%20Benthem%20and%20Alice%20ter%20Meulen%20Handbook%20of%20logic%20and%20language%201996%20Ronald%20J%20Williams%20Simple%20statistical%20gradientfollowing%20algorithms%20for%20connectionist%20reinforcement%20learning%20Machine%20learning%201992%20Luke%20S%20Zettlemoyer%20and%20Michael%20Collins%20Learning%20to%20map%20sentences%20to%20logical%20form%20Structured%20classification%20with%20probabilistic%20categorial%20grammars%20In%20Proceedings%20of%20the%20Conference%20on%20Uncertainty%20in%20Artificial%20Intelligence%20pp%20658666%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Joseph%20Turian%20Lev%20Ratinov%20and%20Yoshua%20Bengio%20Word%20representations%20a%20simple%20and%20general%20method%20for%20semisupervised%20learning%20In%20Proceedings%20of%20the%2048th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20pp%20384394%20Proceedings%20of%20the%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%202010%20JFAK%20van%20Benthem%20and%20Alice%20ter%20Meulen%20Handbook%20of%20logic%20and%20language%201996%20Ronald%20J%20Williams%20Simple%20statistical%20gradientfollowing%20algorithms%20for%20connectionist%20reinforcement%20learning%20Machine%20learning%201992%20Luke%20S%20Zettlemoyer%20and%20Michael%20Collins%20Learning%20to%20map%20sentences%20to%20logical%20form%20Structured%20classification%20with%20probabilistic%20categorial%20grammars%20In%20Proceedings%20of%20the%20Conference%20on%20Uncertainty%20in%20Artificial%20Intelligence%20pp%20658666%202005"
        },
        {
            "id": "The_2014_a",
            "entry": "The model is trained using ADAM (Kingma & Ba, 2014) with a learning rate of.001 and a batch size of 128. Training is ended when the model stops improving on a held-out set.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20model%20is%20trained%20using%20ADAM%20Kingma%20%20Ba%202014%20with%20a%20learning%20rate%20of001%20and%20a%20batch%20size%20of%20128%20Training%20is%20ended%20when%20the%20model%20stops%20improving%20on%20a%20heldout%20set"
        },
        {
            "id": "Word_2011_a",
            "entry": "Word embeddings We train FastText (Bojanowski et al., 2017) on the first 250 million words of the NYT section of Gigaword (Parker et al., 2011). To acquire bigram representations, we pre-process this dataset so that each occurrence of a bigram from the Reddy et al. (2011) dataset is treated as a single word for purposes of estimating word vectors.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Word%20embeddings%20We%20train%20FastText%20Bojanowski%20et%20al%202017%20on%20the%20first%20250%20million%20words%20of%20the%20NYT%20section%20of%20Gigaword%20Parker%20et%20al%202011%20To%20acquire%20bigram%20representations%20we%20preprocess%20this%20dataset%20so%20that%20each%20occurrence%20of%20a%20bigram%20from%20the%20Reddy%20et%20al%202011%20dataset%20is%20treated%20as%20a%20single%20word%20for%20purposes%20of%20estimating%20word%20vectors",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Word%20embeddings%20We%20train%20FastText%20Bojanowski%20et%20al%202017%20on%20the%20first%20250%20million%20words%20of%20the%20NYT%20section%20of%20Gigaword%20Parker%20et%20al%202011%20To%20acquire%20bigram%20representations%20we%20preprocess%20this%20dataset%20so%20that%20each%20occurrence%20of%20a%20bigram%20from%20the%20Reddy%20et%20al%202011%20dataset%20is%20treated%20as%20a%20single%20word%20for%20purposes%20of%20estimating%20word%20vectors"
        },
        {
            "id": "Communication_2014_a",
            "entry": "Communication The encoder and decoder RNNs both use gated recurrent units (Cho et al., 2014) with embeddings and hidden states of size 256. The size of the discrete vocabulary is set to 16 and the maximum message length to 4. Training uses a policy gradient objective with a scalar baseline set to the running average reward; this is optimized using ADAM (Kingma & Ba, 2014) with a learning rate of.001 and a batch size of 256. Each model is trained for 500 steps. Models are trained by sampling from the decoder\u2019s output distribution, but greedy decoding is used to evaluate performance and produce Figure 6.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Communication%20The%20encoder%20and%20decoder%20RNNs%20both%20use%20gated%20recurrent%20units%20Cho%20et%20al%202014%20with%20embeddings%20and%20hidden%20states%20of%20size%20256%20The%20size%20of%20the%20discrete%20vocabulary%20is%20set%20to%2016%20and%20the%20maximum%20message%20length%20to%204%20Training%20uses%20a%20policy%20gradient%20objective%20with%20a%20scalar%20baseline%20set%20to%20the%20running%20average%20reward%20this%20is%20optimized%20using%20ADAM%20Kingma%20%20Ba%202014%20with%20a%20learning%20rate%20of001%20and%20a%20batch%20size%20of%20256%20Each%20model%20is%20trained%20for%20500%20steps%20Models%20are%20trained%20by%20sampling%20from%20the%20decoders%20output%20distribution%20but%20greedy%20decoding%20is%20used%20to%20evaluate%20performance%20and%20produce%20Figure%206",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Communication%20The%20encoder%20and%20decoder%20RNNs%20both%20use%20gated%20recurrent%20units%20Cho%20et%20al%202014%20with%20embeddings%20and%20hidden%20states%20of%20size%20256%20The%20size%20of%20the%20discrete%20vocabulary%20is%20set%20to%2016%20and%20the%20maximum%20message%20length%20to%204%20Training%20uses%20a%20policy%20gradient%20objective%20with%20a%20scalar%20baseline%20set%20to%20the%20running%20average%20reward%20this%20is%20optimized%20using%20ADAM%20Kingma%20%20Ba%202014%20with%20a%20learning%20rate%20of001%20and%20a%20batch%20size%20of%20256%20Each%20model%20is%20trained%20for%20500%20steps%20Models%20are%20trained%20by%20sampling%20from%20the%20decoders%20output%20distribution%20but%20greedy%20decoding%20is%20used%20to%20evaluate%20performance%20and%20produce%20Figure%206"
        }
    ]
}
