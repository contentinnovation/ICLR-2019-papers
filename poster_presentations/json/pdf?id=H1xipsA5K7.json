{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING TWO-LAYER NEURAL NETWORKS WITH SYMMETRIC INPUTS",
        "author": "Rong Ge Computer Science Department Duke University rongge@cs.duke.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1xipsA5K7"
        },
        "abstract": "We give a new algorithm for learning a two-layer neural network under a general class of input distributions. Assuming there is a ground-truth two-layer network y = A\u03c3(W x) + \u03be, where A, W are weight matrices, \u03be represents noise, and the number of neurons in the hidden layer is no larger than the input or output, our algorithm is guaranteed to recover the parameters A, W of the ground-truth network. The only requirement on the input x is that it is symmetric, which still allows highly complicated and structured input. Our algorithm is based on the method-of-moments framework and extends several results in tensor decompositions. We use spectral algorithms to avoid the complicated non-convex optimization in learning neural networks. Experiments show that our algorithm can robustly learn the ground-truth neural network with a small number of samples for many symmetric input distributions."
    },
    "keywords": [
        {
            "term": "Journal of the ACM",
            "url": "https://en.wikipedia.org/wiki/Journal_of_the_ACM"
        },
        {
            "term": "ground truth",
            "url": "https://en.wikipedia.org/wiki/ground_truth"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "polynomial time",
            "url": "https://en.wikipedia.org/wiki/polynomial_time"
        }
    ],
    "abbreviations": {
        "FOCS": "Foundations of Computer Science",
        "JACM": "Journal of the ACM",
        "W W": "we know \u03c3min"
    },
    "highlights": [
        "Deep neural networks have been extremely successful in many tasks related to images, videos and reinforcement learning",
        "Learning a neural network is a complicated non-convex optimization problem, which is hard in the worst-case",
        "If the data is generated according to Equation (1), and the input distribution x \u223c D is symmetric",
        "Optimizing the parameters of a neural network is a difficult problem, especially since the objective function depends on the input distribution which is often unknown and can be very complicated",
        "Our algorithm can learn a network that is of similar complexity as the previous works, while allowing much more general input distributions",
        "Our algorithm shows there is a way to find the global optimal network in polynomial time, does that imply anything about the optimization landscape of the standard objective functions for learning such a neural network, or does it imply there exists an alternative objective function that does not have any local minima? We hope this work can lead to new insights for optimizing a neural network"
    ],
    "key_statements": [
        "Deep neural networks have been extremely successful in many tasks related to images, videos and reinforcement learning",
        "Learning a neural network is a complicated non-convex optimization problem, which is hard in the worst-case",
        "The question of whether we can efficiently learn a neural network still remains generally open, even when the data is drawn from a neural network",
        "If the data is generated according to Equation (1), and the input distribution x \u223c D is symmetric",
        "Later in Section C we will show that requiring a lowerbound on \u03c3min(M ) is not unreasonable: in the smoothed analysis setting where the nature can make a small perturbation on the input distribution D, we show that for any input distribution D, there exists simple perturbations D that are arbitrarily close to D such that \u03c3min(M D ) is lowerbounded.\n3 OUR ALGORITHM",
        "Suppose A, W, E[xx ] and the distinguishing matrix N are all full rank, and Algorithm 2 has access to the exact moments, the network returned by the algorithm computes exactly the same function as the original neural network",
        "We provide experimental results to validate the robustness of our algorithm for both Gaussian input distributions as well as more general symmetric distributions such as symmetric mixtures of Gaussians",
        "Optimizing the parameters of a neural network is a difficult problem, especially since the objective function depends on the input distribution which is often unknown and can be very complicated",
        "Our algorithm can learn a network that is of similar complexity as the previous works, while allowing much more general input distributions",
        "Our algorithm shows there is a way to find the global optimal network in polynomial time, does that imply anything about the optimization landscape of the standard objective functions for learning such a neural network, or does it imply there exists an alternative objective function that does not have any local minima? We hope this work can lead to new insights for optimizing a neural network",
        "Suppose E[xx ], A, W and the augmented distinguishing matrix M are all full rank, and Algorithm 3 has access to the exact moments, the network returned by the algorithm computes exactly the same function as the original neural network",
        "Suppose E[xx ], W, A and the augmented distinguishing matrix M are all full rank, and Algorithm 4 has access to the exact moments, the network returned by the algorithm computes exactly the same function as the original neural network",
        "We show that given any input distribution, after applying (Q, \u03bb)-perturbation with a random Gaussian matrix Q, the smallest singular value of the augmented distinguishing matrix M D is lower bounded"
    ],
    "summary": [
        "Deep neural networks have been extremely successful in many tasks related to images, videos and reinforcement learning.",
        "In this paper we design a new algorithm that is capable of learning a two-layer1 neural network for a general class of input distributions.",
        "The learning algorithm will try to find a neural network f such that f (x) is as close to y as possible over the input distribution D.",
        "When the input distribution is symmetric, we give the first algorithm that can learn a two-layer neural network.",
        "Steps 2 - 5 constructs the pure neuron detector and finds the span of vec\u2217; Steps 7 - 9 performs simultaneous diagonalization to get all the zi\u2019s; Steps 11, 12 calls Algorithm 1 to solve the single-layer problem and outputs the correct result.",
        "Algorithm 2 Learning Two-layer Neural Networks Input: Samples (x1, y1), ..., generated according to Equation (4) Output: Weight matrices W and A.",
        "Suppose A, W, E[xx ] and the distinguishing matrix N are all full rank, and Algorithm 2 has access to the exact moments, the network returned by the algorithm computes exactly the same function as the original neural network.",
        "We generate random orthonormal matrices A and W as the ground truth, and use our algorithm to learn the neural network.",
        "Our algorithm can learn a network that is of similar complexity as the previous works, while allowing much more general input distributions.",
        "It does not affect the last two steps, i.e., finding zi\u2019s from the span (Lemma 5) and learning the reduced single-layer network.",
        "Suppose E[xx ], W, A and the augmented distinguishing matrix M are all full rank, and Algorithm 4 has access to the exact moments, the network returned by the algorithm computes exactly the same function as the original neural network.",
        "For any small enough , for any \u03b4 < 1, given poly \u0393, P1, P2, d, 1/ , 1/\u03b3, 1/\u03b1, 1/\u03b2, 1/\u03b4 number of i.i.d. samples, let the output of Algorithm 3 be V , Z\u22121, we know with probability at least 1 \u2212 \u03b4, A\u03c3(W x) \u2212 Z\u22121\u03c3(Vx) \u2264 , for any input x.",
        "In Lemma 11, we show that with polynomial number of samples, the span of k least singular vectors of Tis close to the null space of T .",
        "Our algorithm shows there is a way to find the global optimal network in polynomial time, does that imply anything about the optimization landscape of the standard objective functions for learning such a neural network, or does it imply there exists an alternative objective function that does not have any local minima? We hope this work can lead to new insights for optimizing a neural network"
    ],
    "headline": "Later in Section C we will show that requiring a lowerbound on \u03c3min(M ) is not unreasonable: in the smoothed analysis setting where the nature can make a small perturbation on the input distribution D, we show that for any input distribution D, there exists simple perturbations D that are arbitrarily close to D such that \u03c3min(M D ) is lowerbounded. 3 OUR ALGORITHM",
    "reference_links": [
        {
            "id": "Anandkumar_et+al_2014_a",
            "entry": "Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773\u20132832, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014"
        },
        {
            "id": "Arora_et+al_2014_a",
            "entry": "Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584\u2013592, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Bhaskara%2C%20Aditya%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Provable%20bounds%20for%20learning%20some%20deep%20representations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Bhaskara%2C%20Aditya%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Provable%20bounds%20for%20learning%20some%20deep%20representations%202014"
        },
        {
            "id": "Bhaskara_et+al_2014_a",
            "entry": "Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pp. 594\u2013603. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "Carbery_2001_a",
            "entry": "A Carbery and J Wright. Distributional and lq norm inequalities for polynomials over convex bodies in Rn. Mathematical research letters, 8(3):233\u2013248, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carbery%2C%20A.%20Wright%2C%20J.%20Distributional%20and%20lq%20norm%20inequalities%20for%20polynomials%20over%20convex%20bodies%20in%20Rn%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carbery%2C%20A.%20Wright%2C%20J.%20Distributional%20and%20lq%20norm%20inequalities%20for%20polynomials%20over%20convex%20bodies%20in%20Rn%202001"
        },
        {
            "id": "Comon_et+al_2009_a",
            "entry": "Pierre Comon, Xavier Luciani, and Andre LF De Almeida. Tensor decompositions, alternating least squares and other tales. Journal of Chemometrics: A Journal of the Chemometrics Society, 23 (7-8):393\u2013405, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Comon%2C%20Pierre%20Luciani%2C%20Xavier%20Almeida%2C%20Andre%20L.F.De%20Tensor%20decompositions%2C%20alternating%20least%20squares%20and%20other%20tales%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Comon%2C%20Pierre%20Luciani%2C%20Xavier%20Almeida%2C%20Andre%20L.F.De%20Tensor%20decompositions%2C%20alternating%20least%20squares%20and%20other%20tales%202009"
        },
        {
            "id": "Daniely_et+al_2016_a",
            "entry": "Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp. 2253\u20132261, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniely%2C%20Amit%20Frostig%2C%20Roy%20Singer%2C%20Yoram%20Toward%20deeper%20understanding%20of%20neural%20networks%3A%20The%20power%20of%20initialization%20and%20a%20dual%20view%20on%20expressivity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniely%2C%20Amit%20Frostig%2C%20Roy%20Singer%2C%20Yoram%20Toward%20deeper%20understanding%20of%20neural%20networks%3A%20The%20power%20of%20initialization%20and%20a%20dual%20view%20on%20expressivity%202016"
        },
        {
            "id": "Lathauwer_et+al_2007_a",
            "entry": "Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. IEEE Transactions on Signal Processing, 55 (6):2965\u20132973, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lathauwer%2C%20Lieven%20De%20Castaing%2C%20Josphine%20Cardoso%2C%20Jean-Franois%20Fourth-order%20cumulant-based%20blind%20identification%20of%20underdetermined%20mixtures%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lathauwer%2C%20Lieven%20De%20Castaing%2C%20Josphine%20Cardoso%2C%20Jean-Franois%20Fourth-order%20cumulant-based%20blind%20identification%20of%20underdetermined%20mixtures%202007"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks with overlaps. arXiv preprint arXiv:1805.07798, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07798"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06129"
        },
        {
            "id": "Du_et+al_2017_b",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "Du_et+al_2018_b",
            "entry": "Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.02054"
        },
        {
            "id": "Gao_et+al_2018_a",
            "entry": "Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one-hiddenlayer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.04133"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimensions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp. 761\u2013770. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Qingqing%20Kakade%2C%20Sham%20M.%20Learning%20mixtures%20of%20gaussians%20in%20high%20dimensions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Qingqing%20Kakade%2C%20Sham%20M.%20Learning%20mixtures%20of%20gaussians%20in%20high%20dimensions%202015"
        },
        {
            "id": "Ge_et+al_0000_a",
            "entry": "Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00708"
        },
        {
            "id": "Ge_et+al_0000_b",
            "entry": "Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Goel_2017_a",
            "entry": "Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06010"
        },
        {
            "id": "Goel_et+al_2016_a",
            "entry": "Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. arXiv preprint arXiv:1611.10258, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10258"
        },
        {
            "id": "Goel_et+al_2018_a",
            "entry": "Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches. arXiv preprint arXiv:1802.02547, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02547"
        },
        {
            "id": "Janzamin_et+al_2015_a",
            "entry": "Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08473"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "Livni_et+al_2014_a",
            "entry": "Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pp. 855\u2013863, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "Ma_et+al_2016_a",
            "entry": "Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-ofsquares. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 438\u2013446. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Tengyu%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Polynomial-time%20tensor%20decompositions%20with%20sum-ofsquares%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Tengyu%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Polynomial-time%20tensor%20decompositions%20with%20sum-ofsquares%202016"
        },
        {
            "id": "Oymak_2018_a",
            "entry": "Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via deep tensor decomposition. arXiv preprint arXiv:1805.06523, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06523"
        },
        {
            "id": "Rudelson_2009_a",
            "entry": "Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 62(12):1707\u20131739, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Smallest%20singular%20value%20of%20a%20random%20rectangular%20matrix.%20Communications%20on%20Pure%20and%20Applied%20Mathematics%3A%20A%20Journal%20Issued%20by%20the%20Courant%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Smallest%20singular%20value%20of%20a%20random%20rectangular%20matrix.%20Communications%20on%20Pure%20and%20Applied%20Mathematics%3A%20A%20Journal%20Issued%20by%20the%20Courant%202009"
        },
        {
            "id": "Safran_2018_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018"
        },
        {
            "id": "Soltanolkotabi_2007_a",
            "entry": "Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing Systems, pp. 2007\u20132017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Learning%20relus%20via%20gradient%20descent%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Learning%20relus%20via%20gradient%20descent%202007"
        },
        {
            "id": "Daniel_2004_a",
            "entry": "Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385\u2013463, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%20A%20Spielman%20and%20ShangHua%20Teng%20Smoothed%20analysis%20of%20algorithms%20Why%20the%20simplex%20algorithm%20usually%20takes%20polynomial%20time%20Journal%20of%20the%20ACM%20JACM%20513385463%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%20A%20Spielman%20and%20ShangHua%20Teng%20Smoothed%20analysis%20of%20algorithms%20Why%20the%20simplex%20algorithm%20usually%20takes%20polynomial%20time%20Journal%20of%20the%20ACM%20JACM%20513385463%202004"
        },
        {
            "id": "Stewart_1977_a",
            "entry": "Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM review, 19(4):634\u2013662, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20Gilbert%20W.%20On%20the%20perturbation%20of%20pseudo-inverses%2C%20projections%20and%20linear%20least%20squares%20problems%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20Gilbert%20W.%20On%20the%20perturbation%20of%20pseudo-inverses%2C%20projections%20and%20linear%20least%20squares%20problems%201977"
        },
        {
            "id": "Stewart_1990_a",
            "entry": "GW Stewart and JG Sun. Computer science and scientific computing. matrix perturbation theory, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20G.W.%20Sun%2C%20J.G.%20Computer%20science%20and%20scientific%20computing.%20matrix%20perturbation%20theory%201990"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "Tropp_2012_a",
            "entry": "Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu networks via gradient descent. arXiv preprint arXiv:1806.07808, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07808"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Yuchen Zhang, Jason D Lee, and Michael I Jordan. 1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pp. 993\u20131001, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%201-regularized%20neural%20networks%20are%20improperly%20learnable%20in%20polynomial%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%201-regularized%20neural%20networks%20are%20improperly%20learnable%20in%20polynomial%20time%202016"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fullyconnected neural networks. In Artificial Intelligence and Statistics, pp. 83\u201391, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20Wainwright%2C%20Martin%20Jordan%2C%20Michael%20On%20the%20learnability%20of%20fullyconnected%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lee%2C%20Jason%20Wainwright%2C%20Martin%20Jordan%2C%20Michael%20On%20the%20learnability%20of%20fullyconnected%20neural%20networks%202017"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "Single-layer:_2018_a",
            "entry": "Single-layer: To get rid of the non-linearities like ReLU, we use the property of the symmetric distribution (similar to (Goel et al., 2018)). Here we provide a more general version (Lemma 6)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singlelayer%20To%20get%20rid%20of%20the%20nonlinearities%20like%20ReLU%20we%20use%20the%20property%20of%20the%20symmetric%20distribution%20similar%20to%20Goel%20et%20al%202018%20Here%20we%20provide%20a%20more%20general%20version%20Lemma%206",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singlelayer%20To%20get%20rid%20of%20the%20nonlinearities%20like%20ReLU%20we%20use%20the%20property%20of%20the%20symmetric%20distribution%20similar%20to%20Goel%20et%20al%202018%20Here%20we%20provide%20a%20more%20general%20version%20Lemma%206"
        },
        {
            "id": "1",
            "entry": "1. For the case where p and q are even numbers, we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20case%20where%20p%20and%20q%20are%20even%20numbers%20we%20have"
        },
        {
            "id": "2",
            "entry": "2. For the other case where p and q are odd numbers, we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20other%20case%20where%20p%20and%20q%20are%20odd%20numbers%20we%20have"
        },
        {
            "id": "2",
            "entry": "2. For any vector vec\u2217(U ) belonging to the span of {vec\u2217(zizi )}, U is a linear combination of zizi \u2019s. Furthermore, T vec\u2217(U ) is a linear combination of T vec\u2217(zizi ). Note that A zi only has one non-zero entry due to the definition of zi, for any i \u2208 [k]. Thus all coefficients in the RHS of (10) are 0. We get T vec\u2217(U ) = 0. Next, we show that we can robustly find zi\u2019s from the span of vec\u2217(zizi )\u2019s. Since this step of the algorithm is the same as the simultaneous diagonalization algorithm for tensor decompositions, we use the robustness of simultaneous diagonalization (Bhaskara et al., 2014) to show that we can find zi\u2019s robustly. The detailed proof is in Section B.2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20any%20vector%20vecU%20%20belonging%20to%20the%20span%20of%20veczizi%20%20U%20is%20a%20linear%20combination%20of%20zizi%20s%20Furthermore%20T%20vecU%20%20is%20a%20linear%20combination%20of%20T%20veczizi%20%20Note%20that%20A%20zi%20only%20has%20one%20nonzero%20entry%20due%20to%20the%20definition%20of%20zi%20for%20any%20i%20%20k%20Thus%20all%20coefficients%20in%20the%20RHS%20of%2010%20are%200%20We%20get%20T%20vecU%20%20%200%20Next%20we%20show%20that%20we%20can%20robustly%20find%20zis%20from%20the%20span%20of%20veczizi%20s%20Since%20this%20step%20of%20the%20algorithm%20is%20the%20same%20as%20the%20simultaneous%20diagonalization%20algorithm%20for%20tensor%20decompositions%20we%20use%20the%20robustness%20of%20simultaneous%20diagonalization%20Bhaskara%20et%20al%202014%20to%20show%20that%20we%20can%20find%20zis%20robustly%20The%20detailed%20proof%20is%20in%20Section%20B2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20any%20vector%20vecU%20%20belonging%20to%20the%20span%20of%20veczizi%20%20U%20is%20a%20linear%20combination%20of%20zizi%20s%20Furthermore%20T%20vecU%20%20is%20a%20linear%20combination%20of%20T%20veczizi%20%20Note%20that%20A%20zi%20only%20has%20one%20nonzero%20entry%20due%20to%20the%20definition%20of%20zi%20for%20any%20i%20%20k%20Thus%20all%20coefficients%20in%20the%20RHS%20of%2010%20are%200%20We%20get%20T%20vecU%20%20%200%20Next%20we%20show%20that%20we%20can%20robustly%20find%20zis%20from%20the%20span%20of%20veczizi%20s%20Since%20this%20step%20of%20the%20algorithm%20is%20the%20same%20as%20the%20simultaneous%20diagonalization%20algorithm%20for%20tensor%20decompositions%20we%20use%20the%20robustness%20of%20simultaneous%20diagonalization%20Bhaskara%20et%20al%202014%20to%20show%20that%20we%20can%20find%20zis%20robustly%20The%20detailed%20proof%20is%20in%20Section%20B2"
        },
        {
            "id": "0",
            "entry": "0. Note that for any 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Note%20that%20for%20any%201"
        },
        {
            "id": "2",
            "entry": "2. The closed form expression for M (as in Lemma 19) has complicated coefficients that are not linear in the vectors wi and wj.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20closed%20form%20expression%20for%20M%20as%20in%20Lemma%2019%20has%20complicated%20coefficients%20that%20are%20not%20linear%20in%20the%20vectors%20wi%20and%20wj"
        },
        {
            "id": "3",
            "entry": "3. The columns of Mij are not independent with each other, so if we condition on all the other columns, Mij is no longer random. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20columns%20of%20Mij%20are%20not%20independent%20with%20each%20other%20so%20if%20we%20condition%20on%20all%20the%20other%20columns%20Mij%20is%20no%20longer%20random"
        }
    ]
}
