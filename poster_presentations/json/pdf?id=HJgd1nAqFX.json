{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DOM-Q-NET: GROUNDED RL ON STRUCTURED LANGUAGE",
        "author": "Sheng Jia University of Toronto Vector Institute sheng.jia@utoronto.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJgd1nAqFX"
        },
        "abstract": "Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks."
    },
    "keywords": [
        {
            "term": "web page",
            "url": "https://en.wikipedia.org/wiki/web_page"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "web navigation",
            "url": "https://en.wikipedia.org/wiki/web_navigation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Document Object Model",
            "url": "https://en.wikipedia.org/wiki/Document_Object_Model"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "q function",
            "url": "https://en.wikipedia.org/wiki/q_function"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "World Wide Web",
            "url": "https://en.wikipedia.org/wiki/World_Wide_Web"
        }
    ],
    "abbreviations": {
        "RL": "reinforcement learning",
        "DOM": "Document Object Model",
        "WWW": "World Wide Web",
        "MLP": "multilayer perceptrons",
        "MDP": "Markov Decision Process",
        "MPNN": "Message Passing Neural Network",
        "GNNs": "graph neural networks",
        "MiniWoB": "Mini World of Bits"
    },
    "highlights": [
        "Deep reinforcement learning (RL) has shown a huge success in solving tasks such as playing arcade games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) and manipulating robotic arms (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>)",
        "We propose to use three separate multilayer perceptrons (MLP) (Rumelhart et al, 1985) to parametrize a factorized Q function for different action categories: \u201cclick\u201d, \u201ctype\u201d and \u201cmode\u201d",
        "Our goal is to design a neural network architecture that effectively captures such invariance for web pages, and yet is flexible to deal with the varying number of Document Object Model elements and goal tokens at different time steps",
        "To address the above problem, we propose a graph neural networks-based reinforcement learning agent that computes the factorized Q-value for each Document Object Model in the current web page, called Document Object Model-Q-NET as shown in Figure 1",
        "Even using imitation learning or guided exploration, the neural network needs to learn a representation that generalizes for unseen diverse Document Object Model states and actions, which our model proves to do",
        "We propose a new architecture for parameterizing factorized Q functions using goal-attention, local word embeddings, and a graph neural network"
    ],
    "key_statements": [
        "Deep reinforcement learning (RL) has shown a huge success in solving tasks such as playing arcade games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) and manipulating robotic arms (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>)",
        "We propose to use three separate multilayer perceptrons (MLP) (Rumelhart et al, 1985) to parametrize a factorized Q function for different action categories: \u201cclick\u201d, \u201ctype\u201d and \u201cmode\u201d",
        "Document Object Model are connected in a tree structure, and we frame web navigation as accessing a Document Object Model and optionally modifying it by the user input",
        "Our goal is to design a neural network architecture that effectively captures such invariance for web pages, and yet is flexible to deal with the varying number of Document Object Model elements and goal tokens at different time steps",
        "To address the above problem, we propose a graph neural networks-based reinforcement learning agent that computes the factorized Q-value for each Document Object Model in the current web page, called Document Object Model-Q-NET as shown in Figure 1",
        "Our model learns a concatenated embedding vector ei = eilocal, eineighbor, eglobal using the low-level and high-level modules that correspond to node-level and graph-level outputs of the graph neural networks",
        "Even using imitation learning or guided exploration, the neural network needs to learn a representation that generalizes for unseen diverse Document Object Model states and actions, which our model proves to do",
        "The faster convergence of Document Object Model-Q-NET to the optimal behaviour indicates the limitation of neighbor module and how global and local module provide shortcuts to the high-level and low-level representations of the web page.\n4.4",
        "Most of the Mini World of Bits tasks have only one desired control policy such as \u201cput a query word in the search, and find the matched link\u201d where the word token for the query and the link have alignments with the Document Object Model",
        "We propose a new architecture for parameterizing factorized Q functions using goal-attention, local word embeddings, and a graph neural network"
    ],
    "summary": [
        "Deep reinforcement learning (RL) has shown a huge success in solving tasks such as playing arcade games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) and manipulating robotic arms (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>).",
        "Our goal is to design a neural network architecture that effectively captures such invariance for web pages, and yet is flexible to deal with the varying number of DOM elements and goal tokens at different time steps.",
        "To address the above problem, we propose a GNN-based RL agent that computes the factorized Q-value for each DOM in the current web page, called DOM-Q-NET as shown in Figure 1.",
        "It uses additional information of tree-structured HTML to guide the learning of state-action representations, embeddings e, which is shared among factorized Q networks.",
        "Our model learns a concatenated embedding vector ei = eilocal, eineighbor, eglobal using the low-level and high-level modules that correspond to node-level and graph-level outputs of the GNN.",
        "Let \u03b8 = (E, wGNN , wdom, wtoken, wmode) be the model parameters including the embedding matrices, the weights of a graph neural network, and weights of the factorized Q-value function.",
        "We perform an ablation study to justify the effectiveness of each representational module, followed by the comparisons of gains in sample efficiency from goal-attention in multitask and single task settings.",
        "We use the Q-learning algorithm, with four components of Rainbow (<a class=\"ref-link\" id=\"cHessel_et+al_2018_a\" href=\"#rHessel_et+al_2018_a\">Hessel et al, 2018</a>), to train our agent because web navigation tasks are sparse reward problems, and an off-policy learning with a replay buffer is more sample-efficient.",
        "Figure 6 shows the two tasks chosen, and the failure case for click-checkboxes shows that DOM selection without the neighbor module will not work because many DOMs have the same attributes, and have exactly the same representations despite the difference in the context.",
        "The faster convergence of DOM-Q-NET to the optimal behaviour indicates the limitation of neighbor module and how global and local module provide shortcuts to the high-level and low-level representations of the web page.",
        "Most of the MiniWoB tasks have only one desired control policy such as \u201cput a query word in the search, and find the matched link\u201d where the word token for the query and the link have alignments with the DOMs. our model solves most of the tasks without feeding the goal representation to the network, with exceptions like click-widget.",
        "This indicates that the agent successfully learns to pay attention to different parts of the DOM tree given different goal instructions when solving multiple tasks.",
        "It solves relatively hard tasks with large action space, and transfers learned behaviours from multitask learning, which are two important factors for web navigation."
    ],
    "headline": "We introduce Document Object Model-Q-NET, a novel architecture for reinforcement learning-based web navigation to address both of these problems",
    "reference_links": [
        {
            "id": "Castro_et+al_2018_a",
            "entry": "Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110.",
            "url": "http://arxiv.org/abs/1812.06110",
            "arxiv_url": "https://arxiv.org/pdf/1812.06110"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.",
            "url": "http://arxiv.org/abs/1406.1078",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Fortunato_et+al_2018_a",
            "entry": "Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fortunato%2C%20Meire%20Azar%2C%20Mohammad%20Gheshlaghi%20Piot%2C%20Bilal%20Menick%2C%20Jacob%20Noisy%20networks%20for%20exploration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fortunato%2C%20Meire%20Azar%2C%20Mohammad%20Gheshlaghi%20Piot%2C%20Bilal%20Menick%2C%20Jacob%20Noisy%20networks%20for%20exploration%202018"
        },
        {
            "id": "Gilmer_et+al_2017_a",
            "entry": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017"
        },
        {
            "id": "Hamrick_et+al_2018_a",
            "entry": "Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B Tenenbaum, and Peter W Battaglia. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01203"
        },
        {
            "id": "Hessel_et+al_2018_a",
            "entry": "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hessel%2C%20Matteo%20Modayil%2C%20Joseph%20Hasselt%2C%20Hado%20Van%20Schaul%2C%20Tom%20Rainbow%3A%20Combining%20improvements%20in%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hessel%2C%20Matteo%20Modayil%2C%20Joseph%20Hasselt%2C%20Hado%20Van%20Schaul%2C%20Tom%20Rainbow%3A%20Combining%20improvements%20in%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kipf_2016_a",
            "entry": "Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.",
            "url": "http://arxiv.org/abs/1609.02907",
            "arxiv_url": "https://arxiv.org/pdf/1609.02907"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. In JMLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Evan%20Zheran%20Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Shi%2C%20Tianlin%20Reinforcement%20learning%20on%20web%20interfaces%20using%20workflow-guided%20exploration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Evan%20Zheran%20Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Shi%2C%20Tianlin%20Reinforcement%20learning%20on%20web%20interfaces%20using%20workflow-guided%20exploration%202018"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. In Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Hinton_1985_a",
            "entry": "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%20Williams%2C%20Ronald%20J.%20Learning%20internal%20representations%20by%20error%20propagation%201985"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. In IEEE Transactions on Neural Networks, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Schaul_et+al_2016_a",
            "entry": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%20Schaul%20John%20Quan%20Ioannis%20Antonoglou%20and%20David%20Silver%20Prioritized%20experience%20replay%20In%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%20Schaul%20John%20Quan%20Ioannis%20Antonoglou%20and%20David%20Silver%20Prioritized%20experience%20replay%20In%20ICLR%202016"
        },
        {
            "id": "Shi_et+al_2017_a",
            "entry": "Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Tianlin%20Karpathy%2C%20Andrej%20Fan%2C%20Linxi%20Hernandez%2C%20Jonathan%20World%20of%20bits%3A%20An%20open-domain%20platform%20for%20web-based%20agents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Tianlin%20Karpathy%2C%20Andrej%20Fan%2C%20Linxi%20Hernandez%2C%20Jonathan%20World%20of%20bits%3A%20An%20open-domain%20platform%20for%20web-based%20agents%202017"
        },
        {
            "id": "Sutton_1988_a",
            "entry": "Richard S Sutton. Learning to predict by the methods of temporal differences. In Machine learning, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences%201988"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Introduction%20to%20reinforcement%20learning%2C%20volume%20135%201998"
        },
        {
            "id": "Hasselt_et+al_2016_a",
            "entry": "Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20qlearning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20qlearning%202016"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "Velickovic_et+al_2018_a",
            "entry": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petar%20Velickovic%20Guillem%20Cucurull%20Arantxa%20Casanova%20Adriana%20Romero%20Pietro%20Lio%20and%20Yoshua%20Bengio%20Graph%20attention%20networks%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petar%20Velickovic%20Guillem%20Cucurull%20Arantxa%20Casanova%20Adriana%20Romero%20Pietro%20Lio%20and%20Yoshua%20Bengio%20Graph%20attention%20networks%20In%20ICLR%202018"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with graph neural networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Tingwu%20Liao%2C%20Renjie%20Ba%2C%20Jimmy%20Fidler%2C%20Sanja%20Nervenet%3A%20Learning%20structured%20policy%20with%20graph%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Tingwu%20Liao%2C%20Renjie%20Ba%2C%20Jimmy%20Fidler%2C%20Sanja%20Nervenet%3A%20Learning%20structured%20policy%20with%20graph%20neural%20networks%202018"
        }
    ]
}
