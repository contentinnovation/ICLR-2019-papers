{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A STATISTICAL APPROACH TO ASSESSING NEURAL NETWORK ROBUSTNESS",
        "author": "Stefan Webb, Department of Engineering Science University of Oxford",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1xcx3C5FX"
        },
        "abstract": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "rare event",
            "url": "https://en.wikipedia.org/wiki/rare_event"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "MCMC",
            "url": "https://en.wikipedia.org/wiki/MCMC"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        }
    ],
    "abbreviations": {
        "AMLS": "adaptive multi-level splitting"
    },
    "highlights": [
        "The robustness of deep neural networks must be guaranteed in mission-critical applications where their failure could have severe real-world implications",
        "Adaptation of the adaptive multi-level splitting method to our verification framework to allow the tractable estimation of our metric for large networks and rare events;",
        "We have introduced a new measure for the intrinsic robustness of a neural network, and have validated its utility on several datasets from the formal verification and deep learning literatures",
        "Our method may fail to find counterexamples because they reside on a subset with probability less than Pmin; the counterexamples may even reside on a subset of the input space with measure zero with respect to the input distribution",
        "There are many practical scenarios, such as those discussed in the introduction, where either it is unrealistic for there to be no counterexamples close to the input, the network is too large to realistically permit formal verification, or where potential counterexamples are generated by chance rather than by an adversary"
    ],
    "key_statements": [
        "The robustness of deep neural networks must be guaranteed in mission-critical applications where their failure could have severe real-world implications",
        "Adaptation of the adaptive multi-level splitting method to our verification framework to allow the tractable estimation of our metric for large networks and rare events;",
        "We have introduced a new measure for the intrinsic robustness of a neural network, and have validated its utility on several datasets from the formal verification and deep learning literatures",
        "Our method may fail to find counterexamples because they reside on a subset with probability less than Pmin; the counterexamples may even reside on a subset of the input space with measure zero with respect to the input distribution",
        "There are many practical scenarios, such as those discussed in the introduction, where either it is unrealistic for there to be no counterexamples close to the input, the network is too large to realistically permit formal verification, or where potential counterexamples are generated by chance rather than by an adversary"
    ],
    "summary": [
        "The robustness of deep neural networks must be guaranteed in mission-critical applications where their failure could have severe real-world implications.",
        "To address the shortfalls of the classic approach, we develop a new measure of intrinsic robustness of neural networks based on the probability that a property is violated under an input distribution model.",
        "The simple approach of constructing a direct Monte Carlo estimate by sampling from the input model and evaluating the property will be expensive and only viable when the event is relatively common.",
        "Reframing neural network verification as the estimation of the probability of a violation, thereby providing a more informative robustness metric for non-verifiable networks;",
        "Adaptation of the AMLS method to our verification framework to allow the tractable estimation of our metric for large networks and rare events;",
        "The framework for our robustness metric is very general, requiring only a) a neural network f\u03b8, b) a property function s(x; f, \u03c6), and c) an input model p(x).",
        "6.1 EMULATION OF FORMAL VERIFICATION In our first experiment1, we aim to test whether our robustness estimation framework is able to effectively emulate formal verification approaches, while providing additional robustness information for SAT properties.",
        "To investigate the effect of the parameters more formally, we further ran AMLS on the SAT properties of COLLISIONDETECTION, varying \u03c1 \u2208 {0.1, 0.25, 0.5}, N \u2208 {103, 104, 105} and M \u2208 {100, 250, 1000}, again comparing to the naive MC estimate for 1010 samples.",
        "To validate the algorithm on a higher-dimensional problem, we first tested adversarial properties on the MNIST and CIFAR\u201310 datasets using a dense ReLU network with two hidden-layer of size 256.",
        "To demonstrate that our approach can be employed on large networks, we tested adversarial properties on the CIFAR\u2013100 dataset and a much larger DenseNet architecture (<a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017a</a>), with depth and growth-rate 40.",
        "As the method of <a class=\"ref-link\" id=\"cWong_2018_a\" href=\"#rWong_2018_a\"><a class=\"ref-link\" id=\"cWong_2018_a\" href=\"#rWong_2018_a\">Wong & Kolter (2018</a></a>) returns the maximum value of the property for each sample over a convex outer bound on the perturbations, it is able to produce certificates-of-robustness for some datapoints.",
        "We compared the fraction of the 50 samples from the test set that are verified by the method of <a class=\"ref-link\" id=\"cWong_2018_a\" href=\"#rWong_2018_a\"><a class=\"ref-link\" id=\"cWong_2018_a\" href=\"#rWong_2018_a\">Wong & Kolter (2018</a></a>), to the fraction that have a negligible volume of adversarial examples, I = Pmin, in their l\u221e -ball neighbourhood.",
        "We have introduced a new measure for the intrinsic robustness of a neural network, and have validated its utility on several datasets from the formal verification and deep learning literatures."
    ],
    "headline": "We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated",
    "reference_links": [
        {
            "id": "Brehier_et+al_2015_a",
            "entry": "Charles-Edouard Brehier, Tony Lelievre, and Mathias Rousset. Analysis of adaptive multilevel splitting algorithms in an idealized case. ESAIM: Probability and Statistics, 19:361\u2013394, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brehier%2C%20Charles-Edouard%20Lelievre%2C%20Tony%20Rousset%2C%20Mathias%20Analysis%20of%20adaptive%20multilevel%20splitting%20algorithms%20in%20an%20idealized%20case%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brehier%2C%20Charles-Edouard%20Lelievre%2C%20Tony%20Rousset%2C%20Mathias%20Analysis%20of%20adaptive%20multilevel%20splitting%20algorithms%20in%20an%20idealized%20case%202015"
        },
        {
            "id": "Bunel_et+al_2018_a",
            "entry": "Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, and M. Pawan Kumar. A unified view of piecewise linear neural network verification. arXiv preprint arXiv:1711.00455v3 [cs.AI], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00455v3"
        },
        {
            "id": "Cheng_et+al_2017_a",
            "entry": "Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess. Verification of binarized neural networks. arXiv preprint arXiv:1710.03107, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03107"
        },
        {
            "id": "Boer_et+al_2005_a",
            "entry": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19\u201367, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005"
        },
        {
            "id": "Gehr_et+al_2018_a",
            "entry": "Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai 2: Safety and robustness certification of neural networks with abstract interpretation. In Security and Privacy (SP), 2018 IEEE Symposium on, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018"
        },
        {
            "id": "Gilks_et+al_1995_a",
            "entry": "Walter R Gilks, Sylvia Richardson, and David Spiegelhalter. Markov chain Monte Carlo in practice. Chapman and Hall/CRC, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilks%2C%20Walter%20R.%20Richardson%2C%20Sylvia%20Spiegelhalter%2C%20David%20Markov%20chain%20Monte%20Carlo%20in%20practice%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilks%2C%20Walter%20R.%20Richardson%2C%20Sylvia%20Spiegelhalter%2C%20David%20Markov%20chain%20Monte%20Carlo%20in%20practice%201995"
        },
        {
            "id": "Goodfellow_2018_a",
            "entry": "Ian Goodfellow. Gradient masking causes clever to overestimate adversarial perturbation size. arXiv preprint arXiv:1804.07870, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07870"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Guyader_et+al_2011_a",
            "entry": "Arnaud Guyader, Nicolas Hengartner, and Eric Matzner-L\u00f8ber. Simulation and estimation of extreme quantiles and extreme probabilities. Applied Mathematics & Optimization, 64(2):171\u2013196, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guyader%2C%20Arnaud%20Hengartner%2C%20Nicolas%20Matzner-L%C3%B8ber%2C%20Eric%20Simulation%20and%20estimation%20of%20extreme%20quantiles%20and%20extreme%20probabilities%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guyader%2C%20Arnaud%20Hengartner%2C%20Nicolas%20Matzner-L%C3%B8ber%2C%20Eric%20Simulation%20and%20estimation%20of%20extreme%20quantiles%20and%20extreme%20probabilities%202011"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Huang_et+al_0000_a",
            "entry": "Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "Springer,_2017_a",
            "entry": "Springer, 2017c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202017c"
        },
        {
            "id": "Kahn_1951_a",
            "entry": "H. Kahn and T.E. Harris. Estimation of particle transmission by random sampling. National Bureau of Standards applied mathematics series, 12:27\u201330, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kahn%2C%20H.%20Harris%2C%20T.E.%20Estimation%20of%20particle%20transmission%20by%20random%20sampling%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kahn%2C%20H.%20Harris%2C%20T.E.%20Estimation%20of%20particle%20transmission%20by%20random%20sampling%201951"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06748"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Neal_2011_a",
            "entry": "Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011"
        },
        {
            "id": "Nowozin_2015_a",
            "entry": "Sebastian Nowozin. Multilevel splitting. http://www.nowozin.net/sebastian/blog/multilevel-splitting.html, 2015.",
            "url": "http://www.nowozin.net/sebastian/blog/multilevel-splitting.html"
        },
        {
            "id": "Roberts_et+al_1997_a",
            "entry": "Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110\u2013120, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roberts%2C%20Gareth%20O.%20Gelman%2C%20Andrew%20Gilks%2C%20Walter%20R.%20Weak%20convergence%20and%20optimal%20scaling%20of%20random%20walk%20metropolis%20algorithms%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roberts%2C%20Gareth%20O.%20Gelman%2C%20Andrew%20Gilks%2C%20Walter%20R.%20Weak%20convergence%20and%20optimal%20scaling%20of%20random%20walk%20metropolis%20algorithms%201997"
        },
        {
            "id": "Rossky_et+al_1978_a",
            "entry": "PJ Rossky, JD Doll, and HL Friedman. Brownian dynamics as smart monte carlo simulation. The Journal of Chemical Physics, 69(10):4628\u20134633, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rossky%2C%20P.J.%20Doll%2C%20J.D.%20Friedman%2C%20H.L.%20Brownian%20dynamics%20as%20smart%20monte%20carlo%20simulation%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rossky%2C%20P.J.%20Doll%2C%20J.D.%20Friedman%2C%20H.L.%20Brownian%20dynamics%20as%20smart%20monte%20carlo%20simulation%201978"
        },
        {
            "id": "Rubinstein_1997_a",
            "entry": "Reuven Y Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89\u2013112, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Reuven%20Y.%20Optimization%20of%20computer%20simulation%20models%20with%20rare%20events%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Reuven%20Y.%20Optimization%20of%20computer%20simulation%20models%20with%20rare%20events%201997"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Jonathan_2019_a",
            "entry": "Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Ruderman, Keith Anderson, Krishnamurthy (Dj) Dvijotham, Nicolas Heess, and Pushmeet Kohli. Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jonathan%20Uesato%20Ananya%20Kumar%20Csaba%20Szepesvari%20Tom%20Erez%20Avraham%20Ruderman%20Keith%20Anderson%20Krishnamurthy%20Dj%20Dvijotham%20Nicolas%20Heess%20and%20Pushmeet%20Kohli%20Rigorous%20agent%20evaluation%20An%20adversarial%20approach%20to%20uncover%20catastrophic%20failures%20In%20International%20Conference%20on%20Learning%20Representations%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jonathan%20Uesato%20Ananya%20Kumar%20Csaba%20Szepesvari%20Tom%20Erez%20Avraham%20Ruderman%20Keith%20Anderson%20Krishnamurthy%20Dj%20Dvijotham%20Nicolas%20Heess%20and%20Pushmeet%20Kohli%20Rigorous%20agent%20evaluation%20An%20adversarial%20approach%20to%20uncover%20catastrophic%20failures%20In%20International%20Conference%20on%20Learning%20Representations%202019"
        },
        {
            "id": "Weng_et+al_0000_a",
            "entry": "Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks. arXiv preprint arXiv:1804.09699, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09699"
        },
        {
            "id": "Weng_et+al_0000_b",
            "entry": "Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. arXiv preprint arXiv:1801.10578, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1801.10578"
        },
        {
            "id": "Wong_2018_a",
            "entry": "Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5283\u20135292, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20Eric%20Kolter%2C%20Zico%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20Eric%20Kolter%2C%20Zico%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018"
        },
        {
            "id": "Xiang_et+al_2018_a",
            "entry": "Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. Output reachable set estimation and verification for multilayer neural networks. IEEE transactions on neural networks and learning systems, (99):1\u20137, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiang%2C%20Weiming%20Tran%2C%20Hoang-Dung%20Johnson%2C%20Taylor%20T.%20Output%20reachable%20set%20estimation%20and%20verification%20for%20multilayer%20neural%20networks.%20IEEE%20transactions%20on%20neural%20networks%20and%20learning%20systems%202018"
        },
        {
            "id": "Zakrzewski_2001_a",
            "entry": "Radosiaw R Zakrzewski. Verification of a trained neural network accuracy. In Neural Networks, 2001. Proceedings. IJCNN\u201901. International Joint Conference on, volume 3, pp. 1657\u20131662. IEEE, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zakrzewski%2C%20Radosiaw%20R.%20Verification%20of%20a%20trained%20neural%20network%20accuracy%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zakrzewski%2C%20Radosiaw%20R.%20Verification%20of%20a%20trained%20neural%20network%20accuracy%202001"
        }
    ]
}
