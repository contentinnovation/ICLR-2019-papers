{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "G-SGD: OPTIMIZING RELU NEURAL NETWORKS IN ITS POSITIVELY SCALE-INVARIANT SPACE",
        "author": "Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Nenghai Yu, Tie-Yan Liu, 1Microsoft Research Asia,3University of Science and Technology of China 4University of Chinese Academy of Sciences 1{meq, huishuai.zhang, wche, qiwye,tie-yan.liu}@microsoft.com 2zhengsx@mail.ustc.edu.cn, 3ynh@ustc.edu.cn, 4mazm@amt.ac.cn",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyxfEn09Y7"
        },
        "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process ? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as G. We show that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and prove that the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as G-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in G-space (abbreviated as G-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that G-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "vector space",
            "url": "https://en.wikipedia.org/wiki/vector_space"
        },
        {
            "term": "transformation group",
            "url": "https://en.wikipedia.org/wiki/transformation_group"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "linear span",
            "url": "https://en.wikipedia.org/wiki/linear_span"
        },
        {
            "term": "scale invariant",
            "url": "https://en.wikipedia.org/wiki/scale_invariant"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        }
    ],
    "abbreviations": {
        "ReLU": "rectified linear hidden units",
        "SGD": "Stochastic Gradient Descent",
        "MLP": "multi-layer perceptron",
        "CNN": "convolutional networks"
    },
    "highlights": [
        "Over the past ten years, neural networks with rectified linear hidden units (ReLU) (<a class=\"ref-link\" id=\"cHahnloser_et+al_2000_a\" href=\"#rHahnloser_et+al_2000_a\"><a class=\"ref-link\" id=\"cHahnloser_et+al_2000_a\" href=\"#rHahnloser_et+al_2000_a\">Hahnloser et al, 2000</a></a>) as activation functions have demonstrated the power in many important applications, such as information system (<a class=\"ref-link\" id=\"cCheng_et+al_2016_a\" href=\"#rCheng_et+al_2016_a\"><a class=\"ref-link\" id=\"cCheng_et+al_2016_a\" href=\"#rCheng_et+al_2016_a\">Cheng et al, 2016</a></a>; Wang et al, 2017), image classification (He et al, 2016a; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>), text understanding (Vaswani et al, 2017), etc",
        "Recent studies (Neyshabur et al, 2015; LeCun et al, 2015) show that rectified linear hidden units networks have positively scale-invariant property, i.e., if the incoming weights of a hidden node with rectified linear hidden units activation are multiplied by a positive constant c and the outgoing weights are divided by c, the neural network with the new weights will generate exactly the same output as the old one for an arbitrary input",
        "We find G-space constituted by the values of the basis paths, which is positively scaleinvariant, can sufficiently represent the rectified linear hidden units neural networks, and has a smaller dimension than the vector space of weights",
        "If A is the structure matrix for a rectified linear hidden units network, we have rank(A) = m \u2212 H, where m is the dimension of weight vector w and H is the total number of hidden nodes for multi-layer perceptron with rectified linear hidden units respectively",
        "We study the G-space for rectified linear hidden units neural networks and propose a novel optimization algorithm called G-Stochastic Gradient Descent",
        "We study the positive scaling operators which forms a transformation group G and prove that the value vector of all the paths is sufficient to represent the neural networks"
    ],
    "key_statements": [
        "Over the past ten years, neural networks with rectified linear hidden units (ReLU) (<a class=\"ref-link\" id=\"cHahnloser_et+al_2000_a\" href=\"#rHahnloser_et+al_2000_a\">Hahnloser et al, 2000</a>) as activation functions have demonstrated the power in many important applications, such as information system (<a class=\"ref-link\" id=\"cCheng_et+al_2016_a\" href=\"#rCheng_et+al_2016_a\">Cheng et al, 2016</a>; Wang et al, 2017), image classification (He et al, 2016a; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a>), text understanding (Vaswani et al, 2017), etc. These networks are usually trained with Stochastic Gradient Descent (SGD), where the gradient of loss function with respect to the weights can be efficiently computed via back propagation method (Rumelhart et al, 1986)",
        "Recent studies (Neyshabur et al, 2015; LeCun et al, 2015) show that rectified linear hidden units networks have positively scale-invariant property, i.e., if the incoming weights of a hidden node with rectified linear hidden units activation are multiplied by a positive constant c and the outgoing weights are divided by c, the neural network with the new weights will generate exactly the same output as the old one for an arbitrary input",
        "We investigate the positively scale-invariant space to sufficiently represent rectified linear hidden units neural networks by the following four steps",
        "We prove that the dimension of G-space is \"H\" smaller comparing to the weight space, where H is the total number of hidden units in a multi-layer perceptron (MLP) or feature maps in a convolutional networks (CNN)",
        "We find G-space constituted by the values of the basis paths, which is positively scaleinvariant, can sufficiently represent the rectified linear hidden units neural networks, and has a smaller dimension than the vector space of weights",
        "If A is the structure matrix for a rectified linear hidden units network, we have rank(A) = m \u2212 H, where m is the dimension of weight vector w and H is the total number of hidden nodes for multi-layer perceptron with rectified linear hidden units respectively",
        "The dimension of G-space is m \u2212 H, where m is the number of weights and H is the total number of hidden nodes for multi-layer perceptron or the total number of feature maps for convolutional networks",
        "We study the G-space for rectified linear hidden units neural networks and propose a novel optimization algorithm called G-Stochastic Gradient Descent",
        "We study the positive scaling operators which forms a transformation group G and prove that the value vector of all the paths is sufficient to represent the neural networks"
    ],
    "summary": [
        "Over the past ten years, neural networks with rectified linear hidden units (ReLU) (<a class=\"ref-link\" id=\"cHahnloser_et+al_2000_a\" href=\"#rHahnloser_et+al_2000_a\"><a class=\"ref-link\" id=\"cHahnloser_et+al_2000_a\" href=\"#rHahnloser_et+al_2000_a\">Hahnloser et al, 2000</a></a>) as activation functions have demonstrated the power in many important applications, such as information system (<a class=\"ref-link\" id=\"cCheng_et+al_2016_a\" href=\"#rCheng_et+al_2016_a\"><a class=\"ref-link\" id=\"cCheng_et+al_2016_a\" href=\"#rCheng_et+al_2016_a\">Cheng et al, 2016</a></a>; Wang et al, 2017), image classification (He et al, 2016a; <a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>), text understanding (Vaswani et al, 2017), etc.",
        "The values of the basis paths are positively scale-invariant and can sufficiently to represent the ReLU neural networks.",
        "We find G-space constituted by the values of the basis paths, which is positively scaleinvariant, can sufficiently represent the ReLU neural networks, and has a smaller dimension than the vector space of weights.",
        "We propose to optimize the ReLU neural networks in its positively scale-invariant space, i.e., G-space.",
        "This is consistent with that, the positive scale-invariance in weight space will negatively influence the optimization and our proposed G-SGD algorithm can effectively solve this problem.",
        "This work is different from ours: 1) they regularize the gradient in weight space by path norm while we optimize the loss function directly in a positively scale-invariant space; 2) they do not consider the dependency between paths and it\u2019s hard for them to compute the exactly path-regularized gradients.",
        "Different from the previous works, we propose to directly optimize the ReLU networks in its positively scale-invariant space, instead of",
        "Theorem 3.4 If A is the structure matrix for a ReLU network, we have rank(A) = m \u2212 H, where m is the dimension of weight vector w and H is the total number of hidden nodes for MLP with ReLU respectively.",
        "This novel algorithm makes use of three methods, named Skeleton Method, Inverse-Chain-Rule (ICR) and WeightAllocation (WA), respectively, to calculate the gradients w.r.t. basis path vector and project the updates back to weights efficiently.",
        "Before the calculation of gradients in G-space, we first design an algorithm called skeleton method to construct skeleton weights and basis paths for MLP whose depth is L and width is h.",
        "From Figure 4, we can see that, 1) for each number of H, G-SGD clearly outperforms SGD on both training loss and test error, which verifies our claim that optimization loss function in G space is a better choice; 2) as H increases, the invariant ratio decreases and \u2206 gradually decreases as well, which provides the evidence for that the positive scaling invariance in weight space improperly influences the optimization.",
        "We study the positive scaling operators which forms a transformation group G and prove that the value vector of all the paths is sufficient to represent the neural networks.",
        "We show that one can identify basis paths and prove that the linear span of their value vectors is an invariant space with lower dimension under the positive scaling group.",
        "We will examine the performance of G-SGD on more large-scale tasks"
    ],
    "headline": "A natural question is: can we construct a new vector space that is positively scale-invariant and sufficient to represent rectified linear hidden units neural networks so as to better facilitate the optimization process ? In this paper, we provide our positive answer to this question",
    "reference_links": [
        {
            "id": "Badrinarayanan_et+al_2015_a",
            "entry": "Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Symmetry-invariant optimization in deep networks. arXiv preprint arXiv:1511.01754, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01754"
        },
        {
            "id": "Cheng_et+al_2016_a",
            "entry": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, pp. 7\u201310. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Heng-Tze%20Koc%2C%20Levent%20Harmsen%2C%20Jeremiah%20Shaked%2C%20Tal%20Wide%20%26%20deep%20learning%20for%20recommender%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Heng-Tze%20Koc%2C%20Levent%20Harmsen%2C%20Jeremiah%20Shaked%2C%20Tal%20Wide%20%26%20deep%20learning%20for%20recommender%20systems%202016"
        },
        {
            "id": "Du_et+al_2018_a",
            "entry": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Hu%2C%20Wei%20Lee%2C%20Jason%20D.%20Algorithmic%20regularization%20in%20learning%20deep%20homogeneous%20models%3A%20Layers%20are%20automatically%20balanced%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Hu%2C%20Wei%20Lee%2C%20Jason%20D.%20Algorithmic%20regularization%20in%20learning%20deep%20homogeneous%20models%3A%20Layers%20are%20automatically%20balanced%202018"
        },
        {
            "id": "Hahnloser_et+al_2000_a",
            "entry": "Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789): 947, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hahnloser%2C%20Richard%20H.R.%20Sarpeshkar%2C%20Rahul%20Mahowald%2C%20Misha%20A.%20Douglas%2C%20Rodney%20J.%20Digital%20selection%20and%20analogue%20amplification%20coexist%20in%20a%20cortex-inspired%20silicon%20circuit%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hahnloser%2C%20Richard%20H.R.%20Sarpeshkar%2C%20Rahul%20Mahowald%2C%20Misha%20A.%20Douglas%2C%20Rodney%20J.%20Digital%20selection%20and%20analogue%20amplification%20coexist%20in%20a%20cortex-inspired%20silicon%20circuit%202000"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        }
    ]
}
