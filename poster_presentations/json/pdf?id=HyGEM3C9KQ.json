{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "IMPROVING DIFFERENTIABLE NEURAL COMPUTERS THROUGH MEMORY MASKING, DE-ALLOCATION, AND LINK DISTRIBUTION SHARPNESS CONTROL",
        "author": "Robert Csordas The Swiss AI Lab, IDSIA / USI / SUPSI robert@idsia.ch",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyGEM3C9KQ"
        },
        "abstract": "The Differentiable Neural Computer (DNC) can learn algorithmic and question answering tasks. An analysis of its internal activation patterns reveals three problems: Most importantly, the lack of key-value separation makes the address distribution resulting from content-based look-up noisy and flat, since the value influences the score calculation, although only the key should. Second, DNC\u2019s deallocation of memory results in aliasing, which is a problem for content-based look-up. Thirdly, chaining memory reads with the temporal linkage matrix exponentially degrades the quality of the address distribution. Our proposed fixes of these problems yield improved performance on arithmetic tasks, and also improve the mean error rate on the bAbI question answering dataset by 43%."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "external memory",
            "url": "https://en.wikipedia.org/wiki/external_memory"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "differentiable neural computer",
            "url": "https://en.wikipedia.org/wiki/differentiable_neural_computer"
        }
    ],
    "abbreviations": {
        "DNC": "Differentiable Neural Computer",
        "RNNs": "Recurrent Neural Networks",
        "Lt": "link matrix"
    },
    "highlights": [
        "Recurrent Neural Networks (RNNs) such as LSTM (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a></a>; <a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\"><a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\">Gers et al, 2000</a></a>) are in theory capable of solving complex algorithmic tasks (<a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\"><a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\">Siegelmann & Sontag, 1992</a></a>), in practice they often struggle to do so",
        "One reason is the large amount of time-varying memory required for many algorithmic tasks, combined with quadratic growth of the number of trainable parameters of a fully connected Recurrent Neural Networks when increasing the size of its internal state",
        "Unlike approaches that achieve state of the art performance on specific tasks, e.g. MemNN (<a class=\"ref-link\" id=\"cSukhbaatar_et+al_2015_a\" href=\"#rSukhbaatar_et+al_2015_a\">Sukhbaatar et al, 2015</a>) or Key-Value Networks (<a class=\"ref-link\" id=\"cMiller_et+al_2016_a\" href=\"#rMiller_et+al_2016_a\">Miller et al, 2016</a>) for the bAbI dataset (<a class=\"ref-link\" id=\"cWeston_et+al_2015_a\" href=\"#rWeston_et+al_2015_a\">Weston et al, 2015</a>), the Differentiable Neural Computer consistently reaches near state of the art performance on all of them",
        "Sharpness enhancement To analyze the problem of degradation of temporal links after successive link matrix updates, we examined the forward and backward link distributions of the model with modified deallocation (DNC-D)",
        "We identified three drawbacks of the traditional Differentiable Neural Computer model, and proposed fixes for them",
        "Two of them are related to content-based addressing: (1) Lack of key-value separation yields uncertain and noisy address distributions resulting from content-based look-up"
    ],
    "key_statements": [
        "Recurrent Neural Networks (RNNs) such as LSTM (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a></a>; <a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\"><a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\">Gers et al, 2000</a></a>) are in theory capable of solving complex algorithmic tasks (<a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\"><a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\">Siegelmann & Sontag, 1992</a></a>), in practice they often struggle to do so",
        "One reason is the large amount of time-varying memory required for many algorithmic tasks, combined with quadratic growth of the number of trainable parameters of a fully connected Recurrent Neural Networks when increasing the size of its internal state",
        "Researchers have tried to address this problem by incorporating an external memory as a useful architectural bias for algorithm learning (<a class=\"ref-link\" id=\"cDas_et+al_1992_a\" href=\"#rDas_et+al_1992_a\">Das et al, 1992</a>; <a class=\"ref-link\" id=\"cMozer_1993_a\" href=\"#rMozer_1993_a\">Mozer & Das, 1993</a>; <a class=\"ref-link\" id=\"cGraves_et+al_2014_a\" href=\"#rGraves_et+al_2014_a\">Graves et al, 2014</a>; 2016)",
        "Unlike approaches that achieve state of the art performance on specific tasks, e.g. MemNN (<a class=\"ref-link\" id=\"cSukhbaatar_et+al_2015_a\" href=\"#rSukhbaatar_et+al_2015_a\">Sukhbaatar et al, 2015</a>) or Key-Value Networks (<a class=\"ref-link\" id=\"cMiller_et+al_2016_a\" href=\"#rMiller_et+al_2016_a\">Miller et al, 2016</a>) for the bAbI dataset (<a class=\"ref-link\" id=\"cWeston_et+al_2015_a\" href=\"#rWeston_et+al_2015_a\">Weston et al, 2015</a>), the Differentiable Neural Computer consistently reaches near state of the art performance on all of them",
        "Three problems with the current Differentiable Neural Computer revolve around the content-based look-up mechanism, which is the main memory addressing system, and the temporal linking used to read memory cells in the same order in which they were written",
        "We evaluate each of the proposed modifications empirically on a benchmark of algorithmic tasks and on bAbI (<a class=\"ref-link\" id=\"cWeston_et+al_2015_a\" href=\"#rWeston_et+al_2015_a\">Weston et al, 2015</a>)",
        "In all cases we find that our model outperforms the Differentiable Neural Computer",
        "We find that improved de-allocation together with sharpness enhancement leads to zero error and 3x faster convergence on the large repeated copy task, while Differentiable Neural Computer is not able to solve it at all",
        "De-allocation is controlled by a gate, which is based on the address distribution of the previous read and decreases the usage counter of each cell",
        "Controlling which part of the key vector to search for is difficult because there is no key-value separation: the entire key and entire cell value are compared to produce the similarity score. This means that the part of the cell value that is unknown during search time and should be retrieved is used in the normalization part of the cosine similarity, resulting in an unpredictable score",
        "The shorter the known part and the longer the part to be retrieved, the worse the problem. This might result in less similar cells having higher scores, and might make the resulting address distribution flat because of the division by the length of the data before the softmax acts as an increased temperature parameter",
        "The resulting score is normalized by the data that is stored along with the tag, which takes most of the memory cell, the resulting score will almost completely be dominated by it",
        "The Differentiable Neural Computer tracks allocation states of memory cells by so-called usage counters which are increased on memory writes and optionally decreased after reads",
        "We propose to zero out the memory contents by multiplying every cell of the memory matrix Mt with the corresponding element of the retention vector",
        "We provide equations for our Differentiable Neural Computer-DMS model in Appendix A",
        "We found that increasing N makes Differentiable Neural Computer fail to solve the task",
        "Sharpness enhancement To analyze the problem of degradation of temporal links after successive link matrix updates, we examined the forward and backward link distributions of the model with modified deallocation (DNC-D)",
        "In Table 1, we present experimental results of multiple versions of the network after 0.5M iterations with batch size 2",
        "Our models converge faster and have both lower error and lower variance than Differentiable Neural Computer. (Note that our goal was not to achieve state of the art performance (<a class=\"ref-link\" id=\"cSantoro_et+al_2017_a\" href=\"#rSantoro_et+al_2017_a\">Santoro et al, 2017</a>; <a class=\"ref-link\" id=\"cHenaff_et+al_2016_a\" href=\"#rHenaff_et+al_2016_a\">Henaff et al, 2016</a>; <a class=\"ref-link\" id=\"cDehghani_et+al_2018_a\" href=\"#rDehghani_et+al_2018_a\">Dehghani et al, 2018</a>) on bAbI",
        "We identified three drawbacks of the traditional Differentiable Neural Computer model, and proposed fixes for them",
        "Two of them are related to content-based addressing: (1) Lack of key-value separation yields uncertain and noisy address distributions resulting from content-based look-up"
    ],
    "summary": [
        "Recurrent Neural Networks (RNNs) such as LSTM (<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter & Schmidhuber, 1997</a></a>; <a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\"><a class=\"ref-link\" id=\"cGers_et+al_2000_a\" href=\"#rGers_et+al_2000_a\">Gers et al, 2000</a></a>) are in theory capable of solving complex algorithmic tasks (<a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\"><a class=\"ref-link\" id=\"cSiegelmann_1992_a\" href=\"#rSiegelmann_1992_a\">Siegelmann & Sontag, 1992</a></a>), in practice they often struggle to do so.",
        "It combines a large external memory with advanced addressing mechanisms such as content-based look-up and temporal linking of memory cells.",
        "Three problems with the current DNC revolve around the content-based look-up mechanism, which is the main memory addressing system, and the temporal linking used to read memory cells in the same order in which they were written.",
        "We find that improved de-allocation together with sharpness enhancement leads to zero error and 3x faster convergence on the large repeated copy task, while DNC is not able to solve it at all.",
        "It compares every cell to a key produced by the controller, resulting in a score, which is later normalized to get an address distribution over the whole memory.",
        "De-allocation is controlled by a gate, which is based on the address distribution of the previous read and decreases the usage counter of each cell.",
        "The final read address distribution is generated as the weighted average of the read content-based look-up distribution and forward and backward temporal links.",
        "The DNC tracks allocation states of memory cells by so-called usage counters which are increased on memory writes and optionally decreased after reads.",
        "The problem is that it affects solely the usage counters and not the actual memory Mt. But memory content plays a vital role in both read and write address generation: the content based look-up still finds de-allocated cells, resulting in memory aliasing.",
        "Fig. 2 shows the block diagram of read address generation in DNC with key masking and sharpness enhancement.",
        "The sharpness enhancement block is inserted into the forward and backward link generation path right before combining them with the content-based look-up distribution.",
        "We use the following notation: DNC is the original network Graves et al (2016), DNC-D has modified de-allocation, DNC-S has added sharpness enhancement, DNC-M has added masking in content based addressing.",
        "We hypothesize that the reason for this is the modified de-allocation: the network can store the beginning of every sequence with similar key without causing look-up conflicts, as it is guaranteed that the previously present key is wiped from memory.",
        "Sharpness enhancement To analyze the problem of degradation of temporal links after successive link matrix updates, we examined the forward and backward link distributions of the model with modified deallocation (DNC-D).",
        "When the network starts repeating a block, the weight of forward links is increased a bit, but as the distribution becomes more blurred, a pure content-based look-up is preferred.",
        "We will investigate the possibility of merging our improvements with related work (<a class=\"ref-link\" id=\"cBen-Ari_2017_a\" href=\"#rBen-Ari_2017_a\"><a class=\"ref-link\" id=\"cBen-Ari_2017_a\" href=\"#rBen-Ari_2017_a\">Ben-Ari & Bekker, 2017</a></a>; <a class=\"ref-link\" id=\"cRae_et+al_2016_a\" href=\"#rRae_et+al_2016_a\"><a class=\"ref-link\" id=\"cRae_et+al_2016_a\" href=\"#rRae_et+al_2016_a\">Rae et al, 2016</a></a>), to further improve the DNC"
    ],
    "headline": "We propose a solution to each of these problems",
    "reference_links": [
        {
            "id": "Ben-Ari_2017_a",
            "entry": "Itamar Ben-Ari and Alan Joseph Bekker. Differentiable memory allocation mechanism for neural computing. MLSLP2017, 2017. URL http://ttic.uchicago.edu/\u0303klivescu/ MLSLP2017/MLSLP2017_ben-ari.pdf.",
            "url": "http://ttic.uchicago.edu/\u0303klivescu/MLSLP2017/MLSLP2017_ben-ari.pdf"
        },
        {
            "id": "Das_et+al_1992_a",
            "entry": "S. Das, C.L. Giles, and G.Z. Sun. Learning context-free grammars: Capabilities and limitations of a neural network with an external stack memory. In Proceedings of the The Fourteenth Annual Conference of the Cognitive Science Society, Bloomington, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20S.%20Giles%2C%20C.L.%20Sun%2C%20G.Z.%20Learning%20context-free%20grammars%3A%20Capabilities%20and%20limitations%20of%20a%20neural%20network%20with%20an%20external%20stack%20memory%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20S.%20Giles%2C%20C.L.%20Sun%2C%20G.Z.%20Learning%20context-free%20grammars%3A%20Capabilities%20and%20limitations%20of%20a%20neural%20network%20with%20an%20external%20stack%20memory%201992"
        },
        {
            "id": "Dehghani_et+al_2018_a",
            "entry": "M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and \u0141. Kaiser. Universal Transformers. ArXiv e-prints, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dehghani%2C%20M.%20Gouws%2C%20S.%20Vinyals%2C%20O.%20Uszkoreit%2C%20J.%20Universal%20Transformers.%20ArXiv%20e-prints%202018-07"
        },
        {
            "id": "Franke_et+al_2018_a",
            "entry": "Jrg Franke, Jan Niehues, and Alex Waibel. Robust and scalable differentiable neural computer for question answering. (arXiv:1807.02658 [cs.CL]), 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.02658"
        },
        {
            "id": "Gers_et+al_2000_a",
            "entry": "F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 12(10):2451\u20132471, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000"
        },
        {
            "id": "Graves_2016_a",
            "entry": "Alex Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983.",
            "url": "http://arxiv.org/abs/1603.08983",
            "arxiv_url": "https://arxiv.org/pdf/1603.08983"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401.",
            "url": "http://arxiv.org/abs/1410.5401",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_b",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): 471\u2013476, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "Henaff_et+al_2016_a",
            "entry": "Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. CoRR, abs/1612.03969, 2016. URL http://arxiv.org/abs/1612.03969.",
            "url": "http://arxiv.org/abs/1612.03969",
            "arxiv_url": "https://arxiv.org/pdf/1612.03969"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735\u2013 1780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997"
        },
        {
            "id": "Miller_et+al_2016_a",
            "entry": "Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. CoRR, abs/1606.03126, 2016. URL http://arxiv.org/abs/1606.03126.",
            "url": "http://arxiv.org/abs/1606.03126",
            "arxiv_url": "https://arxiv.org/pdf/1606.03126"
        },
        {
            "id": "Mozer_1993_a",
            "entry": "Michael C Mozer and Sreerupa Das. A connectionist symbol manipulator that discovers the structure of context-free languages. Advances in Neural Information Processing Systems (NIPS), pp. 863\u2013 863, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mozer%2C%20Michael%20C.%20Das%2C%20Sreerupa%20A%20connectionist%20symbol%20manipulator%20that%20discovers%20the%20structure%20of%20context-free%20languages%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mozer%2C%20Michael%20C.%20Das%2C%20Sreerupa%20A%20connectionist%20symbol%20manipulator%20that%20discovers%20the%20structure%20of%20context-free%20languages%201993"
        },
        {
            "id": "Rae_et+al_2016_a",
            "entry": "Jack W. Rae, Jonathan J. Hunt, Tim Harley, Ivo Danihelka, Andrew W. Senior, Greg Wayne, Alex Graves, and Timothy P. Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. CoRR, abs/1610.09027, 2016. URL http://arxiv.org/abs/1610.09027.",
            "url": "http://arxiv.org/abs/1610.09027",
            "arxiv_url": "https://arxiv.org/pdf/1610.09027"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy P. Lillicrap. A simple neural network module for relational reasoning. CoRR, abs/1706.01427, 2017. URL http://arxiv.org/abs/1706.01427.",
            "url": "http://arxiv.org/abs/1706.01427",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Schmidhuber_2012_a",
            "entry": "J. Schmidhuber. Self-delimiting neural networks. Technical Report IDSIA-08-12, arXiv:1210.0118v1 [cs.NE], The Swiss AI Lab IDSIA, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1210.0118v1"
        },
        {
            "id": "Siegelmann_1992_a",
            "entry": "Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT \u201992, pp. 440\u2013449, New York, NY, USA, 1992. ACM. ISBN 0-89791-497-X. doi: 10.1145/130385.130432. URL http://doi.acm.org/10.1145/130385.130432.",
            "crossref": "https://dx.doi.org/10.1145/130385.130432",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/130385.130432"
        },
        {
            "id": "Sukhbaatar_et+al_2015_a",
            "entry": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. Weakly supervised memory networks. CoRR, abs/1503.08895, 2015. URL http://arxiv.org/abs/1503.08895.",
            "url": "http://arxiv.org/abs/1503.08895",
            "arxiv_url": "https://arxiv.org/pdf/1503.08895"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Rmsprop: divide the gradient by a running average of its recent magnitude. Coursera, pp. 26\u201330, 2012. URL http://www.cs.toronto.edu/\u0303tijmen/csc321/slides/lecture_slides_lec6.pdf.",
            "url": "http://www.cs.toronto.edu/\u0303tijmen/csc321/slides/lecture_slides_lec6.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Rmsprop%3A%20divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Weston_et+al_2015_a",
            "entry": "Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. CoRR, abs/1502.05698, 2015. URL http://arxiv.org/abs/1502.05698.",
            "url": "http://arxiv.org/abs/1502.05698",
            "arxiv_url": "https://arxiv.org/pdf/1502.05698"
        },
        {
            "id": "Here_2016_a",
            "entry": "Here we present the equations for our full model (DNC-MDS). The other models can be easily implemented in line with details of Section 3. We also highlight differences to the DNC by Graves et al. (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Here%20we%20present%20the%20equations%20for%20our%20full%20model%20DNCMDS%20The%20other%20models%20can%20be%20easily%20implemented%20in%20line%20with%20details%20of%20Section%203%20We%20also%20highlight%20differences%20to%20the%20DNC%20by%20Graves%20et%20al%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Here%20we%20present%20the%20equations%20for%20our%20full%20model%20DNCMDS%20The%20other%20models%20can%20be%20easily%20implemented%20in%20line%20with%20details%20of%20Section%203%20We%20also%20highlight%20differences%20to%20the%20DNC%20by%20Graves%20et%20al%202016"
        },
        {
            "id": "Compare_2016_b",
            "entry": "Compare this to the C(M, k, \u03b2, m)[i] = sof tmax(D (k, M )\u03b2) of Graves et al. (2016). Where D is the row-wise cosine similarity with numerical stabilization: u \u00b7 M [i, \u00b7]",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Compare%20this%20to%20the%20CM%20k%20%CE%B2%20mi%20%20sof%20tmaxD%20k%20M%20%CE%B2%20of%20Graves%20et%20al%202016%20Where%20D%20is%20the%20rowwise%20cosine%20similarity%20with%20numerical%20stabilization%20u%20%20M%20i",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Compare%20this%20to%20the%20CM%20k%20%CE%B2%20mi%20%20sof%20tmaxD%20k%20M%20%CE%B2%20of%20Graves%20et%20al%202016%20Where%20D%20is%20the%20rowwise%20cosine%20similarity%20with%20numerical%20stabilization%20u%20%20M%20i"
        },
        {
            "id": "E_2016_a",
            "entry": "Compare this to the Mt = Mt\u22121 (E \u2212 wtwet ) + wtwvt of Graves et al. (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Compare%20this%20to%20the%20Mt%20%20Mt1%20E%20%20wtwet%20%20%20wtwvt%20of%20Graves%20et%20al%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Compare%20this%20to%20the%20Mt%20%20Mt1%20E%20%20wtwet%20%20%20wtwvt%20of%20Graves%20et%20al%202016"
        },
        {
            "id": "Compare_2016_c",
            "entry": "Compare this to the fti = Ltwtr\u2212,i1 and bit = Lt wtr\u2212,i1 of Graves et al. (2016). The read address distribution is given by: crt,i = C Mt, ktr,i, \u03b2tr,i, mtr,i",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Compare%20this%20to%20the%20fti%20%20Ltwtri1%20and%20bit%20%20Lt%20wtri1%20of%20Graves%20et%20al%202016%20The%20read%20address%20distribution%20is%20given%20by%20crti%20%20C%20Mt%20ktri%20%CE%B2tri%20mtri",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Compare%20this%20to%20the%20fti%20%20Ltwtri1%20and%20bit%20%20Lt%20wtri1%20of%20Graves%20et%20al%202016%20The%20read%20address%20distribution%20is%20given%20by%20crti%20%20C%20Mt%20ktri%20%CE%B2tri%20mtri"
        }
    ]
}
