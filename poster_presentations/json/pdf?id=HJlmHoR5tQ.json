{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADVERSARIAL IMITATION VIA VARIATIONAL INVERSE REINFORCEMENT LEARNING",
        "author": "Ahmed H. Qureshi Department of Electrical and Computer Engineering University of California San Diego, La Jolla, CA 92093, USA a1qureshi@ucsd.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJlmHoR5tQ"
        },
        "abstract": "We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns nearoptimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "Mutual information",
            "url": "https://en.wikipedia.org/wiki/Mutual_information"
        },
        {
            "term": "potential function",
            "url": "https://en.wikipedia.org/wiki/potential_function"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "maximum entropy",
            "url": "https://en.wikipedia.org/wiki/maximum_entropy"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "abbreviations": {
        "RL": "Reinforcement learning",
        "IRL": "Inverse reinforcement learning",
        "MaxEnt": "maximum entropy",
        "MDP": "Markov decision process",
        "IL": "imitation learning",
        "BC": "behavior cloning",
        "GAIL": "generative adversarial imitation learning",
        "GANs": "Generative Adversarial Networks",
        "AIRL": "Adversarial Inverse Reinforcement Learning",
        "EAIRL": "empowerment-regularized adversarial inverse reinforcement learning",
        "MI": "Mutual information"
    },
    "highlights": [
        "Reinforcement learning (RL) has emerged as a promising tool for solving complex decision-making and control tasks from predefined high-level reward functions (Sutton et al, 1998)",
        "We propose the empowerment-regularized adversarial inverse reinforcement learning (EAIRL) algorithm1",
        "In the scalable maximum entropy-Inverse reinforcement learning framework (Finn et al, 2016a; <a class=\"ref-link\" id=\"cFu_et+al_2017_a\" href=\"#rFu_et+al_2017_a\">Fu et al, 2017</a>), the normalization term is approximated by importance sampling where the importance-sampler/policy is trained to minimize the KL-divergence from the distribution over expert trajectories",
        "We present an approach to adversarial reward and policy learning from expert demonstrations by regularizing the maximum-entropy inverse reinforcement learning through empowerment",
        "Our method learns the empowerment through variational information maximization in parallel to learning the reward and policy",
        "We show that our method successfully learns near-optimal rewards, policies, and performs significantly better than state-of-the-art Inverse reinforcement learning methods in both imitation learning and challenging transfer learning problems"
    ],
    "key_statements": [
        "Reinforcement learning (RL) has emerged as a promising tool for solving complex decision-making and control tasks from predefined high-level reward functions (Sutton et al, 1998)",
        "We propose the empowerment-regularized adversarial inverse reinforcement learning (EAIRL) algorithm1",
        "We present an inverse reinforcement learning algorithm that learns a robust, transferable reward function and policy from expert demonstrations",
        "This section highlights the importance of empowerment-regularized maximum entropy-Inverse reinforcement learning and modeling rewards as a function of both state and action rather than restricting to state-only formulation on learning rewards and policies from expert demonstrations",
        "In the scalable maximum entropy-Inverse reinforcement learning framework (Finn et al, 2016a; <a class=\"ref-link\" id=\"cFu_et+al_2017_a\" href=\"#rFu_et+al_2017_a\">Fu et al, 2017</a>), the normalization term is approximated by importance sampling where the importance-sampler/policy is trained to minimize the KL-divergence from the distribution over expert trajectories",
        "We present an approach to adversarial reward and policy learning from expert demonstrations by regularizing the maximum-entropy inverse reinforcement learning through empowerment",
        "Our method learns the empowerment through variational information maximization in parallel to learning the reward and policy",
        "We show that our policy is trained to imitate the expert behavior as well to maximize the empowerment of the agent over the environment",
        "We show that our method successfully learns near-optimal rewards, policies, and performs significantly better than state-of-the-art Inverse reinforcement learning methods in both imitation learning and challenging transfer learning problems",
        "The learned rewards are shown to be transferable to environments that are dynamically or structurally different from training environments"
    ],
    "summary": [
        "Reinforcement learning (RL) has emerged as a promising tool for solving complex decision-making and control tasks from predefined high-level reward functions (Sutton et al, 1998).",
        "The results on reward learning show that EAIRL outperforms several state-of-the-art IRL methods by recovering reward functions that leads to optimal, expert-matching behaviors.",
        "An alternative way to express our policy training objective is E\u03c4 [log \u03c0\u03b8(a|s)r\u03c0(s, a, s )], where r\u03c0(s, a, s ) = r(s, a, s ) \u2212 \u03bbI lI (s, a, s ), Algorithm 1: Empowerment-based Adversarial Inverse Reinforcement Learning Initialize parameters of policy \u03c0\u03b8, and inverse model q\u03c6 Initialize parameters of target \u03a6\u03c6 and training \u03a6\u03c6 empowerment, and reward r\u03be functions Obtain expert demonstrations \u03c4E by running expert policy \u03c0E for i \u2190 0 to N do",
        "It can be seen that our method recovers near-optimal reward functions as the policy scores almost reach the expert scores in all five trials even after transfering to unseen testing environments.",
        "This section highlights the importance of empowerment-regularized MaxEnt-IRL and modeling rewards as a function of both state and action rather than restricting to state-only formulation on learning rewards and policies from expert demonstrations.",
        "In the scalable MaxEnt-IRL framework (Finn et al, 2016a; <a class=\"ref-link\" id=\"cFu_et+al_2017_a\" href=\"#rFu_et+al_2017_a\">Fu et al, 2017</a>), the normalization term is approximated by importance sampling where the importance-sampler/policy is trained to minimize the KL-divergence from the distribution over expert trajectories.",
        "The agent shows aggressive behavior and sometimes flips over after few steps, which is the reason that crippled-ant trained with AIRL\u2019s disentangled reward function reaches only the half-way to expert scores as shown in Table 1.",
        "Table 2 shows that methods with state-action reward/discriminator formulation can successfully recover expert-like policies.",
        "Our empirical results show that it is crucial to model reward/discriminator as a function of state-action as otherwise, adversarial imitation learning fails to learn ground-truth rewards and expert-like policies from expert data.",
        "We present an approach to adversarial reward and policy learning from expert demonstrations by regularizing the maximum-entropy inverse reinforcement learning through empowerment.",
        "The proposed regularization prevents premature convergence to local behavior and leads to a generalized policy that in turn guides the reward-learning process to recover near-optimal reward.",
        "We show that our method successfully learns near-optimal rewards, policies, and performs significantly better than state-of-the-art IRL methods in both imitation learning and challenging transfer learning problems.",
        "We plan to extend our method to learn rewards and policies from diverse human/expert demonstrations as the proposed method assumes that a single expert generates the training data.",
        "The learned rewards are shown to be transferable to environments that are dynamically or structurally different from training environments"
    ],
    "headline": "We evaluate our approach on various high-dimensional complex control tasks",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20Proceedings%20of%20the%20twentyfirst%20international%20conference%20on%20Machine%20learning%20pp%201%20ACM%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20Proceedings%20of%20the%20twentyfirst%20international%20conference%20on%20Machine%20learning%20pp%201%20ACM%202004"
        },
        {
            "id": "Agakov_2004_a",
            "entry": "David Barber Felix Agakov. The im algorithm: a variational approach to information maximization. Advances in Neural Information Processing Systems, 16:201, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agakov%2C%20David%20Barber%20Felix%20The%20im%20algorithm%3A%20a%20variational%20approach%20to%20information%20maximization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agakov%2C%20David%20Barber%20Felix%20The%20im%20algorithm%3A%20a%20variational%20approach%20to%20information%20maximization%202004"
        },
        {
            "id": "Argall_et+al_2009_a",
            "entry": "Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Boularias_et+al_2011_a",
            "entry": "Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 182\u2013189, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boularias%2C%20Abdeslam%20Kober%2C%20Jens%20Peters%2C%20Jan%20Relative%20entropy%20inverse%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boularias%2C%20Abdeslam%20Kober%2C%20Jens%20Peters%2C%20Jan%20Relative%20entropy%20inverse%20reinforcement%20learning%202011"
        },
        {
            "id": "Finn_et+al_0000_a",
            "entry": "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03852"
        },
        {
            "id": "Finn_et+al_0000_b",
            "entry": "Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pp. 49\u201358, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Kalakrishnan_et+al_2013_a",
            "entry": "Mrinal Kalakrishnan, Peter Pastor, Ludovic Righetti, and Stefan Schaal. Learning objective functions for manipulation. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pp. 1331\u20131336. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalakrishnan%2C%20Mrinal%20Pastor%2C%20Peter%20Righetti%2C%20Ludovic%20Schaal%2C%20Stefan%20Learning%20objective%20functions%20for%20manipulation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalakrishnan%2C%20Mrinal%20Pastor%2C%20Peter%20Righetti%2C%20Ludovic%20Schaal%2C%20Stefan%20Learning%20objective%20functions%20for%20manipulation%202013"
        },
        {
            "id": "Kuderer_et+al_2015_a",
            "entry": "Markus Kuderer, Shilpa Gulati, and Wolfram Burgard. Learning driving styles for autonomous vehicles from demonstration. In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 2641\u20132646. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuderer%2C%20Markus%20Gulati%2C%20Shilpa%20Burgard%2C%20Wolfram%20Learning%20driving%20styles%20for%20autonomous%20vehicles%20from%20demonstration%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuderer%2C%20Markus%20Gulati%2C%20Shilpa%20Burgard%2C%20Wolfram%20Learning%20driving%20styles%20for%20autonomous%20vehicles%20from%20demonstration%202015"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mohamed_2015_a",
            "entry": "Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 2125\u20132133, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pp. 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Pomerleau_1991_a",
            "entry": "Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88\u201397, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pomerleau%2C%20Dean%20A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pomerleau%2C%20Dean%20A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%201991"
        },
        {
            "id": "Qureshi_et+al_2017_a",
            "entry": "Ahmed. H Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, and Hiroshi Ishiguro. Show, attend and interact: Perceivable human-robot social interaction through neural attention q-network. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 1639\u20131645. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qureshi%2C%20Ahmed%20H.%20Nakamura%2C%20Yutaka%20Yoshikawa%2C%20Yuichiro%20Show%2C%20Hiroshi%20Ishiguro%20attend%20and%20interact%3A%20Perceivable%20human-robot%20social%20interaction%20through%20neural%20attention%20q-network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qureshi%2C%20Ahmed%20H.%20Nakamura%2C%20Yutaka%20Yoshikawa%2C%20Yuichiro%20Show%2C%20Hiroshi%20Ishiguro%20attend%20and%20interact%3A%20Perceivable%20human-robot%20social%20interaction%20through%20neural%20attention%20q-network%202017"
        },
        {
            "id": "Qureshi_et+al_2018_a",
            "entry": "Ahmed. H Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, and Hiroshi Ishiguro. Intrinsically motivated reinforcement learning for human\u2013robot interaction in the real-world. Neural Networks, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qureshi%2C%20Ahmed%20H.%20Nakamura%2C%20Yutaka%20Yoshikawa%2C%20Yuichiro%20Ishiguro%2C%20Hiroshi%20Intrinsically%20motivated%20reinforcement%20learning%20for%20human%E2%80%93robot%20interaction%20in%20the%20real-world%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qureshi%2C%20Ahmed%20H.%20Nakamura%2C%20Yutaka%20Yoshikawa%2C%20Yuichiro%20Ishiguro%2C%20Hiroshi%20Intrinsically%20motivated%20reinforcement%20learning%20for%20human%E2%80%93robot%20interaction%20in%20the%20real-world%202018"
        },
        {
            "id": "Ratliff_2006_a",
            "entry": "Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning, pp. 729\u2013736. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nathan%20D%20Ratliff%20J%20Andrew%20Bagnell%20and%20Martin%20A%20Zinkevich%20Maximum%20margin%20planning%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20729736%20ACM%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nathan%20D%20Ratliff%20J%20Andrew%20Bagnell%20and%20Martin%20A%20Zinkevich%20Maximum%20margin%20planning%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20729736%20ACM%202006"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Introduction%20to%20reinforcement%20learning%2C%20volume%20135%201998"
        },
        {
            "id": "Chicago_2008_a",
            "entry": "Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chicago%20IL%20USA%202008"
        }
    ]
}
