{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PRACTICAL LOSSLESS COMPRESSION WITH LATENT VARIABLES USING BITS BACK CODING",
        "author": "James Townsend, Thomas Bird & David Barber Department of Computer Science University College London {james.townsend,thomas.bird,david.barber}@cs.ucl.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryE98iR5tm"
        },
        "abstract": "Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present \u2018Bits Back with ANS\u2019 (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational autoencoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back."
    },
    "keywords": [
        {
            "term": "information theory",
            "url": "https://en.wikipedia.org/wiki/information_theory"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "latent variable model",
            "url": "https://en.wikipedia.org/wiki/latent_variable_model"
        },
        {
            "term": "arithmetic coding",
            "url": "https://en.wikipedia.org/wiki/arithmetic_coding"
        },
        {
            "term": "lossless compression",
            "url": "https://en.wikipedia.org/wiki/lossless_compression"
        },
        {
            "term": "probabilistic model",
            "url": "https://en.wikipedia.org/wiki/probabilistic_model"
        },
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "asymmetric numeral systems",
            "url": "https://en.wikipedia.org/wiki/asymmetric_numeral_systems"
        },
        {
            "term": "lossy compression",
            "url": "https://en.wikipedia.org/wiki/lossy_compression"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "BB-ANS": "Bits Back with ANS\u2019",
        "VAE": "variational auto-encoder",
        "AC": "Arithmetic coding",
        "ANS": "asymmetric numeral systems",
        "ELBO": "evidence lower bound",
        "MAP": "maximum a posteriori"
    },
    "highlights": [
        "The connections between information theory and machine learning have long been known to be deep, and the two fields are so closely related that they have been described as \u2018two sides of the same coin\u2019 (<a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\"><a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\">Mackay, 2003</a></a>)",
        "The field of machine learning has experienced an explosion of activity in recent years, and we have seen a number of papers looking at applications of modern deep learning methods to lossy compression",
        "We have shown the existence of a scheme, Bits Back with asymmetric numeral systems\u2019, which can be used for lossless compression using latent variable models",
        "We demonstrated Bits Back with asymmetric numeral systems\u2019 by compressing the MNIST dataset, achieving compression rates superior to generic algorithms",
        "We were able to compress to sizes very close to the negative evidence lower bound for a large dataset. This is the first time this has been achieved with a latent variable model, and implies that state-of-the-art latent variable models could be used in conjunction with Bits Back with asymmetric numeral systems\u2019 to achieve significantly better lossless compression rates than current methods",
        "Given that all components of Bits Back with asymmetric numeral systems\u2019 are readily parallelizable, we believe that Bits Back with asymmetric numeral systems\u2019 can be implemented to run on GPU hardware, yielding a fast and powerful lossless compression system"
    ],
    "key_statements": [
        "The connections between information theory and machine learning have long been known to be deep, and the two fields are so closely related that they have been described as \u2018two sides of the same coin\u2019 (<a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\"><a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\">Mackay, 2003</a></a>)",
        "The field of machine learning has experienced an explosion of activity in recent years, and we have seen a number of papers looking at applications of modern deep learning methods to lossy compression",
        "Our scheme improves on existing implementations of bits back coding in terms of compression rate and code complexity, allowing for efficient lossless compression of arbitrarily large datasets with deep latent variable models",
        "We demonstrate the efficiency of Bits Back with asymmetric numeral systems\u2019 by losslessly compressing the MNIST dataset with a variational auto-encoder (VAE), a deep latent variable model with continuous latent variables (Kingma and Welling, 2013; <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a>)",
        "We find that Bits Back with asymmetric numeral systems\u2019 with a variational auto-encoder outperforms generic compression algorithms for both binarized and raw MNIST, even with a very simple model architecture",
        "Arithmetic coding (AC) and asymmetric numeral systems (ANS) are algorithms which solve this problem, providing an encoding from the sequence s to a sequence of bits, and a decoding to recover the original data s",
        "The central insight of this work is to notice that the chaining described in the previous section can be implemented straightforwardly with asymmetric numeral systems with zero compression rate overhead per iteration. This is because of the fact that asymmetric numeral systems is stack-like by nature, which resolves the problems that occur if one tries to implement bits back chaining with Arithmetic coding, which is queue-like. We describe this novel method, which we refer to as \u2018Bits Back with asymmetric numeral systems\u2019 (BB-asymmetric numeral systems)",
        "We present here a derivation, based on <a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\">Mackay (2003</a>), of the surprising fact that continuous latents can be coded with bits back, up to arbitrary precision, without affecting the coding rate",
        "Given the progress within this area, it is of interest to study the application of probabilistic models to lossless compression",
        "We have shown the existence of a scheme, Bits Back with asymmetric numeral systems\u2019, which can be used for lossless compression using latent variable models",
        "We demonstrated Bits Back with asymmetric numeral systems\u2019 by compressing the MNIST dataset, achieving compression rates superior to generic algorithms",
        "We have shown how to handle the issue of latent discretization",
        "We were able to compress to sizes very close to the negative evidence lower bound for a large dataset. This is the first time this has been achieved with a latent variable model, and implies that state-of-the-art latent variable models could be used in conjunction with Bits Back with asymmetric numeral systems\u2019 to achieve significantly better lossless compression rates than current methods",
        "Given that all components of Bits Back with asymmetric numeral systems\u2019 are readily parallelizable, we believe that Bits Back with asymmetric numeral systems\u2019 can be implemented to run on GPU hardware, yielding a fast and powerful lossless compression system"
    ],
    "summary": [
        "The connections between information theory and machine learning have long been known to be deep, and the two fields are so closely related that they have been described as \u2018two sides of the same coin\u2019 (<a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\"><a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\">Mackay, 2003</a></a>).",
        "Our scheme improves on existing implementations of bits back coding in terms of compression rate and code complexity, allowing for efficient lossless compression of arbitrarily large datasets with deep latent variable models.",
        "We find that BB-ANS with a VAE outperforms generic compression algorithms for both binarized and raw MNIST, even with a very simple model architecture.",
        "We describe bits back coding, a method for lossless compression of data using a latent variable model.",
        "Arithmetic coding (AC) and asymmetric numeral systems (ANS) are algorithms which solve this problem, providing an encoding from the sequence s to a sequence of bits, and a decoding to recover the original data s.",
        "We assume access to a coding scheme such as AC or ANS which can be used to encode and decode symbols according to any distribution.",
        "Bits back coding has previously been implemented only for models with discrete latent variables, in Frey (1997).",
        "Note that the number of bits required to generate the latent sample scales with the precision \u2212 log \u03b4y, meaning reasonably small precisions should be preferred in practice.",
        "We use the bits at the top of the ANS stack, which are the result of coding the previous latent y0 according to the prior p(y).",
        "The usual VAE training objective is the ELBO, which, as we noted in Section 2.2, is the negative of the expected message length with bits back coding.",
        "Instead of directly sampling the first latents at random, to simplify our implementation we instead initialize the BB-ANS chain with a supply of \u2018clean\u2019 bits.",
        "This means that we can predict the best currently achievable compression using BB-ANS from the reported values of the negative ELBO for state-of-the-art latent variable models.",
        "In principal, be coded with BB-ANS, but the number of \u2018extra bits\u2019 needed in a naive implementation scales with the length of the chain, which could lead to a highly sub-optimal compression rate in practice.",
        "During encoding/decoding the compression/decompression code is a computational bottleneck, running orders of magnitude slower than the computations of the model probabilities.",
        "We have shown the existence of a scheme, BB-ANS, which can be used for lossless compression using latent variable models.",
        "This is the first time this has been achieved with a latent variable model, and implies that state-of-the-art latent variable models could be used in conjunction with BB-ANS to achieve significantly better lossless compression rates than current methods.",
        "Given that all components of BB-ANS are readily parallelizable, we believe that BB-ANS can be implemented to run on GPU hardware, yielding a fast and powerful lossless compression system"
    ],
    "headline": "We demonstrate this scheme by using it to compress the MNIST dataset with a variational autoencoder model, achieving compression rates superior to standard methods with only a simple variational auto-encoder",
    "reference_links": [
        {
            "id": "Balle_et+al_2017_a",
            "entry": "Balle, J., Laparra, V., and Simoncelli, E. P. (2017). End-to-end Optimized Image Compression. In Proceedings of the International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balle%2C%20J.%20Laparra%2C%20V.%20Simoncelli%2C%20E.P.%20End-to-end%20Optimized%20Image%20Compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balle%2C%20J.%20Laparra%2C%20V.%20Simoncelli%2C%20E.P.%20End-to-end%20Optimized%20Image%20Compression%202017"
        },
        {
            "id": "Balle_et+al_2018_a",
            "entry": "Balle, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston, N. (2018). Variational image compression with a scale hyperprior. In Proceedings of the International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balle%2C%20J.%20Minnen%2C%20D.%20Singh%2C%20S.%20Hwang%2C%20S.J.%20Variational%20image%20compression%20with%20a%20scale%20hyperprior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balle%2C%20J.%20Minnen%2C%20D.%20Singh%2C%20S.%20Hwang%2C%20S.J.%20Variational%20image%20compression%20with%20a%20scale%20hyperprior%202018"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: a large-scale hierarchical image database. In CVPR09.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20a%20large-scale%20hierarchical%20image%20database.%20In%20CVPR09%202009"
        },
        {
            "id": "Duda_2009_a",
            "entry": "Duda, J. (2009). Asymmetric numeral systems. In ArXiv e-prints. arXiv: 0902.0271 [cs.IT]. Frey, B. (1997). Bayesian networks for pattern classification, data compression, and channel coding.",
            "arxiv_url": "https://arxiv.org/pdf/0902.0271"
        },
        {
            "id": "Frey_1996_a",
            "entry": "Frey, B. and Hinton, G. (1996). Free Energy Coding. In Proceedings of the Data Compression",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frey%2C%20B.%20Hinton%2C%20G.%20Free%20Energy%20Coding%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frey%2C%20B.%20Hinton%2C%20G.%20Free%20Energy%20Coding%201996"
        },
        {
            "id": "Giesen_2014_a",
            "entry": "Giesen, F. (2014). Interleaved entropy coders. In ArXiv e-prints. arXiv: 1402.3392 [cs.IT]. Gregor, K., Besse, F., Rezende, D. J., Danihelka, I., and Wierstra, D. (2016). Towards conceptual compression. In Advances in Neural Information Processing Systems (NIPS). Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., and Courville, A. (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1402.3392"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Johnson, M., Duvenaud, D., Wiltschko, A. B., Datta, S. R., and Adams, R. P. (2016). Composing graphical models with neural networks for structured representations and fast inference. In Advances in Neural Information Processing Systems (NIPS). Kingma, D. P. and Welling, M. (2013). Auto-Encoding Variational Bayes. In Proceedings of the International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20M.%20Duvenaud%2C%20D.%20Wiltschko%2C%20A.B.%20Datta%2C%20S.R.%20Composing%20graphical%20models%20with%20neural%20networks%20for%20structured%20representations%20and%20fast%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20M.%20Duvenaud%2C%20D.%20Wiltschko%2C%20A.B.%20Datta%2C%20S.R.%20Composing%20graphical%20models%20with%20neural%20networks%20for%20structured%20representations%20and%20fast%20inference%202016"
        },
        {
            "id": "Krajcevski_et+al_2016_a",
            "entry": "Krajcevski, P., Pratapa, S., and Manocha, D. (2016). GST: GPU-decodable Supercompressed Textures. In Proceedings of ACM SIGGRAPH Asia.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krajcevski%2C%20P.%20Pratapa%2C%20S.%20Manocha%2C%20D.%20GST%3A%20GPU-decodable%20Supercompressed%20Textures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krajcevski%2C%20P.%20Pratapa%2C%20S.%20Manocha%2C%20D.%20GST%3A%20GPU-decodable%20Supercompressed%20Textures%202016"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. In Proceedings of the IEEE. Vol. 86. 11, pp. 2278\u20132324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Mackay_2003_a",
            "entry": "Mackay, D. (2003). Information Theory, Inference and Learning Algorithms. Cambridge University Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mackay%2C%20D.%20Information%20Theory%2C%20Inference%20and%20Learning%20Algorithms%202003"
        },
        {
            "id": "Minnen_et+al_2018_a",
            "entry": "Minnen, D., Balle, J., and Toderici, G. (2018). Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minnen%2C%20D.%20Balle%2C%20J.%20Toderici%2C%20G.%20Joint%20autoregressive%20and%20hierarchical%20priors%20for%20learned%20image%20compression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minnen%2C%20D.%20Balle%2C%20J.%20Toderici%2C%20G.%20Joint%20autoregressive%20and%20hierarchical%20priors%20for%20learned%20image%20compression%202018"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Salakhutdinov_2008_a",
            "entry": "Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of Deep Belief Networks. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Murray%2C%20I%20On%20the%20quantitative%20analysis%20of%20Deep%20Belief%20Networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Murray%2C%20I%20On%20the%20quantitative%20analysis%20of%20Deep%20Belief%20Networks%202008"
        },
        {
            "id": "Shannon_1948_a",
            "entry": "Shannon, C. (1948). A mathematical theory of communication. In Bell System Technical Journal 27, pp. 379\u2013423, 623\u2013656.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.%20A%20mathematical%20theory%20of%20communication.%20In%20Bell%20System%20Technical%201948",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.%20A%20mathematical%20theory%20of%20communication.%20In%20Bell%20System%20Technical%201948"
        },
        {
            "id": "Theis_et+al_2017_a",
            "entry": "Theis, L., Shi, W., Cunningham, A., and Huszar, F. (2017). Lossy image compression with Compressive Autoencoders. In Proceedings of the International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20L.%20Shi%2C%20W.%20Cunningham%2C%20A.%20Huszar%2C%20F.%20Lossy%20image%20compression%20with%20Compressive%20Autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20L.%20Shi%2C%20W.%20Cunningham%2C%20A.%20Huszar%2C%20F.%20Lossy%20image%20compression%20with%20Compressive%20Autoencoders%202017"
        },
        {
            "id": "Tschannen_et+al_2018_a",
            "entry": "Tschannen, M., Agustsson, E., and Lucic, M. (2018). Deep generative models for distributionpreserving lossy compression. In Advances in Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tschannen%2C%20M.%20Agustsson%2C%20E.%20Lucic%2C%20M.%20Deep%20generative%20models%20for%20distributionpreserving%20lossy%20compression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tschannen%2C%20M.%20Agustsson%2C%20E.%20Lucic%2C%20M.%20Deep%20generative%20models%20for%20distributionpreserving%20lossy%20compression%202018"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "Ullrich, K., Meeds, E., and Welling, M. (2017). Soft weight-sharing for neural network compression. In Proceedings of the International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullrich%2C%20K.%20Meeds%2C%20E.%20Welling%2C%20M.%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullrich%2C%20K.%20Meeds%2C%20E.%20Welling%2C%20M.%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel Recurrent Neural Networks. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%20A%20Kalchbrenner%20N%20and%20Kavukcuoglu%20K%202016%20Pixel%20Recurrent%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20ICML",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%20A%20Kalchbrenner%20N%20and%20Kavukcuoglu%20K%202016%20Pixel%20Recurrent%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20ICML"
        },
        {
            "id": "Wallace_1990_a",
            "entry": "Wallace, C. S. (1990). Classification by Minimum-message-length Inference. In Proceedings of the International Conference on Advances in Computing and Information (ICCI), pp. 72\u201381.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wallace%2C%20C.S.%20Classification%20by%20Minimum-message-length%20Inference%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wallace%2C%20C.S.%20Classification%20by%20Minimum-message-length%20Inference%201990"
        },
        {
            "id": "Witten_et+al_1987_a",
            "entry": "Witten, I., Neal, R., and Cleary, J. (1987). Arithmetic coding for data compression. In Communications of the ACM 30.6, pp. 520\u2013540.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Witten%2C%20I.%20Neal%2C%20R.%20Cleary%2C%20J.%20Arithmetic%20coding%20for%20data%20compression.%20In%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Witten%2C%20I.%20Neal%2C%20R.%20Cleary%2C%20J.%20Arithmetic%20coding%20for%20data%20compression.%20In%201987"
        }
    ]
}
