{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NEURAL TTS STYLIZATION WITH ADVERSARIAL AND COLLABORATIVE GAMES",
        "author": "Shuang Ma State University of New York at Buffalo Buffalo, NY shuangma@buffalo.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByzcS3AcYX"
        },
        "abstract": "The modeling of style when synthesizing natural human speech from text has been the focus of significant attention. Some state-of-the-art approaches train an encoder-decoder network on paired text and audio samples xtxt, xaud by encouraging its output to reconstruct xaud. The synthesized audio waveform is expected to contain the verbal content of xtxt and the auditory style of xaud. Unfortunately, modeling style in TTS is somewhat under-determined and training models with a reconstruction loss alone is insufficient to disentangle content and style from other factors of variation. In this work, we introduce an end-to-end TTS model that offers enhanced content-style disentanglement ability and controllability. We achieve this by combining a pairwise training procedure, an adversarial game, and a collaborative game into one training scheme. The adversarial game concentrates the true data distribution, and the collaborative game minimizes the distance between real samples and generated samples in both the original space and the latent space. As a result, our model delivers a highly controllable generator with disentangled representation. Benefiting from the separate modeling of style and content, our model can generate human fidelity speech that satisfies the desired style conditions. Our model achieves start-of-the-art results across multiple tasks, including style transfer (content and style swapping), emotion modeling, and identity transfer (fitting a new speaker\u2019s voice).1"
    },
    "keywords": [
        {
            "term": "mean opinion score",
            "url": "https://en.wikipedia.org/wiki/mean_opinion_score"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        }
    ],
    "abbreviations": {
        "GAN": "generative adversarial networks",
        "WER": "Word Error Rate",
        "MOS": "mean opinion score"
    },
    "highlights": [
        "In the past few years, we have seen exciting developments in Text-To-Speech (TTS) using deep neural networks that learn to synthesize human-like speech from text in an end-to-end fashion",
        "When modeling style in TTS, F shall be conditioned on a style variable, which can be specified in many forms such as a reference audio waveform or a label",
        "Given, the goal is to synthesize a new audio waveform that contains the textual content specified by xtxt and the auditory style specified by xaud",
        "We describe their solution via a conditional probabilistic model admitting two independent sources of variation: a content variable c1:T with T words specified by text xtxt, and a style variable s given by the reference audio xaud",
        "There is a collection of approaches that are based on Tacotron, e.g., Tacotron-prosody (Wang et al, 2017a), prosody-Tacotron (SkerryRyan et al, 2018a) and GST (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>). prosody-Tacotron uses an encoder to compute a style embedding from a reference audio waveform, where the embedding provides style information that is not provided by the text",
        "We demonstrated our approach on two TTS datasets with different auditory styles, and show that our approach establishes state-of-the-art quantitative and qualitative performance on a variety of tasks"
    ],
    "key_statements": [
        "In the past few years, we have seen exciting developments in Text-To-Speech (TTS) using deep neural networks that learn to synthesize human-like speech from text in an end-to-end fashion",
        "Modeling style is of particular importance for many practical applications such as intelligent conversational agents and assistants",
        "It is straightforward to train the model that minimizes the log-likelihood by a reconstruction loss",
        "This method makes it challenging for s to exclusively encode style because no constraints are placed on the disentanglement of style\n1Project webpage: https://researchdemopage.wixsite.com/tts-gan from content within the reference audio",
        "When modeling style in TTS, F shall be conditioned on a style variable, which can be specified in many forms such as a reference audio waveform or a label",
        "Given, the goal is to synthesize a new audio waveform that contains the textual content specified by xtxt and the auditory style specified by xaud",
        "We describe their solution via a conditional probabilistic model admitting two independent sources of variation: a content variable c1:T with T words specified by text xtxt, and a style variable s given by the reference audio xaud",
        "There is a collection of approaches that are based on Tacotron, e.g., Tacotron-prosody (Wang et al, 2017a), prosody-Tacotron (SkerryRyan et al, 2018a) and GST (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a>). prosody-Tacotron uses an encoder to compute a style embedding from a reference audio waveform, where the embedding provides style information that is not provided by the text",
        "GST shows improvement over prosody-Tacotron, which demonstrates the effectiveness of the style token layer that acts as an information bottleneck from audio input to style embeddings",
        "While as our model is adversarially trained, the generative adversarial networks loss regularizes the model, under this case, the style loss and reconstruction loss can help optimizing in a better way.\n5.2",
        "Speaker style modeling: We further evaluate the effectiveness of our approach on modeling styles by means of speaker verification",
        "Despite the fact that the i-vectors are designed for speaker classification, our model can still achieve comparable results, which suggests that our model can produce generic feature representation for various auditory styles including speaker identity",
        "Classification accuracy on synthesized samples: As the style we are modeling are all categorical, we evaluate the synthesized samples by a classification task",
        "For each of the styles individually our model was consistently rated as significantly closer to the reference. These results provide further evidence that our model can synthesize speech with the correct content and distinct auditory styles that are closer to the reference than the state-of-the-art comparison",
        "We evaluate the output using mean opinion score (MOS) naturalness tests",
        "We propose an end-to-end conditional generative model for TTS style modeling",
        "The proposed model is built upon Tacotron, with an enhanced content-style disentanglement ability and controllability",
        "We demonstrated our approach on two TTS datasets with different auditory styles, and show that our approach establishes state-of-the-art quantitative and qualitative performance on a variety of tasks"
    ],
    "summary": [
        "In the past few years, we have seen exciting developments in Text-To-Speech (TTS) using deep neural networks that learn to synthesize human-like speech from text in an end-to-end fashion.",
        "The recently proposed Tacotron-based approaches (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\"><a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a></a>; <a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\">Skerry-Ryan et al, 2018a</a></a></a>) use a piece of reference speech audio to specify the expected style.",
        "\u2192 x, we adopt a pairwise training procedure to enforce our model to correctly map input text to two different audio references, i.e.",
        "Tacotron-based systems (<a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\"><a class=\"ref-link\" id=\"cWang_et+al_2018_a\" href=\"#rWang_et+al_2018_a\">Wang et al, 2018</a></a>; <a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\">Skerry-Ryan et al, 2018a</a></a></a>) solve this with a reconstruction loss by training on paired data xtxt and xaud.",
        "We describe their solution via a conditional probabilistic model admitting two independent sources of variation: a content variable c1:T with T words specified by text xtxt, and a style variable s given by the reference audio xaud.",
        "Prosody-Tacotron uses an encoder to compute a style embedding from a reference audio waveform, where the embedding provides style information that is not provided by the text.",
        "We compare our method with three state-of-the-art approaches: prosody-Tacotron (<a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\"><a class=\"ref-link\" id=\"cSkerry-Ryan_et+al_2018_a\" href=\"#rSkerry-Ryan_et+al_2018_a\">Skerry-Ryan et al, 2018a</a></a></a>) is similar to our model but trained on the reconstruction loss only.",
        "This motivates us to evaluate our model with the task of reconstructing audio samples from style embeddings.",
        "We use precomputed style embeddings from different approaches and train only the decoder network using the reconstruction loss.",
        "Prosody-Tacotron achieves the lowest reconstruction error, suggesting that the approach has the weakest ability to disentangle style from other factors in audio input.",
        "GST shows improvement over prosody-Tacotron, which demonstrates the effectiveness of the style token layer that acts as an information bottleneck from audio input to style embeddings.",
        "While as our model is adversarially trained, the GAN loss regularizes the model, under this case, the style loss and reconstruction loss can help optimizing in a better way.",
        "Despite the fact that the i-vectors are designed for speaker classification, our model can still achieve comparable results, which suggests that our model can produce generic feature representation for various auditory styles including speaker identity.",
        "Each listened to all 60 permutation of content and rated each set of audio style comparing the result of our model versus the prosody-Tacotron model.",
        "These results provide further evidence that our model can synthesize speech with the correct content and distinct auditory styles that are closer to the reference than the state-of-the-art comparison.",
        "The proposed model is built upon Tacotron, with an enhanced content-style disentanglement ability and controllability.",
        "The proposed pairwise training approach that involves a adversarial game and a collaborative game together, result in a highly controllable generator with disentangled representations.",
        "The requirements for a lot of work on aligning text and audios can be much released"
    ],
    "headline": "We introduce an end-to-end TTS model that offers enhanced content-style disentanglement ability and controllability",
    "reference_links": [
        {
            "id": "Arik_et+al_2017_a",
            "entry": "Sercan Omer Arik, Mike Chrzanowski, Adam Coates, Greg Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mohammad Shoeybi. Deep voice: Real-time neural text-to-speech. CoRR, abs/1702.07825, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07825"
        },
        {
            "id": "Barry_2018_a",
            "entry": "Shaun Barry and Youngmoo Kim. style transfer for musical audio using multiple time-frequency representations, 2018. URL https://openreview.net/forum?id=BybQ7zWCb.",
            "url": "https://openreview.net/forum?id=BybQ7zWCb"
        },
        {
            "id": "Cheang_2008_a",
            "entry": "Henry S. Cheang and Marc D. Pell. The sound of sarcasm. Speech Commun., 50(5):366\u2013381, May 2008. ISSN 0167-6393. doi: 10.1016/j.specom.2007.11.003. URL http://dx.doi.org/10.1016/j.specom.2007.11.003.",
            "crossref": "https://dx.doi.org/10.1016/j.specom.2007.11.003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.specom.2007.11.003"
        },
        {
            "id": "2016._0000_a",
            "entry": "2016. URL https://dmitryulyanov.github.io/",
            "url": "https://dmitryulyanov.github.io/"
        },
        {
            "id": "Gatys_et+al_2016_a",
            "entry": "L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414\u20132423, June 2016. doi: 10.1109/CVPR.2016.265.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2016.265",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2016.265"
        },
        {
            "id": "Gibiansky_et+al_2017_a",
            "entry": "Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2962\u20132970. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gibiansky%2C%20Andrew%20Arik%2C%20Sercan%20Diamos%2C%20Gregory%20Miller%2C%20John%20Deep%20voice%202%3A%20Multi-speaker%20neural%20text-to-speech%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gibiansky%2C%20Andrew%20Arik%2C%20Sercan%20Diamos%2C%20Gregory%20Miller%2C%20John%20Deep%20voice%202%3A%20Multi-speaker%20neural%20text-to-speech%202017"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672\u20132680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.",
            "url": "http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Griffin_1984_a",
            "entry": "D. Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, April 1984. ISSN 00963518. doi: 10.1109/TASSP.1984.1164317.",
            "crossref": "https://dx.doi.org/10.1109/TASSP.1984.1164317",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TASSP.1984.1164317"
        },
        {
            "id": "Kameoka_2018_a",
            "entry": "Kou Tanaka Nobukatsu Hojo Hirokazu Kameoka, Takuhiro Kaneko. Stargan-vc: Non-parallel many-to-many voice conversion with star generative adversarial networks. 2018. URL https://arxiv.org/abs/1806.02169.",
            "url": "https://arxiv.org/abs/1806.02169",
            "arxiv_url": "https://arxiv.org/pdf/1806.02169"
        },
        {
            "id": "Sotelo_2017_a",
            "entry": "Kundan Kumar Joao Felipe Santos Kyle Kastner Aaron Courville Yoshua Bengio Jose Sotelo, Soroush Mehri. Char2wav: End-to-end speech synthesis. In International Conference on Learning Representations, workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sotelo%2C%20Kundan%20Kumar%20Joao%20Felipe%20Santos%20Kyle%20Kastner%20Aaron%20Courville%20Yoshua%20Bengio%20Jose%20Mehri%2C%20Soroush%20Char2wav%3A%20End-to-end%20speech%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sotelo%2C%20Kundan%20Kumar%20Joao%20Felipe%20Santos%20Kyle%20Kastner%20Aaron%20Courville%20Yoshua%20Bengio%20Jose%20Mehri%2C%20Soroush%20Char2wav%3A%20End-to-end%20speech%20synthesis%202017"
        },
        {
            "id": "Kinnunen_et+al_2017_a",
            "entry": "T. Kinnunen, L. Juvela, P. Alku, and J. Yamagishi. Non-parallel voice conversion using i-vector plda: towards unifying speaker verification and transformation. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5535\u20135539, March 2017. doi: 10. 1109/ICASSP.2017.7953215.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2017.7953215",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2017.7953215"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. CoRR, abs/1703.00848, 2017. URL http://arxiv.org/abs/1703.00848.",
            "url": "http://arxiv.org/abs/1703.00848",
            "arxiv_url": "https://arxiv.org/pdf/1703.00848"
        },
        {
            "id": "Ma_et+al_2018_a",
            "entry": "Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei. Da-gan: Instance-level image translation by deep attention generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Shuang%20Fu%2C%20Jianlong%20Chen%2C%20Chang%20Wen%20Mei%2C%20Tao%20Da-gan%3A%20Instance-level%20image%20translation%20by%20deep%20attention%20generative%20adversarial%20networks%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Shuang%20Fu%2C%20Jianlong%20Chen%2C%20Chang%20Wen%20Mei%2C%20Tao%20Da-gan%3A%20Instance-level%20image%20translation%20by%20deep%20attention%20generative%20adversarial%20networks%202018-06"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Nachmani_et+al_2018_a",
            "entry": "Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on a short untranscribed sample. CoRR, abs/1802.06984, 2018. URL http://arxiv.org/abs/1802.06984.",
            "url": "http://arxiv.org/abs/1802.06984",
            "arxiv_url": "https://arxiv.org/pdf/1802.06984"
        },
        {
            "id": "Ping_et+al_2018_a",
            "entry": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20voice%203%202000speaker%20neural%20texttospeech%20In%20International%20Conference%20on%20Learning%20Representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20voice%203%202000speaker%20neural%20texttospeech%20In%20International%20Conference%20on%20Learning%20Representations%202018"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Saito_et+al_2017_a",
            "entry": "Y. Saito, S. Takamichi, and H. Saruwatari. Training algorithm to deceive anti-spoofing verification for dnn-based speech synthesis. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4900\u20134904, March 2017. doi: 10.1109/ICASSP.2017.7953088.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2017.7953088",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2017.7953088"
        },
        {
            "id": "Saito_et+al_2018_a",
            "entry": "Yuki Saito; Shinnosuke Takamichi; Hiroshi Saruwatari. Text-to-speech synthesis using stft spectra based on low-/multi-resolution generative adversarial networks. 2018. URL http://sigport.org/2946.",
            "url": "http://sigport.org/2946"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.",
            "url": "http://arxiv.org/abs/1409.1556",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Skerry-Ryan_et+al_2018_a",
            "entry": "R. J. Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 4700\u20134709, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Skerry-Ryan%2C%20R.J.%20Battenberg%2C%20Eric%20Xiao%2C%20Ying%20Wang%2C%20Yuxuan%20Towards%20end-to-end%20prosody%20transfer%20for%20expressive%20speech%20synthesis%20with%20tacotron%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Skerry-Ryan%2C%20R.J.%20Battenberg%2C%20Eric%20Xiao%2C%20Ying%20Wang%2C%20Yuxuan%20Towards%20end-to-end%20prosody%20transfer%20for%20expressive%20speech%20synthesis%20with%20tacotron%202018-07"
        },
        {
            "id": "Skerry-Ryan_et+al_0000_a",
            "entry": "R. J. Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. CoRR, abs/1803.09047, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09047"
        },
        {
            "id": "Taigman_et+al_2016_a",
            "entry": "Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. CoRR, abs/1611.02200, 2016. URL http://arxiv.org/abs/1611.02200.",
            "url": "http://arxiv.org/abs/1611.02200",
            "arxiv_url": "https://arxiv.org/pdf/1611.02200"
        },
        {
            "id": "Taigman_et+al_2018_a",
            "entry": "Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. Voiceloop: Voice fitting and synthesis via a phonological loop. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taigman%2C%20Yaniv%20Wolf%2C%20Lior%20Polyak%2C%20Adam%20Nachmani%2C%20Eliya%20Voiceloop%3A%20Voice%20fitting%20and%20synthesis%20via%20a%20phonological%20loop%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taigman%2C%20Yaniv%20Wolf%2C%20Lior%20Polyak%2C%20Adam%20Nachmani%2C%20Eliya%20Voiceloop%3A%20Voice%20fitting%20and%20synthesis%20via%20a%20phonological%20loop%202018"
        },
        {
            "id": "Ulyanov_et+al_2017_a",
            "entry": "Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. CoRR, abs/1711.10925, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10925"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Arxiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "Van_2008_a",
            "entry": "L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.J.P.%20Hinton%2C%20G.E.%20Visualizing%20high-dimensional%20data%20using%20t-sne%202008"
        },
        {
            "id": "Yuxuan_et+al_2017_a",
            "entry": "Yuxuan Wang, R. J. Skerry-Ryan, Ying Xiao, Daisy Stanton, Joel Shor, Eric Battenberg, Rob Clark, and Rif A. Saurous. Uncovering latent style factors for expressive speech synthesis. CoRR, abs/1711.00520, 2017a. URL http://arxiv.org/abs/1711.00520.",
            "url": "http://arxiv.org/abs/1711.00520",
            "arxiv_url": "https://arxiv.org/pdf/1711.00520"
        },
        {
            "id": "Yuxuan_et+al_0000_a",
            "entry": "Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10135"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A. Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 5167\u20135176, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yuxuan%20Stanton%2C%20Daisy%20Yu%20Zhang%2C%20R.J.-Skerry%20Ryan%20Battenberg%2C%20Eric%20Style%20tokens%3A%20Unsupervised%20style%20modeling%2C%20control%20and%20transfer%20in%20end-to-end%20speech%20synthesis%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yuxuan%20Stanton%2C%20Daisy%20Yu%20Zhang%2C%20R.J.-Skerry%20Ryan%20Battenberg%2C%20Eric%20Style%20tokens%3A%20Unsupervised%20style%20modeling%2C%20control%20and%20transfer%20in%20end-to-end%20speech%20synthesis%202018-07"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Shan Yang, Lei Xie, Xiao Chen, Xiaoyan Lou, Xuan Zhu, Dongyan Huang, and Haizhou Li. Statistical parametric speech synthesis using generative adversarial networks under A multi-task learning framework. CoRR, abs/1707.01670, 2017. URL http://arxiv.org/abs/1707.01670.",
            "url": "http://arxiv.org/abs/1707.01670",
            "arxiv_url": "https://arxiv.org/pdf/1707.01670"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris N. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. CoRR, abs/1612.03242, 2016. URL http://arxiv.org/abs/1612.03242.",
            "url": "http://arxiv.org/abs/1612.03242",
            "arxiv_url": "https://arxiv.org/pdf/1612.03242"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial feature matching for text generation. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 4006\u20134015, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yizhe%20Gan%2C%20Zhe%20Fan%2C%20Kai%20Chen%2C%20Zhi%20Adversarial%20feature%20matching%20for%20text%20generation%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yizhe%20Gan%2C%20Zhe%20Fan%2C%20Kai%20Chen%2C%20Zhi%20Adversarial%20feature%20matching%20for%20text%20generation%202017-08"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017a. URL http://arxiv.org/abs/1703.10593.",
            "url": "http://arxiv.org/abs/1703.10593",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "Zhu_et+al_2017_b",
            "entry": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. CoRR, abs/1711.11586, 2017b. URL http://arxiv.org/abs/1711.11586.",
            "url": "http://arxiv.org/abs/1711.11586",
            "arxiv_url": "https://arxiv.org/pdf/1711.11586"
        }
    ]
}
