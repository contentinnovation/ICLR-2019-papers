{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "APPROXIMABILITY OF DISCRIMINATORS IMPLIES DIVERSITY IN GANS",
        "author": "Yu Bai Stanford University yub@stanford.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJfW5oA5KQ"
        },
        "abstract": "While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs\u2019 statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "metrics",
            "url": "https://en.wikipedia.org/wiki/metrics"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "WGAN": "Wasserstein GAN",
        "F-IPM": "F-Integral Probability Metric",
        "IPM": "Integral Probability Metric"
    },
    "highlights": [
        "In the past few years, we have witnessed great empirical success of Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) in generating high-quality samples in many domains",
        "Do Generative Adversarial Networks learn the target distribution? Recent work (<a class=\"ref-link\" id=\"cArora_et+al_2017_a\" href=\"#rArora_et+al_2017_a\">Arora et al, 2017a</a>;b; <a class=\"ref-link\" id=\"cDumoulin_et+al_2016_a\" href=\"#rDumoulin_et+al_2016_a\">Dumoulin et al, 2016</a>) has both theoretically and empirically brought the concern to light that distributions learned by Generative Adversarial Networks suffer from mode collapse or lack of diversity \u2014 the learned distribution tends to miss a significant amount of modes of the target distribution",
        "We mostly focus on the Wasserstein Generative Adversarial Networks (WGAN) formulation (<a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a>) in this paper",
        "We show that Wasserstein GAN are able to learn both distributions pretty well, and the Integral Probability Metric WF is strongly correlated with the Wasserstein distance W1",
        "We present the first polynomial-in-dimension sample complexity bounds for learning various distributions using Generative Adversarial Networks with convergence guarantees in Wasserstein distance or KL divergence",
        "The analysis technique proceeds via designing discriminators with restricted approximability \u2013 a class of discriminators tailored to the generator class in consideration which have good generalization and mode collapse avoidance properties"
    ],
    "key_statements": [
        "In the past few years, we have witnessed great empirical success of Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) in generating high-quality samples in many domains",
        "We show in this paper that Generative Adversarial Networks can in principle learn distributions in Wasserstein distance with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class",
        "Do Generative Adversarial Networks learn the target distribution? Recent work (<a class=\"ref-link\" id=\"cArora_et+al_2017_a\" href=\"#rArora_et+al_2017_a\">Arora et al, 2017a</a>;b; <a class=\"ref-link\" id=\"cDumoulin_et+al_2016_a\" href=\"#rDumoulin_et+al_2016_a\">Dumoulin et al, 2016</a>) has both theoretically and empirically brought the concern to light that distributions learned by Generative Adversarial Networks suffer from mode collapse or lack of diversity \u2014 the learned distribution tends to miss a significant amount of modes of the target distribution",
        "We mostly focus on the Wasserstein Generative Adversarial Networks (WGAN) formulation (<a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a>) in this paper",
        "Parametric families of functions F such as multi-layer neural networks are used for approximating Lipschitz functions, so that we can empirically optimize this objective eq (2) via gradient-based algorithms as long as distributions in the family G have parameterized samplers. (See Section 2 for more details.)",
        "In Section 4, we study the family of distributions generated by invertible neural networks",
        "We demonstrate in synthetic and controlled experiments that the Integral Probability Metric correlates with the Wasserstein distance for low-dimensional distributions with measure-zero support and correlates with KLdivergence for the invertible generator family (Section 5 and Appendix G.) The theory suggests the possibility that when the KL-divergence or Wasserstein distance is not measurable in more complicated settings, the test Integral Probability Metric could serve as a candidate alternative for measuring the diversity and quality of the learned distribution",
        "The strength in our work is that we develop statistical guarantees in Wasserstein distance for distributions such as injective neural network generators, where the data distribution resides on a low-dimensional manifold and does not have proper density",
        "Our theoretical result and experiments show that successful Generative Adversarial Networks training does imply learning in KL-divergence when the data distribution can be generated by an invertible neural net",
        "We design discriminators with restricted approximability for neural net generators, a family of distributions that are widely used in Generative Adversarial Networks to model real data",
        "We show that Integral Probability Metric is well correlated with the Wasserstein / KL divergence, suggesting that the restricted approximability may hold in practice",
        "We will train Generative Adversarial Networks that learn the unit circle and a \u201cswiss roll\u201d curve (<a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al, 2017</a>) \u2013 both distributions are supported on a one-dimensional manifold in R2, the KL divergence does not exist, but one can use the Wasserstein distance to measure the quality of the learned generator",
        "We show that Wasserstein GAN are able to learn both distributions pretty well, and the Integral Probability Metric WF is strongly correlated with the Wasserstein distance W1",
        "We present the first polynomial-in-dimension sample complexity bounds for learning various distributions using Generative Adversarial Networks with convergence guarantees in Wasserstein distance or KL divergence",
        "The analysis technique proceeds via designing discriminators with restricted approximability \u2013 a class of discriminators tailored to the generator class in consideration which have good generalization and mode collapse avoidance properties"
    ],
    "summary": [
        "In the past few years, we have witnessed great empirical success of Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) in generating high-quality samples in many domains.",
        "Our theoretical result and experiments show that successful GAN training does imply learning in KL-divergence when the data distribution can be generated by an invertible neural net.",
        "Our theory implies that if the data can be generated by an injective neural network (Section 4.2), we can bound the closeness between the learned distribution and the true distribution in Wasserstein distance",
        "We design discriminators with restricted approximability for neural net generators, a family of distributions that are widely used in GANs to model real data.",
        "We let our family G consist of standard -layer feedforward nets x = G\u03b8(z) of the form x = W \u03c3(W \u22121\u03c3(\u00b7 \u00b7 \u00b7 \u03c3(W1z + b1) \u00b7 \u00b7 \u00b7 ) + b \u22121) + b , where Wi \u2208 Rd\u00d7d are invertible, bi \u2208 Rd, and \u03c3 : R \u2192 R is the activation function, on which we make the following assumption: 8Our techniques applies to other parameterized invertible generators but for simplicity we only focus on neural networks.",
        "Combining the restricted approximability and the generalization bound, we immediately obtain that if the training succeeds with small expected IPM, the estimated distribution q is close to the true distribution p in Wasserstein distance.",
        "Our theoretical results on neural network generators in Section 4 convey the message that mode collapse will not happen as long as the discriminator family F has restricted approximability with respect to the generator family G.",
        "Set up suitable generators, and train GANs with either our theoretically proposed discriminator class with restricted approximability, or vanilla neural network discriminators of reasonable capacity.",
        "(a) We learn synthetic 2D datasets with neural net generators and discriminators and show that the IPM is well-correlated with the Wasserstein distance (Section 5.1).",
        "We will train GANs that learn the unit circle and a \u201cswiss roll\u201d curve (<a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al, 2017</a>) \u2013 both distributions are supported on a one-dimensional manifold in R2, the KL divergence does not exist, but one can use the Wasserstein distance to measure the quality of the learned generator.",
        "This would entail designing discriminators that have better restricted approximability bounds, and generally exploring and generalizing approximation theory results in the context of GANs. We hope such explorations will prove as rich and satisfying as they have been in the vanilla functional approximation settings.",
        "We hope such explorations will prove as rich and satisfying as they have been in the vanilla functional approximation settings"
    ],
    "headline": "We show in this paper that Generative Adversarial Networks can in principle learn distributions in Wasserstein distance with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Arora_et+al_0000_a",
            "entry": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In International Conference on Machine Learning, pp. 224\u2013232, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29"
        },
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs actually learn the distribution? do gans learn the distribution? some theory and empirics. ICLR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20actually%20learn%20the%20distribution%3F%20do%20gans%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20actually%20learn%20the%20distribution%3F%20do%20gans%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202017"
        },
        {
            "id": "Bojanowski_et+al_2017_a",
            "entry": "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. arXiv preprint arXiv:1707.05776, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05776"
        },
        {
            "id": "Borji_2018_a",
            "entry": "Ali Borji. Pros and cons of GAN evaluation measures. arXiv preprint arXiv:1802.03446, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03446"
        },
        {
            "id": "Demmel_et+al_2007_a",
            "entry": "James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. Numerische Mathematik, 108(1):59\u201391, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Demmel%2C%20James%20Dumitriu%2C%20Ioana%20Holtz%2C%20Olga%20Fast%20linear%20algebra%20is%20stable%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Demmel%2C%20James%20Dumitriu%2C%20Ioana%20Holtz%2C%20Olga%20Fast%20linear%20algebra%20is%20stable%202007"
        },
        {
            "id": "Di_2017_a",
            "entry": "Xinhan Di and Pengqian Yu. Max-boost-GAN: Max operation to boost generative ability of generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1156\u20131164, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Di%2C%20Xinhan%20Yu%2C%20Pengqian%20Max-boost-GAN%3A%20Max%20operation%20to%20boost%20generative%20ability%20of%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Di%2C%20Xinhan%20Yu%2C%20Pengqian%20Max-boost-GAN%3A%20Max%20operation%20to%20boost%20generative%20ability%20of%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "Durugkar_et+al_2016_a",
            "entry": "I. Durugkar, I. Gemp, and S. Mahadevan. Generative Multi-Adversarial Networks. ArXiv e-prints, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durugkar%2C%20I.%20Gemp%2C%20I.%20Mahadevan%2C%20S.%20Generative%20Multi-Adversarial%20Networks.%20ArXiv%20e-prints%202016-11"
        },
        {
            "id": "Feizi_et+al_2017_a",
            "entry": "Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv preprint arXiv:1710.10793, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10793"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Grover_et+al_2018_a",
            "entry": "Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Combining maximum likelihood and adversarial learning in generative models. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-GAN%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-GAN%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative adversarial networks. In Computer Vision and Patter Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Li%2C%20Yixuan%20Poursaeed%2C%20Omid%20Hopcroft%2C%20John%20Stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Li%2C%20Yixuan%20Poursaeed%2C%20Omid%20Hopcroft%2C%20John%20Stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Im_et+al_2016_a",
            "entry": "D. Jiwoong Im, H. Ma, C. Dongjoo Kim, and G. Taylor. Generative Adversarial Parallelization. ArXiv e-prints, December 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Im%2C%20D.Jiwoong%20Ma%2C%20H.%20Kim%2C%20C.Dongjoo%20Taylor%2C%20G.%20Generative%20Adversarial%20Parallelization.%20ArXiv%20e-prints%202016-12"
        },
        {
            "id": "Ledoux_2013_a",
            "entry": "Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20Talagrand%2C%20Michel%20Probability%20in%20Banach%20Spaces%3A%20isoperimetry%20and%20processes%202013"
        },
        {
            "id": "Liang_2017_a",
            "entry": "Tengyuan Liang. How well can generative adversarial networks (GAN) learn densities: A nonparametric view. arXiv preprint arXiv:1712.08244, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08244"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. PacGAN: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04086"
        },
        {
            "id": "Lopez-Paz_2016_a",
            "entry": "David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.06545"
        },
        {
            "id": "Masarotto_et+al_2018_a",
            "entry": "Valentina Masarotto, Victor M Panaretos, and Yoav Zemel. Procrustes metrics on covariance operators and optimal transportation of gaussian processes. arXiv preprint arXiv:1801.01990, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01990"
        },
        {
            "id": "Muller_1997_a",
            "entry": "Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muller%2C%20Alfred%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997"
        },
        {
            "id": "Nguyen_et+al_2017_a",
            "entry": "Hoi Nguyen, Terence Tao, and Van Vu. Random matrices: tail bounds for gaps between eigenvalues. Probability Theory and Related Fields, 167(3-4):777\u2013816, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Hoi%20Tao%2C%20Terence%20Vu%2C%20Van%20Random%20matrices%3A%20tail%20bounds%20for%20gaps%20between%20eigenvalues%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Hoi%20Tao%2C%20Terence%20Vu%2C%20Van%20Random%20matrices%3A%20tail%20bounds%20for%20gaps%20between%20eigenvalues%202017"
        },
        {
            "id": "Odena_et+al_2016_a",
            "entry": "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09585"
        },
        {
            "id": "Polyanskiy_2016_a",
            "entry": "Yury Polyanskiy and Yihong Wu. Wasserstein continuity of entropy and outer bounds for interference channels. IEEE Transactions on Information Theory, 62(7):3992\u20134002, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyanskiy%2C%20Yury%20Wu%2C%20Yihong%20Wasserstein%20continuity%20of%20entropy%20and%20outer%20bounds%20for%20interference%20channels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyanskiy%2C%20Yury%20Wu%2C%20Yihong%20Wasserstein%20continuity%20of%20entropy%20and%20outer%20bounds%20for%20interference%20channels%202016"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Rumelhart_1986_a",
            "entry": "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20E%20Rumelhart%20Geoffrey%20E%20Hinton%20and%20Ronald%20J%20Williams%20Learning%20representations%20by%20backpropagating%20errors%20nature%203236088533%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=David%20E%20Rumelhart%20Geoffrey%20E%20Hinton%20and%20Ronald%20J%20Williams%20Learning%20representations%20by%20backpropagating%20errors%20nature%203236088533%201986"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Santurkar_et+al_2017_a",
            "entry": "Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry. A classification-based perspective on GAN distributions. arXiv preprint arXiv:1711.00970, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00970"
        },
        {
            "id": "Schmitt_1992_a",
            "entry": "Bernhard A Schmitt. Perturbation bounds for matrix square roots and pythagorean sums. Linear algebra and its applications, 174:215\u2013227, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmitt%2C%20Bernhard%20A.%20Perturbation%20bounds%20for%20matrix%20square%20roots%20and%20pythagorean%20sums%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmitt%2C%20Bernhard%20A.%20Perturbation%20bounds%20for%20matrix%20square%20roots%20and%20pythagorean%20sums%201992"
        },
        {
            "id": "Srivastava_et+al_2017_a",
            "entry": "Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. VeeGAN: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pp. 3310\u20133320, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20VeeGAN%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20VeeGAN%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u2013 31, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-RMSProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-RMSProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scholkopf. AdaGAN: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.02386"
        },
        {
            "id": "Van_2014_a",
            "entry": "Ramon van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Handel%2C%20Ramon%20Probability%20in%20high%20dimension%202014"
        },
        {
            "id": "Vershynin_2010_a",
            "entry": "Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1011.3027"
        },
        {
            "id": "Wainwright_2018_a",
            "entry": "Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. To appear, 2018. URL https://www.stat.berkeley.edu/\u0303wainwrig/nachdiplom/ Chap5_Sep10_2015.pdf.",
            "url": "https://www.stat.berkeley.edu/\u0303wainwrig/nachdiplom/Chap5_Sep10_2015.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20High-dimensional%20statistics%3A%20A%20non-asymptotic%20viewpoint%202018"
        },
        {
            "id": "Weed_2017_a",
            "entry": "Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00087"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00853"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Tao%20Zhang%2C%20Pengchuan%20Huang%2C%20Qiuyuan%20Zhang%2C%20Han%20AttnGAN%3A%20Fine-grained%20text%20to%20image%20generation%20with%20attentional%20generative%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in GANs. arXiv preprint arXiv:1711.02771, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02771"
        },
        {
            "id": "\u03a31_1992_a",
            "entry": "\u03a31 \u2212 \u03a32 op, (cf. (Schmitt, 1992, Lemma 2.2)), we get",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%CE%A31%20%20%CE%A32%20op%20cf%20Schmitt%201992%20Lemma%2022%20we%20get",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%CE%A31%20%20%CE%A32%20op%20cf%20Schmitt%201992%20Lemma%2022%20we%20get"
        },
        {
            "id": "The_2018_a",
            "entry": "The last equality following directly from the closed-form expression of the W2 distance between two Gaus\u221asians (Masarotto et al., 2018, Proposition 3). Thus the claimed lower bound holds with c = 1/(2 2\u03c0).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20last%20equality%20following%20directly%20from%20the%20closedform%20expression%20of%20the%20W2%20distance%20between%20two%20Gaussians%20Masarotto%20et%20al%202018%20Proposition%203%20Thus%20the%20claimed%20lower%20bound%20holds%20with%20c%20%2012%202%CF%80",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20last%20equality%20following%20directly%20from%20the%20closedform%20expression%20of%20the%20W2%20distance%20between%20two%20Gaussians%20Masarotto%20et%20al%202018%20Proposition%203%20Thus%20the%20claimed%20lower%20bound%20holds%20with%20c%20%2012%202%CF%80"
        },
        {
            "id": "As_0000_a",
            "entry": "As \u03c3: R \u2192 R is 1-Lipschitz, by the Rademacher contraction inequality (Ledoux & Talagrand, 2013), we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=As%20%CF%83%20R%20%20R%20is%201Lipschitz%20by%20the%20Rademacher%20contraction%20inequality%20Ledoux%20%20Talagrand%202013%20we%20have"
        },
        {
            "id": "2",
            "entry": "2. Combining this with eq. (17) we complete the proof. The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) will be used here and in later proofs, which we provide for convenience. Lemma C.2 (Gaussian concentration). Suppose X \u223c N(0, Id) and f: Rd \u2192 R is L-Lipschitz, then f (X) is L2-sub-Gaussian.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combining%20this%20with%20eq%2017%20we%20complete%20the%20proof%20The%20Gaussian%20concentration%20result%20Vershynin%202010%20Proposition%20534%20will%20be%20used%20here%20and%20in%20later%20proofs%20which%20we%20provide%20for%20convenience%20Lemma%20C2%20Gaussian%20concentration%20Suppose%20X%20%20N0%20Id%20and%20f%20Rd%20%20R%20is%20LLipschitz%20then%20f%20X%20is%20L2subGaussian",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combining%20this%20with%20eq%2017%20we%20complete%20the%20proof%20The%20Gaussian%20concentration%20result%20Vershynin%202010%20Proposition%20534%20will%20be%20used%20here%20and%20in%20later%20proofs%20which%20we%20provide%20for%20convenience%20Lemma%20C2%20Gaussian%20concentration%20Suppose%20X%20%20N0%20Id%20and%20f%20Rd%20%20R%20is%20LLipschitz%20then%20f%20X%20is%20L2subGaussian"
        },
        {
            "id": "The_2014_a",
            "entry": "The following theorem gives conditions on which the KL divergence can be lower bounded by the Wasserstein 1/2 distance. For a reference see Section 4.1 and 4.4 in van Handel (2014).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20following%20theorem%20gives%20conditions%20on%20which%20the%20KL%20divergence%20can%20be%20lower%20bounded%20by%20the%20Wasserstein%2012%20distance%20For%20a%20reference%20see%20Section%2041%20and%2044%20in%20van%20Handel%202014"
        },
        {
            "id": "(c)_2016_a",
            "entry": "(c) This part is a straightforward extension of (Polyanskiy & Wu, 2016, Proposition 1). For completeness we present the proof here. For any x, y \u2208 Rd we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=c%20This%20part%20is%20a%20straightforward%20extension%20of%20Polyanskiy%20%20Wu%202016%20Proposition%201%20For%20completeness%20we%20present%20the%20proof%20here%20For%20any%20x%20y%20%20Rd%20we%20have",
            "oa_query": "https://api.scholarcy.com/oa_version?query=c%20This%20part%20is%20a%20straightforward%20extension%20of%20Polyanskiy%20%20Wu%202016%20Proposition%201%20For%20completeness%20we%20present%20the%20proof%20here%20For%20any%20x%20y%20%20Rd%20we%20have"
        },
        {
            "id": "Rademacher_2018_b",
            "entry": "Rademacher process, the one-step discretization bound (Wainwright, 2018, Section 5).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rademacher%20process%20the%20onestep%20discretization%20bound%20Wainwright%202018%20Section%205"
        },
        {
            "id": "(See_2010_b",
            "entry": "(See for example (Vershynin, 2010) for such results.) Also, the parameter K is upper bounded by d RW = Cd.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%20for%20example%20Vershynin%202010%20for%20such%20results%20Also%20the%20parameter%20K%20is%20upper%20bounded%20by%20d%20RW%20%20Cd"
        },
        {
            "id": "2",
            "entry": "(2) Can be implemented by a small, Lipschitz network as needed.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=be%2C%20Can%20implemented%20by%20a%20small%2C%20Lipschitz%20network%20as%20needed"
        },
        {
            "id": "1",
            "entry": "1. Hence, ef(z)dz = (1) The KL divergence. As the density of our invertible neural net generator can be analytically computed, we can compute their KL divergence from empirical averages of the difference of the log densities: Dkl(p, p) = EX\u223cp n [log p (X) \u2212 log p(X)], where p and p are the densities of the true generator and the learned generator. We regard the KL divergence as the \u201ccorrect\u201d and rather strong criterion for distributional closeness.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hence%20ef%28z%29dz%20%3D%20%281%29%20The%20KL%20divergence.%20As%20the%20density%20of%20our%20invertible%20neural%20net%20generator%20can%20be%20analytically%20computed%2C%20we%20can%20compute%20their%20KL%20divergence%20from%20empirical%20averages%20of%20the%20difference%20of%20the%20log%20densities%3A%20Dkl%28p%2C%20p%29%20%3D%20EX%E2%88%BCp%20n%20%5Blog%20p%20%28X%29%20%E2%88%92%20log%20p%28X%29%5D%2C%20where%20p%20and%20p%20are%20the%20densities%20of%20the%20true%20generator%20and%20the%20learned%20generator.%20We%20regard%20the%20KL%20divergence%20as%20the%20%E2%80%9Ccorrect%E2%80%9D%20and%20rather%20strong%20criterion%20for%20distributional%20closeness"
        },
        {
            "id": "2",
            "entry": "(2) The training loss (IPM WF train). This is the (unregularized) GAN loss during training. Note: as typically in the training of GANs, we balance carefully the number of steps for discriminator and generators, the training IPM is potentially very far away from the true WF (which requires sufficient training of the discriminators).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20training%20loss%20IPM%20WF%20train%20This%20is%20the%20unregularized%20GAN%20loss%20during%20training%20Note%20as%20typically%20in%20the%20training%20of%20GANs%20we%20balance%20carefully%20the%20number%20of%20steps%20for%20discriminator%20and%20generators%20the%20training%20IPM%20is%20potentially%20very%20far%20away%20from%20the%20true%20WF%20which%20requires%20sufficient%20training%20of%20the%20discriminators"
        },
        {
            "id": "3",
            "entry": "(3) The neural net IPM (WF eval). We report once in a while a separately optimized WGAN loss in which the learned generator is held fixed and the discriminator is trained from scratch to optimality. Unlike the training loss, here the discriminator is trained in norm balls but with no other regularization. By doing this, we are finding f \u2208 F that maximizes the contrast and we regard the f found by stochastic optimization an approximate maximizer, and the loss obtained an approximation of WF.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20neural%20net%20IPM%20WF%20eval%20We%20report%20once%20in%20a%20while%20a%20separately%20optimized%20WGAN%20loss%20in%20which%20the%20learned%20generator%20is%20held%20fixed%20and%20the%20discriminator%20is%20trained%20from%20scratch%20to%20optimality%20Unlike%20the%20training%20loss%20here%20the%20discriminator%20is%20trained%20in%20norm%20balls%20but%20with%20no%20other%20regularization%20By%20doing%20this%20we%20are%20finding%20f%20%20F%20that%20maximizes%20the%20contrast%20and%20we%20regard%20the%20f%20found%20by%20stochastic%20optimization%20an%20approximate%20maximizer%20and%20the%20loss%20obtained%20an%20approximation%20of%20WF"
        },
        {
            "id": "2",
            "entry": "(2) The WF (eval) and the KL divergence are highly correlated with each other, both along each training run and across different runs. In particular, adding gradient penalty improves the optimization significantly (which we see in the KL curve), and this improvement is also reflected by the WF curve. Therefore the quantity WF can serve as a good metric for monitoring convergence and is at least much better than the training loss curve.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20WF%20eval%20and%20the%20KL%20divergence%20are%20highly%20correlated%20with%20each%20other%20both%20along%20each%20training%20run%20and%20across%20different%20runs%20In%20particular%20adding%20gradient%20penalty%20improves%20the%20optimization%20significantly%20which%20we%20see%20in%20the%20KL%20curve%20and%20this%20improvement%20is%20also%20reflected%20by%20the%20WF%20curve%20Therefore%20the%20quantity%20WF%20can%20serve%20as%20a%20good%20metric%20for%20monitoring%20convergence%20and%20is%20at%20least%20much%20better%20than%20the%20training%20loss%20curve"
        }
    ]
}
