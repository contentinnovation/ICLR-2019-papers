{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "OVERCOMING CATASTROPHIC FORGETTING FOR CONTINUAL LEARNING VIA MODEL ADAPTATION",
        "author": "Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao, Dongyan Zhao, Jinwen Ma, and Rui Yan",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryGvcoA5YX"
        },
        "abstract": "Learning multiple tasks sequentially is important for the development of AI and lifelong learning systems. However, standard neural network architectures suffer from catastrophic forgetting which makes it difficult for them to learn a sequence of tasks. Several continual learning methods have been proposed to address the problem. In this paper, we propose a very different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with the problem. The proposed approach learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach."
    },
    "keywords": [
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "Microsoft Research Asia",
            "url": "https://en.wikipedia.org/wiki/Microsoft_Research_Asia"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "multilayer perceptron",
            "url": "https://en.wikipedia.org/wiki/multilayer_perceptron"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "second set",
            "url": "https://en.wikipedia.org/wiki/Second_Set"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "catastrophic forgetting",
            "url": "https://en.wikipedia.org/wiki/Catastrophic_Forgetting"
        }
    ],
    "abbreviations": {
        "PGMA": "parameter generation and model adaptation",
        "NNs": "neural networks",
        "CF": "catastrophic forgetting",
        "DPG": "DYNAMIC PARAMETER GENERATOR",
        "DG": "DATA GENERATOR",
        "CNN": "convolutional neural network",
        "RNN": "recurrent neural network",
        "MLP": "multilayer perceptron",
        "GR": "Generative Replay",
        "LWF": "Learning without forgetting",
        "IMM": "Incremental Moment Matching",
        "GEM": "Gradient Episodic Memory",
        "DEN": "dynamically expandable networks",
        "NSF": "National Science Foundation",
        "MSRA": "Microsoft Research Asia"
    },
    "highlights": [
        "It is well-known that neural networks (NNs) suffer from catastrophic forgetting (CF) (McCloskey & Cohen, 1989), which refers to the phenomenon that when learning a sequence of tasks, the learning of each new task may cause the neural networks to forget the models learned for the previous tasks",
        "We propose a different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with catastrophic forgetting and to significantly reducing accuracy deterioration",
        "The main function is to generate a set of replayed data or samples {xm}m M=1 using its decoder DGD for previous tasks to deal with catastrophic forgetting",
        "This paper proposed a novel approach parameter generation and model adaptation to dealing with catastrophic forgetting",
        "The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each individual test example"
    ],
    "key_statements": [
        "It is well-known that neural networks (NNs) suffer from catastrophic forgetting (CF) (McCloskey & Cohen, 1989), which refers to the phenomenon that when learning a sequence of tasks, the learning of each new task may cause the neural networks to forget the models learned for the previous tasks",
        "An neural networks is hard to adapt to lifelong or continual learning, which is important for AI",
        "Many approaches have been proposed to lessen the effect of catastrophic forgetting (<a class=\"ref-link\" id=\"cChen_2018_a\" href=\"#rChen_2018_a\">Chen & Liu, 2018</a>; <a class=\"ref-link\" id=\"cParisi_et+al_2018_a\" href=\"#rParisi_et+al_2018_a\">Parisi et al, 2018</a>), e.g., dynamically expandable network (DEN) (<a class=\"ref-link\" id=\"cYoon_et+al_2018_a\" href=\"#rYoon_et+al_2018_a\">Yoon et al, 2018</a>), learning without forgetting (LWF) (<a class=\"ref-link\" id=\"cLi_2016_a\" href=\"#rLi_2016_a\">Li & Hoiem, 2016</a>)), elastic weight consolidation (EWC) (Kirkpatrick et al, 2017), incremental moment matching (IMM) (<a class=\"ref-link\" id=\"cLee_et+al_2017_a\" href=\"#rLee_et+al_2017_a\">Lee et al, 2017</a>), gradient episodic memory (GEM) (Lopez-Paz et al, 2017), generative replay (GR) (<a class=\"ref-link\" id=\"cShin_et+al_2017_a\" href=\"#rShin_et+al_2017_a\">Shin et al, 2017</a>), etc. These existing studies except dynamically expandable networks and Learning without forgetting focused on learning a model parameterized by a single joint set of parameters \u03b8\u2217 which is assumed to work well for all tasks",
        "dynamically expandable networks and Learning without forgetting require constantly increasing the number of parameters and can result in a huge and complex model",
        "We propose a different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with catastrophic forgetting and to significantly reducing accuracy deterioration",
        "The main function is to generate a set of replayed data or samples {xm}m M=1 using its decoder DGD for previous tasks to deal with catastrophic forgetting",
        "In training the solver S and DYNAMIC PARAMETER GENERATOR for the new task Ti, both f (\u00b7) and the shared parameters \u03b80 will change, which can cause forgetting in DYNAMIC PARAMETER GENERATOR for previous tasks",
        "To keep the past learned knowledge, the output of the basic units in the solver should not change much when learning a new task with the help of the generated data",
        "Note since we only have one data generator DATA GENERATOR, it has the forgetting problem caused by incremental training of new tasks too",
        "This paper proposed a novel approach parameter generation and model adaptation to dealing with catastrophic forgetting",
        "The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each individual test example"
    ],
    "summary": [
        "It is well-known that neural networks (NNs) suffer from catastrophic forgetting (CF) (McCloskey & Cohen, 1989), which refers to the phenomenon that when learning a sequence of tasks, the learning of each new task may cause the NN to forget the models learned for the previous tasks.",
        "We propose a different approach, called Parameter Generation and Model Adaptation (PGMA), to dealing with CF and to significantly reducing accuracy deterioration.",
        "The main function is to generate a set of replayed data or samples {xm}m M=1 using its decoder DGD for previous tasks to deal with catastrophic forgetting.",
        "Solver S takes xt as input and uses the trained/learned shared parameters \u03b80 and pt to classify xt.",
        "In training the solver S and DPG for the new task Ti, both f (\u00b7) and the shared parameters \u03b80 will change, which can cause forgetting in DPG for previous tasks.",
        "The objective function of the solver S including DPG f and \u03b80 for learning each new classification task Ti is defined as: minimize \u03bc,\u03b80",
        "To keep the past learned knowledge, the output of the basic units in the solver should not change much when learning a new task with the help of the generated data.",
        "It generates the replayed data for previous tasks to deal with forgetting in solver S and DPG.",
        "Note since we only have one data generator DG, it has the forgetting problem caused by incremental training of new tasks too.",
        "M=1 where xm is the replayed data, \u03b8e and \u03b8d are the encoder\u2019s and decoder\u2019s parameters of DG in the process of learning the new task Ti, respectively, and zsmample and M have the same meanings as they are in DPG&S.",
        "Pn and Sn below // denote a batch of generated parameters 10: // and adapted solvers, respectively Compute pn using Eqs. 1 & 6; // Section 2.2 Construct Sn using pn and \u03b80; // Section 2.2 Minimize Lce update f (\u00b7) and S; end for",
        "Table 3 shows the accuracy results when a portion of the parameters in only the last layer of the solver is replaced by the parameters generated by DPG.",
        "This observation indicates that replacing a part of parameters in solver to adapt new input tasks is sufficient.",
        "Learning without forgetting (LWF) (<a class=\"ref-link\" id=\"cLi_2016_a\" href=\"#rLi_2016_a\">Li & Hoiem, 2016</a>) feeds the old network with new training data in new tasks and regards the output as \u201dpseudolabels\u201d.",
        "Our method uses the generated parameters for model adaptation for continual learning.",
        "The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each individual test example.",
        "Experimental results showed that the proposed approach outperformed the existing baseline methods markedly"
    ],
    "headline": "We propose a very different approach, called Parameter Generation and Model Adaptation, to dealing with the problem",
    "reference_links": [
        {
            "id": "Aljundi_et+al_2017_a",
            "entry": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. arXiv preprint arXiv:1711.09601, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09601"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05344"
        },
        {
            "id": "Camoriano_et+al_2017_a",
            "entry": "Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale, Lorenzo Rosasco, and Giorgio Metta. Incremental robot learning of new objects with fixed update time. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 3207\u20133214. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camoriano%2C%20Raffaello%20Pasquale%2C%20Giulia%20Ciliberto%2C%20Carlo%20Natale%2C%20Lorenzo%20Incremental%20robot%20learning%20of%20new%20objects%20with%20fixed%20update%20time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camoriano%2C%20Raffaello%20Pasquale%2C%20Giulia%20Ciliberto%2C%20Carlo%20Natale%2C%20Lorenzo%20Incremental%20robot%20learning%20of%20new%20objects%20with%20fixed%20update%20time%202017"
        },
        {
            "id": "Chen_2018_a",
            "entry": "Zhiyuan Chen and Bing Liu. Lifelong machine learning. Morgan & Claypool Publishers, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Zhiyuan%20Liu%2C%20Bing%20Lifelong%20machine%20learning%202018"
        },
        {
            "id": "Denil_et+al_2013_a",
            "entry": "Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep learning. In Advances in neural information processing systems, pp. 2148\u20132156, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "Furlanello_et+al_2016_a",
            "entry": "Tommaso Furlanello, Jiaping Zhao, Andrew M Saxe, Laurent Itti, and Bosco S Tjan. Active long term memory networks. arXiv preprint arXiv:1606.02355, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02355"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016-06"
        },
        {
            "id": "He_2018_a",
            "entry": "Xu He and Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropagation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Xu%20Jaeger%2C%20Herbert%20Overcoming%20catastrophic%20interference%20using%20conceptor-aided%20backpropagation%202018"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pp. 4652\u20134662, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017"
        },
        {
            "id": "Lehmann_et+al_2015_a",
            "entry": "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Soren Auer, et al. Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6(2):167\u2013195, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20Dbpedia%E2%80%93a%20largescale%2C%20multilingual%20knowledge%20base%20extracted%20from%20wikipedia%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehmann%2C%20Jens%20Isele%2C%20Robert%20Jakob%2C%20Max%20Jentzsch%2C%20Anja%20Dbpedia%E2%80%93a%20largescale%2C%20multilingual%20knowledge%20base%20extracted%20from%20wikipedia%202015"
        },
        {
            "id": "Li_et+al_2006_a",
            "entry": "Jingyang Li, Maosong Sun, and Xian Zhang. A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization. In ACL, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jingyang%20Sun%2C%20Maosong%20Zhang%2C%20Xian%20A%20comparison%20and%20semi-quantitative%20analysis%20of%20words%20and%20character-bigrams%20as%20features%20in%20chinese%20text%20categorization%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jingyang%20Sun%2C%20Maosong%20Zhang%2C%20Xian%20A%20comparison%20and%20semi-quantitative%20analysis%20of%20words%20and%20character-bigrams%20as%20features%20in%20chinese%20text%20categorization%202006"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and Xiaoyong Du. Analogical reasoning on chinese morphological and semantic relations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 138\u2013143. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/P18-2023.",
            "url": "http://aclweb.org/anthology/P18-2023",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Shen%20Zhao%2C%20Zhe%20Hu%2C%20Renfen%20Li%2C%20Wensi%20Analogical%20reasoning%20on%20chinese%20morphological%20and%20semantic%20relations%202018"
        },
        {
            "id": "Li_2016_a",
            "entry": "Zhizhong Li and Derek Hoiem. Learning Without Forgetting. ECCV, 9908(1):614\u2013629, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20Without%20Forgetting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20Without%20Forgetting%202016"
        },
        {
            "id": "Lopez-Paz_2017_a",
            "entry": "David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467\u20136476, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017"
        },
        {
            "id": "Masse_et+al_2018_a",
            "entry": "Nicolas Y Masse, Gregory D Grant, and David J Freedman. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. arXiv preprint arXiv:1802.01569, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01569"
        },
        {
            "id": "Parisi_et+al_2018_a",
            "entry": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. arXiv preprint arXiv:1802.07569, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07569"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532\u20131543, 2014. URL http://www.aclweb.org/anthology/D14-1162.",
            "url": "http://www.aclweb.org/anthology/D14-1162",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Rebuffi_et+al_2017_a",
            "entry": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20Lampert%2C%20Christoph%20H.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20Lampert%2C%20Christoph%20H.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "Schwarz_et+al_2018_a",
            "entry": "Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. arXiv preprint arXiv:1805.06370, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06370"
        },
        {
            "id": "Seff_et+al_2017_a",
            "entry": "Ari Seff, Alex Beatson, Daniel Suo, and Han Liu. Continual learning in generative adversarial nets. arXiv preprint arXiv:1705.08395, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08395"
        },
        {
            "id": "Serra_et+al_2018_a",
            "entry": "Joan Serra, D\u0131dac Sur\u0131s, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01423"
        },
        {
            "id": "Shin_et+al_2017_a",
            "entry": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990\u20132999, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01558"
        },
        {
            "id": "Venkatesan_et+al_2017_a",
            "entry": "Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan, and Baoxin Li. A strategy for an uncompromising incremental learner. arXiv preprint arXiv:1705.00744, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.00744"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 189\u2013198, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017"
        },
        {
            "id": "Yoon_et+al_2018_a",
            "entry": "Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoon%2C%20Jaehong%20Yang%2C%20Eunho%20Lee%2C%20Jeongtae%20Hwang%2C%20Sung%20Ju%20Lifelong%20learning%20with%20dynamically%20expandable%20networks%202018"
        },
        {
            "id": "Zenke_et+al_2017_a",
            "entry": "Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04200"
        }
    ]
}
