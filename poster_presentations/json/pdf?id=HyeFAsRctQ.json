{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "VERIFICATION OF NON-LINEAR SPECIFICATIONS FOR NEURAL NETWORKS",
        "author": "Chongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O\u2019Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HyeFAsRctQ"
        },
        "abstract": "Prior work on neural network verification has focused on specifications that are linear functions of the output of the network, e.g., invariance of the classifier output under adversarial perturbations of the input. In this paper, we extend verification algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable specifications, which constitute nonlinear specifications that can be verified using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classifier\u2019s output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these specifications. Moreover, our evaluation exposes the failure modes in models which cannot be verified to satisfy these specifications. Thus, emphasizing the importance of training models not just to fit training data but also to be consistent with specifications."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        }
    ],
    "abbreviations": {
        "SMT": "Satisfiability Modulo Theory"
    },
    "highlights": [
        "Deep learning has been shown to be effective for problems in a wide variety of different fields from computer vision to machine translation; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al (2014</a></a>); <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al (2012</a></a>))",
        "We extend neural network verification algorithms to handle a broader class of convex-relaxable specifications",
        "The three example specifications that we study in this paper are: a) Physics specifications: Learned models of physical systems should be consistent with the laws of physics, b) Downstream task specifications: A system that uses image classifiers on handwritten digits to predict the sum of these digits should make only small errors on the final sum under adversarial perturbations of the input and c) Semantic specifications: The expected semantic change between the label predicted by a neural network and that of the true label should be small under adversarial perturbations of the input",
        "We have developed verification algorithms for a new class of convex-relaxable specifications, that can model many specifications of interest in prediction tasks",
        "We have shown experimentally that our verification algorithms can verify these specifications with high precision while still being tractable",
        "Inspired by (Wong & Kolter, 2018; Wong et al, 2018; <a class=\"ref-link\" id=\"cRaghunathan_et+al_2018_a\" href=\"#rRaghunathan_et+al_2018_a\">Raghunathan et al, 2018</a>), we believe that folding these verification algorithms can help significantly in training models that are consistent with various specifications of interest"
    ],
    "key_statements": [
        "Deep learning has been shown to be effective for problems in a wide variety of different fields from computer vision to machine translation; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al (2014</a>); <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al (2012</a>))",
        "An instance of this was demonstrated by <a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al (2013</a>), who showed that neural networks for image classification produced incorrect results on inputs which were modified by small, but carefully chosen, perturbations",
        "We extend neural network verification algorithms to handle a broader class of convex-relaxable specifications",
        "The three example specifications that we study in this paper are: a) Physics specifications: Learned models of physical systems should be consistent with the laws of physics, b) Downstream task specifications: A system that uses image classifiers on handwritten digits to predict the sum of these digits should make only small errors on the final sum under adversarial perturbations of the input and c) Semantic specifications: The expected semantic change between the label predicted by a neural network and that of the true label should be small under adversarial perturbations of the input",
        "We have proposed a novel set of specifications to be verified as well as new verification algorithms that can verify whether these specifications are satisfied by neural networks",
        "We show that our verification algorithm can produce tight verification results, i.e., the fraction of instances on which the verification algorithm fails to find a proof of a correctness but the falsification algorithm cannot find a counter-example is small",
        "We have developed verification algorithms for a new class of convex-relaxable specifications, that can model many specifications of interest in prediction tasks",
        "We have shown experimentally that our verification algorithms can verify these specifications with high precision while still being tractable",
        "We have shown that verifying these specifications can provide valuable diagnostic information regarding the ultimate behavior of models in downstream prediction tasks",
        "Inspired by (Wong & Kolter, 2018; Wong et al, 2018; <a class=\"ref-link\" id=\"cRaghunathan_et+al_2018_a\" href=\"#rRaghunathan_et+al_2018_a\">Raghunathan et al, 2018</a>), we believe that folding these verification algorithms can help significantly in training models that are consistent with various specifications of interest"
    ],
    "summary": [
        "Deep learning has been shown to be effective for problems in a wide variety of different fields from computer vision to machine translation; <a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al (2014</a></a>); <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al (2012</a></a>)).",
        "We extend neural network verification algorithms to handle a broader class of convex-relaxable specifications.",
        "We present several examples of verification of neural networks beyond the usual case of robustness under adversarial perturbations of the input \u2013 all of which are non-linear specifications.",
        "Most prior work on neural network verification has assumed specifications of the form: F (x, y) = cT y + d \u2264 0 \u2200x \u2208 Sin, y = f (x)",
        "Given a neural network f , input set Sin, and a convex-relaxable specification F , we have that F (x, y) \u2264 0 for all x \u2208 Sin, y = f (x) if the optimal value of (13) is smaller than 0 and further (13) it is a convex optimization problem.",
        "The specifications on label semantics from (1) can be modeled as linear functions of the softmax layer of the neural network: P (j|x) = eyj / k eyk , y = f (x).",
        "In order to validate our contributions experimentally, we perform two sets of experiments: the first set of experiments tests the ability of the convex relaxation approach we develop here to verify nonlinear specifications and the second set of experiments tests that the specifications we verify provide useful information on the ability of the ML model to solve the task of interest.",
        "Tightness of verification: We proposed incomplete verification algorithms that can produce conservative results, i.e., even if the model satisfies the specification our approach may not be able to prove it.",
        "We define the following metrics which are used in our experiments to gauge the tightness and effectiveness our verification algorithm: Verification bound: This is the fraction of test examples xnom for which the specification is provably satisfied over the set Sin using our verification algorithm.",
        "Adversarial bound: This is the fraction of test examples xnom for which the falsification algorithm based on (8) was not able to find a counter-example in the set Sin. The real quantity of interest is the fraction of test examples for which the specification is satisfied denoted as \u03b2.",
        "At a perturbation radius up until 0.02, the exhaustive verification can prove that the specification is not violated for 73.98% of the test data points while using our convex relaxation approach achieved a lower bound of 70.00%.",
        "We have developed verification algorithms for a new class of convex-relaxable specifications, that can model many specifications of interest in prediction tasks.",
        "We believe that such an advance will significantly aid and accelerate deployment of AI in applications with safety and security constraints"
    ],
    "headline": "We extend verification algorithms to be able to certify richer properties of neural networks",
    "reference_links": [
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Bar-Sinai_et+al_2018_a",
            "entry": "Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Data-driven discretization: a method for systematic coarse graining of partial differential equations. arXiv preprint arXiv:1808.04930, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04930"
        },
        {
            "id": "Bunel_et+al_2017_a",
            "entry": "Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and M. Pawan Kumar. Piecewise linear neural network verification: A comparative study. CoRR, abs/1711.00455, 2017. URL http://arxiv.org/abs/1711.00455.",
            "url": "http://arxiv.org/abs/1711.00455",
            "arxiv_url": "https://arxiv.org/pdf/1711.00455"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3\u201314. ACM, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "Carlini_2017_b",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357. IEEE, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Chandrasekaran_2016_a",
            "entry": "V. Chandrasekaran and P. Shah. Relative entropy relaxations for signomial optimization. SIAM Journal on Optimization, 26(2):1147\u20131173, 2016. doi: 10.1137/140988978. URL https://doi.org/10.1137/140988978.",
            "crossref": "https://dx.doi.org/10.1137/140988978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/140988978"
        },
        {
            "id": "Dvijotham_et+al_0000_a",
            "entry": "Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O\u2019Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned verifiers. CoRR, abs/1805.10265, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10265"
        },
        {
            "id": "Dvijotham_et+al_0000_b",
            "entry": "Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06567"
        },
        {
            "id": "Ehrhardt_et+al_2017_a",
            "entry": "Sebastien Ehrhardt, Aron Monszpart, Niloy J Mitra, and Andrea Vedaldi. Learning a physical longterm predictor. arXiv preprint arXiv:1703.00247, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00247"
        },
        {
            "id": "Gehr_et+al_2018_a",
            "entry": "Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai 2: Safety and robustness certification of neural networks with abstract interpretation. In Security and Privacy (SP), 2018 IEEE Symposium on, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%2C%20volume%201%202016"
        },
        {
            "id": "Gowal_et+al_2018_a",
            "entry": "Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.12715"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "Luo_et+al_2010_a",
            "entry": "Zhi-Quan Luo, Wing-Kin Ma, Anthony Man-Cho So, Yinyu Ye, and Shuzhong Zhang. Semidefinite relaxation of quadratic optimization problems. IEEE Signal Processing Magazine, 27(3):20\u201334, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Zhi-Quan%20Ma%2C%20Wing-Kin%20So%2C%20Anthony%20Man-Cho%20Ye%2C%20Yinyu%20Semidefinite%20relaxation%20of%20quadratic%20optimization%20problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Zhi-Quan%20Ma%2C%20Wing-Kin%20So%2C%20Anthony%20Man-Cho%20Ye%2C%20Yinyu%20Semidefinite%20relaxation%20of%20quadratic%20optimization%20problems%202010"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "George_1995_a",
            "entry": "George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39\u201341, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=George%20A%20Miller%20Wordnet%20a%20lexical%20database%20for%20english%20Communications%20of%20the%20ACM%203811%203941%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=George%20A%20Miller%20Wordnet%20a%20lexical%20database%20for%20english%20Communications%20of%20the%20ACM%203811%203941%201995"
        },
        {
            "id": "Mirman_et+al_2018_a",
            "entry": "Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3575\u20133583, Stockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018a. PMLR. URL http://proceedings.mlr.press/v80/mirman18b.html.",
            "url": "http://proceedings.mlr.press/v80/mirman18b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirman%2C%20Matthew%20Gehr%2C%20Timon%20Vechev%2C%20Martin%20Differentiable%20abstract%20interpretation%20for%20provably%20robust%20neural%20networks%202018-07"
        },
        {
            "id": "Mirman_et+al_2018_b",
            "entry": "Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3578\u20133586, Stockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018b. PMLR. URL http://proceedings.mlr.press/v80/mirman18b.html.",
            "url": "http://proceedings.mlr.press/v80/mirman18b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirman%2C%20Matthew%20Gehr%2C%20Timon%20Vechev%2C%20Martin%20Differentiable%20abstract%20interpretation%20for%20provably%20robust%20neural%20networks%202018-07"
        },
        {
            "id": "Moosavi-Dezfooli_et+al_2016_a",
            "entry": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574\u20132582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "Stewart_2017_a",
            "entry": "Russell Stewart and Stefano Ermon. Label-free supervision of neural networks with physics and domain knowledge. In AAAI, volume 1, pp. 1\u20137, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20Russell%20Ermon%2C%20Stefano%20Label-free%20supervision%20of%20neural%20networks%20with%20physics%20and%20domain%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20Russell%20Ermon%2C%20Stefano%20Label-free%20supervision%20of%20neural%20networks%20with%20physics%20and%20domain%20knowledge%202017"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3104\u20133112. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.",
            "url": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tassa_et+al_2018_a",
            "entry": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        },
        {
            "id": "Tjeng_2017_a",
            "entry": "Vincent Tjeng and Russ Tedrake. Verifying neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07356"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Uesato_et+al_2018_a",
            "entry": "Jonathan Uesato, Brendan O\u2019Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05666"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20TsuiWei%20Weng%20Huan%20Zhang%20Hongge%20Chen%20Zhao%20Song%20ChoJui%20Hsieh%20Luca%20Daniel%20Duane"
        },
        {
            "id": "Boning_2018_a",
            "entry": "Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5273\u20135282, Stockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/weng18a.html. Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.5283\u20135292, Stockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/wong18a.html. Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. CoRR, abs/1805.12514, 2018.",
            "url": "http://proceedings.mlr.press/v80/weng18a.html",
            "arxiv_url": "https://arxiv.org/pdf/1805.12514"
        }
    ]
}
