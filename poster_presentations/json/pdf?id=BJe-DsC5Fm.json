{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SIGNSGD VIA ZEROTH-ORDER ORACLE",
        "author": "Sijia Liu, Pin-Yu Chen, Xiangyi Chen, \u2020MIT-IBM Watson AI Lab, IBM Research \u2021University of Minnesota, Twin Cities",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJe-DsC5Fm"
        },
        "abstract": "In this paper, we design and analyze a new zeroth-order (ZO) stochastic optimization algorithm, ZO-signSGD, which enjoys dual advantages of gradient-free operations and signSGD. The latter requires only the sign information of gradient estimates but is able to achieve a comparable or even better convergen\u221ace speed than SGD-type algorithms. Our study shows that ZO-signSGD req\u221auire\u221as d times more iterations than signSGD, leading to a convergence rate of O( d/ T ) under some mild conditions, where d is the number of optimization variables, and T is the number of iterations. In addition, we analyze the effects of different types of gradient estimators on the con\u221averg\u221aence of ZO-signSGD, and propose several variants of ZO-signSGD with O( d/ T ) convergence rate. On the application side we explore the connection between ZO-signSGD and black-box adversarial attacks in robust deep learning. Our empirical evaluations on image classification datasets MNIST and CIFAR-10 demonstrate the superior performance of ZO-signSGD on the generation of adversarial examples from black-box neural networks."
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "ADMM",
            "url": "https://en.wikipedia.org/wiki/ADMM"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "natural evolution strategy",
            "url": "https://en.wikipedia.org/wiki/natural_evolution_strategy"
        },
        {
            "term": "d/ t",
            "url": "https://en.wikipedia.org/wiki/D_T"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "ML": "machine learning",
        "DNNs": "deep neural networks",
        "ZO-SCD": "ZO stochastic coordinate descent",
        "DNN": "deep neural network",
        "NES": "natural evolution strategy"
    },
    "highlights": [
        "Zeroth-order optimization has attracted an increasing amount of attention for solving machine learning (ML) problems in scenarios where explicit expressions for the gradients are difficult or infeasible to obtain",
        "The black-box optimization nature limits the practical design of adversarial examples, where internal configurations and operating mechanism of public machine learning systems (e.g., Google Cloud Vision API) are not revealed to practitioners and the only mode of interaction with the system is via submitting inputs and receiving the corresponding predicted outputs (<a class=\"ref-link\" id=\"cPapernot_et+al_2017_a\" href=\"#rPapernot_et+al_2017_a\">Papernot et al, 2017</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>; <a class=\"ref-link\" id=\"cTu_et+al_2018_a\" href=\"#rTu_et+al_2018_a\">Tu et al, 2018</a>; Ilyas et al, 2018b; <a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\">Cheng et al, 2018</a>; <a class=\"ref-link\" id=\"cBhagoji_et+al_2018_a\" href=\"#rBhagoji_et+al_2018_a\">Bhagoji et al, 2018</a>)",
        "We demonstrate the superior performance of ZO-signSGD for generating adversarial examples from black-box deep neural networks",
        "We show that the generation of adversarial examples in (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a>; <a class=\"ref-link\" id=\"cKurakin_et+al_2017_a\" href=\"#rKurakin_et+al_2017_a\">Kurakin et al, 2017</a>) can be interpreted through signSGD",
        "Generating black-box adversarial examples Here we study adversarial robustness by generating adversarial examples from a black-box image classifier trained by a deep neural network (DNN) model; see details on problem formulation in Appendix 8.2",
        "Motivated by the impressive convergence behavior of signSGD and the empirical success in c\u221arafti\u221ang adversarial examples from black-box machine learning models, in this paper we rigorously prove the O( d/ T ) convergence rate of ZO-signSGD and its variants under mild conditions"
    ],
    "key_statements": [
        "Zeroth-order optimization has attracted an increasing amount of attention for solving machine learning (ML) problems in scenarios where explicit expressions for the gradients are difficult or infeasible to obtain",
        "The black-box optimization nature limits the practical design of adversarial examples, where internal configurations and operating mechanism of public machine learning systems (e.g., Google Cloud Vision API) are not revealed to practitioners and the only mode of interaction with the system is via submitting inputs and receiving the corresponding predicted outputs (<a class=\"ref-link\" id=\"cPapernot_et+al_2017_a\" href=\"#rPapernot_et+al_2017_a\">Papernot et al, 2017</a>; <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>; <a class=\"ref-link\" id=\"cTu_et+al_2018_a\" href=\"#rTu_et+al_2018_a\">Tu et al, 2018</a>; Ilyas et al, 2018b; <a class=\"ref-link\" id=\"cCheng_et+al_2018_a\" href=\"#rCheng_et+al_2018_a\">Cheng et al, 2018</a>; <a class=\"ref-link\" id=\"cBhagoji_et+al_2018_a\" href=\"#rBhagoji_et+al_2018_a\">Bhagoji et al, 2018</a>)",
        "This paper proposes a zeroth-order (ZO) sign-based descent algorithm for solving black-box optimization problems, e.g. design of black-box adversarial examples",
        "We demonstrate the superior performance of ZO-signSGD for generating adversarial examples from black-box deep neural networks",
        "We show that the commonly-used methods for generating adversarial attacks fall into the framework of signSGD",
        "We show that the generation of adversarial examples in (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a>; <a class=\"ref-link\" id=\"cKurakin_et+al_2017_a\" href=\"#rKurakin_et+al_2017_a\">Kurakin et al, 2017</a>) can be interpreted through signSGD",
        "Compared to SGD-type methods, the fast empirical convergence of signSGD and ZO-signSGD has been shown in the application of generating white-box and black-box adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a>; <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al, 2018</a>; <a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\">Ilyas et al, 2018a</a>)",
        "In Appendix 1, we provide two concrete examples (Fig. A1 and Fig. A2) to confirm the aforementioned analysis",
        "In Fig. A1, we show the robustness of ZO-signSGD against sparse noise perturbation through a toy quadratic optimization problem, originally introduced in (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a>) to motivate the fast convergence of signSGD against SGD",
        "Recall that signSGD in (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a>) achieves O(1/ T ) convergence rate given the condition that the mini-batch size is sufficiently large, b = O(T )",
        "Gk \u2212 \u2207f\u03bc. This crucial relationship presented in Proposition 1 holds for a general class of signSGD-type algorithms that use different ZO gradient estimators",
        "(5), we investigate the second-order moment of gk in Proposition 2",
        "Compared to the variance bound (\u03c32/b) of the stochastic gradient estimate of f in signSGD (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a>), Proposition 2 provides a general result for the ZO gradient estimate gk",
        "We demonstrate the convergence rate of ZO-signSGD in Theorem 1",
        "Note that the convergence rate of ZO-signSGD relies on the learning rate \u03b4k, the problem size d, the smoothing parameter \u03bc, the mini-batch size b, and the number of random perturbations q for",
        "We provide several key insights on the convergence rate of ZO-signSGD through (9)",
        "We study the gradient estimator (11), whose sign is equivalent to the majority vote of signs of individual gradient estimates {\u2207\u02c6 fi(x; ui,j)}",
        "Generating black-box adversarial examples Here we study adversarial robustness by generating adversarial examples from a black-box image classifier trained by a deep neural network (DNN) model; see details on problem formulation in Appendix 8.2",
        "We find that ZO-signSGD usually takes significantly less iterations than other methods to find the first successful adversarial example with a similar attacking loss",
        "Motivated by the impressive convergence behavior of signSGD and the empirical success in c\u221arafti\u221ang adversarial examples from black-box machine learning models, in this paper we rigorously prove the O( d/ T ) convergence rate of ZO-signSGD and its variants under mild conditions"
    ],
    "summary": [
        "Zeroth-order optimization has attracted an increasing amount of attention for solving machine learning (ML) problems in scenarios where explicit expressions for the gradients are difficult or infeasible to obtain.",
        "It was observed in both white-box and black-box settings1 that leveraging the sign information of gradient estimates of an attacking loss can achieve superior empirical performance in generating adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a></a>; <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\"><a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al, 2018</a></a>; <a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\"><a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\">Ilyas et al, 2018a</a></a>).",
        "This paper proposes a zeroth-order (ZO) sign-based descent algorithm for solving black-box optimization problems, e.g. design of black-box adversarial examples.",
        "In the first-order setting, a sign-based stochastic gradient descent method, known as signSGD, was analyzed by (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a></a></a></a>; <a class=\"ref-link\" id=\"cBalles_2017_a\" href=\"#rBalles_2017_a\">Balles & Hennig, 2017</a>).",
        "We carefully study the effects of different types of gradient estimators on the convergence of ZO-signSGD, and propose three variants of ZO-signSGD for both centralized and distributed ZO optimization.",
        "Compared to SGD-type methods, the fast empirical convergence of signSGD and ZO-signSGD has been shown in the application of generating white-box and black-box adversarial examples (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a></a>; <a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\"><a class=\"ref-link\" id=\"cMadry_et+al_2018_a\" href=\"#rMadry_et+al_2018_a\">Madry et al, 2018</a></a>; <a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\"><a class=\"ref-link\" id=\"cIlyas_et+al_2018_a\" href=\"#rIlyas_et+al_2018_a\">Ilyas et al, 2018a</a></a>).",
        "Recall that signSGD in (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a></a></a></a>) achieves O(1/ T ) convergence rate given the condition that the mini-batch size is sufficiently large, b = O(T ).",
        "This crucial relationship presented in Proposition 1 holds for a general class of signSGD-type algorithms that use different ZO gradient estimators.",
        "Compared to the variance bound (\u03c32/b) of the stochastic gradient estimate of f in signSGD (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a></a></a></a>), Proposition 2 provides a general result for the ZO gradient estimate gk.",
        "Proposition 2 implies that the use of large b and q reduces the variance of the gradient estimate, and will further improve the convergence rate.",
        "Note that the convergence rate of ZO-signSGD relies on the learning rate \u03b4k, the problem size d, the smoothing parameter \u03bc, the mini-batch size b, and the number of random perturbations q for",
        "Corollary 1 Suppose that the conditions in Theorem 1 hold, ZO-signSGD with gradient estimator (10) yields the same convergence rate of ZO-signSGD that uses the estimator (3).",
        "It was shown in (<a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\"><a class=\"ref-link\" id=\"cBernstein_et+al_2018_a\" href=\"#rBernstein_et+al_2018_a\">Bernstein et al, 2018</a></a></a></a>) that signSGD with majority vote has a better convergence rate under additional assumptions of unimodal symmetric noise distribution of coordinate-wise gradient estimates.",
        "Motivated by the impressive convergence behavior of signSGD and the empirical success in c\u221arafti\u221ang adversarial examples from black-box ML models, in this paper we rigorously prove the O( d/ T ) convergence rate of ZO-signSGD and its variants under mild conditions.",
        "We would like to generalize our analysis to nonsmooth and nonconvex constrained optimization problems"
    ],
    "headline": "On the application side we explore the connection between ZO-signSGD and black-box adversarial attacks in robust deep learning",
    "reference_links": [
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Balles_2017_a",
            "entry": "L. Balles and P. Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients. arXiv preprint arXiv:1705.07774, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07774"
        },
        {
            "id": "Bernstein_et+al_2018_a",
            "entry": "J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04434"
        },
        {
            "id": "Bhagoji_et+al_2018_a",
            "entry": "A. N. Bhagoji, W. He, B. Li, and D. Song. Practical black-box attacks on deep neural networks using efficient query mechanisms. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 154\u2013169, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhagoji%2C%20A.N.%20He%2C%20W.%20Li%2C%20B.%20Song%2C%20D.%20Practical%20black-box%20attacks%20on%20deep%20neural%20networks%20using%20efficient%20query%20mechanisms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhagoji%2C%20A.N.%20He%2C%20W.%20Li%2C%20B.%20Song%2C%20D.%20Practical%20black-box%20attacks%20on%20deep%20neural%20networks%20using%20efficient%20query%20mechanisms%202018"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, pp. 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15\u201326. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02941"
        },
        {
            "id": "Cheng_et+al_2018_a",
            "entry": "M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh. Query-efficient hard-label black-box attack: An optimization-based approach. arXiv preprint arXiv:1807.04457, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04457"
        },
        {
            "id": "Duchi_et+al_2012_a",
            "entry": "J. C. Duchi, P. L. Bartlett, and M. J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674\u2013701, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Bartlett%2C%20P.L.%20Wainwright%2C%20M.J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Bartlett%2C%20P.L.%20Wainwright%2C%20M.J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012"
        },
        {
            "id": "Duchi_et+al_2015_a",
            "entry": "J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788\u20132806, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Jordan%2C%20M.I.%20Wainwright%2C%20M.J.%20Wibisono%2C%20A.%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Jordan%2C%20M.I.%20Wainwright%2C%20M.J.%20Wibisono%2C%20A.%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%202015"
        },
        {
            "id": "Gao_et+al_2014_a",
            "entry": "X. Gao, B. Jiang, and S. Zhang. On the information-adaptive variants of the ADMM: an iteration complexity perspective. Optimization Online, 12, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20X.%20Jiang%2C%20B.%20Zhang%2C%20S.%20On%20the%20information-adaptive%20variants%20of%20the%20ADMM%3A%20an%20iteration%20complexity%20perspective%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20X.%20Jiang%2C%20B.%20Zhang%2C%20S.%20On%20the%20information-adaptive%20variants%20of%20the%20ADMM%3A%20an%20iteration%20complexity%20perspective%202014"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "Ghadimi_et+al_2016_a",
            "entry": "S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267\u2013305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. 2015 ICLR, arXiv preprint arXiv:1412.6572, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "B. Gu, Z. Huo, and H. Huang. Zeroth-order asynchronous doubly stochastic algorithm with variance reduction. arXiv preprint arXiv:1612.01425, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01425"
        },
        {
            "id": "Hajinezhad_et+al_2017_a",
            "entry": "D. Hajinezhad, M. Hong, and A. Garcia. Zenith: A zeroth-order distributed algorithm for multi-agent nonconvex optimization. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hajinezhad%2C%20D.%20Hong%2C%20M.%20Garcia%2C%20A.%20Zenith%3A%20A%20zeroth-order%20distributed%20algorithm%20for%20multi-agent%20nonconvex%20optimization%202017"
        },
        {
            "id": "Ilyas_et+al_2018_a",
            "entry": "A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversarial attacks with limited queries and information. In International Conference on Machine Learning, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilyas%2C%20A.%20Engstrom%2C%20L.%20Athalye%2C%20A.%20Lin%2C%20J.%20Black-box%20adversarial%20attacks%20with%20limited%20queries%20and%20information%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilyas%2C%20A.%20Engstrom%2C%20L.%20Athalye%2C%20A.%20Lin%2C%20J.%20Black-box%20adversarial%20attacks%20with%20limited%20queries%20and%20information%202018"
        },
        {
            "id": "Ilyas_et+al_0000_a",
            "entry": "A. Ilyas, L. Engstrom, and A. Madry. Prior convictions: Black-box adversarial attacks with bandits and priors. arXiv preprint arXiv:1807.07978, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1807.07978"
        },
        {
            "id": "Jamieson_et+al_2012_a",
            "entry": "K. G. Jamieson, R. Nowak, and B. Recht. Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20K.G.%20Nowak%2C%20R.%20Recht%2C%20B.%20Query%20complexity%20of%20derivative-free%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20K.G.%20Nowak%2C%20R.%20Recht%2C%20B.%20Query%20complexity%20of%20derivative-free%20optimization%202012"
        },
        {
            "id": "Kinga_2015_a",
            "entry": "D. Kinga and J. B. Adam. A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kinga%2C%20D.%20Adam%2C%20J.B.%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kinga%2C%20D.%20Adam%2C%20J.B.%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kurakin_et+al_2017_a",
            "entry": "Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017 ICLR, arXiv preprint arXiv:1611.01236, 2017. URL http://arxiv.org/abs/1611.01236.",
            "url": "http://arxiv.org/abs/1611.01236",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "Lei_et+al_2017_a",
            "entry": "L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pp. 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "Lian_et+al_2016_a",
            "entry": "X. Lian, H. Zhang, C.-J. Hsieh, Y. Huang, and J. Liu. A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order. In Advances in Neural Information Processing Systems, pp. 3054\u20133062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016"
        },
        {
            "id": "Lin_et+al_2019_a",
            "entry": "J. Lin, C. Gan, and S. Han. Defensive quantization: When efficiency meets robustness. In International Conference on Learning Representations (ICLR), 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20J.%20Gan%2C%20C.%20Han%2C%20S.%20Defensive%20quantization%3A%20When%20efficiency%20meets%20robustness%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20J.%20Gan%2C%20C.%20Han%2C%20S.%20Defensive%20quantization%3A%20When%20efficiency%20meets%20robustness%202019"
        },
        {
            "id": "Liu_et+al_0000_a",
            "entry": "L. Liu, M. Cheng, C.-J. Hsieh, and D. Tao. Stochastic zeroth-order optimization via variance reduction method. arXiv preprint arXiv:1805.11811, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11811"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "S. Liu, J. Chen, P.-Y. Chen, and A. O. Hero. Zeroth-order online ADMM: Convergence analysis and applications. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pp. 288\u2013297, April 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20S.%20Chen%2C%20J.%20Chen%2C%20P.-Y.%20Hero%2C%20A.O.%20Zeroth-order%20online%20ADMM%3A%20Convergence%20analysis%20and%20applications%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20S.%20Chen%2C%20J.%20Chen%2C%20P.-Y.%20Hero%2C%20A.O.%20Zeroth-order%20online%20ADMM%3A%20Convergence%20analysis%20and%20applications%202018-04"
        },
        {
            "id": "Liu_et+al_0000_b",
            "entry": "S. Liu, B. Kailkhura, P.-Y. Chen, P. Ting, S. Chang, and L. Amini. Zeroth-order stochastic variance reduction for nonconvex optimization. arXiv preprint arXiv:1805.10367, 2018c.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10367"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box attacks. ICLR, arXiv preprint arXiv:1611.02770, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "Madry_et+al_2018_a",
            "entry": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "Nesterov_2015_a",
            "entry": "Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 2(17):527\u2013566, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%202015"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Shaham_et+al_2018_a",
            "entry": "U. Shaham, Y. Yamada, and S. Negahban. Understanding adversarial training: Increasing local stability of supervised models through robust optimization. Neurocomputing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaham%2C%20U.%20Yamada%2C%20Y.%20Negahban%2C%20S.%20Understanding%20adversarial%20training%3A%20Increasing%20local%20stability%20of%20supervised%20models%20through%20robust%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaham%2C%20U.%20Yamada%2C%20Y.%20Negahban%2C%20S.%20Understanding%20adversarial%20training%3A%20Increasing%20local%20stability%20of%20supervised%20models%20through%20robust%20optimization%202018"
        },
        {
            "id": "Shamir_2017_a",
            "entry": "O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning Research, 18(52):1\u201311, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20O.%20An%20optimal%20algorithm%20for%20bandit%20and%20zero-order%20convex%20optimization%20with%20two-point%20feedback%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20O.%20An%20optimal%20algorithm%20for%20bandit%20and%20zero-order%20convex%20optimization%20with%20two-point%20feedback%202017"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tu_et+al_2018_a",
            "entry": "C.-C. Tu, P. Ting, P.-Y. Chen, S. Liu, H. Zhang, J. Yi, C.-J. Hsieh, and S.-M. Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. arXiv preprint arXiv:1805.11770, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11770"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Y. Wang, S. Du, S. Balakrishnan, and A. Singh. Stochastic zeroth-order optimization in high dimensions. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pp. 1356\u20131365. PMLR, April 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Du%2C%20S.%20Balakrishnan%2C%20S.%20Singh%2C%20A.%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Du%2C%20S.%20Balakrishnan%2C%20S.%20Singh%2C%20A.%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%202018-04"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "P. Xu, F. Roosta-Khorasan, and M. W. Mahoney. Second-order optimization for non-convex machine learning: An empirical study. arXiv preprint arXiv:1708.07827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07827"
        }
    ]
}
