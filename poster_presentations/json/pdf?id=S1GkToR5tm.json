{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "author": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei",
        "date": 2015,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=S1GkToR5tm",
            "doi": "10.1007/s11263-015-0816-y"
        },
        "journal": "International Journal of Computer Vision",
        "volume": "115",
        "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm\u2014called Discriminator Rejection Sampling (DRS)\u2014that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the state of the art SAGAN model. On ImageNet, we train an improved baseline that increases the best published Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.",
        "pages": "211-252"
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "data distribution",
            "url": "https://en.wikipedia.org/wiki/data_distribution"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "rejection sampling",
            "url": "https://en.wikipedia.org/wiki/rejection_sampling"
        }
    ],
    "abbreviations": {
        "DRS": "Discriminator Rejection Sampling",
        "GANs": "Generative Adversarial Networks",
        "GAN": "generative adversarial network",
        "IS": "INCEPTION SCORE",
        "FID": "Frechet Inception Distance",
        "SAGAN": "Self-Attention GAN"
    },
    "highlights": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are a powerful tool for image synthesis",
        "The Generative Adversarial Networks training procedure is a two-player differentiable game, and the game dynamics are largely what distinguishes the study of Generative Adversarial Networks from the study of other generative models",
        "In Section 3, we explain how to apply this rejection sampling algorithm to the Generative Adversarial Networks framework: in brief, we draw samples from the trained generator, pg(x), and reject some of those samples using the discriminator to attain a closer approximation to the true data distribution, pd(x)",
        "We have proposed a rejection sampling scheme using the Generative Adversarial Networks discriminator to approximately correct errors in the Generative Adversarial Networks generator distribution",
        "There\u2019s no reason that our scheme can only be applied to Generative Adversarial Networks generators",
        "It seems worth investigating whether rejection sampling can improve e.g. VAE decoders"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are a powerful tool for image synthesis",
        "The Generative Adversarial Networks training procedure is a two-player differentiable game, and the game dynamics are largely what distinguishes the study of Generative Adversarial Networks from the study of other generative models",
        "These game dynamics have well-known and heavily studied stability issues. Addressing these issues is an active area of research (<a class=\"ref-link\" id=\"cMao_et+al_2017_a\" href=\"#rMao_et+al_2017_a\">Mao et al, 2017</a>; <a class=\"ref-link\" id=\"cArjovsky_et+al_2017_a\" href=\"#rArjovsky_et+al_2017_a\">Arjovsky et al, 2017</a>; <a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al, 2017</a>; <a class=\"ref-link\" id=\"cOdena_et+al_2018_a\" href=\"#rOdena_et+al_2018_a\">Odena et al, 2018</a>; <a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al, 2017</a>)",
        "We focus on using the discriminator as part of a probabilistic rejection sampling scheme",
        "We propose a rejection sampling scheme using the Generative Adversarial Networks discriminator to approximately correct errors in the Generative Adversarial Networks generator distribution",
        "We show that under quite strict assumptions, this scheme allows us to recover the data distribution exactly",
        "We examine where those strict assumptions break down and design a practical algorithm \u2013 called Discriminator Rejection Sampling \u2013 that takes this into account",
        "In Section 3, we explain how to apply this rejection sampling algorithm to the Generative Adversarial Networks framework: in brief, we draw samples from the trained generator, pg(x), and reject some of those samples using the discriminator to attain a closer approximation to the true data distribution, pd(x)",
        "This section describes the Discriminator Rejection Sampling (DRS) procedure, which is an adjustment of the idealized procedure, meant to address the above issues",
        "We justify the modifications made to the idealized algorithm. We do this by conducting two experiments in which we show that (according to popular measures of how well a Generative Adversarial Networks has",
        "MIXTURE OF 25 GAUSSIANS We investigate the impact of Discriminator Rejection Sampling on a low-dimensional synthetic data set consisting of a mixture of twenty-five 2D isotropic Gaussian distributions arranged in a grid (<a class=\"ref-link\" id=\"cDumoulin_et+al_2016_a\" href=\"#rDumoulin_et+al_2016_a\">Dumoulin et al, 2016</a>; Srivastava et al, 2017; <a class=\"ref-link\" id=\"cLin_et+al_2017_a\" href=\"#rLin_et+al_2017_a\">Lin et al, 2017</a>)",
        "We have proposed a rejection sampling scheme using the Generative Adversarial Networks discriminator to approximately correct errors in the Generative Adversarial Networks generator distribution",
        "There\u2019s no reason that our scheme can only be applied to Generative Adversarial Networks generators",
        "It seems worth investigating whether rejection sampling can improve e.g. VAE decoders",
        "It would be interesting to theoretically characterize the efficacy of rejection sampling under the breakdown-of-assumptions that we have described earlier"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are a powerful tool for image synthesis.",
        "The GAN training procedure pits two neural networks against each other, a generator and a discriminator.",
        "The discriminator is trained to distinguish between samples from the target distribution and samples from the generator.",
        "It\u2019s well known that the equilibrium of this training procedure is reached when sampling from the generator is identical to sampling from the target distribution and the discriminator always outputs 1/2.",
        "If there is useful information left in the discriminator, why doesn\u2019t it find its way into the generator via the training procedure?",
        "In Section 3, we explain how to apply this rejection sampling algorithm to the GAN framework: in brief, we draw samples from the trained generator, pg(x), and reject some of those samples using the discriminator to attain a closer approximation to the true data distribution, pd(x).",
        "We introduce our proposed rejection sampling scheme for GANs. We\u2019ll first derive an idealized version of the algorithm that will rely on assumptions that don\u2019t necessarily hold in realistic settings.",
        "5. Rejection sampling is known to have too low an acceptance probability when the target distribution is high dimensional (<a class=\"ref-link\" id=\"cMackay_2003_a\" href=\"#rMackay_2003_a\">MacKay, 2003</a>).",
        "This section describes the Discriminator Rejection Sampling (DRS) procedure, which is an adjustment of the idealized procedure, meant to address the above issues.",
        "If DM \u2217 is very large, the acceptance probability eD\u2217(x)\u2212DM \u2217 will be close to zero, and almost all samples will be rejected, which is undesirable.",
        "We do this by conducting two experiments in which we show that Discriminator Rejection Sampling yields improvements for actual GANs. We start with a toy example that yields insight into how DRS can help, after which we demonstrate DRS on the ImageNet dataset (Russakovsky et al, 2015).",
        "We train a GAN model where the generator and discriminator are neural networks with four fully connected layers with ReLu activations.",
        "Since SAGAN uses a hinge loss and DRS requires a sigmoid output, we added a fully-connected layer \u201con top of\u201d the trained discriminator and trained it to distinguish real images from fake ones using the binary cross-entropy loss.",
        "Figure 4 shows that the subjective visual quality of samples with high acceptance probability is considerably better.",
        "We have proposed a rejection sampling scheme using the GAN discriminator to approximately correct errors in the GAN generator distribution.",
        "This seems like it might help, because VAEs may have trouble with \u201cspreading mass around\u201d too much"
    ],
    "headline": "We propose a rejection sampling scheme using the discriminator of a Generative Adversarial Networks to approximately correct errors in the Generative Adversarial Networks generator distribution",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. ArXiv e-prints, January 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20GAN%2C%20L.Bottou%20Wasserstein%20ArXiv%20e-prints%202017-01"
        },
        {
            "id": "Arora_2017_a",
            "entry": "Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. CoRR, abs/1706.08224, 2017. URL http://arxiv.org/abs/1706.08224.",
            "url": "http://arxiv.org/abs/1706.08224",
            "arxiv_url": "https://arxiv.org/pdf/1706.08224"
        },
        {
            "id": "Azadi_et+al_2018_a",
            "entry": "Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. Multi-content gan for few-shot font style transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 11, pp. 13, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azadi%2C%20Samaneh%20Fisher%2C%20Matthew%20Kim%2C%20Vladimir%20Wang%2C%20Zhaowen%20Multi-content%20gan%20for%20few-shot%20font%20style%20transfer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azadi%2C%20Samaneh%20Fisher%2C%20Matthew%20Kim%2C%20Vladimir%20Wang%2C%20Zhaowen%20Multi-content%20gan%20for%20few-shot%20font%20style%20transfer%202018"
        },
        {
            "id": "Casella_et+al_2004_a",
            "entry": "George Casella, Christian P Robert, Martin T Wells, et al. Generalized accept-reject sampling schemes. In A Festschrift for Herman Rubin, pp. 342\u2013347. Institute of Mathematical Statistics, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Casella%2C%20George%20Robert%2C%20Christian%20P.%20Wells%2C%20Martin%20T.%20Generalized%20accept-reject%20sampling%20schemes.%20In%20A%20Festschrift%20for%20Herman%20Rubin%202004"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially Learned Inference. ArXiv e-prints, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20V.%20Belghazi%2C%20I.%20Poole%2C%20B.%20Lamb%2C%20A.%20Adversarially%20Learned%20Inference.%20ArXiv%20e-prints%202016-06"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. ArXiv e-prints, June 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20J%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20Adversarial%20Networks%20ArXiv%20eprints%20June%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20J%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20Adversarial%20Networks%20ArXiv%20eprints%20June%202014"
        },
        {
            "id": "Grover_et+al_2018_a",
            "entry": "Aditya Grover, Ramki Gummadi, Miguel Lazaro-Gredilla, Dale Schuurmans, and Stefano Ermon. Variational rejection sampling. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20Aditya%20Gummadi%2C%20Ramki%20Lazaro-Gredilla%2C%20Miguel%20Schuurmans%2C%20Dale%20Variational%20rejection%20sampling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20Aditya%20Gummadi%2C%20Ramki%20Lazaro-Gredilla%2C%20Miguel%20Schuurmans%2C%20Dale%20Variational%20rejection%20sampling%202018"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Mart\u0131n Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017. URL http://arxiv.org/abs/1704.00028.",
            "url": "http://arxiv.org/abs/1704.00028",
            "arxiv_url": "https://arxiv.org/pdf/1704.00028"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. ArXiv e-prints, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20Trained%20by%20a%20Two%20Time-Scale%20Update%20Rule%20Converge%20to%20a%20Local%20Nash%20Equilibrium.%20ArXiv%20e-prints%202017-06"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "Kumar_et+al_2017_a",
            "entry": "A. Kumar, P. Sattigeri, and P. T. Fletcher. Improved Semi-supervised Learning with GANs using Manifold Invariances. ArXiv e-prints, May 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20A.%20Sattigeri%2C%20P.%20Fletcher%2C%20P.T.%20Improved%20Semi-supervised%20Learning%20with%20GANs%20using%20Manifold%20Invariances.%20ArXiv%20e-prints%202017-05"
        },
        {
            "id": "Ledig_et+al_2017_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, volume 2, pp. 4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pp. 2203\u20132213, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "Lim_2017_a",
            "entry": "Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02894"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04086"
        },
        {
            "id": "Mackay_2003_a",
            "entry": "David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20J.C.%20Information%20theory%2C%20inference%20and%20learning%20algorithms%202003"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "T. Miyato and M. Koyama. cGANs with Projection Discriminator. ArXiv e-prints, February 2018. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-. A. Odena. Semi-Supervised Learning with Generative Adversarial Networks. ArXiv e-prints, June 2016.",
            "url": "https://openreview.net/forum?id=B1QRgziT-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Koyama%2C%20M.%20cGANs%20with%20Projection%20Discriminator.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "Odena_et+al_2018_a",
            "entry": "A. Odena, J. Buckman, C. Olsson, T. B. Brown, C. Olah, C. Raffel, and I. Goodfellow. Is Generator Conditioning Causally Related to GAN Performance? ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20A.%20Buckman%2C%20J.%20Olsson%2C%20C.%20Brown%2C%20T.B.%20Is%20Generator%20Conditioning%20Causally%20Related%20to%20GAN%20Performance%3F%20ArXiv%20e-prints%202018-02"
        }
    ]
}
