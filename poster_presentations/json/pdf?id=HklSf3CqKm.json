{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SUBGRADIENT DESCENT LEARNS ORTHOGONAL DIC-",
        "author": "Yu Bai, Qijia Jiang & Ju Sun Stanford University {yub,qjiang,sunju}@stanford.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HklSf3CqKm"
        },
        "abstract": "This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can recover orthogonal dictionaries on a natural nonsmooth, nonconvex 1 minimization formulation of the problem, under mild statistical assumption on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among other applications. Preliminary synthetic and real experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries."
    },
    "keywords": [
        {
            "term": "sparse coding",
            "url": "https://en.wikipedia.org/wiki/sparse_coding"
        },
        {
            "term": "riemannian manifold",
            "url": "https://en.wikipedia.org/wiki/riemannian_manifold"
        },
        {
            "term": "Dictionary learning",
            "url": "https://en.wikipedia.org/wiki/Dictionary_learning"
        }
    ],
    "abbreviations": {
        "DL": "Dictionary learning"
    },
    "highlights": [
        "Dictionary learning (DL), i.e. , sparse coding, concerns the problem of learning compact representations, i.e., given data Y , one tries to find a representation basis A and coefficients X, so that Y \u2248 AX where X is most sparse",
        "The latter methods are invariably based on nonconvex optimization with model-dependent initialization, rendering their practicality on real data questionable",
        "As long as the initialization belongs to S1(i/+(5)log n) or S1(i/\u2212(5)log n), our finding-one-basis result in Theorem 3.8 guarantees that Riemannian subgradient descent will converge to ei or \u2212ei respectively",
        "This paper presents the first theoretical guarantee for orthogonal dictionary learning using subgradient descent on a natural 1 minimization formulation"
    ],
    "key_statements": [
        "Dictionary learning (DL), i.e. , sparse coding, concerns the problem of learning compact representations, i.e., given data Y , one tries to find a representation basis A and coefficients X, so that Y \u2248 AX where X is most sparse",
        "The latter methods are invariably based on nonconvex optimization with model-dependent initialization, rendering their practicality on real data questionable",
        "As long as the initialization belongs to S1(i/+(5)log n) or S1(i/\u2212(5)log n), our finding-one-basis result in Theorem 3.8 guarantees that Riemannian subgradient descent will converge to ei or \u2212ei respectively",
        "This paper presents the first theoretical guarantee for orthogonal dictionary learning using subgradient descent on a natural 1 minimization formulation"
    ],
    "summary": [
        "Dictionary learning (DL), i.e. , sparse coding, concerns the problem of learning compact representations, i.e., given data Y , one tries to find a representation basis A and coefficients X, so that Y \u2248 AX where X is most sparse.",
        "We show that with sufficiently many samples, the optimization landscape of formulation (1.1) is benign with high probability, and a simple Riemannian subgradient descent algorithm can provably recover A in polynomial time.",
        "For m \u2265 \u03a9(\u03b8\u22122n4 log4 n), the following holds with high probability: there exists a poly(m, \u22121)time algorithm, which runs Riemannian subgradient descent on formulation (1.1) from at most O(n log n) independent, uniformly random initial points, and outputs a set of vectors {a1, .",
        "With probability at least 1 \u2212 exp \u2212cm\u03b83n\u22123 log\u22123 m \u2212 exp (\u2212c R/n), an algorithm which runs Riemannian subgradient descent R = C n log n times with independent random initializations on Sn\u22121 outputs a set of vectors {a1, .",
        "The benign geometry of the empirical objective allows a simple Riemannian subgradient descent algorithm to find one basis vector a time.",
        "The above optimization result (Theorem 3.8) shows that Riemannian subgradient descent is able to find the basis vector en when initialized in the associated region S1(n/(+5)log n).",
        "As long as the initialization belongs to S1(i/+(5)log n) or S1(i/\u2212(5)log n), our finding-one-basis result in Theorem 3.8 guarantees that Riemannian subgradient descent will converge to ei or \u2212ei respectively.",
        "If we run the algorithm with independent, uniformly random initializations on the sphere multiple times, by a coupon collector\u2019s argument, we will recover all the basis vectors.",
        "A uniformly random initialization on Sn\u22121, and choose the provided that R \u2265 C n log n, all standard basis vectors step size as \u03b7(k) will be recovered",
        "As long as we are able to bound the Hausdorff distance, all directional differences between the subdifferentials are simultaneously bounded, which is exactly what we want to show to carry the benign geometry from the population to the empirical objective.",
        "We \u221arun the Riemannian subgradient descent algorithm Eq (3.16) with decaying step size \u03b7(k) = 1/ k, corresponding to the boundary case \u03b1 = 1/2 in Theorem 3.8 with a much better base size.",
        "Result From our simulations, Riemannian subgradient descent succeeds in recovering the dictionary as long as m \u2265 Cn2 (Fig. 2), across different sparsity level \u03b8.",
        "As our main geometric result Theorem 3.6 already achieved tight bounds on the directional derivatives, further sample complexity improvement could potentially come out of utilizing second-order information such as the strong negative curvature (Lemma B.2), or careful algorithm-dependent analysis."
    ],
    "headline": "We show that a subgradient descent algorithm, with random initialization, can recover orthogonal dictionaries on a natural nonsmooth, nonconvex 1 minimization formulation of the problem, under mild statistical assumption on the data",
    "reference_links": [
        {
            "id": "Agarwal_et+al_2017_a",
            "entry": "Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli. A clustering approach to learning sparsely used overcomplete dictionaries. IEEE Trans. Information Theory, 63(1):575\u2013592, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Anandkumar%2C%20Animashree%20Netrapalli%2C%20Praneeth%20A%20clustering%20approach%20to%20learning%20sparsely%20used%20overcomplete%20dictionaries%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Anandkumar%2C%20Animashree%20Netrapalli%2C%20Praneeth%20A%20clustering%20approach%20to%20learning%20sparsely%20used%20overcomplete%20dictionaries%202017"
        },
        {
            "id": "Arora_et+al_2014_a",
            "entry": "Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In Conference on Learning Theory, pp. 779\u2013806, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Moitra%2C%20Ankur%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Moitra%2C%20Ankur%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014"
        },
        {
            "id": "Arora_et+al_2015_a",
            "entry": "Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Simple%2C%20Ankur%20Moitra%20and%20neural%20algorithms%20for%20sparse%20coding%202015"
        },
        {
            "id": "Aubin_1998_a",
            "entry": "Jean-Pierre Aubin. Optima and equilibria: an introduction to nonlinear analysis, volume 140. Springer Science & Business Media, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aubin%2C%20Jean-Pierre%20Optima%20and%20equilibria%3A%20an%20introduction%20to%20nonlinear%20analysis%2C%20volume%20140%201998"
        },
        {
            "id": "Awasthi_2018_a",
            "entry": "Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dictionaries with arbitrary supports. arXiv preprint arXiv:1804.08603, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08603"
        },
        {
            "id": "Bagirov_et+al_2014_a",
            "entry": "Adil Bagirov, Napsu Karmitsa, and Marko M Makela. Introduction to Nonsmooth Optimization: theory, practice and software. Springer, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagirov%2C%20Adil%20Karmitsa%2C%20Napsu%20Makela%2C%20Marko%20M.%20Introduction%20to%20Nonsmooth%20Optimization%3A%20theory%2C%20practice%20and%20software%202014"
        },
        {
            "id": "Barak_et+al_2015_a",
            "entry": "Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp. 143\u2013151. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barak%2C%20Boaz%20Kelner%2C%20Jonathan%20A.%20Steurer%2C%20David%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barak%2C%20Boaz%20Kelner%2C%20Jonathan%20A.%20Steurer%2C%20David%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015"
        },
        {
            "id": "Boucheron_et+al_2005_a",
            "entry": "Stephane Boucheron, Olivier Bousquet, and Gabor Lugosi. Theory of classification: A survey of some recent advances. ESAIM: probability and statistics, 9:323\u2013375, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boucheron%2C%20Stephane%20Bousquet%2C%20Olivier%20Lugosi%2C%20Gabor%20Theory%20of%20classification%3A%20A%20survey%20of%20some%20recent%20advances%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boucheron%2C%20Stephane%20Bousquet%2C%20Olivier%20Lugosi%2C%20Gabor%20Theory%20of%20classification%3A%20A%20survey%20of%20some%20recent%20advances%202005"
        },
        {
            "id": "Burke_et+al_2005_a",
            "entry": "James V Burke, Adrian S Lewis, and Michael L Overton. A robust gradient sampling algorithm for nonsmooth, nonconvex optimization. SIAM Journal on Optimization, 15(3):751\u2013779, 2005. doi: 10.1137/030601296.",
            "crossref": "https://dx.doi.org/10.1137/030601296",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/030601296"
        },
        {
            "id": "Burke_et+al_2018_a",
            "entry": "James V. Burke, Frank E. Curtis, Adrian S. Lewis, Michael L. Overton, and Lucas E. A. Simes. Gradient sampling methods for nonsmooth optimization. arXiv:1804.11003, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11003"
        },
        {
            "id": "Candes_2014_a",
            "entry": "Emmanuel Candes. Mathematics of sparsity (and a few other things). In Proceedings of the International Congress of Mathematicians, volume 123, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20Mathematics%20of%20sparsity%20%28and%20a%20few%20other%20things%29%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20Mathematics%20of%20sparsity%20%28and%20a%20few%20other%20things%29%202014"
        },
        {
            "id": "Chatterji_1997_a",
            "entry": "Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random initialization. In Advances in Neural Information Processing Systems, pp. 1997\u20132006, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chatterji%2C%20Niladri%20Bartlett%2C%20Peter%20L.%20Alternating%20minimization%20for%20dictionary%20learning%20with%20random%20initialization%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chatterji%2C%20Niladri%20Bartlett%2C%20Peter%20L.%20Alternating%20minimization%20for%20dictionary%20learning%20with%20random%20initialization%201997"
        },
        {
            "id": "Clarke_1990_a",
            "entry": "Frank H Clarke. Optimization and nonsmooth analysis. SIAM, 1990. doi: 10.1137/1.9781611971309.",
            "crossref": "https://dx.doi.org/10.1137/1.9781611971309",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/1.9781611971309"
        },
        {
            "id": "Curtis_2015_a",
            "entry": "Frank E Curtis and Xiaocun Que. A quasi-newton algorithm for nonconvex, nonsmooth optimization with global convergence guarantees. Mathematical Programming Computation, 7(4):399\u2013428, 2015. doi: 10.1007/s12532-015-0086-2.",
            "crossref": "https://dx.doi.org/10.1007/s12532-015-0086-2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s12532-015-0086-2"
        },
        {
            "id": "Curtis_et+al_2017_a",
            "entry": "Frank E Curtis, Tim Mitchell, and Michael L Overton. A bfgs-sqp method for nonsmooth, nonconvex, constrained optimization and its evaluation using relative minimization profiles. Optimization Methods and Software, 32(1):148\u2013181, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curtis%2C%20Frank%20E.%20Mitchell%2C%20Tim%20Overton%2C%20Michael%20L.%20A%20bfgs-sqp%20method%20for%20nonsmooth%2C%20nonconvex%2C%20constrained%20optimization%20and%20its%20evaluation%20using%20relative%20minimization%20profiles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Curtis%2C%20Frank%20E.%20Mitchell%2C%20Tim%20Overton%2C%20Michael%20L.%20A%20bfgs-sqp%20method%20for%20nonsmooth%2C%20nonconvex%2C%20constrained%20optimization%20and%20its%20evaluation%20using%20relative%20minimization%20profiles%202017"
        },
        {
            "id": "Davis_et+al_2017_a",
            "entry": "Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase retrieval. arxiv:1711.03247, 2017. URL http://arxiv.org/abs/1711.03247.",
            "url": "http://arxiv.org/abs/1711.03247",
            "arxiv_url": "https://arxiv.org/pdf/1711.03247"
        },
        {
            "id": "Duchi_2017_a",
            "entry": "John C. Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval. arxiv:1705.02356, 2017. URL http://arxiv.org/abs/1705.02356.",
            "url": "http://arxiv.org/abs/1705.02356",
            "arxiv_url": "https://arxiv.org/pdf/1705.02356"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arxiv:1711.00501, 2017. URL http://arxiv.org/abs/1711.00501.",
            "url": "http://arxiv.org/abs/1711.00501",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Gilboa_et+al_2018_a",
            "entry": "Dar Gilboa, Sam Buchanan, and John Wright. Efficient dictionary learning with gradient descent. In ICML 2018 Workshop on Modern Trends in Nonconvex Optimization for Machine Learning, 2018. URL https://arxiv.org/abs/1809.10313.",
            "url": "https://arxiv.org/abs/1809.10313",
            "arxiv_url": "https://arxiv.org/pdf/1809.10313"
        },
        {
            "id": "Grohs_2015_a",
            "entry": "P Grohs and S Hosseini. Nonsmooth trust region algorithms for locally Lipschitz functions on Riemannian manifolds. IMA Journal of Numerical Analysis, 36(3):1167\u20131192, 2015. doi: 10. 1093/imanum/drv043.",
            "crossref": "https://dx.doi.org/10.1093/imanum/drv043",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1093/imanum/drv043"
        },
        {
            "id": "Grohs_2016_a",
            "entry": "Philipp Grohs and Seyedehsomayeh Hosseini. \u03b5-subgradient algorithms for locally Lipschitz functions on Riemannian manifolds. Advances in Computational Mathematics, 42(2):333\u2013360, 2016. doi: 10.1007/s10444-015-9426-z.",
            "crossref": "https://dx.doi.org/10.1007/s10444-015-9426-z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10444-015-9426-z"
        },
        {
            "id": "Hiriart-Urruty_2001_a",
            "entry": "Jean-Baptiste Hiriart-Urruty and Claude Lemarchal. Fundamentals of Convex Analysis. SpringerVerlag Berlin Heidelberg, 2001. doi: 10.1007/978-3-642-56468-0.",
            "crossref": "https://dx.doi.org/10.1007/978-3-642-56468-0"
        },
        {
            "id": "Hosseini_2011_a",
            "entry": "S Hosseini and MR Pouryayevali. Generalized gradients and characterization of epi-lipschitz sets in Riemannian manifolds. Nonlinear Analysis: Theory, Methods & Applications, 74(12):3884\u20133895, 2011. doi: 10.1016/j.na.2011.02.023.",
            "crossref": "https://dx.doi.org/10.1016/j.na.2011.02.023",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.na.2011.02.023"
        },
        {
            "id": "Hosseini_2015_a",
            "entry": "Seyedehsomayeh Hosseini. Optimality conditions for global minima of nonconvex functions on Riemannian manifolds. Pacific Journal of Optimization, 2015. URL http://uschmajew.ins.uni-bonn.de/research/pub/hosseini/3.pdf.",
            "url": "http://uschmajew.ins.uni-bonn.de/research/pub/hosseini/3.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hosseini%2C%20Seyedehsomayeh%20Optimality%20conditions%20for%20global%20minima%20of%20nonconvex%20functions%20on%20Riemannian%20manifolds%202015"
        },
        {
            "id": "Hosseini_2017_a",
            "entry": "Seyedehsomayeh Hosseini and Andre Uschmajew. A Riemannian gradient sampling algorithm for nonsmooth optimization on manifolds. SIAM Journal on Optimization, 27(1):173\u2013189, 2017. doi: 10.1137/16M1069298.",
            "crossref": "https://dx.doi.org/10.1137/16M1069298",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/16M1069298"
        },
        {
            "id": "Kakade_2018_a",
            "entry": "Sham Kakade and Jason D. Lee. Provably correct automatic subdifferentiation for qualified programs. arXiv:1809.08530, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.08530"
        },
        {
            "id": "Laurent_2017_a",
            "entry": "Thomas Laurent and James von Brecht. The multilinear structure of relu networks. arxiv:1712.10132, 2017. URL http://arxiv.org/abs/1712.10132.",
            "url": "http://arxiv.org/abs/1712.10132",
            "arxiv_url": "https://arxiv.org/pdf/1712.10132"
        },
        {
            "id": "Ledyaev_2007_a",
            "entry": "Yu Ledyaev and Qiji Zhu. Nonsmooth analysis on smooth manifolds. Transactions of the American Mathematical Society, 359(8):3687\u20133732, 2007. doi: 10.1090/S0002-9947-07-04075-5.",
            "crossref": "https://dx.doi.org/10.1090/S0002-9947-07-04075-5",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1090/S0002-9947-07-04075-5"
        },
        {
            "id": "Li_2018_a",
            "entry": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv:1808.01204, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.01204"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arxiv:1705.09886, 2017. URL http://arxiv.org/abs/1705.09886.",
            "url": "http://arxiv.org/abs/1705.09886",
            "arxiv_url": "https://arxiv.org/pdf/1705.09886"
        },
        {
            "id": "Loh_2015_a",
            "entry": "Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima. Journal of Machine Learning Research, 16:559\u2013616, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loh%2C%20Po-Ling%20Wainwright%2C%20Martin%20J.%20Regularized%20m-estimators%20with%20nonconvexity%3A%20Statistical%20and%20algorithmic%20theory%20for%20local%20optima%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loh%2C%20Po-Ling%20Wainwright%2C%20Martin%20J.%20Regularized%20m-estimators%20with%20nonconvexity%3A%20Statistical%20and%20algorithmic%20theory%20for%20local%20optima%202015"
        },
        {
            "id": "Ma_et+al_2016_a",
            "entry": "Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-ofsquares. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Tengyu%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Polynomial-time%20tensor%20decompositions%20with%20sum-ofsquares%202016"
        },
        {
            "id": "Mairal_et+al_2014_a",
            "entry": "Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. Foundations and Trends R in Computer Graphics and Vision, 8(2-3):85\u2013283, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014"
        },
        {
            "id": "Mei_et+al_2018_a",
            "entry": "Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. arXiv:1804.06561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06561"
        },
        {
            "id": "Olshausen_1996_a",
            "entry": "Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996"
        },
        {
            "id": "O_1983_a",
            "entry": "Barrett O\u2019neill. Semi-Riemannian geometry with applications to relativity, volume 103. Academic press, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99neill%2C%20Barrett%20Semi-Riemannian%20geometry%20with%20applications%20to%20relativity%2C%20volume%20103%201983"
        },
        {
            "id": "Ravishankar_2013_a",
            "entry": "Saiprasad Ravishankar and Yoram Bresler. Learning sparsifying transforms. IEEE Transactions on Signal Processing, 61(5):1072\u20131086, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravishankar%2C%20Saiprasad%20Bresler%2C%20Yoram%20Learning%20sparsifying%20transforms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravishankar%2C%20Saiprasad%20Bresler%2C%20Yoram%20Learning%20sparsifying%20transforms%202013"
        },
        {
            "id": "Schramm_2017_a",
            "entry": "Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to dictionary learning. arXiv:1706.08672, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08672"
        },
        {
            "id": "Spielman_et+al_2012_a",
            "entry": "Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory, pp. 37\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spielman%2C%20Daniel%20A.%20Wang%2C%20Huan%20Wright%2C%20John%20Exact%20recovery%20of%20sparsely-used%20dictionaries%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spielman%2C%20Daniel%20A.%20Wang%2C%20Huan%20Wright%2C%20John%20Exact%20recovery%20of%20sparsely-used%20dictionaries%202012"
        },
        {
            "id": "Sternberg_2013_a",
            "entry": "Shlomo Sternberg. Dynamical Systems. Dover Publications, Inc, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sternberg%2C%20Shlomo%20Dynamical%20Systems%202013"
        },
        {
            "id": "Sun_et+al_2015_a",
            "entry": "Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. arxiv:1504.06785, 2015. URL http://arxiv.org/abs/1504.06785.",
            "url": "http://arxiv.org/abs/1504.06785",
            "arxiv_url": "https://arxiv.org/pdf/1504.06785"
        },
        {
            "id": "Der_2009_a",
            "entry": "Aad Van Der Vaart and Jon A Wellner. A note on bounds for VC dimensions. Institute of Mathematical Statistics collections, 5:103, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Der%2C%20Aad%20Van%20Vaart%20and%20Jon%20A%20Wellner.%20A%20note%20on%20bounds%20for%20VC%20dimensions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Der%2C%20Aad%20Van%20Vaart%20and%20Jon%20A%20Wellner.%20A%20note%20on%20bounds%20for%20VC%20dimensions%202009"
        },
        {
            "id": "Vershynin_0000_a",
            "entry": "Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596.",
            "crossref": "https://dx.doi.org/10.1017/9781108231596",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1017/9781108231596"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. arxiv:1706.03175, 2017. URL http://arxiv.org/abs/1706.03175.",
            "url": "http://arxiv.org/abs/1706.03175",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "See_2013_a",
            "entry": "See, e.g., Sec. 7.1 of Sternberg (2013) for a proof. Lemma A.1 (Restatement of Lemma A.1). For convex compact sets X, Y \u2282 Rn, we have dH (X, Y ) = sup |hX (u) \u2212 hY (u)|, u\u2208Sn\u22121 where hS(u) =. supx\u2208S x, u is the support function associated with the set S.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%2C%20eg%20Sec%20for%20a%20proof.%20Lemma%20A.1%20%28Restatement%20of%20Lemma%20A.1%29.%20For%20convex%20compact%20sets%20X%2C%20Y%20%E2%8A%82%20Rn%2C%20we%20have%20dH%20%28X%2C%20Y%20%29%20%3D%20sup%20%7ChX%20%28u%29%20%E2%88%92%20hY%20%28u%29%7C%2C%20u%E2%88%88Sn%E2%88%921%20where%20hS%28u%29%20%3D.%20supx%E2%88%88S%20x%2C%20u%20is%20the%20support%20function%20associated%20with%20the%20set%20S%202013"
        },
        {
            "id": "Proposition_2018_a",
            "entry": "Proposition A.2 (Talagrand\u2019s comparison inequality, Corollary 8.6.3 and Exercise 8.6.5 of Vershynin (2018)). Let {Xx}x\u2208T be a zero-mean random process on a subset T \u2282 Rn. Assume that for all x, y \u2208 T we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proposition%20A2%20Talagrands%20comparison%20inequality%20Corollary%20863%20and%20Exercise%20865%20of%20Vershynin%202018%20Let%20XxxT%20be%20a%20zeromean%20random%20process%20on%20a%20subset%20T%20%20Rn%20Assume%20that%20for%20all%20x%20y%20%20T%20we%20have"
        },
        {
            "id": "Proposition_2018_b",
            "entry": "Proposition A.3 (Deviation inequality for sub-Gaussian matrices, Theorem 9.1.1 and Exercise 9.1.8 of Vershynin (2018)). Let A be an n \u00d7 m matrix whose rows Ai\u2019s are independent, isotropic, and sub-Gaussian random vectors in Rm. Then for any subset T \u2282 Rm, we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proposition%20A3%20Deviation%20inequality%20for%20subGaussian%20matrices%20Theorem%20911%20and%20Exercise%20918%20of%20Vershynin%202018%20Let%20A%20be%20an%20n%20%20m%20matrix%20whose%20rows%20Ais%20are%20independent%20isotropic%20and%20subGaussian%20random%20vectors%20in%20Rm%20Then%20for%20any%20subset%20T%20%20Rm%20we%20have"
        },
        {
            "id": "and_2001_a",
            "entry": "and the fact that \u2202E [f ] (q) = \u2202E [ q\u03a9 2] = E [\u2202 q\u03a9 2] as the sub-differential and expectation can be exchanged for convex functions (Hiriart-Urruty & Lemarchal, 2001). By the same exchangability result, we also have E [\u2202f (q)] = \u2202E [f ] (q).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%20the%20fact%20that%20E%20f%20%20q%20%20E%20%20q%CE%A9%202%20%20E%20%20q%CE%A9%202%20as%20the%20subdifferential%20and%20expectation%20can%20be%20exchanged%20for%20convex%20functions%20HiriartUrruty%20%20Lemarchal%202001%20By%20the%20same%20exchangability%20result%20we%20also%20have%20E%20f%20q%20%20E%20f%20%20q"
        },
        {
            "id": "For_2015_a",
            "entry": "For analysis purposes, we introduce the reparametrization w = q1:(n\u22121) in the region S0(n+), following (Sun et al., 2015). With this reparametrization, the problem becomes minimizew\u2208Rn\u22121 g (w) =.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20analysis%20purposes%20we%20introduce%20the%20reparametrization%20w%20%20q1n1%20in%20the%20region%20S0n%20following%20Sun%20et%20al%202015%20With%20this%20reparametrization%20the%20problem%20becomes%20minimizewRn1%20g%20w",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20analysis%20purposes%20we%20introduce%20the%20reparametrization%20w%20%20q1n1%20in%20the%20region%20S0n%20following%20Sun%20et%20al%202015%20With%20this%20reparametrization%20the%20problem%20becomes%20minimizewRn1%20g%20w"
        },
        {
            "id": "This_1983_a",
            "entry": "This fact can be proved either directly, see, e.g., page 12 of this online notes: http://www.math.mcgill.ca/drury/notes354.pdf, or by realizing that the angle equal to the geodesic length, which is the Riemmannian distance over the sphere; see, e.g., Riemannian Distance of Chapter 5 of the book O\u2019neill (1983).",
            "url": "http://www.math.mcgill.ca/drury/notes354.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=This%20fact%20can%20be%20proved%20either%20directly%20see%20eg%20page%2012%20of%20this%20online%20notes%20httpwwwmathmcgillcadrurynotes354pdf%20or%20by%20realizing%20that%20the%20angle%20equal%20to%20the%20geodesic%20length%20which%20is%20the%20Riemmannian%20distance%20over%20the%20sphere%20see%20eg%20Riemannian%20Distance%20of%20Chapter%205%20of%20the%20book%20Oneill%201983"
        },
        {
            "id": "and_2005_a",
            "entry": "and let dvc(H) be the VC-dimension of H. From concentration results for VC-classes (see, e.g., Eq (3) and Theorem 3.4 of Boucheron et al. (2005)), we have",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%20let%20dvcH%20be%20the%20VCdimension%20of%20H%20From%20concentration%20results%20for%20VCclasses%20see%20eg%20Eq%203%20and%20Theorem%2034%20of%20Boucheron%20et%20al%202005%20we%20have",
            "oa_query": "https://api.scholarcy.com/oa_version?query=and%20let%20dvcH%20be%20the%20VCdimension%20of%20H%20From%20concentration%20results%20for%20VCclasses%20see%20eg%20Eq%203%20and%20Theorem%2034%20of%20Boucheron%20et%20al%202005%20we%20have"
        },
        {
            "id": "Note_2009_a",
            "entry": "Note that H0 has VC-dimension n + 1. Applying bounds on the VC-dimension of unions and intersections (Theorem 1.1, Van Der Vaart & Wellner (2009)), we get that dvc(H) \u2264 C1dvc(H0 H0) \u2264 C2dvc(H0) \u2264 C3n.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Note%20that%20H0%20has%20VCdimension%20n%20%201%20Applying%20bounds%20on%20the%20VCdimension%20of%20unions%20and%20intersections%20Theorem%2011%20Van%20Der%20Vaart%20%20Wellner%202009%20we%20get%20that%20dvcH%20%20C1dvcH0%20H0%20%20C2dvcH0%20%20C3n",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Note%20that%20H0%20has%20VCdimension%20n%20%201%20Applying%20bounds%20on%20the%20VCdimension%20of%20unions%20and%20intersections%20Theorem%2011%20Van%20Der%20Vaart%20%20Wellner%202009%20we%20get%20that%20dvcH%20%20C1dvcH0%20H0%20%20C2dvcH0%20%20C3n"
        }
    ]
}
