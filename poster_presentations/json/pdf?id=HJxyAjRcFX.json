{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HARMONIZING MAXIMUM LIKELIHOOD WITH GANS FOR MULTIMODAL CONDITIONAL GENERATION",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJxyAjRcFX"
        },
        "abstract": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "mean absolute deviation",
            "url": "https://en.wikipedia.org/wiki/mean_absolute_deviation"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        },
        {
            "term": "image synthesis",
            "url": "https://en.wikipedia.org/wiki/image_synthesis"
        },
        {
            "term": "super resolution",
            "url": "https://en.wikipedia.org/wiki/super_resolution"
        },
        {
            "term": "maximum likelihood estimations",
            "url": "https://en.wikipedia.org/wiki/Maximum_Likelihood_Estimation"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        }
    ],
    "abbreviations": {
        "MLE": "maximum likelihood estimation",
        "MLEs": "maximum likelihood estimations",
        "MAD": "mean absolute deviation",
        "MR": "Moment Reconstruction",
        "NRF": "National Research Foundation of Korea"
    },
    "highlights": [
        "Active research has led to a huge progress on conditional image generation, whose typical tasks include image-to-image translation (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al (2017</a>)), image inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al (2016</a>)), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al (2017</a>)) and video prediction (<a class=\"ref-link\" id=\"cMathieu_et+al_2016_a\" href=\"#rMathieu_et+al_2016_a\">Mathieu et al (2016</a>))",
        "Almost all previous models in conditional image generation exploit the reconstruction loss such as 1/ 2 loss in addition to the GAN loss. Using these two types of losses is synergetic in that the GAN loss complements the weakness of the reconstruction loss that output samples are blurry and lack high-frequency structure, while the reconstruction loss offers the training stability required for convergence",
        "We propose a variant called Proxy Moment Reconstruction loss and a conditional GAN with the loss named proxy Moment Reconstruction-GAN whose overall training scheme is depicted in Figure 2(c)",
        "We proposed a set of novel loss functions named Moment Reconstruction loss and proxy Moment Reconstruction loss that enable conditional GAN models to accomplish both stability of training and multimodal generation",
        "We showed that our loss functions were successfully integrated with multiple state-of-the-art models for image translation, super-resolution and image inpainting tasks, for which our method generated realistic image samples of high visual fidelity and variability on Cityscapes and CelebA dataset"
    ],
    "key_statements": [
        "Active research has led to a huge progress on conditional image generation, whose typical tasks include image-to-image translation (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al (2017</a>)), image inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al (2016</a>)), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al (2017</a>)) and video prediction (<a class=\"ref-link\" id=\"cMathieu_et+al_2016_a\" href=\"#rMathieu_et+al_2016_a\">Mathieu et al (2016</a>))",
        "Almost all previous models in conditional image generation exploit the reconstruction loss such as 1/ 2 loss in addition to the GAN loss. Using these two types of losses is synergetic in that the GAN loss complements the weakness of the reconstruction loss that output samples are blurry and lack high-frequency structure, while the reconstruction loss offers the training stability required for convergence",
        "The objective of this paper is to propose a new set of losses named moment reconstruction losses that can replace the reconstruction loss with losing neither the visual fidelity nor diversity in output samples",
        "We propose novel alternatives for the reconstruction loss that are applicable to virtually any conditional generation tasks",
        "In conventional conditional GANs, maximum likelihood estimation losses are applied to the generator\u2019s objective to make sure that it generates output samples well matched to their ground-truth",
        "We propose a variant called Proxy Moment Reconstruction loss and a conditional GAN with the loss named proxy Moment Reconstruction-GAN whose overall training scheme is depicted in Figure 2(c)",
        "In order to show the generality of our methods, we apply them to three conditional generation tasks: image-to-image translation, super-resolution and image inpainting, for each of which we select Pix2Pix, SRGAN (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a>) and GLCIC (<a class=\"ref-link\" id=\"cIizuka_et+al_2017_a\" href=\"#rIizuka_et+al_2017_a\">Iizuka et al, 2017</a>) as base models, respectively",
        "We proposed a set of novel loss functions named Moment Reconstruction loss and proxy Moment Reconstruction loss that enable conditional GAN models to accomplish both stability of training and multimodal generation",
        "We showed that our loss functions were successfully integrated with multiple state-of-the-art models for image translation, super-resolution and image inpainting tasks, for which our method generated realistic image samples of high visual fidelity and variability on Cityscapes and CelebA dataset",
        "Using the statistics of high-level features may capture additional correlations that cannot be represented with pixel-level statistics"
    ],
    "summary": [
        "Active research has led to a huge progress on conditional image generation, whose typical tasks include image-to-image translation (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al (2017</a></a>)), image inpainting (<a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\"><a class=\"ref-link\" id=\"cPathak_et+al_2016_a\" href=\"#rPathak_et+al_2016_a\">Pathak et al (2016</a></a>)), super-resolution (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\"><a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al (2017</a></a>)) and video prediction (<a class=\"ref-link\" id=\"cMathieu_et+al_2016_a\" href=\"#rMathieu_et+al_2016_a\"><a class=\"ref-link\" id=\"cMathieu_et+al_2016_a\" href=\"#rMathieu_et+al_2016_a\">Mathieu et al (2016</a></a>)).",
        "We propose two novel loss functions that enable the model to accomplish both training stability and multimodal output generation.",
        "Our methods replace the reconstruction loss, and are applicable to any conditional generation tasks.",
        "Conditional VAE-GANs and disentanglement-based methods both leverage the latent variable to prevent the model from discarding the multimodality of output samples.",
        "We briefly review the objective of conditional GANs in section 3.1 and discuss why the two loss terms cause the loss of modality in the sample distribution of the generator in section 3.2.",
        "We train four models that use different combinations of loss terms, and generate four samples with different noise input.",
        "As shown in Figure 1(c), the model trained with only the GAN loss fails to generate realistic images, since the signal from the discriminator is too unstable to learn the translation task.",
        "In conventional conditional GANs, MLE losses are applied to the generator\u2019s objective to make sure that it generates output samples well matched to their ground-truth.",
        "The predictor is trained prior to the generator by the MLE loss in Eq(7) with ground-truth y to predict conditional mean and variance, i.e. \u03bcand \u03c32.",
        "The sets of optimal generators for the GAN loss and the 2 loss, denoted by G and R respectively, can be formulated as follows: G = {G|pdata(y|x) = pG(y|x)}, R = {G| arg min Ex,y,z[L2(G(x, z), y)]}.",
        "We discuss why our approach does not suffer from loss of diversity in the output samples unlike the reconstruction loss in terms of the set of optimal generators.",
        "In order to show the generality of our methods, we apply them to three conditional generation tasks: image-to-image translation, super-resolution and image inpainting, for each of which we select Pix2Pix, SRGAN (<a class=\"ref-link\" id=\"cLedig_et+al_2017_a\" href=\"#rLedig_et+al_2017_a\">Ledig et al, 2017</a>) and GLCIC (<a class=\"ref-link\" id=\"cIizuka_et+al_2017_a\" href=\"#rIizuka_et+al_2017_a\">Iizuka et al, 2017</a>) as base models, respectively.",
        "We proposed a set of novel loss functions named MR loss and proxy MR loss that enable conditional GAN models to accomplish both stability of training and multimodal generation.",
        "We showed that our loss functions were successfully integrated with multiple state-of-the-art models for image translation, super-resolution and image inpainting tasks, for which our method generated realistic image samples of high visual fidelity and variability on Cityscapes and CelebA dataset.",
        "There are other conditional generation tasks that we did not cover, such as text-to-image synthesis, text-to-speech synthesis and video prediction, for which our methods can be directly applied to generate diverse, high-quality samples.",
        "Using the statistics of high-level features may capture additional correlations that cannot be represented with pixel-level statistics"
    ],
    "headline": "We reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples",
    "reference_links": [
        {
            "id": "Bao_et+al_2017_a",
            "entry": "Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. CVAE-GAN: Fine-Grained image generation through asymmetric training. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bao%2C%20Jianmin%20Chen%2C%20Dong%20Wen%2C%20Fang%20Li%2C%20Houqiang%20CVAE-GAN%3A%20Fine-Grained%20image%20generation%20through%20asymmetric%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bao%2C%20Jianmin%20Chen%2C%20Dong%20Wen%2C%20Fang%20Li%2C%20Houqiang%20CVAE-GAN%3A%20Fine-Grained%20image%20generation%20through%20asymmetric%20training%202017"
        },
        {
            "id": "Bhattacharjee_2017_a",
            "entry": "Prateep Bhattacharjee and Sukhendu Das. Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhattacharjee%2C%20Prateep%20Das%2C%20Sukhendu%20Temporal%20coherency%20based%20criteria%20for%20predicting%20video%20frames%20using%20deep%20multi-stage%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhattacharjee%2C%20Prateep%20Das%2C%20Sukhendu%20Temporal%20coherency%20based%20criteria%20for%20predicting%20video%20frames%20using%20deep%20multi-stage%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Bishop_2006_a",
            "entry": "Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20Christopher%20M.%20Pattern%20Recognition%20and%20Machine%20Learning%202006"
        },
        {
            "id": "Bloesch_et+al_2018_a",
            "entry": "Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew Davison. CodeSLAM - learning a compact, optimisable representation for dense visual SLAM. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloesch%2C%20Michael%20Czarnowski%2C%20Jan%20Clark%2C%20Ronald%20Leutenegger%2C%20Stefan%20CodeSLAM%20-%20learning%20a%20compact%2C%20optimisable%20representation%20for%20dense%20visual%20SLAM%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloesch%2C%20Michael%20Czarnowski%2C%20Jan%20Clark%2C%20Ronald%20Leutenegger%2C%20Stefan%20CodeSLAM%20-%20learning%20a%20compact%2C%20optimisable%20representation%20for%20dense%20visual%20SLAM%202018"
        },
        {
            "id": "Bruna_et+al_2016_a",
            "entry": "Joan Bruna, Pablo Sprechmann, and LeCun, Yann. Super-Resolution with deep convolutional sufficient statistics. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruna%2C%20Joan%20Sprechmann%2C%20Pablo%20LeCun%2C%20Yann%20Super-Resolution%20with%20deep%20convolutional%20sufficient%20statistics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruna%2C%20Joan%20Sprechmann%2C%20Pablo%20LeCun%2C%20Yann%20Super-Resolution%20with%20deep%20convolutional%20sufficient%20statistics%202016"
        },
        {
            "id": "Bulat_et+al_2018_a",
            "entry": "Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a GAN to learn how to do image degradation first. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bulat%2C%20Adrian%20Yang%2C%20Jing%20Tzimiropoulos%2C%20Georgios%20To%20learn%20image%20super-resolution%2C%20use%20a%20GAN%20to%20learn%20how%20to%20do%20image%20degradation%20first%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bulat%2C%20Adrian%20Yang%2C%20Jing%20Tzimiropoulos%2C%20Georgios%20To%20learn%20image%20super-resolution%2C%20use%20a%20GAN%20to%20learn%20how%20to%20do%20image%20degradation%20first%202018"
        },
        {
            "id": "Cordts_et+al_2016_a",
            "entry": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "Goodfellow_2016_a",
            "entry": "Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20NIPS%202016%20tutorial%20Generative%20adversarial%20networks%20arXiv%202016"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Pouget-Abadie, Jean, Mehdi Mirza, Bing Xu, Warde-Farley, David, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20networks%202014"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised Image-toImage translation. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Liu%2C%20Ming-Yu%20Belongie%2C%20Serge%20Kautz%2C%20Jan%20Multimodal%20unsupervised%20Image-toImage%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Liu%2C%20Ming-Yu%20Belongie%2C%20Serge%20Kautz%2C%20Jan%20Multimodal%20unsupervised%20Image-toImage%20translation%202018"
        },
        {
            "id": "Iizuka_et+al_2017_a",
            "entry": "Satoshi Iizuka, Simo-Serra, Edgar, and Hiroshi Ishikawa. Globally and locally consistent image completion. SIGGRAPH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20Satoshi%20Simo-Serra%2C%20Edgar%20Ishikawa%2C%20Hiroshi%20Globally%20and%20locally%20consistent%20image%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20Satoshi%20Simo-Serra%2C%20Edgar%20Ishikawa%2C%20Hiroshi%20Globally%20and%20locally%20consistent%20image%20completion%202017"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-Image translation with conditional adversarial networks. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-Image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-Image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "James_2003_a",
            "entry": "Gareth James. Variance and bias for general loss functions. Machine Learning, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=James%2C%20Gareth%20Variance%20and%20bias%20for%20general%20loss%20functions%202003"
        },
        {
            "id": "Jang_et+al_2018_a",
            "entry": "Yunseok Jang, Gunhee Kim, and Yale Song. Video prediction with appearance and motion conditions. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Yunseok%20Kim%2C%20Gunhee%20Song%2C%20Yale%20Video%20prediction%20with%20appearance%20and%20motion%20conditions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20Yunseok%20Kim%2C%20Gunhee%20Song%2C%20Yale%20Video%20prediction%20with%20appearance%20and%20motion%20conditions%202018"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Alexandre Alahi, and Fei-Fei, Li. Perceptual losses for Real-Time style transfer and Super-Resolution. ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20Real-Time%20style%20transfer%20and%20Super-Resolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Alahi%2C%20Alexandre%20Fei-Fei%2C%20Li%20Perceptual%20losses%20for%20Real-Time%20style%20transfer%20and%20Super-Resolution%202016"
        },
        {
            "id": "Kendall_2017_a",
            "entry": "Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Max Welling. Auto-Encoding variational bayes. ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-Encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-Encoding%20variational%20bayes%202014"
        },
        {
            "id": "Larsen_et+al_2016_a",
            "entry": "Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsen%2C%20Anders%20Boesen%20Lindbo%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Larochelle%2C%20Hugo%20Winther%2C%20Ole%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016"
        },
        {
            "id": "Ledig_et+al_2017_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. PhotoRealistic single image Super-Resolution using a generative adversarial network. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20PhotoRealistic%20single%20image%20Super-Resolution%20using%20a%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Huszar%2C%20Ferenc%20Caballero%2C%20Jose%20PhotoRealistic%20single%20image%20Super-Resolution%20using%20a%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Alex Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. arXiv, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Alex%20Zhang%2C%20Richard%20Ebert%2C%20Frederik%20Abbeel%2C%20Pieter%20Stochastic%20adversarial%20video%20prediction%202018"
        },
        {
            "id": "Lee_et+al_2018_b",
            "entry": "Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse Image-to-Image translation via disentangled representations. ECCV, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Hsin-Ying%20Tseng%2C%20Hung-Yu%20Huang%2C%20Jia-Bin%20Singh%2C%20Maneesh%20Diverse%20Image-to-Image%20translation%20via%20disentangled%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Hsin-Ying%20Tseng%2C%20Hung-Yu%20Huang%2C%20Jia-Bin%20Singh%2C%20Maneesh%20Diverse%20Image-to-Image%20translation%20via%20disentangled%20representations%202018"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang. Generative face completion. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yijun%20Liu%2C%20Sifei%20Yang%2C%20Jimei%20Yang%2C%20Ming-Hsuan%20Generative%20face%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yijun%20Liu%2C%20Sifei%20Yang%2C%20Jimei%20Yang%2C%20Ming-Hsuan%20Generative%20face%20completion%202017"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yifan Liu, Zengchang Qin, Zhenbo Luo, and Hua Wang. Auto-painter: Cartoon image generation from sketch by using conditional generative adversarial networks. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yifan%20Qin%2C%20Zengchang%20Luo%2C%20Zhenbo%20Wang%2C%20Hua%20Auto-painter%3A%20Cartoon%20image%20generation%20from%20sketch%20by%20using%20conditional%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "Lu_et+al_2017_a",
            "entry": "Chaochao Lu, Michael Hirsch, and Bernhard Scholkopf. Flexible Spatio-Temporal networks for video prediction. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Chaochao%20Hirsch%2C%20Michael%20Scholkopf%2C%20Bernhard%20Flexible%20Spatio-Temporal%20networks%20for%20video%20prediction%202017"
        },
        {
            "id": "Lyu_et+al_2017_a",
            "entry": "Pengyuan Lyu, Xiang Bai, Cong Yao, Zhen Zhu, Tengteng Huang, and Wenyu Liu. Auto-Encoder guided GAN for chinese calligraphy synthesis. ICDAR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lyu%2C%20Pengyuan%20Bai%2C%20Xiang%20Yao%2C%20Cong%20Zhu%2C%20Zhen%20Auto-Encoder%20guided%20GAN%20for%20chinese%20calligraphy%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lyu%2C%20Pengyuan%20Bai%2C%20Xiang%20Yao%2C%20Cong%20Zhu%2C%20Zhen%20Auto-Encoder%20guided%20GAN%20for%20chinese%20calligraphy%20synthesis%202017"
        },
        {
            "id": "Mahapatra_et+al_2017_a",
            "entry": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Sajini Hewavitharanage, and Rahil Garnavi. Image super resolution using generative adversarial networks and local saliency maps for retinal image analysis. MICCAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahapatra%2C%20Dwarikanath%20Bozorgtabar%2C%20Behzad%20Hewavitharanage%2C%20Sajini%20Garnavi%2C%20Rahil%20Image%20super%20resolution%20using%20generative%20adversarial%20networks%20and%20local%20saliency%20maps%20for%20retinal%20image%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahapatra%2C%20Dwarikanath%20Bozorgtabar%2C%20Behzad%20Hewavitharanage%2C%20Sajini%20Garnavi%2C%20Rahil%20Image%20super%20resolution%20using%20generative%20adversarial%20networks%20and%20local%20saliency%20maps%20for%20retinal%20image%20analysis%202017"
        },
        {
            "id": "Mathieu_et+al_2016_a",
            "entry": "Michael Mathieu, Camille Couprie, and LeCun, Yann. Deep multi-scale video prediction beyond mean square error. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20Michael%20Couprie%2C%20Camille%20LeCun%2C%20Yann%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirza%2C%20Mehdi%20Osindero%2C%20Simon%20Conditional%20generative%20adversarial%20nets%202014"
        },
        {
            "id": "Olszewski_et+al_2017_a",
            "entry": "Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li. Realistic dynamic facial textures from a single image using GANs. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olszewski%2C%20Kyle%20Li%2C%20Zimo%20Yang%2C%20Chao%20Zhou%2C%20Yi%20Realistic%20dynamic%20facial%20textures%20from%20a%20single%20image%20using%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olszewski%2C%20Kyle%20Li%2C%20Zimo%20Yang%2C%20Chao%20Zhou%2C%20Yi%20Realistic%20dynamic%20facial%20textures%20from%20a%20single%20image%20using%20GANs%202017"
        },
        {
            "id": "Ouyang_et+al_2018_a",
            "entry": "Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, and Pan Zhou. Pedestrian-Synthesis-GAN: generating pedestrian data in real scene and beyond. arXiv preprint arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ouyang%2C%20Xi%20Cheng%2C%20Yu%20Jiang%2C%20Yifan%20Li%2C%20Chun-Liang%20Pedestrian-Synthesis-GAN%3A%20generating%20pedestrian%20data%20in%20real%20scene%20and%202018"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "Sabini_2018_a",
            "entry": "Mark Sabini and Gili Rusak. Painting outside the box: Image outpainting with GANs. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabini%2C%20Mark%20Rusak%2C%20Gili%20Painting%20outside%20the%20box%3A%20Image%20outpainting%20with%20GANs%202018"
        },
        {
            "id": "Sajjadi_et+al_2017_a",
            "entry": "Mehdi S. M. Sajjadi, Bernhard B Sch, and Michael Hirsch. EnhanceNet: single image SuperResolution through automated texture synthesis. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20S.M.%20Sch%2C%20Bernhard%20B.%20Hirsch%2C%20Michael%20EnhanceNet%3A%20single%20image%20SuperResolution%20through%20automated%20texture%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20S.M.%20Sch%2C%20Bernhard%20B.%20Hirsch%2C%20Michael%20EnhanceNet%3A%20single%20image%20SuperResolution%20through%20automated%20texture%20synthesis%202017"
        },
        {
            "id": "Sangkloy_et+al_2017_a",
            "entry": "Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Scribbler: Controlling deep image synthesis with sketch and color. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sangkloy%2C%20Patsorn%20Lu%2C%20Jingwan%20Fang%2C%20Chen%20Yu%2C%20Fisher%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sangkloy%2C%20Patsorn%20Lu%2C%20Jingwan%20Fang%2C%20Chen%20Yu%2C%20Fisher%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017"
        },
        {
            "id": "Song_et+al_2018_a",
            "entry": "Yuhang Song, Chao Yang, Yejin Shen, Peng Wang, Qin Huang, and C.-C. Jay Kuo. SPG-Net: segmentation prediction and guidance network for image inpainting. arXiv preprint arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Yuhang%20Yang%2C%20Chao%20Shen%2C%20Yejin%20Wang%2C%20Peng%20SPG-Net%3A%20segmentation%20prediction%20and%20guidance%20network%20for%20image%20inpainting%202018"
        },
        {
            "id": "Villegas_et+al_2017_a",
            "entry": "Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and content for natural video sequence prediction. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Hong%2C%20Seunghoon%20Lin%2C%20Xunyu%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017"
        },
        {
            "id": "Vondrick_2017_a",
            "entry": "Carl Vondrick and Antonio Torralba. Generating the future with adversarial transformers. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20Carl%20Torralba%2C%20Antonio%20Generating%20the%20future%20with%20adversarial%20transformers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20Carl%20Torralba%2C%20Antonio%20Generating%20the%20future%20with%20adversarial%20transformers%202017"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. HighResolution image synthesis and semantic manipulation with conditional GANs. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Tao%2C%20Andrew%20HighResolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20GANs%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-Video synthesis. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ting-Chun%20Liu%2C%20Ming-Yu%20Zhu%2C%20Jun-Yan%20Liu%2C%20Guilin%20Video-to-Video%20synthesis%202018"
        },
        {
            "id": "Xian_et+al_2017_a",
            "entry": "Wenqi Xian, Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. TextureGAN: controlling deep image synthesis with texture patches. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xian%2C%20Wenqi%20Sangkloy%2C%20Patsorn%20Lu%2C%20Jingwan%20Fang%2C%20Chen%20TextureGAN%3A%20controlling%20deep%20image%20synthesis%20with%20texture%20patches%202017"
        },
        {
            "id": "Xie_et+al_2018_a",
            "entry": "You Xie, Erik Franz, Mengyu Chu, and Nils Thuerey. tempoGAN: a temporally coherent, volumetric GAN for super-resolution fluid flow. SIGGRAPH, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20You%20Franz%2C%20Erik%20Chu%2C%20Mengyu%20Thuerey%2C%20Nils%20tempoGAN%3A%20a%20temporally%20coherent%2C%20volumetric%20GAN%20for%20super-resolution%20fluid%20flow%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20You%20Franz%2C%20Erik%20Chu%2C%20Mengyu%20Thuerey%2C%20Nils%20tempoGAN%3A%20a%20temporally%20coherent%2C%20volumetric%20GAN%20for%20super-resolution%20fluid%20flow%202018"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Xiangyu Xu, Deqing Sun, Jinshan Pan, Yujin Zhang, Hanspeter Pfister, and Ming-Hsuan Yang. Learning to Super-Resolve blurry face and text images. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Xiangyu%20Sun%2C%20Deqing%20Pan%2C%20Jinshan%20Zhang%2C%20Yujin%20Learning%20to%20Super-Resolve%20blurry%20face%20and%20text%20images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Xiangyu%20Sun%2C%20Deqing%20Pan%2C%20Jinshan%20Zhang%2C%20Yujin%20Learning%20to%20Super-Resolve%20blurry%20face%20and%20text%20images%202017"
        },
        {
            "id": "Yang_et+al_2018_a",
            "entry": "Chao Yang, Yuhang Song, Xiaofeng Liu, Qingming Tang, and C.-C. Jay Kuo. Image inpainting using block-wise procedural training with annealed adversarial counterpart. arXiv preprint arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Chao%20Song%2C%20Yuhang%20Liu%2C%20Xiaofeng%20Tang%2C%20Qingming%20Image%20inpainting%20using%20block-wise%20procedural%20training%20with%20annealed%20adversarial%20counterpart%202018"
        },
        {
            "id": "Yeh_et+al_2017_a",
            "entry": "Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Hasegawa-Johnson, Mark, and Minh N. Do. Semantic image inpainting with deep generative models. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-Form image inpainting with gated convolution. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Jiahui%20Lin%2C%20Zhe%20Yang%2C%20Jimei%20Shen%2C%20Xiaohui%20Free-Form%20image%20inpainting%20with%20gated%20convolution%202018"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "He Zhang, Vishwanath Sindagi, and Vishal Patel. Image de-raining using a conditional generative adversarial network. arXiv, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20He%20Sindagi%2C%20Vishwanath%20Patel%2C%20Vishal%20Image%20de-raining%20using%20a%20conditional%20generative%20adversarial%20network%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Richard%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20Shechtman%2C%20Eli%20The%20unreasonable%20effectiveness%20of%20deep%20features%20as%20a%20perceptual%20metric%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Richard%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20Shechtman%2C%20Eli%20The%20unreasonable%20effectiveness%20of%20deep%20features%20as%20a%20perceptual%20metric%202018"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Shu Zhang, Ran He, and Tieniu Tan. DeMeshNet: blind face inpainting for deep MeshFace verification. TIFS, 2017b. Yipin Zhou and Tamara L. Berg. Learning temporal transformations from Time-Lapse videos.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Shu%20Zhang%20Ran%20He%20and%20Tieniu%20Tan%20DeMeshNet%20blind%20face%20inpainting%20for%20deep%20MeshFace%20verification%20TIFS%202017b%20Yipin%20Zhou%20and%20Tamara%20L%20Berg%20Learning%20temporal%20transformations%20from%20TimeLapse%20videos"
        },
        {
            "id": "Eccv_0000_a",
            "entry": "ECCV, 2016. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, and Eli",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ECCV%202016%20JunYan%20Zhu%20Richard%20Zhang%20Deepak%20Pathak%20Trevor%20Darrell%20Alexei%20A%20Efros%20Oliver%20Wang%20and%20Eli",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ECCV%202016%20JunYan%20Zhu%20Richard%20Zhang%20Deepak%20Pathak%20Trevor%20Darrell%20Alexei%20A%20Efros%20Oliver%20Wang%20and%20Eli"
        },
        {
            "id": "Shechtman_2017_a",
            "entry": "Shechtman. Toward multimodal image-to-image translation. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shechtman%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shechtman%20Toward%20multimodal%20image-to-image%20translation%202017"
        },
        {
            "id": "We_2016_a",
            "entry": "We elaborate on the algorithms of all eight variants of our methods in detail from Algorithm 5 to Algorithm 4. The presented algorithms assume a single input per update, although we use mini-batch training in practice. Also, we use non-saturating GAN loss, \u2212 log D(x, y) (Goodfellow, 2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20elaborate%20on%20the%20algorithms%20of%20all%20eight%20variants%20of%20our%20methods%20in%20detail%20from%20Algorithm%205%20to%20Algorithm%204%20The%20presented%20algorithms%20assume%20a%20single%20input%20per%20update%20although%20we%20use%20minibatch%20training%20in%20practice%20Also%20we%20use%20nonsaturating%20GAN%20loss%20%20log%20Dx%20y%20Goodfellow%202016"
        },
        {
            "id": "Our_0000_b",
            "entry": "Our Pix2Pix variant is based on the U-net generator from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.",
            "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
        },
        {
            "id": "Our_0000_c",
            "entry": "Our SRGAN variant is based on the PyTorch implementation of SRGAN from https://github.com/zijundeng/SRGAN.",
            "url": "https://github.com/zijundeng/SRGAN"
        }
    ]
}
