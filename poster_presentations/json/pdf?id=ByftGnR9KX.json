{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "FLOWQA: GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION",
        "author": "Hsin-Yuan Huang, California Institute of Technology hsinyuan@caltech.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByftGnR9KX"
        },
        "abstract": "Conversational machine comprehension requires the understanding of the conversation history, such as previous question/answer pairs, the document context and the current question. To enable traditional, single-turn models to encode the history comprehensively, we introduce FLOW, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions/answers as input, FLOW integrates the latent semantics of the conversation history more deeply. Our model, FLOWQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of FLOW also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FLOWQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy."
    },
    "keywords": [
        {
            "term": "Catholic",
            "url": "https://en.wikipedia.org/wiki/Catholic"
        },
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "FLOW",
            "url": "https://en.wikipedia.org/wiki/FLOW"
        },
        {
            "term": "single turn",
            "url": "https://en.wikipedia.org/wiki/single_turn"
        }
    ],
    "abbreviations": {
        "MC": "machine comprehension",
        "HEQ": "Human Equivalence Score"
    },
    "highlights": [
        "MACHINE COMPREHENSION<br/><br/>we introduce the task formulations of machine comprehension in both single-turn and conversational settings, and discuss the main ideas of state-of-the-art machine comprehension models.<br/><br/>2.1 TASK FORMULATION<br/><br/>Given an evidence document and a question, the task is to find the answer to the question based on the context",
        "We present FLOWQA, a model designed for conversational machine comprehension",
        "FLOWQA consists of two main components: a base neural model for single-turn machine comprehension and a FLOW mechanism that encodes the conversation history",
        "The larger gain on CoQA, which contains longer dialog chains,7 suggests that our FLOW architecture can capture long-range conversation history more effectively",
        "Table 3 shows the contributions of three components: (1) QHierRNN, the hierarchical LSTM layers for encoding past questions, (2) FLOW, augmenting the intermediate representation from the machine reasoning process in the conversation history, and (3) N -Ans, marking the gold answers to the previous N questions in the context",
        "We presented a novel FLOW component for conversational machine comprehension"
    ],
    "key_statements": [
        "MACHINE COMPREHENSION<br/><br/>we introduce the task formulations of machine comprehension in both single-turn and conversational settings, and discuss the main ideas of state-of-the-art machine comprehension models.<br/><br/>2.1 TASK FORMULATION<br/><br/>Given an evidence document and a question, the task is to find the answer to the question based on the context",
        "We present FLOWQA, a model designed for conversational machine comprehension",
        "FLOWQA consists of two main components: a base neural model for single-turn machine comprehension and a FLOW mechanism that encodes the conversation history",
        "The larger gain on CoQA, which contains longer dialog chains,7 suggests that our FLOW architecture can capture long-range conversation history more effectively",
        "Table 3 shows the contributions of three components: (1) QHierRNN, the hierarchical LSTM layers for encoding past questions, (2) FLOW, augmenting the intermediate representation from the machine reasoning process in the conversation history, and (3) N -Ans, marking the gold answers to the previous N questions in the context",
        "We find that FLOW is a critical component",
        "We consider the task of understanding a sequence of natural language instructions. We reduce this problem to a conversational machine comprehension task and apply FLOWQA",
        "Reduction We reduce sequential instruction understanding to machine comprehension as follows",
        "We presented a novel FLOW component for conversational machine comprehension"
    ],
    "summary": [
        "MACHINE COMPREHENSION<br/><br/>we introduce the task formulations of machine comprehension in both single-turn and conversational settings, and discuss the main ideas of state-of-the-art MC models.<br/><br/>2.1 TASK FORMULATION<br/><br/>Given an evidence document and a question, the task is to find the answer to the question based on the context.",
        "Existing approaches take a single-turn MC model and augment the current question and context with the previous questions and answers (<a class=\"ref-link\" id=\"cChoi_et+al_2018_a\" href=\"#rChoi_et+al_2018_a\">Choi et al, 2018</a>; <a class=\"ref-link\" id=\"cReddy_et+al_2019_a\" href=\"#rReddy_et+al_2019_a\">Reddy et al, 2019</a>).",
        "FLOWQA consists of two main components: a base neural model for single-turn MC and a FLOW mechanism that encodes the conversation history.",
        "For single-turn MC, many top-performing models share a similar architecture, consisting of four major components: (1) question encoding, (2) context encoding, (3) reasoning, and (4) answer prediction.",
        "To adapt to the conversational setting, existing methods incorporate previous question/answer pairs into the current question and context encoding without modifying higher-level layers of the model.",
        "Our model integrates both previous question/answer pairs and FLOW, the intermediate context representation from conversation history (See Fig. 1).",
        "Our model considers these intermediate representations, Cih, generated during the h-th context integration layer of the reasoning component for the i-th question.",
        "We pass each sequence into a GRU3 so the entire intermediate representation for answering the previous questions can be used when processing the current question.",
        "Integration-Flow We concatenate the output from the previous IF layer with the attended question vector, and pass it as an input.",
        "CoQA baselines append the most recent previous question and answer to the current question.6 <a class=\"ref-link\" id=\"cChoi_et+al_2018_a\" href=\"#rChoi_et+al_2018_a\">Choi et al (2018</a>) applied BiDAF++, a strong extractive QA model to QuAC dataset.",
        "FLOWQA (N -Ans) is our model: similar to BiDAF++ (N -ctx), we append the binary feature vector encoding previous N answer spans to the context embeddings.",
        "Table 3 shows the contributions of three components: (1) QHierRNN, the hierarchical LSTM layers for encoding past questions, (2) FLOW, augmenting the intermediate representation from the machine reasoning process in the conversation history, and (3) N -Ans, marking the gold answers to the previous N questions in the context.",
        "We encode the history of instructions explicitly by concatenating preceding questions and the current one and by marking previous answers in the current context similar to N -Ans in conversational MC tasks.",
        "We propose a new architecture suitable for multi-turn MC tasks by passing the hidden model representations of preceding questions using the FLOW design.",
        "By applying FLOW to a state-of-the-art machine comprehension model, our model encodes the conversation history more comprehensively, and yields better performance.",
        "When evaluated on two recently proposed conversational challenge datasets and three domains of a sequential instruction understanding task, FLOWQA outperforms existing models.",
        "We would like to investigate more efficient and fine-grained ways to model the conversation flow, as well as methods that enable machines to engage more active and natural conversational behaviors, such as asking clarification questions"
    ],
    "headline": "Single-turn models to encode the history comprehensively, we introduce FLOW, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure",
    "reference_links": [
        {
            "id": "Antoine_2017_a",
            "entry": "Antoine Bordes, Y-Lan Boureau, and Jason Weston. Learning end-to-end goal-oriented dialog. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antoine%20Bordes%2C%20Y.-Lan%20Boureau%20Weston%2C%20Jason%20Learning%20end-to-end%20goal-oriented%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antoine%20Bordes%2C%20Y.-Lan%20Boureau%20Weston%2C%20Jason%20Learning%20end-to-end%20goal-oriented%20dialog%202017"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer opendomain questions. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Danqi%20Fisch%2C%20Adam%20Weston%2C%20Jason%20and%20Antoine%20Bordes.%20Reading%20Wikipedia%20to%20answer%20opendomain%20questions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Danqi%20Fisch%2C%20Adam%20Weston%2C%20Jason%20and%20Antoine%20Bordes.%20Reading%20Wikipedia%20to%20answer%20opendomain%20questions%202017"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Yun-Nung Chen, Dilek Z. Hakkani-Tur, Gokhan Tur, Jianfeng Gao, and Li Deng. End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding. In INTERSPEECH, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yun-Nung%20Hakkani-Tur%2C%20Dilek%20Z.%20Tur%2C%20Gokhan%20Gao%2C%20Jianfeng%20End-to-end%20memory%20networks%20with%20knowledge%20carryover%20for%20multi-turn%20spoken%20language%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yun-Nung%20Hakkani-Tur%2C%20Dilek%20Z.%20Tur%2C%20Gokhan%20Gao%2C%20Jianfeng%20End-to-end%20memory%20networks%20with%20knowledge%20carryover%20for%20multi-turn%20spoken%20language%20understanding%202016"
        },
        {
            "id": "Choi_et+al_2018_a",
            "entry": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC: Question answering in context. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eunsol%20Choi%20He%20He%20Mohit%20Iyyer%20Mark%20Yatskar%20Wentau%20Yih%20Yejin%20Choi%20Percy%20Liang%20and%20Luke%20Zettlemoyer%20QuAC%20Question%20answering%20in%20context%20In%20EMNLP%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eunsol%20Choi%20He%20He%20Mohit%20Iyyer%20Mark%20Yatskar%20Wentau%20Yih%20Yejin%20Choi%20Percy%20Liang%20and%20Luke%20Zettlemoyer%20QuAC%20Question%20answering%20in%20context%20In%20EMNLP%202018"
        },
        {
            "id": "Elgohary_et+al_2018_a",
            "entry": "Ahmed Elgohary, Chen Zhao, and Jordan Boyd-Garber. A dataset and baselines for sequential open-domain question answering. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elgohary%2C%20Ahmed%20Zhao%2C%20Chen%20Boyd-Garber%2C%20Jordan%20A%20dataset%20and%20baselines%20for%20sequential%20open-domain%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elgohary%2C%20Ahmed%20Zhao%2C%20Chen%20Boyd-Garber%2C%20Jordan%20A%20dataset%20and%20baselines%20for%20sequential%20open-domain%20question%20answering%202018"
        },
        {
            "id": "Fried_et+al_2018_a",
            "entry": "Daniel Fried, Jacob Andreas, and Dan Klein. Unified pragmatic models for generating and following instructions. In NAACL-HLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fried%2C%20Daniel%20Andreas%2C%20Jacob%20Klein%2C%20Dan%20Unified%20pragmatic%20models%20for%20generating%20and%20following%20instructions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fried%2C%20Daniel%20Andreas%2C%20Jacob%20Klein%2C%20Dan%20Unified%20pragmatic%20models%20for%20generating%20and%20following%20instructions%202018"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Ghazvininejad_et+al_2018_a",
            "entry": "Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, William B. Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghazvininejad%2C%20Marjan%20Brockett%2C%20Chris%20Chang%2C%20Ming-Wei%20Dolan%2C%20William%20B.%20A%20knowledge-grounded%20neural%20conversation%20model%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghazvininejad%2C%20Marjan%20Brockett%2C%20Chris%20Chang%2C%20Ming-Wei%20Dolan%2C%20William%20B.%20A%20knowledge-grounded%20neural%20conversation%20model%202018"
        },
        {
            "id": "Guu_et+al_2017_a",
            "entry": "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Liu%2C%20Evan%20Zheran%20Liang%2C%20Percy%20From%20language%20to%20programs%3A%20Bridging%20reinforcement%20learning%20and%20maximum%20marginal%20likelihood%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Liu%2C%20Evan%20Zheran%20Liang%2C%20Percy%20From%20language%20to%20programs%3A%20Bridging%20reinforcement%20learning%20and%20maximum%20marginal%20likelihood%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. FusionNet: Fusing via fullyaware attention with application to machine comprehension. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Hsin-Yuan%20Zhu%2C%20Chenguang%20Shen%2C%20Yelong%20Chen%2C%20Weizhu%20FusionNet%3A%20Fusing%20via%20fullyaware%20attention%20with%20application%20to%20machine%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Hsin-Yuan%20Zhu%2C%20Chenguang%20Shen%2C%20Yelong%20Chen%2C%20Weizhu%20FusionNet%3A%20Fusing%20via%20fullyaware%20attention%20with%20application%20to%20machine%20comprehension%202018"
        },
        {
            "id": "Iyyer_et+al_2017_a",
            "entry": "Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential question answering. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyyer%2C%20Mohit%20Yih%2C%20Wen-tau%20Chang%2C%20Ming-Wei%20Search-based%20neural%20structured%20learning%20for%20sequential%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyyer%2C%20Mohit%20Yih%2C%20Wen-tau%20Chang%2C%20Ming-Wei%20Search-based%20neural%20structured%20learning%20for%20sequential%20question%20answering%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Lewis_et+al_2017_a",
            "entry": "Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no Deal? End-toend learning for negotiation dialogues. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20Mike%20Yarats%2C%20Denis%20Dauphin%2C%20Yann%20Parikh%2C%20Devi%20Deal%20or%20no%20Deal%3F%20End-toend%20learning%20for%20negotiation%20dialogues%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20Mike%20Yarats%2C%20Denis%20Dauphin%2C%20Yann%20Parikh%2C%20Devi%20Deal%20or%20no%20Deal%3F%20End-toend%20learning%20for%20negotiation%20dialogues%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Daniel Jurafsky. Adversarial learning for neural dialogue generation. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jiwei%20Monroe%2C%20Will%20Shi%2C%20Tianlin%20Ritter%2C%20Alan%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jiwei%20Monroe%2C%20Will%20Shi%2C%20Tianlin%20Ritter%2C%20Alan%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017"
        },
        {
            "id": "Long_et+al_2016_a",
            "entry": "Reginald Long, Panupong Pasupat, and Percy Liang. Simpler context-dependent logical forms via model projections. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Reginald%20Pasupat%2C%20Panupong%20Liang%2C%20Percy%20Simpler%20context-dependent%20logical%20forms%20via%20model%20projections%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Reginald%20Pasupat%2C%20Panupong%20Liang%2C%20Percy%20Simpler%20context-dependent%20logical%20forms%20via%20model%20projections%202016"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%202017"
        },
        {
            "id": "Park_et+al_2018_a",
            "entry": "Yookoon Park, Jaemin Cho, and Gunhee Kim. A hierarchical latent structure for variational conversation modeling. In NAACL-HLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20Yookoon%20Cho%2C%20Jaemin%20Kim%2C%20Gunhee%20A%20hierarchical%20latent%20structure%20for%20variational%20conversation%20modeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20Yookoon%20Cho%2C%20Jaemin%20Kim%2C%20Gunhee%20A%20hierarchical%20latent%20structure%20for%20variational%20conversation%20modeling%202018"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Reddy_et+al_2019_a",
            "entry": "Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. TACL, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddy%2C%20Siva%20Chen%2C%20Danqi%20Manning%2C%20Christopher%20D.%20CoQA%3A%20A%20conversational%20question%20answering%20challenge%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddy%2C%20Siva%20Chen%2C%20Danqi%20Manning%2C%20Christopher%20D.%20CoQA%3A%20A%20conversational%20question%20answering%20challenge%202019"
        },
        {
            "id": "Ritter_et+al_2011_a",
            "entry": "Alan Ritter, Colin Cherry, and William B. Dolan. Data-driven response generation in social media. In EMNLP, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ritter%2C%20Alan%20Cherry%2C%20Colin%20Dolan%2C%20William%20B.%20Data-driven%20response%20generation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ritter%2C%20Alan%20Cherry%2C%20Colin%20Dolan%2C%20William%20B.%20Data-driven%20response%20generation%202011"
        },
        {
            "id": "Saeidi_et+al_2018_a",
            "entry": "Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Signh, Tim Rocktschel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. Interpretation of natural language rules in conversational machine reading. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marzieh%20Saeidi%20Max%20Bartolo%20Patrick%20Lewis%20Sameer%20Signh%20Tim%20Rocktschel%20Mike%20Sheldon%20Guillaume%20Bouchard%20and%20Sebastian%20Riedel%20Interpretation%20of%20natural%20language%20rules%20in%20conversational%20machine%20reading%20In%20EMNLP%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marzieh%20Saeidi%20Max%20Bartolo%20Patrick%20Lewis%20Sameer%20Signh%20Tim%20Rocktschel%20Mike%20Sheldon%20Guillaume%20Bouchard%20and%20Sebastian%20Riedel%20Interpretation%20of%20natural%20language%20rules%20in%20conversational%20machine%20reading%20In%20EMNLP%202018"
        },
        {
            "id": "Saha_et+al_2018_a",
            "entry": "Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saha%2C%20Amrita%20Pahuja%2C%20Vardaan%20Khapra%2C%20Mitesh%20M.%20Sankaranarayanan%2C%20Karthik%20Complex%20sequential%20question%20answering%3A%20Towards%20learning%20to%20converse%20over%20linked%20question%20answer%20pairs%20with%20a%20knowledge%20graph%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saha%2C%20Amrita%20Pahuja%2C%20Vardaan%20Khapra%2C%20Mitesh%20M.%20Sankaranarayanan%2C%20Karthik%20Complex%20sequential%20question%20answering%3A%20Towards%20learning%20to%20converse%20over%20linked%20question%20answer%20pairs%20with%20a%20knowledge%20graph%202018"
        },
        {
            "id": "Seo_et+al_2016_a",
            "entry": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Minjoon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Minjoon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202016"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Suhr_2018_a",
            "entry": "Alane Suhr and Yoav Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suhr%2C%20Alane%20Artzi%2C%20Yoav%20Situated%20mapping%20of%20sequential%20instructions%20to%20actions%20with%20single-step%20reward%20observation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suhr%2C%20Alane%20Artzi%2C%20Yoav%20Situated%20mapping%20of%20sequential%20instructions%20to%20actions%20with%20single-step%20reward%20observation%202018"
        },
        {
            "id": "Suhr_et+al_2018_b",
            "entry": "Alane Suhr, Srinivasan Iyer, and Yoav Artzi. Learning to map context-dependent sentences to executable formal queries. In NAACL-HLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suhr%2C%20Alane%20Iyer%2C%20Srinivasan%20Artzi%2C%20Yoav%20Learning%20to%20map%20context-dependent%20sentences%20to%20executable%20formal%20queries%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suhr%2C%20Alane%20Iyer%2C%20Srinivasan%20Artzi%2C%20Yoav%20Learning%20to%20map%20context-dependent%20sentences%20to%20executable%20formal%20queries%202018"
        },
        {
            "id": "Talmor_2018_a",
            "entry": "A. Talmor and J. Berant. The Web as knowledge-base for answering complex questions. In NAACLHLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talmor%2C%20A.%20J.%20Berant.%20The%20Web%20as%20knowledge-base%20for%20answering%20complex%20questions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talmor%2C%20A.%20J.%20Berant.%20The%20Web%20as%20knowledge-base%20for%20answering%20complex%20questions%202018"
        },
        {
            "id": "Wang_2017_a",
            "entry": "Shuohang Wang and Jing Jiang. Machine comprehension using match-LSTM and answer pointer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shuohang%20Jiang%2C%20Jing%20Machine%20comprehension%20using%20match-LSTM%20and%20answer%20pointer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shuohang%20Jiang%2C%20Jing%20Machine%20comprehension%20using%20match-LSTM%20and%20answer%20pointer%202017"
        },
        {
            "id": "Wang_et+al_2017_b",
            "entry": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks for reading comprehension and question answering. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017"
        },
        {
            "id": "Yatskar_2018_a",
            "entry": "Mark Yatskar. A qualitative comparison of CoQA, SQuAD 2.0 and QuAC. CoRR, abs/1809.10735, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.10735"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Adams%20Wei%20Dohan%2C%20David%20Luong%2C%20Minh-Thang%20Zhao%2C%20Rui%20QANet%3A%20Combining%20local%20convolution%20with%20global%20self-attention%20for%20reading%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Adams%20Wei%20Dohan%2C%20David%20Luong%2C%20Minh-Thang%20Zhao%2C%20Rui%20QANet%3A%20Combining%20local%20convolution%20with%20global%20self-attention%20for%20reading%20comprehension%202018"
        },
        {
            "id": "The_2019_a",
            "entry": "The highlighted part of the context indicates the QA model\u2019s guess on the current conversation topic and relevant information. Notice that this is not attention; it is instead a visualization on how the hidden memory is changing over time. The example is from CoQA (Reddy et al., 2019).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20highlighted%20part%20of%20the%20context%20indicates%20the%20QA%20models%20guess%20on%20the%20current%20conversation%20topic%20and%20relevant%20information%20Notice%20that%20this%20is%20not%20attention%20it%20is%20instead%20a%20visualization%20on%20how%20the%20hidden%20memory%20is%20changing%20over%20time%20The%20example%20is%20from%20CoQA%20Reddy%20et%20al%202019"
        },
        {
            "id": "We_2019_b",
            "entry": "We present two examples, where each one of them consists of a passage that the dialog talks about, a sequence of questions and model predictions, and an analysis. For both examples, the original conversation is much longer. Here we only present a subset of the questions to demonstrate the different behaviors of BiDAF++ and FLOWQA. The examples are from CoQA (Reddy et al., 2019).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20present%20two%20examples%20where%20each%20one%20of%20them%20consists%20of%20a%20passage%20that%20the%20dialog%20talks%20about%20a%20sequence%20of%20questions%20and%20model%20predictions%20and%20an%20analysis%20For%20both%20examples%20the%20original%20conversation%20is%20much%20longer%20Here%20we%20only%20present%20a%20subset%20of%20the%20questions%20to%20demonstrate%20the%20different%20behaviors%20of%20BiDAF%20and%20FLOWQA%20The%20examples%20are%20from%20CoQA%20Reddy%20et%20al%202019"
        },
        {
            "id": "B_2009_a",
            "entry": "B.2 EXAMPLE 2: Context: TV talent show star Susan Boyle will sing for Pope Benedict XVI during his visit to Scotland next month, the Catholic Church in Scotland said Wednesday. A church spokesman said in June they were negotiating with the singing phenomenon to perform. Benedict is due to visit England and Scotland from September 16-19. Boyle will perform three times at Bellahouston Park in Glasgow on Thursday, Sept. 16, the Scottish Catholic Media Office said. She will also sing with the 800-strong choir at the open-air Mass there. In the pre-Mass program, Boyle plans to sing the hymn \u201cHow Great Thou Art\u201d as well as her signature song, \u201cI Dreamed a Dream,\u201d the tune from the musical \u201cLes Miserables\u201d that shot her to fame in April 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B2%20EXAMPLE%202%3A%20Context%3A%20TV%20talent%20show%20star%20Susan%20Boyle%20will%20sing%20for%20Pope%20Benedict%20XVI%20during%20his%20visit%20to%20Scotland%20next%20month%2C%20the%20Catholic%20Church%20in%20Scotland%20said%20Wednesday.%20A%20church%20spokesman%20said%20in%20June%20they%20were%20negotiating%20with%20the%20singing%20phenomenon%20to%20perform.%20Benedict%20is%20due%20to%20visit%20England%20and%20Scotland%20from%20September%2016-19.%20Boyle%20will%20perform%20three%20times%20at%20Bellahouston%20Park%20in%20Glasgow%20on%20Thursday%2C%20Sept.%2016%2C%20the%20Scottish%20Catholic%20Media%20Office%20said.%20She%20will%20also%20sing%20with%20the%20800-strong%20choir%20at%20the%20open-air%20Mass%20there.%20In%20the%20pre-Mass%20program%2C%20Boyle%20plans%20to%20sing%20the%20hymn%20%E2%80%9CHow%20Great%20Thou%20Art%E2%80%9D%20as%20well%20as%20her%20signature%20song%2C%20%E2%80%9CI%20Dreamed%20a%20Dream%2C%E2%80%9D%20the%20tune%20from%20the%20musical%20%E2%80%9CLes%20Miserables%E2%80%9D%20that%20shot%20her%20to%20fame%20in%20April%202009"
        },
        {
            "id": "\u201cTo_1982_a",
            "entry": "\u201cTo be able to sing for the pope is a great honor and something I\u2019ve always dreamed of \u2013 it\u2019s indescribable,\u201d Boyle, a Catholic, said in a statement. \u201cI think the 16th of September will stand out in my memory as something I\u2019ve always wanted to do. I\u2019ve always wanted to sing for His Holiness and I can\u2019t really put into words my happiness that this wish has come true at last.\u201d Boyle said her late mother was at the same Glasgow park when Pope John Paul II visited in 1982. After the final hymn at the end of the Mass, Boyle will sing a farewell song to the pope as he leaves to go to the airport for his flight to London, the church said.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=To%20be%20able%20to%20sing%20for%20the%20pope%20is%20a%20great%20honor%20and%20something%20Ive%20always%20dreamed%20of%20%20its%20indescribable%20Boyle%20a%20Catholic%20said%20in%20a%20statement%20I%20think%20the%2016th%20of%20September%20will%20stand%20out%20in%20my%20memory%20as%20something%20Ive%20always%20wanted%20to%20do%20Ive%20always%20wanted%20to%20sing%20for%20His%20Holiness%20and%20I%20cant%20really%20put%20into%20words%20my%20happiness%20that%20this%20wish%20has%20come%20true%20at%20last%20Boyle%20said%20her%20late%20mother%20was%20at%20the%20same%20Glasgow%20park%20when%20Pope%20John%20Paul%20II%20visited%20in%201982%20After%20the%20final%20hymn%20at%20the%20end%20of%20the%20Mass%20Boyle%20will%20sing%20a%20farewell%20song%20to%20the%20pope%20as%20he%20leaves%20to%20go%20to%20the%20airport%20for%20his%20flight%20to%20London%20the%20church%20said"
        },
        {
            "id": "Question-specific_2017_a",
            "entry": "Question-specific context input representation: We restate how the question-specific context input representation Ci0 is generated, following DrQA (Chen et al., 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Questionspecific%20context%20input%20representation%20We%20restate%20how%20the%20questionspecific%20context%20input%20representation%20Ci0%20is%20generated%20following%20DrQA%20Chen%20et%20al%202017"
        },
        {
            "id": "Wang_2017_c",
            "entry": "Answer Span Selection Method: We restate how the answer span selection method is performed (following (Wang et al., 2017; Wang & Jiang, 2017; Huang et al., 2018)) to estimate the start and end probabilities PiS,j, PiE,j of the j-th context token for the i-th question.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%20Jiang%20Answer%20Span%20Selection%20Method%3A%20We%20restate%20how%20the%20answer%20span%20selection%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%20Jiang%20Answer%20Span%20Selection%20Method%3A%20We%20restate%20how%20the%20answer%20span%20selection%20method%202017"
        }
    ]
}
