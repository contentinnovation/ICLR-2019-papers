{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Robot Skill Learning: From Reinforcement Learning to Evolution Strategies",
        "author": "Freek Stulp, Olivier Sigaud",
        "date": 2015,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkeU5j0ctQ",
            "doi": "10.2478/pjbr-2013-0003"
        },
        "journal": "Paladyn, Journal of Behavioral Robotics",
        "volume": "4",
        "abstract": "Policy improvement methods seek to optimize the parameters of a policy with respect to a utility function. Owing to current trends involving searching in parameter space (rather than action space) and using reward-weighted averaging (rather than gradient estimation), reinforcement learning algorithms for policy improvement, e.g. PoWER and PI"
    },
    "keywords": [
        {
            "term": "Estimation of Distribution Algorithms",
            "url": "https://en.wikipedia.org/wiki/Estimation_of_Distribution_Algorithms"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Covariance Matrix Adaptation Evolution Strategy",
            "url": "https://en.wikipedia.org/wiki/Covariance_Matrix_Adaptation_Evolution_Strategy"
        },
        {
            "term": "Cross-Entropy Method",
            "url": "https://en.wikipedia.org/wiki/Cross-Entropy_Method"
        },
        {
            "term": "covariance matrix",
            "url": "https://en.wikipedia.org/wiki/covariance_matrix"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "evolution strategy",
            "url": "https://en.wikipedia.org/wiki/evolution_strategy"
        },
        {
            "term": "evolution strategies",
            "url": "https://en.wikipedia.org/wiki/evolution_strategies"
        },
        {
            "term": "evolutionary algorithm",
            "url": "https://en.wikipedia.org/wiki/evolutionary_algorithm"
        }
    ],
    "abbreviations": {
        "deep RL": "deep reinforcement learning",
        "ESs": "evolution strategies",
        "EDAs": "Estimation of Distribution Algorithms",
        "CEM": "Cross-Entropy Method",
        "CMA-ES": "Covariance Matrix Adaptation Evolution Strategy",
        "GEP": "goal exploration process",
        "DDPG": "Deep Deterministic Policy Gradient"
    },
    "highlights": [
        "Policy search is the problem of finding a policy or controller maximizing some unknown utility function",
        "After presenting some background in Section 3, we propose in Section 4 a new combination method that combines the cross-entropy method (CEM) with Deep Deterministic Policy Gradient or TD3, an off-policy deep reinforcement learning algorithm which improves over Deep Deterministic Policy Gradient",
        "Evolution strategies can be seen as specific evolutionary algorithms where only one individual is retained from one generation to the this individual being the mean of the distribution from which new individuals are drawn",
        "Another difference is that in Cross-Entropy Method-RL gradient steps are applied at each iteration whereas in ERL, a deep reinforcement learning actor is only injected to the population from time to time",
        "Despite being mainly an evolutionary method, Cross-Entropy Method-RL is competitive to the state-of-the-art even when considering sample efficiency, which is not the case of other deep neuroevolution methods (<a class=\"ref-link\" id=\"cSalimans_et+al_2017_a\" href=\"#rSalimans_et+al_2017_a\">Salimans et al, 2017</a>; Petroski Such et al, 2017)",
        "As suggested in Section 5.2.3, another avenue for future work will consist in designing an ERL algorithm based on Cross-Entropy Method rather than on an ad hoc evolutionary algorithm"
    ],
    "key_statements": [
        "Policy search is the problem of finding a policy or controller maximizing some unknown utility function",
        "Research on policy search methods has witnessed a surge of interest due to the combination with deep neural networks, making it possible to find good enough continuous action policies in large domains",
        "After presenting some background in Section 3, we propose in Section 4 a new combination method that combines the cross-entropy method (CEM) with Deep Deterministic Policy Gradient or TD3, an off-policy deep reinforcement learning algorithm which improves over Deep Deterministic Policy Gradient",
        "In Section 5, we investigate experimentally the properties of this CEMRL method, showing its advantages both over the components taken separately and over a competing approach",
        "By contrast with the works outlined above, the method presented here combines Cross-Entropy Method and TD3 in such a way that our algorithm benefits from the gradient-based policy improvement mechanism of TD3, from the better stability of evolution strategies, and may even benefit from the better sample efficiency brought by importance mixing, as described in Appendix B",
        "We provide a quick overview of the evolutionary and deep reinforcement learning methods used throughout the paper.\n3.1",
        "Evolution strategies can be seen as specific evolutionary algorithms where only one individual is retained from one generation to the this individual being the mean of the distribution from which new individuals are drawn",
        "Among evolution strategies, Estimation of Distribution Algorithms (EDAs) are a specific family where the population is represented as a distribution using a covariance matrix \u03a3 (<a class=\"ref-link\" id=\"cLarranaga_2001_a\" href=\"#rLarranaga_2001_a\">Larranaga & Lozano, 2001</a>)",
        "Another difference is that in Cross-Entropy Method-RL gradient steps are applied at each iteration whereas in ERL, a deep reinforcement learning actor is only injected to the population from time to time",
        "We first compare Cross-Entropy Method-TD3 to TD3, TD3 and a multi-actor variant of TD3, Cross-Entropy Method-RL to ERL based on several benchmarks",
        "A third section is devoted to additional results which have been rejected in appendices to comply with space constraints.\n5.2.1",
        "For TD3 and its multi-actor variant, we report the average of the score of the agent over 10 episodes for every 5000 steps performed in the environment",
        "For Cross-Entropy Method and Cross-Entropy Method-TD3, we report after each generation the average of the score of the new average individual of the population over 10 episodes",
        "Cross-Entropy Method outperforms all other algorithms on SWIMMER-V2, as covered in Appendix E",
        "We considered a population of 5 actors initialized as in Cross-Entropy Method-TD3, but just following the gradient given by the TD3 critic",
        "We outline the main messages arising from further studies that have been rejected in appendices in order to comply with space constraints",
        "In Appendix B, we investigate the influence of the importance mixing mechanism over the evolution of performance, for Cross-Entropy Method and Cross-Entropy Method-RL",
        "Results show that importance mixing has a limited impact on the sample efficiency of Cross-Entropy Method-TD3 on the benchmarks studied here, in contradiction with results from <a class=\"ref-link\" id=\"cPourchot_et+al_2018_a\" href=\"#rPourchot_et+al_2018_a\">Pourchot et al (2018</a>) obtained using various standard evolutionary strategies",
        "Unlike what Khadka & Tumer (2018b) suggested using ERL, we did not find any conclusive evidence that action space noise improves performance with Cross-Entropy Method-TD3. This may be due to the fact that, as further studied in Appendix D, the evolutionary algorithm in ERL tends to converge to a unique individual, additional noise is welcome, whereas evolutionary strategies like Cross-Entropy Method more maintain some exploration",
        "We further investigate the different dynamics of parameter space exploration provided by the ERL and Cross-Entropy Method-TD3 algorithms in Appendix D",
        "The most likely explanation is that, in SWIMMER-V2, any deep reinforcement learning method provides a deceptive gradient information which is detrimental to convergence towards efficient actor parameters",
        "Despite being mainly an evolutionary method, Cross-Entropy Method-RL is competitive to the state-of-the-art even when considering sample efficiency, which is not the case of other deep neuroevolution methods (<a class=\"ref-link\" id=\"cSalimans_et+al_2017_a\" href=\"#rSalimans_et+al_2017_a\">Salimans et al, 2017</a>; Petroski Such et al, 2017)",
        "As suggested in Section 5.2.3, another avenue for future work will consist in designing an ERL algorithm based on Cross-Entropy Method rather than on an ad hoc evolutionary algorithm"
    ],
    "summary": [
        "Policy search is the problem of finding a policy or controller maximizing some unknown utility function.",
        "When the gradient-based policy improvement mechanism of DDPG is efficient, this individual outperforms its evolutionary siblings, it gets selected into the generation and draws the whole population towards higher performance.",
        "By contrast with the works outlined above, the method presented here combines CEM and TD3 in such a way that our algorithm benefits from the gradient-based policy improvement mechanism of TD3, from the better stability of ESs, and may even benefit from the better sample efficiency brought by importance mixing, as described in Appendix B.",
        "The CEM algorithm takes the top-performing half of the resulting global population to compute its new \u03c0\u03bc and \u03a3.",
        "In cases where the gradient steps decrease performance, the resulting actors are ignored by CEM, which instead focuses on standard samples around \u03c0\u03bc.",
        "By contrast with Khadka & Tumer (2018b), gradient steps are directly applied to several samples, and using the CEM algorithm makes it possible to use importance mixing, as described in Appendix B.",
        "Another difference is that in CEM-RL gradient steps are applied at each iteration whereas in ERL, a deep RL actor is only injected to the population from time to time.",
        "Figure 3 shows that after performing 1 million steps, both CEM-RL methods outperform ERL on HALF-CHEETAH-V2, HOPPER-V2 and WALKER2D-V2.",
        "Results show that importance mixing has a limited impact on the sample efficiency of CEM-TD3 on the benchmarks studied here, in contradiction with results from <a class=\"ref-link\" id=\"cPourchot_et+al_2018_a\" href=\"#rPourchot_et+al_2018_a\">Pourchot et al (2018</a>) obtained using various standard evolutionary strategies.",
        "This may be due to the fact that, as further studied in Appendix D, the evolutionary algorithm in ERL tends to converge to a unique individual, additional noise is welcome, whereas evolutionary strategies like CEM more maintain some exploration.",
        "The most likely explanation is that, in SWIMMER-V2, any deep RL method provides a deceptive gradient information which is detrimental to convergence towards efficient actor parameters.",
        "We have proposed such a combination, the CEM-RL method, and showed that in most cases it was outperforming not only some evolution strategies and some sample efficient offpolicy deep RL algorithms, but another combination, the ERL algorithm.",
        "Given the impact of the neural architecture on our results, we believe that a more systemic search of architectures through techniques such as neural architecture search (<a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\"><a class=\"ref-link\" id=\"cZoph_2016_a\" href=\"#rZoph_2016_a\">Zoph & Le, 2016</a></a>; <a class=\"ref-link\" id=\"cElsken_et+al_2018_a\" href=\"#rElsken_et+al_2018_a\"><a class=\"ref-link\" id=\"cElsken_et+al_2018_a\" href=\"#rElsken_et+al_2018_a\">Elsken et al, 2018</a></a>) may provide important progress in performance of deep policy search algorithms."
    ],
    "headline": "After presenting some background in Section 3, we propose in Section 4 a new combination method that combines the cross-entropy method with Deep Deterministic Policy Gradient or TD3, an off-policy deep reinforcement learning algorithm which improves over Deep Deterministic Policy Gradient",
    "reference_links": [
        {
            "id": "Arnold_et+al_2011_a",
            "entry": "L. Arnold, A. Auger, N. Hansen, and Y. Ollivier. Information-geometric optimization algorithms: A unifying picture via invariance principles. Technical report, INRIA Saclay, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arnold%2C%20L.%20Auger%2C%20A.%20Hansen%2C%20N.%20Ollivier%2C%20Y.%20Information-geometric%20optimization%20algorithms%3A%20A%20unifying%20picture%20via%20invariance%20principles%202011"
        },
        {
            "id": "Back_1996_a",
            "entry": "Thomas Back. Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms. Oxford university press, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Back%2C%20Thomas%20Evolutionary%20algorithms%20in%20theory%20and%20practice%3A%20evolution%20strategies%2C%20evolutionary%20programming%2C%20genetic%20algorithms%201996"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Colas_et+al_2018_a",
            "entry": "Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. arXiv preprint arXiv:1802.05054, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05054"
        },
        {
            "id": "Conti_et+al_2017_a",
            "entry": "Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. arXiv preprint arXiv:1712.06560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06560"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. OpenAI baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. arXiv preprint arXiv:1604.06778, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.06778"
        },
        {
            "id": "Elsken_et+al_2018_a",
            "entry": "T. Elsken, J. Hendrik Metzen, and F. Hutter. Neural Architecture Search: A Survey. ArXiv e-prints, August 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elsken%2C%20T.%20Metzen%2C%20J.Hendrik%20Hutter%2C%20F.%20Neural%20Architecture%20Search%3A%20A%20Survey.%20ArXiv%20e-prints%202018-08"
        },
        {
            "id": "Fujimoto_et+al_2018_a",
            "entry": "Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic methods. CoRR, abs/1802.09477, 2018. URL http://arxiv.org/abs/1802.09477.",
            "url": "http://arxiv.org/abs/1802.09477",
            "arxiv_url": "https://arxiv.org/pdf/1802.09477"
        },
        {
            "id": "Gupta_et+al_2018_a",
            "entry": "Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07245"
        },
        {
            "id": "Hansen_2016_a",
            "entry": "Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00772"
        },
        {
            "id": "Henderson_et+al_2017_a",
            "entry": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06560"
        },
        {
            "id": "Houthooft_et+al_2018_a",
            "entry": "Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. arXiv preprint arXiv:1802.04821, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04821"
        },
        {
            "id": "Khadka_0000_a",
            "entry": "Shauharda Khadka and Kagan Tumer. Evolutionary reinforcement learning. arXiv preprint arXiv:1805.07917, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07917"
        },
        {
            "id": "Khadka_2018_a",
            "entry": "Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. In Neural Information Processing Systems, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khadka%2C%20Shauharda%20Tumer%2C%20Kagan%20Evolution-guided%20policy%20gradient%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khadka%2C%20Shauharda%20Tumer%2C%20Kagan%20Evolution-guided%20policy%20gradient%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Larranaga_2001_a",
            "entry": "Pedro Larranaga and Jose A Lozano. Estimation of distribution algorithms: A new tool for evolutionary computation, volume 2. Springer Science & Business Media, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larranaga%2C%20Pedro%20Lozano%2C%20Jose%20A.%20Estimation%20of%20distribution%20algorithms%3A%20A%20new%20tool%20for%20evolutionary%20computation%2C%20volume%202%202001"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Maheswaranathan_et+al_2018_a",
            "entry": "Niru Maheswaranathan, Luke Metz, George Tucker, and Jascha Sohl-Dickstein. Guided evolutionary strategies: escaping the curse of dimensionality in random search. arXiv preprint arXiv:1806.10230, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10230"
        },
        {
            "id": "Osband_et+al_2016_a",
            "entry": "Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pp. 4026\u20134034, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016"
        },
        {
            "id": "Such_et+al_2017_a",
            "entry": "Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "Plappert_et+al_2017_a",
            "entry": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "Pourchot_et+al_2018_a",
            "entry": "Alo\u0131s Pourchot, Nicolas Perrin, and Olivier Sigaud. Importance mixing: Improving sample reuse in evolutionary policy search methods. arXiv preprint arXiv:1808.05832, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05832"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05477"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sigaud_2018_a",
            "entry": "Olivier Sigaud and Freek Stulp. Policy search in continuous action domains: an overview. arXiv preprint arXiv:1803.04706, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.04706"
        },
        {
            "id": "Simpson_1953_a",
            "entry": "George Gaylord Simpson. The Baldwin effect. Evolution, 7(2):110\u2013117, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simpson%2C%20George%20Gaylord%20The%20Baldwin%20effect%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simpson%2C%20George%20Gaylord%20The%20Baldwin%20effect%201953"
        },
        {
            "id": "Stulp_2012_a",
            "entry": "Freek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix adaptation. In Proceedings of the 29th International Conference on Machine Learning, pp. 1\u20138, Edinburgh, Scotland, 2012a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stulp%2C%20Freek%20Sigaud%2C%20Olivier%20Path%20integral%20policy%20improvement%20with%20covariance%20matrix%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stulp%2C%20Freek%20Sigaud%2C%20Olivier%20Path%20integral%20policy%20improvement%20with%20covariance%20matrix%20adaptation%202012"
        },
        {
            "id": "Stulp_0000_a",
            "entry": "Freek Stulp and Olivier Sigaud. Policy improvement methods: Between black-box optimization and episodic reinforcement learning. Technical report, hal-00738463, 2012b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stulp%2C%20Freek%20Sigaud%2C%20Olivier%20Policy%20improvement%20methods%3A%20Between%20black-box%20optimization%20and%20episodic%20reinforcement%20learning"
        },
        {
            "id": "Stulp_2013_a",
            "entry": "Freek Stulp and Olivier Sigaud. Robot skill learning: From reinforcement learning to evolution strategies. Paladyn Journal of Behavioral Robotics, 4(1):49\u201361, august 2013. doi: 10.2478/ pjbr-2013-0003.",
            "crossref": "https://dx.doi.org/10.2478/pjbr-2013-0003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.2478/pjbr-2013-0003"
        },
        {
            "id": "Sun_et+al_2009_a",
            "entry": "Yi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Efficient natural evolution strategies. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pp. 539\u2013 546. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Yi%20Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Schmidhuber%2C%20Juergen%20Efficient%20natural%20evolution%20strategies%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Yi%20Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Schmidhuber%2C%20Juergen%20Efficient%20natural%20evolution%20strategies%202009"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "Zoph_2016_a",
            "entry": "Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        },
        {
            "id": "Our_2018_a",
            "entry": "Our network architectures are very similar to the ones described in Fujimoto et al. (2018). In particular, the size of the layers remains the same. The only differences resides in the non-linearities. We use tanh operations in the actor between each layer, where Fujimoto et al. use RELU and we use leaky RELU in the critic, where Fujimoto et al. use simple RELU. Reasons for this choice are presented in Appendix F.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Our%20network%20architectures%20are%20very%20similar%20to%20the%20ones%20described%20in%20Fujimoto%20et%20al%202018%20In%20particular%20the%20size%20of%20the%20layers%20remains%20the%20same%20The%20only%20differences%20resides%20in%20the%20nonlinearities%20We%20use%20tanh%20operations%20in%20the%20actor%20between%20each%20layer%20where%20Fujimoto%20et%20al%20use%20RELU%20and%20we%20use%20leaky%20RELU%20in%20the%20critic%20where%20Fujimoto%20et%20al%20use%20simple%20RELU%20Reasons%20for%20this%20choice%20are%20presented%20in%20Appendix%20F",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Our%20network%20architectures%20are%20very%20similar%20to%20the%20ones%20described%20in%20Fujimoto%20et%20al%202018%20In%20particular%20the%20size%20of%20the%20layers%20remains%20the%20same%20The%20only%20differences%20resides%20in%20the%20nonlinearities%20We%20use%20tanh%20operations%20in%20the%20actor%20between%20each%20layer%20where%20Fujimoto%20et%20al%20use%20RELU%20and%20we%20use%20leaky%20RELU%20in%20the%20critic%20where%20Fujimoto%20et%20al%20use%20simple%20RELU%20Reasons%20for%20this%20choice%20are%20presented%20in%20Appendix%20F"
        }
    ]
}
