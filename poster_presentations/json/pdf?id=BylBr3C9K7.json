{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ENERGY-CONSTRAINED COMPRESSION FOR DEEP NEURAL NETWORKS VIA WEIGHTED SPARSE PROJECTION AND LAYER INPUT MASKING",
        "author": "Haichuan Yang, Yuhao Zhu, and Ji Liu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BylBr3C9K7"
        },
        "abstract": "Deep Neural Networks (DNNs) are increasingly deployed in highly energyconstrained environments such as autonomous drones and wearable devices while at the same time must operate in real-time. Therefore, reducing the energy consumption has become a major design consideration in DNN training. This paper proposes the first end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the DNN training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. We integrate the quantitative DNN energy estimation into the DNN training process to assist the constrained optimization. We prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, our framework trains DNNs that provide higher accuracies under same or lower energy budgets."
    },
    "keywords": [
        {
            "term": "energy budget",
            "url": "https://en.wikipedia.org/wiki/energy_budget"
        },
        {
            "term": "Tensor Processing Unit",
            "url": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit"
        },
        {
            "term": "autonomous drone",
            "url": "https://en.wikipedia.org/wiki/autonomous_drone"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "language processing",
            "url": "https://en.wikipedia.org/wiki/language_processing"
        },
        {
            "term": "Dynamic Random Access Memory",
            "url": "https://en.wikipedia.org/wiki/Dynamic_Random_Access_Memory"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "energy consumption",
            "url": "https://en.wikipedia.org/wiki/energy_consumption"
        }
    ],
    "abbreviations": {
        "DNNs": "Deep Neural Networks",
        "EAP": "energy-aware pruning",
        "TPU": "Tensor Processing Unit",
        "FC": "fully connected",
        "ReLU": "Rectified Linear Unit",
        "DRAM": "Dynamic Random Access Memory",
        "RF": "Register File",
        "MP": "magnitude-based pruning",
        "SSL": "sparsity learning",
        "SBP": "structured bayesian pruning",
        "BC": "bayesian compression"
    },
    "highlights": [
        "Deep Neural Networks (DNNs) have become the fundamental building blocks of many emerging application domains such as computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and natural language processing (<a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\"><a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\">Goldberg, 2016</a></a>)",
        "We model a common, three-level memory hierarchy composed of a Dynamic Random Access Memory (DRAM), a Cache, and a Register File (RF)",
        "Given the energy model presented in Section 3, we propose a new energy-constrained Deep Neural Networks model that bounds the energy consumption of a Deep Neural Networks\u2019s inference",
        "To include the sparsity of X in our training framework, we introduce a trainable binary mask M that is of the same shape of X, and is multiplied with X before X is fed into CONV or fully connected layers, or equivalently, at the end of the previous layer",
        "This paper demonstrates that it is possible to train Deep Neural Networks with quantitative energy guarantees in an end-to-end fashion",
        "Our training framework generates Deep Neural Networks with higher accuracies under the same or lower energy budgets compared to prior art"
    ],
    "key_statements": [
        "Deep Neural Networks (DNNs) have become the fundamental building blocks of many emerging application domains such as computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and natural language processing (<a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\"><a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\">Goldberg, 2016</a></a>)",
        "We model the Deep Neural Networks energy consumption after the popular systolic array hardware architecture (<a class=\"ref-link\" id=\"cKung_1982_a\" href=\"#rKung_1982_a\">Kung, 1982</a>) that is increasingly adopted in today\u2019s Deep Neural Networks hardware chips such as Google\u2019s Tensor Processing Unit (TPU) (<a class=\"ref-link\" id=\"cJouppi_et+al_2017_a\" href=\"#rJouppi_et+al_2017_a\">Jouppi et al, 2017</a>), NVidia\u2019s Tensor Cores, and ARM\u2019s ML Processor",
        "We propose a quantitative model to estimate the energy consumption of Deep Neural Networks inference on Tensor Processing Unit-like hardware",
        "We present the detailed per-layer energy modeling (Section 3.2 and Section 3.3), which allow us to derive the overall Deep Neural Networks energy consumption (Section 3.4)",
        "That model sparsity is not the end goal of our paper; rather we focus on reducing the energy consumption directly",
        "We focus mainly on modeling the energy consumption of the CONV and fully connected layers",
        "Since we mainly use pruning as the energy reduction technique, we model how Ecomp and Edata are affected by Deep Neural Networks sparsity.\n3.2",
        "We model a common, three-level memory hierarchy composed of a Dynamic Random Access Memory (DRAM), a Cache, and a Register File (RF)",
        "Given the energy model presented in Section 3, we propose a new energy-constrained Deep Neural Networks model that bounds the energy consumption of a Deep Neural Networks\u2019s inference",
        "To include the sparsity of X in our training framework, we introduce a trainable binary mask M that is of the same shape of X, and is multiplied with X before X is fed into CONV or fully connected layers, or equivalently, at the end of the previous layer",
        "Table 1 shows the top-5 test accuracy drop and energy consumption of various methods compared to the dense model",
        "This paper demonstrates that it is possible to train Deep Neural Networks with quantitative energy guarantees in an end-to-end fashion",
        "Our training framework generates Deep Neural Networks with higher accuracies under the same or lower energy budgets compared to prior art"
    ],
    "summary": [
        "Deep Neural Networks (DNNs) have become the fundamental building blocks of many emerging application domains such as computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>; <a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\"><a class=\"ref-link\" id=\"cSimonyan_2014_a\" href=\"#rSimonyan_2014_a\">Simonyan & Zisserman, 2014</a></a>), speech recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>), and natural language processing (<a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\"><a class=\"ref-link\" id=\"cGoldberg_2016_a\" href=\"#rGoldberg_2016_a\">Goldberg, 2016</a></a>).",
        "Our learning algorithm directly trains a DNN model that meets a given energy budget while maximizing model accuracy without incremental hyper-parameter tuning.",
        "A DNN model, once is trained, by design meets the energy budget while maximizing the accuracy.",
        "Given the DNN energy estimation, we formulate DNN training as an optimization problem that minimizes the accuracy loss under the constraint of a certain energy budget.",
        "We propose a quantitative model to estimate the energy consumption of DNN inference on TPU-like hardware.",
        "We formulate a new optimization problem for energy-constrained DNN training and present a general optimization algorithm that solves the problem.",
        "Our work integrates the energy budget as an optimization constraint in model training.",
        "Since we mainly use pruning as the energy reduction technique, we model how Ecomp and Edata are affected by DNN sparsity.",
        "This section formulates training an energy-constrained DNN as an optimization problem.",
        "We first formulate the optimization constraint by introducing a trainable mask variable into the energy modeling to enforce layer input sparsity.",
        "Controlling Input Sparsity Using Input Mask The objective of training an energy-constrained DNN is to minimize the accuracy loss while ensuring that the DNN inference energy is below a given budget, Ebudget.",
        "Training an energy-constrained DNN model is formulated as an optimization problem: min",
        "Filter pruning methods (<a class=\"ref-link\" id=\"cLi_et+al_2016_a\" href=\"#rLi_et+al_2016_a\">Li et al, 2016</a>; <a class=\"ref-link\" id=\"cHe_et+al_2017_a\" href=\"#rHe_et+al_2017_a\">He et al, 2017</a>) require a sparsity ratio to be set for each layer, and these sparsity hyper-parameters will determine the energy cost of the DNN.",
        "Considering manually setting all these hyper-parameters in energy-constrained compression is not trivial, we directly compare against NetAdapt (Yang et al, 2018) which automatically searches such sparsity ratios and use filter pruning to compress DNN models.",
        "We initialize all the DNNs by a pre-trained dense model, which is used to set up the knowledge distillation regularization.",
        "Table 1 shows the top-5 test accuracy drop and energy consumption of various methods compared to the dense model.",
        "Masks lets us control the sparsity of the layer inputs and further reduce energy than merely pruning model parameters as in conventional methods.",
        "Leveraging the energy model, we augment the conventional DNN training with an energy-constrained optimization process, which minimizes the accuracy loss under the constraint of a given energy budget.",
        "Our training framework generates DNNs with higher accuracies under the same or lower energy budgets compared to prior art."
    ],
    "headline": "This paper proposes the first end-to-end Deep Neural Networks training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking",
    "reference_links": [
        {
            "id": "Ba_2014_a",
            "entry": "Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pp. 2654\u20132662, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "Chan_2018_a",
            "entry": "Timothy M Chan. Approximation schemes for 0-1 knapsack. In OASIcs-OpenAccess Series in Informatics, volume 61. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20Timothy%20M.%20Approximation%20schemes%20for%200-1%20knapsack%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20Timothy%20M.%20Approximation%20schemes%20for%200-1%20knapsack%202018"
        },
        {
            "id": "Chellapilla_et+al_2006_a",
            "entry": "Kumar Chellapilla, Sidd Puri, and Patrice Simard. High performance convolutional neural networks for document processing. In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chellapilla%2C%20Kumar%20Puri%2C%20Sidd%20Simard%2C%20Patrice%20High%20performance%20convolutional%20neural%20networks%20for%20document%20processing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chellapilla%2C%20Kumar%20Puri%2C%20Sidd%20Simard%2C%20Patrice%20High%20performance%20convolutional%20neural%20networks%20for%20document%20processing%202006"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Yu-Hsin Chen, Joel Emer, and Vivienne Sze. Eyeriss: A Spatial Architecture for Energy-efficient Dataflow for Convolutional Neural Networks. In Proc. of ISCA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yu-Hsin%20Emer%2C%20Joel%20Sze%2C%20Vivienne%20Eyeriss%3A%20A%20Spatial%20Architecture%20for%20Energy-efficient%20Dataflow%20for%20Convolutional%20Neural%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yu-Hsin%20Emer%2C%20Joel%20Sze%2C%20Vivienne%20Eyeriss%3A%20A%20Spatial%20Architecture%20for%20Energy-efficient%20Dataflow%20for%20Convolutional%20Neural%20Networks%202016"
        },
        {
            "id": "Chetlur_et+al_2014_a",
            "entry": "Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.0759"
        },
        {
            "id": "Courbariaux_et+al_2015_a",
            "entry": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "Dantzig_1957_a",
            "entry": "George B Dantzig. Discrete-variable extremum problems. Operations research, 5(2):266\u2013288, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dantzig%2C%20George%20B.%20Discrete-variable%20extremum%20problems%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dantzig%2C%20George%20B.%20Discrete-variable%20extremum%20problems%201957"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Goldberg_2016_a",
            "entry": "Yoav Goldberg. A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57:345\u2013420, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberg%2C%20Yoav%20A%20primer%20on%20neural%20network%20models%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberg%2C%20Yoav%20A%20primer%20on%20neural%20network%20models%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Gong_et+al_2014_a",
            "entry": "Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "Han_et+al_2015_b",
            "entry": "Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pp. 1135\u20131143, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "Han_et+al_2016_a",
            "entry": "Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark Horowitz, and William Dally. EIE: Efficient Inference Engine on Compressed Deep Neural Network. In Proc. of ISCA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%20Han%20Xingyu%20Liu%20Huizi%20Mao%20Jing%20Pu%20Ardavan%20Pedram%20Mark%20Horowitz%20and%20William%20Dally%20EIE%20Efficient%20Inference%20Engine%20on%20Compressed%20Deep%20Neural%20Network%20In%20Proc%20of%20ISCA%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%20Han%20Xingyu%20Liu%20Huizi%20Mao%20Jing%20Pu%20Ardavan%20Pedram%20Mark%20Horowitz%20and%20William%20Dally%20EIE%20Efficient%20Inference%20Engine%20on%20Compressed%20Deep%20Neural%20Network%20In%20Proc%20of%20ISCA%202016"
        },
        {
            "id": "He_et+al_2017_a",
            "entry": "Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV), volume 2, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Yihui%20Zhang%2C%20Xiangyu%20Sun%2C%20Jian%20Channel%20pruning%20for%20accelerating%20very%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Yihui%20Zhang%2C%20Xiangyu%20Sun%2C%20Jian%20Channel%20pruning%20for%20accelerating%20very%20deep%20neural%20networks%202017"
        },
        {
            "id": "He_et+al_2018_a",
            "entry": "Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 784\u2013800, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Yihui%20Lin%2C%20Ji%20Liu%2C%20Zhijian%20Wang%2C%20Hanrui%20Amc%3A%20Automl%20for%20model%20compression%20and%20acceleration%20on%20mobile%20devices%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Yihui%20Lin%2C%20Ji%20Liu%2C%20Zhijian%20Wang%2C%20Hanrui%20Amc%3A%20Automl%20for%20model%20compression%20and%20acceleration%20on%20mobile%20devices%202018"
        },
        {
            "id": "Hennessy_2011_a",
            "entry": "John L Hennessy and David A Patterson. Computer architecture: a quantitative approach. Elsevier, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hennessy%2C%20John%20L.%20Patterson%2C%20David%20A.%20Computer%20architecture%3A%20a%20quantitative%20approach%202011"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Jouppi_et+al_2017_a",
            "entry": "Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pp. 1\u201312. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jouppi%2C%20Norman%20P.%20Young%2C%20Cliff%20Patil%2C%20Nishant%20Patterson%2C%20David%20In-datacenter%20performance%20analysis%20of%20a%20tensor%20processing%20unit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jouppi%2C%20Norman%20P.%20Young%2C%20Cliff%20Patil%2C%20Nishant%20Patterson%2C%20David%20In-datacenter%20performance%20analysis%20of%20a%20tensor%20processing%20unit%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kung_1982_a",
            "entry": "Hsiang-Tsung Kung. Why systolic architectures? IEEE computer, 15(1):37\u201346, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kung%2C%20Hsiang-Tsung%20Why%20systolic%20architectures%3F%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kung%2C%20Hsiang-Tsung%20Why%20systolic%20architectures%3F%201982"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08710"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806\u2013814, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Louizos_et+al_2017_a",
            "entry": "Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pp. 3288\u20133298, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Ullrich%2C%20Karen%20Welling%2C%20Max%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Mishra_2017_a",
            "entry": "Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. arXiv preprint arXiv:1711.05852, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05852"
        },
        {
            "id": "Neklyudov_et+al_2017_a",
            "entry": "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. In Advances in Neural Information Processing Systems, pp. 6775\u20136784, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017"
        },
        {
            "id": "Parashar_et+al_2017_a",
            "entry": "Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally. SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In Proc. of ISCA, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parashar%2C%20Angshuman%20Rhu%2C%20Minsoo%20Mukkara%2C%20Anurag%20Puglielli%2C%20Antonio%20SCNN%3A%20An%20Accelerator%20for%20Compressed-sparse%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parashar%2C%20Angshuman%20Rhu%2C%20Minsoo%20Mukkara%2C%20Anurag%20Puglielli%2C%20Antonio%20SCNN%3A%20An%20Accelerator%20for%20Compressed-sparse%202017"
        },
        {
            "id": "Samajdar_et+al_2018_a",
            "entry": "Ananda Samajdar, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar Krishna. Scale-sim: Systolic cnn accelerator. arXiv preprint arXiv:1811.02883, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.02883"
        },
        {
            "id": "Sandler_et+al_2018_a",
            "entry": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510\u20134520, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Tschannen_et+al_2017_a",
            "entry": "Michael Tschannen, Aran Khanna, and Anima Anandkumar. Strassennets: Deep learning with a multiplication budget. arXiv preprint arXiv:1712.03942, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03942"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074\u20132082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4820\u20134828, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5687\u20135695, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tien-Ju%20Chen%2C%20Yu-Hsin%20Sze%2C%20Vivienne%20Designing%20energy-efficient%20convolutional%20neural%20networks%20using%20energy-aware%20pruning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tien-Ju%20Chen%2C%20Yu-Hsin%20Sze%2C%20Vivienne%20Designing%20energy-efficient%20convolutional%20neural%20networks%20using%20energy-aware%20pruning%202017"
        },
        {
            "id": "Chen_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20TienJu%20Yang%20Andrew%20Howard%20Bo%20Chen%20Xiao%20Zhang%20Alec%20Go%20Vivienne%20Sze%20and%20Hartwig%20Adam",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20TienJu%20Yang%20Andrew%20Howard%20Bo%20Chen%20Xiao%20Zhang%20Alec%20Go%20Vivienne%20Sze%20and%20Hartwig%20Adam"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Netadapt: Platform-aware neural network adaptation for mobile applications. arXiv preprint arXiv:1804.03230, 2018. Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In European Conference on Computer Vision, pp. 662\u2013677.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03230"
        },
        {
            "id": "Springer_2018_a",
            "entry": "Springer, 2016. Yuhao Zhu, Anand Samajdar, Matthew Mattina, and Paul Whatmough. Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision. In Proc. of ISCA, 2018. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low-bitwidth convolutional neural networks. In other words, 2:2, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016%20Yuhao%20Zhu%20Anand%20Samajdar%20Matthew%20Mattina%20and%20Paul%20Whatmough%20Euphrates%20AlgorithmSoC%20CoDesign%20for%20LowPower%20Mobile%20Continuous%20Vision%20In%20Proc%20of%20ISCA%202018%20Bohan%20Zhuang%20Chunhua%20Shen%20Mingkui%20Tan%20Lingqiao%20Liu%20and%20Ian%20Reid%20Towards%20effective%20lowbitwidth%20convolutional%20neural%20networks%20In%20other%20words%2022%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%202016%20Yuhao%20Zhu%20Anand%20Samajdar%20Matthew%20Mattina%20and%20Paul%20Whatmough%20Euphrates%20AlgorithmSoC%20CoDesign%20for%20LowPower%20Mobile%20Continuous%20Vision%20In%20Proc%20of%20ISCA%202018%20Bohan%20Zhuang%20Chunhua%20Shen%20Mingkui%20Tan%20Lingqiao%20Liu%20and%20Ian%20Reid%20Towards%20effective%20lowbitwidth%20convolutional%20neural%20networks%20In%20other%20words%2022%202018"
        }
    ]
}
