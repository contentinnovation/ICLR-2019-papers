{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS",
        "author": "Sanjeev Arora Princeton University and Institute for Advanced Study arora@cs.princeton.edu",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkMQg3C5K7"
        },
        "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x \u2192 WN WN\u22121 \u00b7 \u00b7 \u00b7 W1x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (<a class=\"ref-link\" id=\"cBartlett_et+al_2018_a\" href=\"#rBartlett_et+al_2018_a\">Bartlett et al., 2018</a>)."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "global optimum",
            "url": "https://en.wikipedia.org/wiki/global_optimum"
        },
        {
            "term": "critical point",
            "url": "https://en.wikipedia.org/wiki/critical_point"
        },
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Deep learning builds upon the mysterious ability of gradient-based optimization methods to solve related non-convex problems",
        "We show that when minimizing 2 loss of a deep linear network over a whitened dataset, gradient descent converges to the global minimum, at a linear rate, provided that the following conditions hold: (i) the dimensions of hidden layers are greater than or equal to the minimum between those of the input and output; layers are initialized to be approximately balanced \u2014 this is met under commonplace near-zero, as well as residual initializations; and the initial loss is smaller than any loss obtainable with rank deficiencies \u2014 this condition will hold with probability close to 0.5 if the output dimension is 1 and standard near-zero initialization is employed",
        "We began by experimenting in the setting covered by our analysis \u2014 linear neural networks trained via gradient descent minimization of 2 loss over whitened data",
        "For deep linear neural networks, we have rigorously proven convergence of gradient descent to global minima, at a linear rate, provided that the initial weight matrices are approximately balanced and the initial end-to-end matrix has positive deficiency margin",
        "For networks with output dimension 1, we have shown that a balanced initialization, i.e. a random choice of the end-to-end matrix followed by a balanced partition across all layers, leads assumptions to be met, and convergence to take place, with constant probability",
        "Through simple experiments we have shown that the latter can lead to favorable convergence in deep learning practice, as it does in theory"
    ],
    "key_statements": [
        "Deep learning builds upon the mysterious ability of gradient-based optimization methods to solve related non-convex problems",
        "Several papers (e.g. <a class=\"ref-link\" id=\"cGe_et+al_2015_a\" href=\"#rGe_et+al_2015_a\">Ge et al (2015</a>); <a class=\"ref-link\" id=\"cLee_et+al_2016_a\" href=\"#rLee_et+al_2016_a\">Lee et al (2016</a>)) have shown that it suffices for critical points to meet the following two conditions: (i) no poor local minima \u2014 every local minimum is close in its objective value to a global minimum; and strict saddle property \u2014 every critical point that is not a local minimum has at least one negative eigenvalue to its Hessian",
        "We show that when minimizing 2 loss of a deep linear network over a whitened dataset, gradient descent converges to the global minimum, at a linear rate, provided that the following conditions hold: (i) the dimensions of hidden layers are greater than or equal to the minimum between those of the input and output; layers are initialized to be approximately balanced \u2014 this is met under commonplace near-zero, as well as residual initializations; and the initial loss is smaller than any loss obtainable with rank deficiencies \u2014 this condition will hold with probability close to 0.5 if the output dimension is 1 and standard near-zero initialization is employed",
        "In Section 2 we present the problem of gradient descent training a deep linear neural network by minimizing the 2 loss over a whitened dataset",
        "We focus on studying the process of training a deep linear neural network by gradient descent, i.e. of tackling the optimization problem in Equation (3) by iteratively applying the following updates:",
        "We establish convergence of gradient descent for deep linear neural networks (Equations (4) and (3)) by directly analyzing the trajectories taken by the algorithm",
        "For a setting of random near-zero initialization, we present in Subsection 3.3 a scheme that, when the output dimension is 1, ensures assumptions are satisfied with constant probability",
        "We began by experimenting in the setting covered by our analysis \u2014 linear neural networks trained via gradient descent minimization of 2 loss over whitened data",
        "This work is similar to ours in the sense that it treats linear neural networks trained via minimization of 2 loss over whitened data, and proves linear convergence for gradient descent",
        "For deep linear neural networks, we have rigorously proven convergence of gradient descent to global minima, at a linear rate, provided that the initial weight matrices are approximately balanced and the initial end-to-end matrix has positive deficiency margin",
        "For networks with output dimension 1, we have shown that a balanced initialization, i.e. a random choice of the end-to-end matrix followed by a balanced partition across all layers, leads assumptions to be met, and convergence to take place, with constant probability",
        "Through simple experiments we have shown that the latter can lead to favorable convergence in deep learning practice, as it does in theory"
    ],
    "summary": [
        "Deep learning builds upon the mysterious ability of gradient-based optimization methods to solve related non-convex problems.",
        "We establish convergence of gradient descent for deep linear neural networks (Equations (4) and (3)) by directly analyzing the trajectories taken by the algorithm.",
        "We began by experimenting in the setting covered by our analysis \u2014 linear neural networks trained via gradient descent minimization of 2 loss over whitened data.",
        "Starting with the customary initialization of layer-wise independent random Gaussian perturbations centered at zero, we trained a three layer network (N = 3) with hidden widths (d1, d2) set to 32, and measured the time it takes to converge under different choices of standard deviation for the initialization.",
        "From the perspective of our analysis, a possible explanation for the aggravation is as follows: under layer-wise independent initialization, the magnitude of the end-to-end matrix W1:N depends on the standard deviation in a manner that is exponential in depth, for large depths the range of standard deviations that lead to moderately sized W1:N is limited, and within this range, there may not be many standard deviations small enough to ensure approximate balancedness.",
        "Notice that our theoretical analysis does not cover non-linear activation, softmax-cross-entropy loss and stochastic optimization, the conclusion of balanced initialization leading to improved convergence carries over to this setting.",
        "We turned to the MNIST tutorial built into TensorFlow (<a class=\"ref-link\" id=\"cAbadi_et+al_2016_a\" href=\"#rAbadi_et+al_2016_a\">Abadi et al, 2016</a>),9 which comprises a fully-connected neural network with two hidden layers and ReLU activation (<a class=\"ref-link\" id=\"cNair_2010_a\" href=\"#rNair_2010_a\">Nair and Hinton, 2010</a>), trained through stochastic gradient descent with batch size 100, initialized via customary layer-wise independent Gaussian perturbations centered at zero.",
        "Our theoretical analysis does not cover non-linear activation, softmax-cross-entropy loss or stochasticity in optimization, its conclusion of balanced initialization leading to improved convergence carried over to such setting.",
        "This work is similar to ours in the sense that it treats linear neural networks trained via minimization of 2 loss over whitened data, and proves linear convergence for gradient descent.",
        "For deep linear neural networks, we have rigorously proven convergence of gradient descent to global minima, at a linear rate, provided that the initial weight matrices are approximately balanced and the initial end-to-end matrix has positive deficiency margin.",
        "For networks with output dimension 1, we have shown that a balanced initialization, i.e. a random choice of the end-to-end matrix followed by a balanced partition across all layers, leads assumptions to be met, and convergence to take place, with constant probability.",
        "Further investigation of balanced initialization, including development of variants for convolutional layers, is regarded as a promising direction for future research"
    ],
    "headline": "Our results significantly extend previous analyses, e.g., of deep linear residual networks",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, pages 244\u2013253, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Cohen%2C%20Nadav%20Hazan%2C%20Elad%20On%20the%20optimization%20of%20deep%20networks%3A%20Implicit%20acceleration%20by%20overparameterization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Cohen%2C%20Nadav%20Hazan%2C%20Elad%20On%20the%20optimization%20of%20deep%20networks%3A%20Implicit%20acceleration%20by%20overparameterization%202018"
        },
        {
            "id": "Baldi_1989_a",
            "entry": "Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53\u201358, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "Bartlett_et+al_2018_a",
            "entry": "Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520\u2013529, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20Helmbold%2C%20Dave%20Long%2C%20Phil%20Gradient%20descent%20with%20identity%20initialization%20efficiently%20learns%20positive%20definite%20linear%20transformations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20Helmbold%2C%20Dave%20Long%2C%20Phil%20Gradient%20descent%20with%20identity%20initialization%20efficiently%20learns%20positive%20definite%20linear%20transformations%202018"
        },
        {
            "id": "Bhatia_1997_a",
            "entry": "Rajendra Bhatia. Matrix analysis. Springer-Verlag, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatia%2C%20Rajendra%20Matrix%20analysis%201997"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20optimization%202004"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In International Conference on Machine Learning, pages 605\u2013614, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017"
        },
        {
            "id": "Brutzkus_et+al_2018_a",
            "entry": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20Alon%20Globerson%2C%20Amir%20Malach%2C%20Eran%20Shalev-Shwartz%2C%20Shai%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018"
        },
        {
            "id": "Carbery_2001_a",
            "entry": "A Carbery and J Wright. Distributional and lq norm inequalities for polynomials over convex bodies in Rn. Mathematics Research Letters, 8(3):233\u2013248, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carbery%2C%20A.%20Wright%2C%20J.%20Distributional%20and%20lq%20norm%20inequalities%20for%20polynomials%20over%20convex%20bodies%20in%20Rn%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carbery%2C%20A.%20Wright%2C%20J.%20Distributional%20and%20lq%20norm%20inequalities%20for%20polynomials%20over%20convex%20bodies%20in%20Rn%202001"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Chudnov_1986_a",
            "entry": "Alexander Chudnov. On minimax signal generation and reception algorithms. Problems of Information Transmission, 22(4):49\u201354, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chudnov%2C%20Alexander%20On%20minimax%20signal%20generation%20and%20reception%20algorithms%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chudnov%2C%20Alexander%20On%20minimax%20signal%20generation%20and%20reception%20algorithms%201986"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. In International Conference on Machine Learning, pages 1329\u20131338, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018"
        },
        {
            "id": "Du_et+al_0000_a",
            "entry": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00900"
        },
        {
            "id": "Du_et+al_2018_b",
            "entry": "Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? International Conference on Learning Representations, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20When%20is%20a%20convolutional%20filter%20easy%20to%20learn%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20When%20is%20a%20convolutional%20filter%20easy%20to%20learn%3F%202018"
        },
        {
            "id": "Du_et+al_2018_c",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns onehidden-layer cnn: Dont be afraid of spurious local minima. In International Conference on Machine Learning, pages 1339\u20131348, 2018c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Singh%2C%20Aarti%20Gradient%20descent%20learns%20onehidden-layer%20cnn%3A%20Dont%20be%20afraid%20of%20spurious%20local%20minima%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Lee%2C%20Jason%20D.%20Tian%2C%20Yuandong%20Singh%2C%20Aarti%20Gradient%20descent%20learns%20onehidden-layer%20cnn%3A%20Dont%20be%20afraid%20of%20spurious%20local%20minima%202018"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20pointsonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20pointsonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2016_a",
            "entry": "Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973\u20132981, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Lee%2C%20Jason%20D.%20Ma%2C%20Tengyu%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Lee%2C%20Jason%20D.%20Ma%2C%20Tengyu%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "Haeffele_2017_a",
            "entry": "Benjamin Haeffele and Rene Vidal. Global optimality in neural network training. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4390\u20134398, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20Benjamin%20Vidal%2C%20Rene%20Global%20optimality%20in%20neural%20network%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20Benjamin%20Vidal%2C%20Rene%20Global%20optimality%20in%20neural%20network%20training%202017"
        },
        {
            "id": "Hardt_2016_a",
            "entry": "Moritz Hardt and Tengyu Ma. Identity matters in deep learning. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Horn_1990_a",
            "entry": "Roger Horn and Charles Johnson. Matrix analysis. Cambridge university press, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horn%2C%20Roger%20Johnson%2C%20Charles%20Matrix%20analysis%201990"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Laurent_2000_a",
            "entry": "Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302\u20131338, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Beatrice%20Massart%2C%20Pascal%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Beatrice%20Massart%2C%20Pascal%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000"
        },
        {
            "id": "Laurent_2018_a",
            "entry": "Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are global. In International Conference on Machine Learning, pages 2908\u20132913, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Thomas%20Brecht%2C%20James%20Deep%20linear%20networks%20with%20arbitrary%20loss%3A%20All%20local%20minima%20are%20global%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Thomas%20Brecht%2C%20James%20Deep%20linear%20networks%20with%20arbitrary%20loss%3A%20All%20local%20minima%20are%20global%202018"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pages 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Proceedings of the 31st Conference On Learning Theory, pages 2\u201347, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Ma%2C%20Tengyu%20Zhang%2C%20Hongyang%20Algorithmic%20regularization%20in%20over-parameterized%20matrix%20sensing%20and%20neural%20networks%20with%20quadratic%20activations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Ma%2C%20Tengyu%20Zhang%2C%20Hongyang%20Algorithmic%20regularization%20in%20over-parameterized%20matrix%20sensing%20and%20neural%20networks%20with%20quadratic%20activations%202018"
        },
        {
            "id": "Liao_et+al_2018_a",
            "entry": "Zhenyu Liao, Yacine Chitour, and Romain Couillet. Almost global convergence to global minima for gradient descent in deep linear networks. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liao%2C%20Zhenyu%20Chitour%2C%20Yacine%20Couillet%2C%20Romain%20Almost%20global%20convergence%20to%20global%20minima%20for%20gradient%20descent%20in%20deep%20linear%20networks%202018"
        },
        {
            "id": "Lovett_2010_a",
            "entry": "Shachar Lovett. An elementary proof of anti-concentration of polynomials in Gaussian variables. Electronic Colloquium on Computational Complexity, page 182, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lovett%2C%20Shachar%20An%20elementary%20proof%20of%20anti-concentration%20of%20polynomials%20in%20Gaussian%20variables%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lovett%2C%20Shachar%20An%20elementary%20proof%20of%20anti-concentration%20of%20polynomials%20in%20Gaussian%20variables%202010"
        },
        {
            "id": "Meka_et+al_2016_a",
            "entry": "Raghu Meka, Oanh Nguyen, and Van Vu. Anti-concentration for Polynomials of Independent Random Variables. Theory of Computing, 12:17, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meka%2C%20Raghu%20Nguyen%2C%20Oanh%20Vu%2C%20Van%20Anti-concentration%20for%20Polynomials%20of%20Independent%20Random%20Variables%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meka%2C%20Raghu%20Nguyen%2C%20Oanh%20Vu%2C%20Van%20Anti-concentration%20for%20Polynomials%20of%20Independent%20Random%20Variables%202016"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pages 2603\u20132612, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10928"
        },
        {
            "id": "Panageas_2017_a",
            "entry": "Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions. In Innovations in Theoretical Computer Science, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panageas%2C%20Ioannis%20Piliouras%2C%20Georgios%20Gradient%20descent%20only%20converges%20to%20minimizers%3A%20Non-isolated%20critical%20points%20and%20invariant%20regions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panageas%2C%20Ioannis%20Piliouras%2C%20Georgios%20Gradient%20descent%20only%20converges%20to%20minimizers%3A%20Non-isolated%20critical%20points%20and%20invariant%20regions%202017"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Rodriguez-Lujan_et+al_2014_a",
            "entry": "Irene Rodriguez-Lujan, Jordi Fonollosa, Alexander Vergara, Margie Homer, and Ramon Huerta. On the calibration of sensor arrays for pattern recognition using the minimal number of experiments. Chemometrics and Intelligent Laboratory Systems, 130:123\u2013134, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodriguez-Lujan%2C%20Irene%20Fonollosa%2C%20Jordi%20Vergara%2C%20Alexander%20Homer%2C%20Margie%20On%20the%20calibration%20of%20sensor%20arrays%20for%20pattern%20recognition%20using%20the%20minimal%20number%20of%20experiments%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodriguez-Lujan%2C%20Irene%20Fonollosa%2C%20Jordi%20Vergara%2C%20Alexander%20Homer%2C%20Margie%20On%20the%20calibration%20of%20sensor%20arrays%20for%20pattern%20recognition%20using%20the%20minimal%20number%20of%20experiments%202014"
        },
        {
            "id": "Safran_2018_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018"
        },
        {
            "id": "Saxe_et+al_2014_a",
            "entry": "Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20M.%20McClelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20M.%20McClelland%2C%20James%20L.%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014"
        },
        {
            "id": "Shamir_2018_a",
            "entry": "Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. arXiv preprint arXiv:1809.08587, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.08587"
        },
        {
            "id": "Soudry_2016_a",
            "entry": "Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "Sutskever_et+al_2013_a",
            "entry": "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pages 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "Vergara_et+al_2012_a",
            "entry": "Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ramon Huerta. Chemical gas sensor drift compensation using classifier ensembles. Sensors and Actuators B: Chemical, 166:320\u2013329, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vergara%2C%20Alexander%20Vembu%2C%20Shankar%20Ayhan%2C%20Tuba%20Ryan%2C%20Margaret%20A.%20Chemical%20gas%20sensor%20drift%20compensation%20using%20classifier%20ensembles%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vergara%2C%20Alexander%20Vembu%2C%20Shankar%20Ayhan%2C%20Tuba%20Ryan%2C%20Margaret%20A.%20Chemical%20gas%20sensor%20drift%20compensation%20using%20classifier%20ensembles%202012"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for onehidden-layer neural networks. In International Conference on Machine Learning, pages 4140\u20134149, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20Kai%20Song%2C%20Zhao%20Jain%2C%20Prateek%20Bartlett%2C%20Peter%20L.%20Recovery%20guarantees%20for%20onehidden-layer%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20Kai%20Song%2C%20Zhao%20Jain%2C%20Prateek%20Bartlett%2C%20Peter%20L.%20Recovery%20guarantees%20for%20onehidden-layer%20neural%20networks%202017"
        },
        {
            "id": "In_2013_a",
            "entry": "In this appendix we show that the assumptions on initialization facilitating our main convergence result (Theorem 1) \u2014 approximate balancedness and deficiency margin \u2014 are both necessary, by demonstrating cases where violating each of them leads to convergence failure. This accords with widely observed empirical phenomena, by which successful optimization in deep learning crucially depends on careful initialization (cf. Sutskever et al. (2013)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20appendix%20we%20show%20that%20the%20assumptions%20on%20initialization%20facilitating%20our%20main%20convergence%20result%20Theorem%201%20%20approximate%20balancedness%20and%20deficiency%20margin%20%20are%20both%20necessary%20by%20demonstrating%20cases%20where%20violating%20each%20of%20them%20leads%20to%20convergence%20failure%20This%20accords%20with%20widely%20observed%20empirical%20phenomena%20by%20which%20successful%20optimization%20in%20deep%20learning%20crucially%20depends%20on%20careful%20initialization%20cf%20Sutskever%20et%20al%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20this%20appendix%20we%20show%20that%20the%20assumptions%20on%20initialization%20facilitating%20our%20main%20convergence%20result%20Theorem%201%20%20approximate%20balancedness%20and%20deficiency%20margin%20%20are%20both%20necessary%20by%20demonstrating%20cases%20where%20violating%20each%20of%20them%20leads%20to%20convergence%20failure%20This%20accords%20with%20widely%20observed%20empirical%20phenomena%20by%20which%20successful%20optimization%20in%20deep%20learning%20crucially%20depends%20on%20careful%20initialization%20cf%20Sutskever%20et%20al%202013"
        },
        {
            "id": "In_2018_a",
            "entry": "In terms of deficiency margin, we provide (by adapting Theorem 4 in Bartlett et al. (2018)) a different, somewhat stronger result \u2014 there exist settings where initialization violates the assumption of deficiency margin, and despite being perfectly balanced, leads to convergence failure, for any choice of learning rate:17",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20terms%20of%20deficiency%20margin%20we%20provide%20by%20adapting%20Theorem%204%20in%20Bartlett%20et%20al%202018%20a%20different%20somewhat%20stronger%20result%20%20there%20exist%20settings%20where%20initialization%20violates%20the%20assumption%20of%20deficiency%20margin%20and%20despite%20being%20perfectly%20balanced%20leads%20to%20convergence%20failure%20for%20any%20choice%20of%20learning%20rate17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20terms%20of%20deficiency%20margin%20we%20provide%20by%20adapting%20Theorem%204%20in%20Bartlett%20et%20al%202018%20a%20different%20somewhat%20stronger%20result%20%20there%20exist%20settings%20where%20initialization%20violates%20the%20assumption%20of%20deficiency%20margin%20and%20despite%20being%20perfectly%20balanced%20leads%20to%20convergence%20failure%20for%20any%20choice%20of%20learning%20rate17"
        },
        {
            "id": "Proof._1990_a",
            "entry": "Proof. Recall that for any matrices A and B of compatible sizes \u03c3min(A + B) \u2265 \u03c3min(A) \u2212 \u03c3max(B), and that the Frobenius norm of a matrix is always lower bounded by its largest singular value (Horn and Johnson (1990)). Using these facts, we have: \u03c3min(W ) = \u03c3min \u03a6 + (W \u2212 \u03a6) \u2265 \u03c3min(\u03a6) \u2212 \u03c3max(W \u2212 \u03a6) \u2265 \u03c3min(\u03a6) \u2212 W \u2212 \u03a6 F \u2265 \u03c3min(\u03a6) \u2212 W \u2212 \u03a6 F \u2265 \u03c3min(\u03a6) \u2212 (\u03c3min(\u03a6) \u2212 c) = c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20Recall%20that%20for%20any%20matrices%20A%20and%20B%20of%20compatible%20sizes%20%CF%83minA%20%20B%20%20%CF%83minA%20%20%CF%83maxB%20and%20that%20the%20Frobenius%20norm%20of%20a%20matrix%20is%20always%20lower%20bounded%20by%20its%20largest%20singular%20value%20Horn%20and%20Johnson%201990%20Using%20these%20facts%20we%20have%20%CF%83minW%20%20%20%CF%83min%20%CE%A6%20%20W%20%20%CE%A6%20%20%CF%83min%CE%A6%20%20%CF%83maxW%20%20%CE%A6%20%20%CF%83min%CE%A6%20%20W%20%20%CE%A6%20F%20%20%CF%83min%CE%A6%20%20W%20%20%CE%A6%20F%20%20%CF%83min%CE%A6%20%20%CF%83min%CE%A6%20%20c%20%20c",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20Recall%20that%20for%20any%20matrices%20A%20and%20B%20of%20compatible%20sizes%20%CF%83minA%20%20B%20%20%CF%83minA%20%20%CF%83maxB%20and%20that%20the%20Frobenius%20norm%20of%20a%20matrix%20is%20always%20lower%20bounded%20by%20its%20largest%20singular%20value%20Horn%20and%20Johnson%201990%20Using%20these%20facts%20we%20have%20%CF%83minW%20%20%20%CF%83min%20%CE%A6%20%20W%20%20%CE%A6%20%20%CF%83min%CE%A6%20%20%CF%83maxW%20%20%CE%A6%20%20%CF%83min%CE%A6%20%20W%20%20%CE%A6%20F%20%20%CF%83min%CE%A6%20%20W%20%20%CE%A6%20F%20%20%CF%83min%CE%A6%20%20%CF%83min%CE%A6%20%20c%20%20c"
        },
        {
            "id": "Lemma_1997_a",
            "entry": "Lemma 3 (Bhatia (1997), Exercise IV.3.5). For any two matrices A, B of the same size, Sing(A)\u2212 Sing(B) \u03c3 \u2264 A \u2212 B \u03c3 and Sing(A) \u2212 Sing(B) F \u2264 A \u2212 B F.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%28Bhatia%2C%20Lemma%203%20Exercise%20IV.3.5%29.%20For%20any%20two%20matrices%20A%2C%20B%20of%20the%20same%20size%2C%20Sing%28A%29%E2%88%92%20Sing%28B%29%20%CF%83%20%E2%89%A4%20A%20%E2%88%92%20B%20%CF%83%20and%20Sing%28A%29%20%E2%88%92%20Sing%28B%29%20F%20%E2%89%A4%20A%20%E2%88%92%20B%20F%201997"
        }
    ]
}
