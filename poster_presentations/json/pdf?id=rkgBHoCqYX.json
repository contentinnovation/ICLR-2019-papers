{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A KERNEL RANDOM MATRIX-BASED APPROACH FOR SPARSE PCA",
        "author": "Mohamed El Amine Seddik, Mohamed Tamaazousti, & Romain Couillet,\u2217 1CEA List, 2CentraleSup\u00e9lec, 3GIPSA-Lab University of GrenobleAlpes {mohamedelamine.seddik,mohamed.tamaazousti}@cea.fr romain.couillet@gipsa-lab.grenoble-inp.fr",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkgBHoCqYX"
        },
        "journal": "P\u00e9ch\u00e9",
        "abstract": "In this paper, we present a random matrix approach to recover sparse principal components from n p-dimensional vectors. Specifically, considering the large dimensional setting where n, p \u2192 \u221e with p/n \u2192 c \u2208 (0, \u221e) and under Gaussian vector observations, we study kernel random matrices of the type f (C ), where f is a three-times continuously differentiable function applied entry-wise to the sample covariance matrix Cof the data. Then, assuming that the principal components are sparse, we show that taking f in such a way that f (0) = f (0) = 0 allows for powerful recovery of the principal components, thereby generalizing previous ideas involving more specific f functions such as the soft-thresholding function."
    },
    "keywords": [
        {
            "term": "semi-definite programming",
            "url": "https://en.wikipedia.org/wiki/semi-definite_programming"
        },
        {
            "term": "inner product",
            "url": "https://en.wikipedia.org/wiki/inner_product"
        },
        {
            "term": "random matrix",
            "url": "https://en.wikipedia.org/wiki/random_matrix"
        },
        {
            "term": "Principal component analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        },
        {
            "term": "sample covariance matrix",
            "url": "https://en.wikipedia.org/wiki/sample_covariance_matrix"
        }
    ],
    "abbreviations": {
        "PCA": "Principal component analysis",
        "CT": "covariance thresholding",
        "SDP": "semi-definite programming"
    },
    "highlights": [
        "Principal component analysis (PCA) is extensively used in data analysis and machine learning applications",
        "It is a dimension reduction technique that aims to project a given dataset onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix (<a class=\"ref-link\" id=\"cWold_et+al_1987_a\" href=\"#rWold_et+al_1987_a\">Wold et al, 1987</a>), which represent the principal modes of variance",
        "The statistical interpretation of Principal component analysis lies in the fact that most of the variance in the data is captured by these modes",
        "The standard Principal component analysis method requires the computation of the sample covariance matrix C = Y Y /n and estimates the first principal components u1, u2, . . . by the ordered eigenvectors u1, u2, . . . of C . (<a class=\"ref-link\" id=\"cJohnstone_2009_a\" href=\"#rJohnstone_2009_a\">Johnstone & Lu, 2009</a>) demonstrated that, in the high dimensional regime where n, p \u2192 \u221e with p/n \u2192 c > 0, the principal component u1 estimated by standard Principal component analysis is inconsistent",
        "While restricting ourselves to a setting where p and n grow at a controlled joint rate, we provide an elementary argument, based on a matrix-wise Taylor expansion controlled through a concentration of measure approach, that generalizes the covariance thresholding method to a large family of kernel-based\n1We use the kernel-based terminology to highlight that our work falls within the framework of kernel random matrices and should not be confused with the standard kernel Principal component analysis",
        "Our methodology can be generalized to other sparse covariance matrix-based contexts, in the same vein as the works in (<a class=\"ref-link\" id=\"cBickel_2008_a\" href=\"#rBickel_2008_a\">Bickel & Levina, 2008</a>; El Karoui, 2008)"
    ],
    "key_statements": [
        "Principal component analysis (PCA) is extensively used in data analysis and machine learning applications",
        "It is a dimension reduction technique that aims to project a given dataset onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix (<a class=\"ref-link\" id=\"cWold_et+al_1987_a\" href=\"#rWold_et+al_1987_a\">Wold et al, 1987</a>), which represent the principal modes of variance",
        "The statistical interpretation of Principal component analysis lies in the fact that most of the variance in the data is captured by these modes",
        "The standard Principal component analysis method requires the computation of the sample covariance matrix C = Y Y /n and estimates the first principal components u1, u2, . . . by the ordered eigenvectors u1, u2, . . . of C . (<a class=\"ref-link\" id=\"cJohnstone_2009_a\" href=\"#rJohnstone_2009_a\">Johnstone & Lu, 2009</a>) demonstrated that, in the high dimensional regime where n, p \u2192 \u221e with p/n \u2192 c > 0, the principal component u1 estimated by standard Principal component analysis is inconsistent",
        "We show that the soft-thresholding method falls within a broader class of kernel-based1 Principal component analysis algorithms that are suited to sparse Principal component analysis recovery",
        "In Section 2, we present the related work on sparse Principal component analysis",
        "Based on the well-known power method, (<a class=\"ref-link\" id=\"cYuan_2013_a\" href=\"#rYuan_2013_a\">Yuan & Zhang, 2013</a>) introduced an efficient sparse Principal component analysis approximation to obtain the exact level of required sparsity, by truncating to zero the principal components iteratively except for their largest entries",
        "While restricting ourselves to a setting where p and n grow at a controlled joint rate, we provide an elementary argument, based on a matrix-wise Taylor expansion controlled through a concentration of measure approach, that generalizes the covariance thresholding method to a large family of kernel-based\n1We use the kernel-based terminology to highlight that our work falls within the framework of kernel random matrices and should not be confused with the standard kernel Principal component analysis",
        "We provide some experiments in the context of sparse Principal component analysis, where we consider the spiked model presented in Section 4.1",
        "Our methodology can be generalized to other sparse covariance matrix-based contexts, in the same vein as the works in (<a class=\"ref-link\" id=\"cBickel_2008_a\" href=\"#rBickel_2008_a\">Bickel & Levina, 2008</a>; El Karoui, 2008)"
    ],
    "summary": [
        "Principal component analysis (PCA) is extensively used in data analysis and machine learning applications.",
        "The standard PCA method requires the computation of the sample covariance matrix C = Y Y /n and estimates the first principal components u1, u2, .",
        "Based on the well-known power method, (<a class=\"ref-link\" id=\"cYuan_2013_a\" href=\"#rYuan_2013_a\">Yuan & Zhang, 2013</a>) introduced an efficient sparse PCA approximation to obtain the exact level of required sparsity, by truncating to zero the principal components iteratively except for their largest entries.",
        "We have the concentration of Gaussian random vectors in the sense of Definition 2 in the following proposition (<a class=\"ref-link\" id=\"cTao_2012_a\" href=\"#rTao_2012_a\">Tao, 2012</a>, Theorem 2.1.12).",
        "According to Definition 2, given a Lipschitz application F : Rp \u2192 Rq for q \u2208 N\u2217, Proposition 3 provides the normal concentration of all the random vectors F (Z).",
        "When considering a large-dimensional random matrix setting, the notion of sparsity for such matrices is attached to the choice of the matrix norm.2 (El Karoui, 2008) introduced a definition (\u03b5-sparsity) for sparsity of matrices that is compatible with spectral analysis, and adapted to the operator norm.",
        "As Definition 3 is based on a graph defined by its corresponding adjacency matrix, we have the following property: given an \u03b5-sparse matrix M and a function f such that f (0) = 0 and f (x) =x=0 0, the matrix f (M ), resulting from the application of f entry-wise to M , remains \u03b5-sparse; this is a consequence of A(M ) = A(f (M )).",
        "Theorem 2 gives a general result concerning the estimation of \u03b5-sparse covariance matrices.",
        "As our method consists in computing the sparse eigenvectors of a p \u00d7 p matrix which can be done by power method, the complexity of estimating the principal component is about O where s is the sparsity level.",
        "As suggested theoretically and verified experimentally, our proposed method strongly attenuates the \u201cnoise component\u201d of the sample covariance matrix and consistently estimates the principal components.",
        "Our approach gives equivalent results to the CT method while generalizing it to the class of smooth functions f such that f (0) = f (0) = 0, in the considered regime.",
        "We tackled the problem of sparse PCA through a random matrix perspective thereby generalizing recent ideas to a broader kernel-based method.",
        "Given a spiked covariance model Cand a smooth function f , we gave in this paper sufficient conditions on f to consistently estimate the principal components through the matrix f (C ).",
        "Our methodology can be generalized to other sparse covariance matrix-based contexts, in the same vein as the works in (<a class=\"ref-link\" id=\"cBickel_2008_a\" href=\"#rBickel_2008_a\"><a class=\"ref-link\" id=\"cBickel_2008_a\" href=\"#rBickel_2008_a\">Bickel & Levina, 2008</a></a>; El Karoui, 2008)"
    ],
    "headline": "We present a random matrix approach to recover sparse principal components from n p-dimensional vectors",
    "reference_links": [
        {
            "id": "Anderson_1963_a",
            "entry": "Theodore Wilbur Anderson. Asymptotic theory for principal component analysis. The Annals of Mathematical Statistics, 34(1):122\u2013148, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Theodore%20Wilbur%20Asymptotic%20theory%20for%20principal%20component%20analysis%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Theodore%20Wilbur%20Asymptotic%20theory%20for%20principal%20component%20analysis%201963"
        },
        {
            "id": "Asteris_et+al_2014_a",
            "entry": "Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse pca with provable guarantees. In International Conference on Machine Learning, pp. 1728\u20131736, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asteris%2C%20Megasthenis%20Papailiopoulos%2C%20Dimitris%20Dimakis%2C%20Alexandros%20Nonnegative%20sparse%20pca%20with%20provable%20guarantees%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asteris%2C%20Megasthenis%20Papailiopoulos%2C%20Dimitris%20Dimakis%2C%20Alexandros%20Nonnegative%20sparse%20pca%20with%20provable%20guarantees%202014"
        },
        {
            "id": "Bai_1998_a",
            "entry": "Zhi-Dong Bai and Jack W Silverstein. No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices. Annals of probability, pp. 316\u2013345, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bai%2C%20Zhi-Dong%20Silverstein%2C%20Jack%20W.%20No%20eigenvalues%20outside%20the%20support%20of%20the%20limiting%20spectral%20distribution%20of%20large-dimensional%20sample%20covariance%20matrices%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bai%2C%20Zhi-Dong%20Silverstein%2C%20Jack%20W.%20No%20eigenvalues%20outside%20the%20support%20of%20the%20limiting%20spectral%20distribution%20of%20large-dimensional%20sample%20covariance%20matrices%201998"
        },
        {
            "id": "Baik_et+al_2005_a",
            "entry": "Jinho Baik, G\u00e9rard Ben Arous, Sandrine P\u00e9ch\u00e9, et al. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643\u20131697, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baik%2C%20Jinho%20Arous%2C%20G%C3%A9rard%20Ben%20P%C3%A9ch%C3%A9%2C%20Sandrine%20Phase%20transition%20of%20the%20largest%20eigenvalue%20for%20nonnull%20complex%20sample%20covariance%20matrices%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baik%2C%20Jinho%20Arous%2C%20G%C3%A9rard%20Ben%20P%C3%A9ch%C3%A9%2C%20Sandrine%20Phase%20transition%20of%20the%20largest%20eigenvalue%20for%20nonnull%20complex%20sample%20covariance%20matrices%202005"
        },
        {
            "id": "Benaych-Georges_2011_a",
            "entry": "Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494\u2013521, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benaych-Georges%2C%20Florent%20Nadakuditi%2C%20Raj%20Rao%20The%20eigenvalues%20and%20eigenvectors%20of%20finite%2C%20low%20rank%20perturbations%20of%20large%20random%20matrices%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benaych-Georges%2C%20Florent%20Nadakuditi%2C%20Raj%20Rao%20The%20eigenvalues%20and%20eigenvectors%20of%20finite%2C%20low%20rank%20perturbations%20of%20large%20random%20matrices%202011"
        },
        {
            "id": "Bickel_2008_a",
            "entry": "Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics, pp. 2577\u20132604, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bickel%2C%20Peter%20J.%20Levina%2C%20Elizaveta%20Covariance%20regularization%20by%20thresholding%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bickel%2C%20Peter%20J.%20Levina%2C%20Elizaveta%20Covariance%20regularization%20by%20thresholding%202008"
        },
        {
            "id": "Cadima_1995_a",
            "entry": "Jorge Cadima and Ian T Jolliffe. Loading and correlations in the interpretation of principle compenents. Journal of Applied Statistics, 22(2):203\u2013214, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cadima%2C%20Jorge%20Jolliffe%2C%20Ian%20T.%20Loading%20and%20correlations%20in%20the%20interpretation%20of%20principle%20compenents%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cadima%2C%20Jorge%20Jolliffe%2C%20Ian%20T.%20Loading%20and%20correlations%20in%20the%20interpretation%20of%20principle%20compenents%201995"
        },
        {
            "id": "Capitaine_et+al_2009_a",
            "entry": "Mireille Capitaine, Catherine Donati-Martin, and Delphine F\u00e9ral. The largest eigenvalues of finite rank deformation of large wigner matrices: convergence and nonuniversality of the fluctuations. The Annals of Probability, pp. 1\u201347, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Capitaine%2C%20Mireille%20Donati-Martin%2C%20Catherine%20F%C3%A9ral%2C%20Delphine%20The%20largest%20eigenvalues%20of%20finite%20rank%20deformation%20of%20large%20wigner%20matrices%3A%20convergence%20and%20nonuniversality%20of%20the%20fluctuations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Capitaine%2C%20Mireille%20Donati-Martin%2C%20Catherine%20F%C3%A9ral%2C%20Delphine%20The%20largest%20eigenvalues%20of%20finite%20rank%20deformation%20of%20large%20wigner%20matrices%3A%20convergence%20and%20nonuniversality%20of%20the%20fluctuations%202009"
        },
        {
            "id": "Cheng_2013_a",
            "entry": "Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random Matrices: Theory and Applications, 2(04):1350010, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Xiuyuan%20Singer%2C%20Amit%20The%20spectrum%20of%20random%20inner-product%20kernel%20matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Xiuyuan%20Singer%2C%20Amit%20The%20spectrum%20of%20random%20inner-product%20kernel%20matrices%202013"
        },
        {
            "id": "D_et+al_2005_a",
            "entry": "Alexandre d\u2019Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet. A direct formulation for sparse pca using semidefinite programming. In Advances in neural information processing systems, pp. 41\u201348, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20E.%20Jordan%2C%20Michael%20I.%20Lanckriet%2C%20Gert%20R.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20E.%20Jordan%2C%20Michael%20I.%20Lanckriet%2C%20Gert%20R.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202005"
        },
        {
            "id": "Davis_1970_a",
            "entry": "Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davis%2C%20Chandler%20Kahan%2C%20William%20Morton%20The%20rotation%20of%20eigenvectors%20by%20a%20perturbation.%20iii%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davis%2C%20Chandler%20Kahan%2C%20William%20Morton%20The%20rotation%20of%20eigenvectors%20by%20a%20perturbation.%20iii%201970"
        },
        {
            "id": "Deshpande_2014_a",
            "entry": "Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. In Advances in Neural Information Processing Systems, pp. 334\u2013342, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202014"
        },
        {
            "id": "Deshpande_2016_a",
            "entry": "Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. J. Mach. Learn. Res., 17(1):4913\u20134953, January 2016. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202016-01"
        },
        {
            "id": "Eisenstat_1998_a",
            "entry": "Stanley C Eisenstat and Ilse CF Ipsen. Three absolute perturbation bounds for matrix eigenvalues imply relative bounds. SIAM Journal on Matrix Analysis and Applications, 20(1):149\u2013158, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eisenstat%2C%20Stanley%20C.%20Ipsen%2C%20Ilse%20C.F.%20Three%20absolute%20perturbation%20bounds%20for%20matrix%20eigenvalues%20imply%20relative%20bounds%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eisenstat%2C%20Stanley%20C.%20Ipsen%2C%20Ilse%20C.F.%20Three%20absolute%20perturbation%20bounds%20for%20matrix%20eigenvalues%20imply%20relative%20bounds%201998"
        },
        {
            "id": "Karoui_2008_a",
            "entry": "Noureddine El Karoui. Operator norm consistent estimation of large-dimensional sparse covariance matrices. The Annals of Statistics, pp. 2717\u20132756, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karoui%2C%20Noureddine%20El%20Operator%20norm%20consistent%20estimation%20of%20large-dimensional%20sparse%20covariance%20matrices%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karoui%2C%20Noureddine%20El%20Operator%20norm%20consistent%20estimation%20of%20large-dimensional%20sparse%20covariance%20matrices%202008"
        },
        {
            "id": "Karoui_2010_a",
            "entry": "Noureddine El Karoui. On information plus noise kernel random matrices. The Annals of Statistics, 38(5):3191\u20133216, 2010a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karoui%2C%20Noureddine%20El%20On%20information%20plus%20noise%20kernel%20random%20matrices%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karoui%2C%20Noureddine%20El%20On%20information%20plus%20noise%20kernel%20random%20matrices%202010"
        },
        {
            "id": "Karoui_0000_a",
            "entry": "Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1): 1\u201350, 2010b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karoui%2C%20Noureddine%20El%20The%20spectrum%20of%20kernel%20random%20matrices",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karoui%2C%20Noureddine%20El%20The%20spectrum%20of%20kernel%20random%20matrices"
        },
        {
            "id": "F_2007_a",
            "entry": "Delphine F\u00e9ral and Sandrine P\u00e9ch\u00e9. The largest eigenvalue of rank one deformation of large wigner matrices. Communications in mathematical physics, 272(1):185\u2013228, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=F%C3%A9ral%2C%20Delphine%20P%C3%A9ch%C3%A9%2C%20Sandrine%20The%20largest%20eigenvalue%20of%20rank%20one%20deformation%20of%20large%20wigner%20matrices%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=F%C3%A9ral%2C%20Delphine%20P%C3%A9ch%C3%A9%2C%20Sandrine%20The%20largest%20eigenvalue%20of%20rank%20one%20deformation%20of%20large%20wigner%20matrices%202007"
        },
        {
            "id": "Johnstone_2009_a",
            "entry": "Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486):682\u2013693, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnstone%2C%20Iain%20M.%20Lu%2C%20Arthur%20Yu%20On%20consistency%20and%20sparsity%20for%20principal%20components%20analysis%20in%20high%20dimensions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnstone%2C%20Iain%20M.%20Lu%2C%20Arthur%20Yu%20On%20consistency%20and%20sparsity%20for%20principal%20components%20analysis%20in%20high%20dimensions%202009"
        },
        {
            "id": "Kammoun_2017_a",
            "entry": "Abla J Kammoun and Romain Couillet. Subspace kernel clustering of large dimensional data. (submitted to) Annals of Applied Probability, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kammoun%2C%20Abla%20J.%20Couillet%2C%20Romain%20Subspace%20kernel%20clustering%20of%20large%20dimensional%20data.%20%28submitted%20to%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kammoun%2C%20Abla%20J.%20Couillet%2C%20Romain%20Subspace%20kernel%20clustering%20of%20large%20dimensional%20data.%20%28submitted%20to%29%202017"
        },
        {
            "id": "Knowles_2013_a",
            "entry": "Antti Knowles and Jun Yin. The isotropic semicircle law and deformation of wigner matrices. Communications on Pure and Applied Mathematics, 66(11):1663\u20131749, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knowles%2C%20Antti%20Yin%2C%20Jun%20The%20isotropic%20semicircle%20law%20and%20deformation%20of%20wigner%20matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knowles%2C%20Antti%20Yin%2C%20Jun%20The%20isotropic%20semicircle%20law%20and%20deformation%20of%20wigner%20matrices%202013"
        },
        {
            "id": "Krauthgamer_et+al_2015_a",
            "entry": "Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse pca up to the information limit? Ann. Statist., 43(3):1300\u20131322, 06 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krauthgamer%2C%20Robert%20Nadler%2C%20Boaz%20Vilenchik%2C%20Dan%20Do%20semidefinite%20relaxations%20solve%20sparse%20pca%20up%20to%20the%20information%20limit%3F%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krauthgamer%2C%20Robert%20Nadler%2C%20Boaz%20Vilenchik%2C%20Dan%20Do%20semidefinite%20relaxations%20solve%20sparse%20pca%20up%20to%20the%20information%20limit%3F%202015"
        },
        {
            "id": "Ledoux_2005_a",
            "entry": "Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc., 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20The%20concentration%20of%20measure%20phenomenon.%20Number%2089%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ledoux%2C%20Michel%20The%20concentration%20of%20measure%20phenomenon.%20Number%2089%202005"
        },
        {
            "id": "Ma_2013_a",
            "entry": "Zongming Ma et al. Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41(2):772\u2013801, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Zongming%20Sparse%20principal%20component%20analysis%20and%20iterative%20thresholding%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Zongming%20Sparse%20principal%20component%20analysis%20and%20iterative%20thresholding%202013"
        },
        {
            "id": "Vladimir_1967_a",
            "entry": "Vladimir A Marcenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):457, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vladimir%20A%20Marcenko%20and%20Leonid%20Andreevich%20Pastur%20Distribution%20of%20eigenvalues%20for%20some%20sets%20of%20random%20matrices%20Mathematics%20of%20the%20USSRSbornik%2014457%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vladimir%20A%20Marcenko%20and%20Leonid%20Andreevich%20Pastur%20Distribution%20of%20eigenvalues%20for%20some%20sets%20of%20random%20matrices%20Mathematics%20of%20the%20USSRSbornik%2014457%201967"
        },
        {
            "id": "Moghaddam_et+al_2006_a",
            "entry": "Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral bounds for sparse pca: Exact and greedy algorithms. In Advances in neural information processing systems, pp. 915\u2013922, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moghaddam%2C%20Baback%20Weiss%2C%20Yair%20Avidan%2C%20Shai%20Spectral%20bounds%20for%20sparse%20pca%3A%20Exact%20and%20greedy%20algorithms%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moghaddam%2C%20Baback%20Weiss%2C%20Yair%20Avidan%2C%20Shai%20Spectral%20bounds%20for%20sparse%20pca%3A%20Exact%20and%20greedy%20algorithms%202006"
        },
        {
            "id": "Papailiopoulos_et+al_2013_a",
            "entry": "Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse pca through lowrank approximations. In International Conference on Machine Learning, pp. 747\u2013755, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papailiopoulos%2C%20Dimitris%20Dimakis%2C%20Alexandros%20Korokythakis%2C%20Stavros%20Sparse%20pca%20through%20lowrank%20approximations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papailiopoulos%2C%20Dimitris%20Dimakis%2C%20Alexandros%20Korokythakis%2C%20Stavros%20Sparse%20pca%20through%20lowrank%20approximations%202013"
        },
        {
            "id": "Paul_2007_a",
            "entry": "Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance model. Statistica Sinica, 17:1617\u20131642, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paul%2C%20Debashis%20Asymptotics%20of%20sample%20eigenstructure%20for%20a%20large%20dimensional%20spiked%20covariance%20model%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paul%2C%20Debashis%20Asymptotics%20of%20sample%20eigenstructure%20for%20a%20large%20dimensional%20spiked%20covariance%20model%202007"
        },
        {
            "id": "Shen_2008_a",
            "entry": "Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of multivariate analysis, 99(6):1015\u20131034, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Haipeng%20Huang%2C%20Jianhua%20Z.%20Sparse%20principal%20component%20analysis%20via%20regularized%20low%20rank%20matrix%20approximation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Haipeng%20Huang%2C%20Jianhua%20Z.%20Sparse%20principal%20component%20analysis%20via%20regularized%20low%20rank%20matrix%20approximation%202008"
        },
        {
            "id": "Tao_2012_a",
            "entry": "Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Society Providence, RI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20Terence%20Topics%20in%20random%20matrix%20theory%2C%20volume%20132%202012"
        },
        {
            "id": "Ali_et+al_2018_a",
            "entry": "Hafis Tiomoko Ali, Abla Kammoun, and Romain Couillet. Random matrix asymptotics of inner product spectral clustering. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ali%2C%20Hafis%20Tiomoko%20Kammoun%2C%20Abla%20Couillet%2C%20Romain%20Random%20matrix%20asymptotics%20of%20inner%20product%20spectral%20clustering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ali%2C%20Hafis%20Tiomoko%20Kammoun%2C%20Abla%20Couillet%2C%20Romain%20Random%20matrix%20asymptotics%20of%20inner%20product%20spectral%20clustering%202018"
        },
        {
            "id": "Wold_et+al_1987_a",
            "entry": "Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37\u201352, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wold%2C%20Svante%20Esbensen%2C%20Kim%20Geladi%2C%20Paul%20Principal%20component%20analysis%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wold%2C%20Svante%20Esbensen%2C%20Kim%20Geladi%2C%20Paul%20Principal%20component%20analysis%201987"
        },
        {
            "id": "Wright_et+al_2009_a",
            "entry": "John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in neural information processing systems, pp. 2080\u20132088, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wright%2C%20John%20Ganesh%2C%20Arvind%20Rao%2C%20Shankar%20Peng%2C%20Yigang%20Robust%20principal%20component%20analysis%3A%20Exact%20recovery%20of%20corrupted%20low-rank%20matrices%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wright%2C%20John%20Ganesh%2C%20Arvind%20Rao%2C%20Shankar%20Peng%2C%20Yigang%20Robust%20principal%20component%20analysis%3A%20Exact%20recovery%20of%20corrupted%20low-rank%20matrices%20via%20convex%20optimization%202009"
        },
        {
            "id": "Yuan_2013_a",
            "entry": "Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. Journal of Machine Learning Research, 14(Apr):899\u2013925, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Xiao-Tong%20Zhang%2C%20Tong%20Truncated%20power%20method%20for%20sparse%20eigenvalue%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Xiao-Tong%20Zhang%2C%20Tong%20Truncated%20power%20method%20for%20sparse%20eigenvalue%20problems%202013"
        },
        {
            "id": "Zass_2007_a",
            "entry": "Ron Zass and Amnon Shashua. Nonnegative sparse pca. In Advances in Neural Information Processing Systems, pp. 1561\u20131568, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zass%2C%20Ron%20Shashua%2C%20Amnon%20Nonnegative%20sparse%20pca%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zass%2C%20Ron%20Shashua%2C%20Amnon%20Nonnegative%20sparse%20pca%202007"
        },
        {
            "id": "Zou_et+al_2006_a",
            "entry": "Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265\u2013286, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Sparse%20principal%20component%20analysis%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Sparse%20principal%20component%20analysis%202006"
        },
        {
            "id": "In_2009_a",
            "entry": "In this appendix we provide the proofs of the different results presented in the paper and some additional experiments that validate our findings. It includes the following items: (i) the proof of Theorem 1 (Section A.2); (ii) The proof of Theorem 2 concerning the analysis of the sparse case (Section A.3); and finally (iii) further experiments which confirm the consistency of our method and the necessity of the conditions f (0) = f (0) = 0 on the kernel function f, using the signals of Johnstone et al. (Johnstone & Lu, 2009) (Section A.4). For convenience, we make the present appendix self-contained by recalling the preliminaries and the results presented in the main paper.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20appendix%20we%20provide%20the%20proofs%20of%20the%20different%20results%20presented%20in%20the%20paper%20and%20some%20additional%20experiments%20that%20validate%20our%20findings%20It%20includes%20the%20following%20items%20i%20the%20proof%20of%20Theorem%201%20Section%20A2%20ii%20The%20proof%20of%20Theorem%202%20concerning%20the%20analysis%20of%20the%20sparse%20case%20Section%20A3%20and%20finally%20iii%20further%20experiments%20which%20confirm%20the%20consistency%20of%20our%20method%20and%20the%20necessity%20of%20the%20conditions%20f%200%20%20f%200%20%200%20on%20the%20kernel%20function%20f%20using%20the%20signals%20of%20Johnstone%20et%20al%20Johnstone%20%20Lu%202009%20Section%20A4%20For%20convenience%20we%20make%20the%20present%20appendix%20selfcontained%20by%20recalling%20the%20preliminaries%20and%20the%20results%20presented%20in%20the%20main%20paper",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20this%20appendix%20we%20provide%20the%20proofs%20of%20the%20different%20results%20presented%20in%20the%20paper%20and%20some%20additional%20experiments%20that%20validate%20our%20findings%20It%20includes%20the%20following%20items%20i%20the%20proof%20of%20Theorem%201%20Section%20A2%20ii%20The%20proof%20of%20Theorem%202%20concerning%20the%20analysis%20of%20the%20sparse%20case%20Section%20A3%20and%20finally%20iii%20further%20experiments%20which%20confirm%20the%20consistency%20of%20our%20method%20and%20the%20necessity%20of%20the%20conditions%20f%200%20%20f%200%20%200%20on%20the%20kernel%20function%20f%20using%20the%20signals%20of%20Johnstone%20et%20al%20Johnstone%20%20Lu%202009%20Section%20A4%20For%20convenience%20we%20make%20the%20present%20appendix%20selfcontained%20by%20recalling%20the%20preliminaries%20and%20the%20results%20presented%20in%20the%20main%20paper"
        },
        {
            "id": "Proposition_2012_a",
            "entry": "Proposition 3 (Normal Concentration of Gaussian Random Vectors (Tao, 2012, Theorem 2.1.12)).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proposition%203%20Normal%20Concentration%20of%20Gaussian%20Random%20Vectors%20Tao%202012%20Theorem%202112"
        },
        {
            "id": "Definition_2008_a",
            "entry": "Definition 3 (\u03b5-sparse matrices (El Karoui, 2008, Definition 1)). A sequence of covariance matrices {\u03a3p}p\u221e=1 is said to be \u03b5-sparse if the sequence of their associated graphs8 {Gp}p\u221e=1 satisfies, for all k \u2208 2N",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Definition%203%20%CE%B5sparse%20matrices%20El%20Karoui%202008%20Definition%201%20A%20sequence%20of%20covariance%20matrices%20%CE%A3pp1%20is%20said%20to%20be%20%CE%B5sparse%20if%20the%20sequence%20of%20their%20associated%20graphs8%20Gpp1%20satisfies%20for%20all%20k%20%202N",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Definition%203%20%CE%B5sparse%20matrices%20El%20Karoui%202008%20Definition%201%20A%20sequence%20of%20covariance%20matrices%20%CE%A3pp1%20is%20said%20to%20be%20%CE%B5sparse%20if%20the%20sequence%20of%20their%20associated%20graphs8%20Gpp1%20satisfies%20for%20all%20k%20%202N"
        },
        {
            "id": "Proof_2008_a",
            "entry": "Proof. The proof needs the introduction of the following two lemmas, that can be found in (El Karoui, 2008, Lemma A.1 and A.2) and which are a consequence of the \u03b5-sparsity notion11",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20The%20proof%20needs%20the%20introduction%20of%20the%20following%20two%20lemmas%2C%20that%20can%20be%20found%20in%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20The%20proof%20needs%20the%20introduction%20of%20the%20following%20two%20lemmas%2C%20that%20can%20be%20found%20in%202008"
        },
        {
            "id": "A.4_2009_b",
            "entry": "A.4 FURTHER EXPERIMENTS A.4.1 HIGHER RANK CASE In this section, we provide further experiments by considering a rank three case and by using the \u201cThree Peak\u201d, \u201cPiece Poly\u201d and \u201cStep New\u201d signals of Johnstone et al. (Johnstone & Lu, 2009), in the \u201cSymmlet 8\u201d wavelet basis, as principal components. We compare the estimated PCs by our method with the kernel function in equation 8 to the estimated ones through standard PCA and the CT method (Deshpande & Montanari, 2016). As shown in Figure 4, the proposed method retrieves consistently the principal components compared to a standard PCA. In particular, we obtain results that are similar to the ones obtained by the CT method while generalizing it to the class of smooth functions with f (0) = f (0) = 0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A4%20FURTHER%20EXPERIMENTS%20A41%20HIGHER%20RANK%20CASE%20In%20this%20section%20we%20provide%20further%20experiments%20by%20considering%20a%20rank%20three%20case%20and%20by%20using%20the%20Three%20Peak%20Piece%20Poly%20and%20Step%20New%20signals%20of%20Johnstone%20et%20al%20Johnstone%20%20Lu%202009%20in%20the%20Symmlet%208%20wavelet%20basis%20as%20principal%20components%20We%20compare%20the%20estimated%20PCs%20by%20our%20method%20with%20the%20kernel%20function%20in%20equation%208%20to%20the%20estimated%20ones%20through%20standard%20PCA%20and%20the%20CT%20method%20Deshpande%20%20Montanari%202016%20As%20shown%20in%20Figure%204%20the%20proposed%20method%20retrieves%20consistently%20the%20principal%20components%20compared%20to%20a%20standard%20PCA%20In%20particular%20we%20obtain%20results%20that%20are%20similar%20to%20the%20ones%20obtained%20by%20the%20CT%20method%20while%20generalizing%20it%20to%20the%20class%20of%20smooth%20functions%20with%20f%200%20%20f%200%20%200",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A4%20FURTHER%20EXPERIMENTS%20A41%20HIGHER%20RANK%20CASE%20In%20this%20section%20we%20provide%20further%20experiments%20by%20considering%20a%20rank%20three%20case%20and%20by%20using%20the%20Three%20Peak%20Piece%20Poly%20and%20Step%20New%20signals%20of%20Johnstone%20et%20al%20Johnstone%20%20Lu%202009%20in%20the%20Symmlet%208%20wavelet%20basis%20as%20principal%20components%20We%20compare%20the%20estimated%20PCs%20by%20our%20method%20with%20the%20kernel%20function%20in%20equation%208%20to%20the%20estimated%20ones%20through%20standard%20PCA%20and%20the%20CT%20method%20Deshpande%20%20Montanari%202016%20As%20shown%20in%20Figure%204%20the%20proposed%20method%20retrieves%20consistently%20the%20principal%20components%20compared%20to%20a%20standard%20PCA%20In%20particular%20we%20obtain%20results%20that%20are%20similar%20to%20the%20ones%20obtained%20by%20the%20CT%20method%20while%20generalizing%20it%20to%20the%20class%20of%20smooth%20functions%20with%20f%200%20%20f%200%20%200"
        }
    ]
}
