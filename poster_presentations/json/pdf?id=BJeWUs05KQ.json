{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DIRECTED-INFO GAIL: LEARNING HIERARCHICAL POLICIES FROM UNSEGMENTED DEMONSTRATIONS USING DIRECTED INFORMATION",
        "author": "Mohit Sharma, Arjun Sharma, Nick Rhinehart, Kris M. Kitani Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {mohits,arjuns,nrhineha,kkitani}@cs.cmu.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJeWUs05KQ"
        },
        "abstract": "The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting stateaction trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies."
    },
    "keywords": [
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "directed information",
            "url": "https://en.wikipedia.org/wiki/directed_information"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "hierarchical structure",
            "url": "https://en.wikipedia.org/wiki/hierarchical_structure"
        }
    ],
    "abbreviations": {
        "MDP": "Markov Decision Process",
        "GAIL": "Generative Adversarial Imitation Learning",
        "GAN": "Generative Adversarial Networks",
        "DP-GMM": "Dirichlet Process Gaussian Mixture Model",
        "VAE": "variational auto-encoder"
    },
    "highlights": [
        "Complex human activities can often be broken down into various simpler sub-activities or sub-tasks that can serve as the basic building blocks for completing a variety of complicated tasks",
        "We show that our work, while derived using the information theoretic perspective of maximizing directed information, bears a close resemblance to applying the options framework in a generative adversarial imitation setting",
        "We extend existing generative adversarial imitation learning frameworks to allow for learning of sub-task specific policies by maximizing directed information in a causal graph of subactivity latent variables and observed trajectory variables",
        "Learning separate sub-task policies can help improve the performance of imitation learning when the demonstrated task is complex and has a hierarchical structure",
        "We model the problem of imitation learning as a directed graph with sub-task latent variables and observed trajectory variables",
        "We further show theoretical connections with the options literature as used in hierarchical reinforcement and imitation learning"
    ],
    "key_statements": [
        "Complex human activities can often be broken down into various simpler sub-activities or sub-tasks that can serve as the basic building blocks for completing a variety of complicated tasks",
        "We develop an imitation learning framework that can learn a policy for each of these sub-tasks given unsegmented activity demonstrations and learn a macro-policy which dictates switching from one sub-task policy to another",
        "We show that our work, while derived using the information theoretic perspective of maximizing directed information, bears a close resemblance to applying the options framework in a generative adversarial imitation setting",
        "We extend existing generative adversarial imitation learning frameworks to allow for learning of sub-task specific policies by maximizing directed information in a causal graph of subactivity latent variables and observed trajectory variables",
        "Learning separate sub-task policies can help improve the performance of imitation learning when the demonstrated task is complex and has a hierarchical structure",
        "We model the problem of imitation learning as a directed graph with sub-task latent variables and observed trajectory variables",
        "We further show theoretical connections with the options literature as used in hierarchical reinforcement and imitation learning"
    ],
    "summary": [
        "Complex human activities can often be broken down into various simpler sub-activities or sub-tasks that can serve as the basic building blocks for completing a variety of complicated tasks.",
        "We extend existing generative adversarial imitation learning frameworks to allow for learning of sub-task specific policies by maximizing directed information in a causal graph of subactivity latent variables and observed trajectory variables.",
        "We draw connections between previous works on imitation learning with sub-task policies using options and show that our proposed approach can be seen as option learning in a generative adversarial setting.",
        "To learn such sub-task policies from unsegmented deomonstrations we use the graphical model in Figure 1 (Right), i.e., consider a set of expert demonstrations, each of which is represented by \u03c4 = {\u03c41, \u00b7 \u00b7 \u00b7 , \u03c4T } where \u03c4t is the state-action pair observed at time t.",
        "To enforce the model to use this latent variable, these approaches propose to maximize the mutual information between the demonstrated sequence of state-action pairs and the latent embedding of the nature of the sub-activity.",
        "In this work we propose to force the policy to generate trajectories that maximize the directed or causal information flow from trajectories to the sequence of latent sub-activity variables instead.",
        "Our proposed Directed-Info GAIL can be be considered as the generative adversarial variant of imitation learning using the options framework.",
        "Through this experiment we aim to see whether our proposed approach is able to infer sub-tasks which correspond to meaningful navigation strategies and combine them to plan paths to different goal states.",
        "As can be seen in Figure 4(a, b), when using two sub-task latent variables, our method learns to segment the demonstrations into",
        "Figure 5(b) shows that our approach learns to assign separate latent variable values for different action primitives such as, jumping, mid-air and landing phases of these tasks, with the latent variable changing approximately periodically as the agent performs the periodic hopping/walking motion.",
        "While our method was able to learn to segment the expert demonstrations into the Pick and Place sub-tasks correctly, as can be seen in Figure 6 and the videos at https://sites.google.com/view/directedinfo-gail/home#h.p_ 4dsbuC5expkZ, neither our approach, nor GAIL was able to successfully complete the task.",
        "We present an algorithm that infers these latent sub-task policies directly from given unstructured and unlabelled expert demonstrations.",
        "We model the problem of imitation learning as a directed graph with sub-task latent variables and observed trajectory variables.",
        "We use the notion of directed information in a generative adversarial imitation learning framework to learn sub-task and macro policies.",
        "Our experiments show that our method is able to segment the expert demonstrations into different sub-tasks, learn sub-task specific policies and learn a macro-policy that can combines these sub-task"
    ],
    "headline": "We discover the interaction between sub-tasks from their resulting stateaction trajectory sequences using a directed graphical model",
    "reference_links": [
        {
            "id": "Andreas_et+al_2017_a",
            "entry": "Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166\u2013175. JMLR. org, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Klein%2C%20Dan%20Levine%2C%20Sergey%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Daniel_et+al_2016_a",
            "entry": "Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3):337\u2013357, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016"
        },
        {
            "id": "Fox_et+al_2017_a",
            "entry": "Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options. arXiv preprint arXiv:1703.08294, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08294"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Graves_et+al_2006_a",
            "entry": "Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine learning, pp. 369\u2013376. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Fernandez%2C%20Santiago%20Gomez%2C%20Faustino%20Schmidhuber%2C%20Jurgen%20Connectionist%20temporal%20classification%3A%20labelling%20unsegmented%20sequence%20data%20with%20recurrent%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Fernandez%2C%20Santiago%20Gomez%2C%20Faustino%20Schmidhuber%2C%20Jurgen%20Connectionist%20temporal%20classification%3A%20labelling%20unsegmented%20sequence%20data%20with%20recurrent%20neural%20networks%202006"
        },
        {
            "id": "Hausman_et+al_2017_a",
            "entry": "Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph J Lim. Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 1235\u20131245, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20Karol%20Chebotar%2C%20Yevgen%20Schaal%2C%20Stefan%20Sukhatme%2C%20Gaurav%20Multi-modal%20imitation%20learning%20from%20unstructured%20demonstrations%20using%20generative%20adversarial%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20Karol%20Chebotar%2C%20Yevgen%20Schaal%2C%20Stefan%20Sukhatme%2C%20Gaurav%20Multi-modal%20imitation%20learning%20from%20unstructured%20demonstrations%20using%20generative%20adversarial%20nets%202017"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kramer_1998_a",
            "entry": "Gerhard Kramer. Directed information for channels with feedback. PhD thesis, Eidgenossiche Technische Hochschule Zurich, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kramer%2C%20Gerhard%20Directed%20information%20for%20channels%20with%20feedback%201998"
        },
        {
            "id": "Krishnan_et+al_2017_a",
            "entry": "Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. DDCO: Discovery of deep continuous options for robot learning from demonstrations. In 1st Conference on Robot Learning (CoRL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Sanjay%20Fox%2C%20Roy%20Stoica%2C%20Ion%20Goldberg%2C%20Ken%20DDCO%3A%20Discovery%20of%20deep%20continuous%20options%20for%20robot%20learning%20from%20demonstrations%202017"
        },
        {
            "id": "Kroemer_et+al_2014_a",
            "entry": "O. Kroemer, H. van Hoof, G. Neumann, and J. Peters. Learning to predict phases of manipulation tasks as hidden states. In Proceedings of 2014 IEEE International Conference on Robotics and Automation, pp. 4009\u20134014. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kroemer%2C%20O.%20van%20Hoof%2C%20H.%20Neumann%2C%20G.%20Peters%2C%20J.%20Learning%20to%20predict%20phases%20of%20manipulation%20tasks%20as%20hidden%20states%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kroemer%2C%20O.%20van%20Hoof%2C%20H.%20Neumann%2C%20G.%20Peters%2C%20J.%20Learning%20to%20predict%20phases%20of%20manipulation%20tasks%20as%20hidden%20states%202014"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems, pp. 3815\u20133825, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yunzhu%20Song%2C%20Jiaming%20Ermon%2C%20Stefano%20Infogail%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yunzhu%20Song%2C%20Jiaming%20Ermon%2C%20Stefano%20Infogail%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017"
        },
        {
            "id": "Mcgovern_2001_a",
            "entry": "Amy McGovern and Andrew G Barto. Accelerating reinforcement learning through the discovery of useful subgoals. 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McGovern%2C%20Amy%20Barto%2C%20Andrew%20G.%20Accelerating%20reinforcement%20learning%20through%20the%20discovery%20of%20useful%20subgoals%202001"
        },
        {
            "id": "Moon_1996_a",
            "entry": "Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6): 47\u201360, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moon%2C%20Todd%20K.%20The%20expectation-maximization%20algorithm%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moon%2C%20Todd%20K.%20The%20expectation-maximization%20algorithm%201996"
        },
        {
            "id": "Niekum_2011_a",
            "entry": "Scott Niekum and Andrew G Barto. Clustering via dirichlet process mixture models for portable skill discovery. In Advances in neural information processing systems, pp. 1818\u20131826, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niekum%2C%20Scott%20Barto%2C%20Andrew%20G.%20Clustering%20via%20dirichlet%20process%20mixture%20models%20for%20portable%20skill%20discovery%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niekum%2C%20Scott%20Barto%2C%20Andrew%20G.%20Clustering%20via%20dirichlet%20process%20mixture%20models%20for%20portable%20skill%20discovery%202011"
        },
        {
            "id": "Dean_1989_a",
            "entry": "Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, pp. 305\u2013313, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%20A%20Pomerleau%20Alvinn%20An%20autonomous%20land%20vehicle%20in%20a%20neural%20network%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20305313%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%20A%20Pomerleau%20Alvinn%20An%20autonomous%20land%20vehicle%20in%20a%20neural%20network%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20305313%201989"
        },
        {
            "id": "Ranchod_et+al_2015_a",
            "entry": "Pravesh Ranchod, Benjamin Rosman, and George Konidaris. Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pp. 471\u2013477. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranchod%2C%20Pravesh%20Rosman%2C%20Benjamin%20Konidaris%2C%20George%20Nonparametric%20bayesian%20reward%20segmentation%20for%20skill%20discovery%20using%20inverse%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranchod%2C%20Pravesh%20Rosman%2C%20Benjamin%20Konidaris%2C%20George%20Nonparametric%20bayesian%20reward%20segmentation%20for%20skill%20discovery%20using%20inverse%20reinforcement%20learning%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Shiarlis_et+al_2018_a",
            "entry": "Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, and Ingmar Posner. Taco: Learning task decomposition via temporal alignment for control. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shiarlis%2C%20Kyriacos%20Wulfmeier%2C%20Markus%20Salter%2C%20Sasha%20Whiteson%2C%20Shimon%20Taco%3A%20Learning%20task%20decomposition%20via%20temporal%20alignment%20for%20control%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shiarlis%2C%20Kyriacos%20Wulfmeier%2C%20Markus%20Salter%2C%20Sasha%20Whiteson%2C%20Shimon%20Taco%3A%20Learning%20task%20decomposition%20via%20temporal%20alignment%20for%20control%202018"
        },
        {
            "id": "Simsek_et+al_2005_a",
            "entry": "Ozgur Simsek, Alicia P Wolfe, and Andrew G Barto. Identifying useful subgoals in reinforcement learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning, pp. 816\u2013823. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simsek%2C%20Ozgur%20Wolfe%2C%20Alicia%20P.%20Barto%2C%20Andrew%20G.%20Identifying%20useful%20subgoals%20in%20reinforcement%20learning%20by%20local%20graph%20partitioning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simsek%2C%20Ozgur%20Wolfe%2C%20Alicia%20P.%20Barto%2C%20Andrew%20G.%20Identifying%20useful%20subgoals%20in%20reinforcement%20learning%20by%20local%20graph%20partitioning%202005"
        },
        {
            "id": "Sutton_et+al_1998_a",
            "entry": "Richard S Sutton, Doina Precup, and Satinder P Singh. Intra-option learning about temporally abstract actions. In ICML, volume 98, pp. 556\u2013564, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20P.%20Intra-option%20learning%20about%20temporally%20abstract%20actions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20P.%20Intra-option%20learning%20about%20temporally%20abstract%20actions%201998"
        }
    ]
}
