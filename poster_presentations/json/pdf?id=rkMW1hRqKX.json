{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "OPTIMAL COMPLETION DISTILLATION FOR SEQUENCE LEARNING",
        "author": "Sara Sabour, William Chan, Mohammad Norouzi {sasabour, williamchan, mnorouzi}@google.com Google Brain",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkMW1hRqKX"
        },
        "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we define a target distribution that puts an equal probability on the first token of each optimal suffix. OCD achieves the state-of-theart performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving 9.3% and 4.5% word error rates, respectively."
    },
    "keywords": [
        {
            "term": "Maximum Likelihood Estimation",
            "url": "https://en.wikipedia.org/wiki/Maximum_Likelihood_Estimation"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "Wall Street Journal",
            "url": "https://en.wikipedia.org/wiki/Wall_Street_Journal"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "edit distance",
            "url": "https://en.wikipedia.org/wiki/edit_distance"
        }
    ],
    "abbreviations": {
        "OCD": "Optimal Completion Distillation",
        "MLE": "Maximum Likelihood Estimation",
        "CER": "Character Error Rate",
        "WER": "Word Error Rate",
        "RNNs": "recurrent neural networks",
        "DQN": "Deep Q-Network",
        "RL": "Reinforcement Learning",
        "EMBR": "Edit-based Minimum Bayes Risk",
        "RAML": "Reward Augmented Maximum Likelihood",
        "WSJ": "Wall Street Journal",
        "OCT": "Optimal Completion Target",
        "BPE": "Byte Pair Encoding"
    },
    "highlights": [
        "Recent advances in natural language processing and speech recognition hinge on the development of expressive neural network architectures for sequence to sequence learning (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a></a>)",
        "Despite many attempts to mitigate the limitations of Maximum Likelihood Estimation (MLE) (<a class=\"ref-link\" id=\"cRanzato_et+al_2016_a\" href=\"#rRanzato_et+al_2016_a\">Ranzato et al, 2016</a>; <a class=\"ref-link\" id=\"cWiseman_2016_a\" href=\"#rWiseman_2016_a\">Wiseman and Rush, 2016</a>; <a class=\"ref-link\" id=\"cNorouzi_et+al_2016_a\" href=\"#rNorouzi_et+al_2016_a\">Norouzi et al, 2016</a>; <a class=\"ref-link\" id=\"cBahdanau_et+al_2017_a\" href=\"#rBahdanau_et+al_2017_a\">Bahdanau et al, 2017</a>; <a class=\"ref-link\" id=\"cLeblond_et+al_2018_a\" href=\"#rLeblond_et+al_2018_a\">Leblond et al, 2018</a>), Maximum Likelihood Estimation is still considered the dominant approach for training seq2seq models",
        "We propose Optimal Completion Distillation, a stand-alone algorithm for optimizing seq2seq models based on edit distance",
        "We show that Optimal Completion Distillation achieves significantly better Character Error Rate and Word Error Rate over the other optimization strategies compared in Table 3",
        "This paper presents Optimal Completion Distillation (OCD), a training procedure for optimizing autoregressive sequence models base on edit distance",
        "Optimal Completion Distillation is applicable to on-policy or off-policy trajectories, and in this paper, we demonstrate its effectiveness on samples drawn from the model in an online fashion"
    ],
    "key_statements": [
        "Recent advances in natural language processing and speech recognition hinge on the development of expressive neural network architectures for sequence to sequence learning (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a></a>)",
        "Despite many attempts to mitigate the limitations of Maximum Likelihood Estimation (MLE) (<a class=\"ref-link\" id=\"cRanzato_et+al_2016_a\" href=\"#rRanzato_et+al_2016_a\">Ranzato et al, 2016</a>; <a class=\"ref-link\" id=\"cWiseman_2016_a\" href=\"#rWiseman_2016_a\">Wiseman and Rush, 2016</a>; <a class=\"ref-link\" id=\"cNorouzi_et+al_2016_a\" href=\"#rNorouzi_et+al_2016_a\">Norouzi et al, 2016</a>; <a class=\"ref-link\" id=\"cBahdanau_et+al_2017_a\" href=\"#rBahdanau_et+al_2017_a\">Bahdanau et al, 2017</a>; <a class=\"ref-link\" id=\"cLeblond_et+al_2018_a\" href=\"#rLeblond_et+al_2018_a\">Leblond et al, 2018</a>), Maximum Likelihood Estimation is still considered the dominant approach for training seq2seq models",
        "We propose Optimal Completion Distillation, a stand-alone algorithm for optimizing seq2seq models based on edit distance",
        "We demonstrate the effectiveness of Optimal Completion Distillation on end-to-end speech recognition using attentionbased seq2seq models",
        "On Librispeech, Optimal Completion Distillation achieves state-of-the-art Word Error Rate of 4.5% on \u201ctest-clean\u201d and 13.3% on \u201ctest-other\u201d sets (Table 5).\n2 BACKGROUND: SEQUENCE LEARNING WITH Maximum Likelihood Estimation",
        "Our goal is to identify the set of all optimal suffixes y \u2208 Y that result in a full sequences [y<i, y] with a minimum edit distance v.s. y\u2217",
        "We show that Optimal Completion Distillation achieves significantly better Character Error Rate and Word Error Rate over the other optimization strategies compared in Table 3",
        "This paper presents Optimal Completion Distillation (OCD), a training procedure for optimizing autoregressive sequence models base on edit distance",
        "Optimal Completion Distillation is applicable to on-policy or off-policy trajectories, and in this paper, we demonstrate its effectiveness on samples drawn from the model in an online fashion"
    ],
    "summary": [
        "Recent advances in natural language processing and speech recognition hinge on the development of expressive neural network architectures for sequence to sequence learning (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a></a>).",
        "Our key observation is that given an arbitrary prefix, we can exactly and efficiently identify all of the suffixes that result in a minimum total edit distance (v.s. the ground truth target).",
        "We always train on prefixes generated by sampling from the model that is being optimized.",
        "2. For each generated prefix, we identify all of the optimal suffixes that result in a minimum total edit distance v.s. the ground truth target using an efficient dynamic programming algorithm.",
        "We propose OCD, a stand-alone algorithm for optimizing seq2seq models based on edit distance.",
        "One optimizes the log-probability of the ground truth output sequence, which is often different from the task evaluation metric.",
        "We always train on sequences generated by sampling from the current model that is being optimized.",
        "Applying MLE to autoregressive models casts the problem of sequence learning as optimizing a mapping (x, y<\u2217 t) \u2192 yt\u2217 from ground truth prefixes to correct tokens.",
        "OCD encourages the model to extend each prefix with the set of optimal choices for the token.",
        "Table 1 includes an example ground truth target from the Wall Street Journal dataset and the corresponding generated sample from a model.",
        "Distance is the evaluation metric, we develop a dynamic programming algorithm to calculate optimal Q-values exactly and efficiently for all prefixes of a sequence y, discussed below.",
        "Our goal is to identify the set of all optimal suffixes y \u2208 Y that result in a full sequences [y<i, y] with a minimum edit distance v.s. y\u2217.",
        "On both WSJ and Librispeech, our proposed OCD (Optimal Completion Distillation) algorithm significantly outperforms our own strong baselines including MLE (Maximum Likelihood Estimation with label smoothing) and SS.",
        "We emphasize that during training, the generated prefixes sampled from the model do not match the ground truth sequence, even at the end of training.",
        "We emphasize that unlike SS, we do not need to tune an exploration schedule, OCD prefixes are always sampled from the model from the start of training.",
        "In OCT, we optimize the log-likelihood of one target, which at each step we pick dynamically based on the minimum edit distance completion similar to OCD.",
        "This paper presents Optimal Completion Distillation (OCD), a training procedure for optimizing autoregressive sequence models base on edit distance.",
        "OCD outperforms all published work on end-to-end speech recognition, including our own well-tuned MLE and scheduled sampling baselines without introducing new hyper-parameters."
    ],
    "headline": "We present Optimal Completion Distillation, a training procedure for optimizing sequence to sequence models based on edit distance",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20a%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20a%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Bahdanau_et+al_2016_a",
            "entry": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-toEnd Attention-based Large Vocabulary Speech Recognition. ICASSP, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Chorowski%2C%20Jan%20Serdyuk%2C%20Dmitriy%20Brakel%2C%20Philemon%20End-toEnd%20Attention-based%20Large%20Vocabulary%20Speech%20Recognition%202016"
        },
        {
            "id": "Bahdanau_et+al_2016_b",
            "entry": "Dzmitry Bahdanau, Dmitriy Serdyuk, Philemon Brakel, Nan Rosemary Ke, Jan Chorowski, Aaron Courville, and Yoshua Bengio. Task Loss Estimation for Sequence Prediction. ICLR Workshop, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Serdyuk%2C%20Dmitriy%20Brakel%2C%20Philemon%20Ke%2C%20Nan%20Rosemary%20Task%20Loss%20Estimation%20for%20Sequence%20Prediction.%20ICLR%20Workshop%202016"
        },
        {
            "id": "Bahdanau_et+al_2017_a",
            "entry": "Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An Actor-Critic Algorithm for Sequence Prediction. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Brakel%2C%20Philemon%20Xu%2C%20Kelvin%20Goyal%2C%20Anirudh%20An%20Actor-Critic%20Algorithm%20for%20Sequence%20Prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Brakel%2C%20Philemon%20Xu%2C%20Kelvin%20Goyal%2C%20Anirudh%20An%20Actor-Critic%20Algorithm%20for%20Sequence%20Prediction%202017"
        },
        {
            "id": "Bengio_et+al_2015_a",
            "entry": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam M. Shazeer. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20M.%20Scheduled%20Sampling%20for%20Sequence%20Prediction%20with%20Recurrent%20Neural%20Networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20M.%20Scheduled%20Sampling%20for%20Sequence%20Prediction%20with%20Recurrent%20Neural%20Networks%202015"
        },
        {
            "id": "Britz_et+al_2017_a",
            "entry": "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration of neural machine translation architectures. arXiv:1703.03906, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03906"
        },
        {
            "id": "Chan_et+al_2016_a",
            "entry": "William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition. ICASSP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20William%20Jaitly%2C%20Navdeep%20Le%2C%20Quoc%20Listen%2C%20Oriol%20Vinyals%20Attend%20and%20Spell%3A%20A%20Neural%20Network%20for%20Large%20Vocabulary%20Conversational%20Speech%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20William%20Jaitly%2C%20Navdeep%20Le%2C%20Quoc%20Listen%2C%20Oriol%20Vinyals%20Attend%20and%20Spell%3A%20A%20Neural%20Network%20for%20Large%20Vocabulary%20Conversational%20Speech%20Recognition%202016"
        },
        {
            "id": "Chan_et+al_2017_a",
            "entry": "William Chan, Yu Zhang, and Navdeep Jaitly. Latent Sequence Decompositions. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20William%20Zhang%2C%20Yu%20Jaitly%2C%20Navdeep%20Latent%20Sequence%20Decompositions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20William%20Zhang%2C%20Yu%20Jaitly%2C%20Navdeep%20Latent%20Sequence%20Decompositions%202017"
        },
        {
            "id": "Chang_et+al_2015_a",
            "entry": "Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\u00e9 III, and John Langford. Learning to Search Better Than Your Teacher. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daum%C3%A9%2C%20III%2C%20Hal%20Learning%20to%20Search%20Better%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daum%C3%A9%2C%20III%2C%20Hal%20Learning%20to%20Search%20Better%202015"
        },
        {
            "id": "Cheng_2018_a",
            "entry": "Ching-An Cheng and Byron Boots. Convergence of value aggregation for imitation learning. arXiv preprint arXiv:1801.07292, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07292"
        },
        {
            "id": "Chiu_et+al_2017_a",
            "entry": "Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Katya Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. arXiv:1712.01769, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01769"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20Merri%C3%ABnboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20Merri%C3%ABnboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Chorowski_2017_a",
            "entry": "Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in sequence to sequence models. INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chorowski%2C%20Jan%20Jaitly%2C%20Navdeep%20Towards%20better%20decoding%20and%20language%20model%20integration%20in%20sequence%20to%20sequence%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chorowski%2C%20Jan%20Jaitly%2C%20Navdeep%20Towards%20better%20decoding%20and%20language%20model%20integration%20in%20sequence%20to%20sequence%20models%202017"
        },
        {
            "id": "Collobert_et+al_2016_a",
            "entry": "Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to-end convnet-based speech recognition system. arXiv preprint arXiv:1609.03193, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03193"
        },
        {
            "id": "Hal_2009_a",
            "entry": "Hal Daum\u00e9, III, John Langford, and Daniel Marcu. Search-based structured prediction. Mach. Learn. J., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hal%20Daum%C3%A9%2C%20III%2C%20John%20Langford%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hal%20Daum%C3%A9%2C%20III%2C%20John%20Langford%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009"
        },
        {
            "id": "Daum_2005_a",
            "entry": "Hal Daum\u00e9 III and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. ICML, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daum%C3%A9%2C%20III%2C%20Hal%20Marcu%2C%20Daniel%20Learning%20as%20search%20optimization%3A%20Approximate%20large%20margin%20methods%20for%20structured%20prediction%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daum%C3%A9%2C%20III%2C%20Hal%20Marcu%2C%20Daniel%20Learning%20as%20search%20optimization%3A%20Approximate%20large%20margin%20methods%20for%20structured%20prediction%202005"
        },
        {
            "id": "Ding_2017_a",
            "entry": "Nan Ding and Radu Soricut. Cold-Start Reinforcement Learning with Softmax Policy Gradient. In Advances in Neural Information Processing Systems, pages 2817\u20132826, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Nan%20Soricut%2C%20Radu%20Cold-Start%20Reinforcement%20Learning%20with%20Softmax%20Policy%20Gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Nan%20Soricut%2C%20Radu%20Cold-Start%20Reinforcement%20Learning%20with%20Softmax%20Policy%20Gradient%202017"
        },
        {
            "id": "Edunov_et+al_2018_a",
            "entry": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\u2019Aurelio Ranzato. Classical structured prediction losses for sequence to sequence learning. NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edunov%2C%20Sergey%20Ott%2C%20Myle%20Auli%2C%20Michael%20Grangier%2C%20David%20Classical%20structured%20prediction%20losses%20for%20sequence%20to%20sequence%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edunov%2C%20Sergey%20Ott%2C%20Myle%20Auli%2C%20Michael%20Grangier%2C%20David%20Classical%20structured%20prediction%20losses%20for%20sequence%20to%20sequence%20learning%202018"
        },
        {
            "id": "Elbayad_et+al_2018_a",
            "entry": "Maha Elbayad, Laurent Besacier, and Jakob Verbeek. Token-level and sequence-level loss smoothing for rnn language models. ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elbayad%2C%20Maha%20Besacier%2C%20Laurent%20Verbeek%2C%20Jakob%20Token-level%20and%20sequence-level%20loss%20smoothing%20for%20rnn%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elbayad%2C%20Maha%20Besacier%2C%20Laurent%20Verbeek%2C%20Jakob%20Token-level%20and%20sequence-level%20loss%20smoothing%20for%20rnn%20language%20models%202018"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional Sequence to Sequence Learning. In arXiv:1705.03122, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodman_et+al_2016_a",
            "entry": "James Goodman, Andreas Vlachos, and Jason Naradowsky. Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1\u201311, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20James%20Vlachos%2C%20Andreas%20Naradowsky%2C%20Jason%20Noise%20reduction%20and%20targeted%20exploration%20in%20imitation%20learning%20for%20abstract%20meaning%20representation%20parsing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodman%2C%20James%20Vlachos%2C%20Andreas%20Naradowsky%2C%20Jason%20Noise%20reduction%20and%20targeted%20exploration%20in%20imitation%20learning%20for%20abstract%20meaning%20representation%20parsing%202016"
        },
        {
            "id": "Graves_2014_a",
            "entry": "Alex Graves and Navdeep Jaitly. Towards End-to-End Speech Recognition with Recurrent Neural Networks. ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Jaitly%2C%20Navdeep%20Towards%20End-to-End%20Speech%20Recognition%20with%20Recurrent%20Neural%20Networks%202014"
        },
        {
            "id": "Hassan_et+al_2018_a",
            "entry": "Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving human parity on automatic chinese to english news translation. arXiv:1803.05567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05567"
        },
        {
            "id": "Hinton_et+al_2014_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. In Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20Knowledge%20in%20a%20Neural%20Network%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20Knowledge%20in%20a%20Neural%20Network%202014"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. Neural Comput., 9, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20Short-Term%20Memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20Short-Term%20Memory%201997"
        },
        {
            "id": "Karita_et+al_2018_a",
            "entry": "Shigeki Karita, Atsunori Ogawa, Marc Delcroix, and Tomohiro Nakatani. Sequence Training of Encoder-decoder Model Using Policy Gradient for End- To-end Speech Recognition. In ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karita%2C%20Shigeki%20Ogawa%2C%20Atsunori%20Delcroix%2C%20Marc%20Nakatani%2C%20Tomohiro%20Sequence%20Training%20of%20Encoder-decoder%20Model%20Using%20Policy%20Gradient%20for%20End-%20To-end%20Speech%20Recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karita%2C%20Shigeki%20Ogawa%2C%20Atsunori%20Delcroix%2C%20Marc%20Nakatani%2C%20Tomohiro%20Sequence%20Training%20of%20Encoder-decoder%20Model%20Using%20Policy%20Gradient%20for%20End-%20To-end%20Speech%20Recognition%202018"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning. ICASSP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Suyoun%20Hori%2C%20Takaaki%20Watanabe%2C%20Shinji%20Joint%20CTC-Attention%20based%20End-to-End%20Speech%20Recognition%20using%20Multi-task%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Suyoun%20Hori%2C%20Takaaki%20Watanabe%2C%20Shinji%20Joint%20CTC-Attention%20based%20End-to-End%20Speech%20Recognition%20using%20Multi-task%20Learning%202017"
        },
        {
            "id": "Koehn_et+al_2007_a",
            "entry": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source toolkit for statistical machine translation. Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koehn%2C%20Philipp%20Hoang%2C%20Hieu%20Birch%2C%20Alexandra%20Callison-Burch%2C%20Chris%20Moses%3A%20Open%20source%20toolkit%20for%20statistical%20machine%20translation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koehn%2C%20Philipp%20Hoang%2C%20Hieu%20Birch%2C%20Alexandra%20Callison-Burch%2C%20Chris%20Moses%3A%20Open%20source%20toolkit%20for%20statistical%20machine%20translation%202007"
        },
        {
            "id": "Leblond_et+al_2018_a",
            "entry": "R\u00e9mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, and Simon Lacoste-Julien. SEARNN: Training RNNs with global-local losses. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leblond%2C%20R%C3%A9mi%20Alayrac%2C%20Jean-Baptiste%20Osokin%2C%20Anton%20Lacoste-Julien%2C%20Simon%20SEARNN%3A%20Training%20RNNs%20with%20global-local%20losses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leblond%2C%20R%C3%A9mi%20Alayrac%2C%20Jean-Baptiste%20Osokin%2C%20Anton%20Lacoste-Julien%2C%20Simon%20SEARNN%3A%20Training%20RNNs%20with%20global-local%20losses%202018"
        },
        {
            "id": "Levenshtein_1966_a",
            "entry": "Vladimir I Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707\u2013710, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levenshtein%2C%20Vladimir%20I.%20Binary%20codes%20capable%20of%20correcting%20deletions%2C%20insertions%2C%20and%20reversals%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levenshtein%2C%20Vladimir%20I.%20Binary%20codes%20capable%20of%20correcting%20deletions%2C%20insertions%2C%20and%20reversals%201966"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "Davis Liang, Zhiheng Huang, and Zachary C Lipton. Learning noise-invariant representations for robust speech recognition. arXiv preprint arXiv:1807.06610, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06610"
        },
        {
            "id": "Liptchinsky_et+al_2017_a",
            "entry": "Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert. Letter-Based Speech Recognition with Gated ConvNets. In arXiv:1712.09444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09444"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Hairong Liu, Zhenyao Zhu, Xiangang Li, and Sanjeev Satheesh. Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hairong%20Zhu%2C%20Zhenyao%20Li%2C%20Xiangang%20Satheesh%2C%20Sanjeev%20Gram-CTC%3A%20Automatic%20Unit%20Selection%20and%20Target%20Decomposition%20for%20Sequence%20Labelling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hairong%20Zhu%2C%20Zhenyao%20Li%2C%20Xiangang%20Satheesh%2C%20Sanjeev%20Gram-CTC%3A%20Automatic%20Unit%20Selection%20and%20Target%20Decomposition%20for%20Sequence%20Labelling%202017"
        },
        {
            "id": "Ma_et+al_2017_a",
            "entry": "Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, and Eduard Hovy. Softmax q-distribution estimation for structured prediction: A theoretical interpretation for raml. arXiv:1705.07136, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07136"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Norouzi_et+al_2016_a",
            "entry": "Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, and Dale Schuurmans. Reward Augmented Maximum Likelihood for Neural Structured Prediction. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Norouzi%2C%20Mohammad%20Bengio%2C%20Samy%20Chen%2C%20Zhifeng%20Jaitly%2C%20Navdeep%20Reward%20Augmented%20Maximum%20Likelihood%20for%20Neural%20Structured%20Prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Norouzi%2C%20Mohammad%20Bengio%2C%20Samy%20Chen%2C%20Zhifeng%20Jaitly%2C%20Navdeep%20Reward%20Augmented%20Maximum%20Likelihood%20for%20Neural%20Structured%20Prediction%202016"
        },
        {
            "id": "Panayotov_et+al_2015_a",
            "entry": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Librispeech%3A%20an%20asr%20corpus%20based%20on%20public%20domain%20audio%20books%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Librispeech%3A%20an%20asr%20corpus%20based%20on%20public%20domain%20audio%20books%202015"
        },
        {
            "id": "Paul_1992_a",
            "entry": "Douglas B Paul and Janet M Baker. The design for the wall street journal-based csr corpus. In Proceedings of the workshop on Speech and Natural Language, pages 357\u2013362. Association for Computational Linguistics, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paul%2C%20Douglas%20B.%20Baker%2C%20Janet%20M.%20The%20design%20for%20the%20wall%20street%20journal-based%20csr%20corpus%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paul%2C%20Douglas%20B.%20Baker%2C%20Janet%20M.%20The%20design%20for%20the%20wall%20street%20journal-based%20csr%20corpus%201992"
        },
        {
            "id": "Pereyra_et+al_2017_a",
            "entry": "Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing Neural Networks by Penalizing Confident Output Distributions. ICLR Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pereyra%2C%20Gabriel%20Tucker%2C%20George%20Chorowski%2C%20Jan%20Kaiser%2C%20%C5%81ukasz%20Regularizing%20Neural%20Networks%20by%20Penalizing%20Confident%20Output%20Distributions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pereyra%2C%20Gabriel%20Tucker%2C%20George%20Chorowski%2C%20Jan%20Kaiser%2C%20%C5%81ukasz%20Regularizing%20Neural%20Networks%20by%20Penalizing%20Confident%20Output%20Distributions%202017"
        },
        {
            "id": "Povey_et+al_2011_a",
            "entry": "Daniel Povey, Arnab Ghoshal, Gilles Boulianne, et al. The Kaldi Speech Recognition Toolkit. ASRU, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%20Povey%20Arnab%20Ghoshal%20Gilles%20Boulianne%20et%20al%20The%20Kaldi%20Speech%20Recognition%20Toolkit%20ASRU%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%20Povey%20Arnab%20Ghoshal%20Gilles%20Boulianne%20et%20al%20The%20Kaldi%20Speech%20Recognition%20Toolkit%20ASRU%202011"
        },
        {
            "id": "Prabhavalkar_et+al_2018_a",
            "entry": "Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, and Anjuli Kannan. Minimum Word Error Rate Training for Attention-based Sequence-toSequence Models. In ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prabhavalkar%2C%20Rohit%20Sainath%2C%20Tara%20N.%20Wu%2C%20Yonghui%20Nguyen%2C%20Patrick%20Minimum%20Word%20Error%20Rate%20Training%20for%20Attention-based%20Sequence-toSequence%20Models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prabhavalkar%2C%20Rohit%20Sainath%2C%20Tara%20N.%20Wu%2C%20Yonghui%20Nguyen%2C%20Patrick%20Minimum%20Word%20Error%20Rate%20Training%20for%20Attention-based%20Sequence-toSequence%20Models%202018"
        },
        {
            "id": "Ranzato_et+al_2016_a",
            "entry": "Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence Level Training with Recurrent Neural Networks. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Chopra%2C%20Sumit%20Auli%2C%20Michael%20Zaremba%2C%20Wojciech%20Sequence%20Level%20Training%20with%20Recurrent%20Neural%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Chopra%2C%20Sumit%20Auli%2C%20Michael%20Zaremba%2C%20Wojciech%20Sequence%20Level%20Training%20with%20Recurrent%20Neural%20Networks%202016"
        },
        {
            "id": "Rennie_et+al_2017_a",
            "entry": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical Sequence Training for Image Captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Self-critical%20Sequence%20Training%20for%20Image%20Captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Self-critical%20Sequence%20Training%20for%20Image%20Captioning%202017"
        },
        {
            "id": "Ross_2014_a",
            "entry": "Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5979"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Stephane Ross, Geoffrey J Gordon, and J Andrew Bagnell. A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20J.%20Bagnell%2C%20J.Andrew%20A%20Reduction%20of%20Imitation%20Learning%20and%20Structured%20Prediction%20to%20No-Regret%20Online%20Learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20J.%20Bagnell%2C%20J.Andrew%20A%20Reduction%20of%20Imitation%20Learning%20and%20Structured%20Prediction%20to%20No-Regret%20Online%20Learning%202011"
        },
        {
            "id": "Rush_et+al_2015_a",
            "entry": "Alexander M. Rush, Sumit Chopra, and Jason Weston. A Neural Attention Model for Abstractive Sentence Summarization. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization%202015"
        },
        {
            "id": "Rusu_et+al_2016_a",
            "entry": "Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrei%20A%20Rusu%20Sergio%20Gomez%20Colmenarejo%20Caglar%20Gulcehre%20Guillaume%20Desjardins%20James%20Kirkpatrick%20Razvan%20Pascanu%20Volodymyr%20Mnih%20Koray%20Kavukcuoglu%20and%20Raia%20Hadsell%20Policy%20Distillation%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrei%20A%20Rusu%20Sergio%20Gomez%20Colmenarejo%20Caglar%20Gulcehre%20Guillaume%20Desjardins%20James%20Kirkpatrick%20Razvan%20Pascanu%20Volodymyr%20Mnih%20Koray%20Kavukcuoglu%20and%20Raia%20Hadsell%20Policy%20Distillation%20ICLR%202016"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with Subword Units. ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20Rico%20Haddow%2C%20Barry%20Birch%2C%20Alexandra%20Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units%202016"
        },
        {
            "id": "Serdyuk_et+al_2018_a",
            "entry": "Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris Pal, and Yoshua Bengio. Twin Networks: Matching the Future for Sequence Generation. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serdyuk%2C%20Dmitriy%20Ke%2C%20Nan%20Rosemary%20Sordoni%2C%20Alessandro%20Trischler%2C%20Adam%20Twin%20Networks%3A%20Matching%20the%20Future%20for%20Sequence%20Generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serdyuk%2C%20Dmitriy%20Ke%2C%20Nan%20Rosemary%20Sordoni%2C%20Alessandro%20Trischler%2C%20Adam%20Twin%20Networks%3A%20Matching%20the%20Future%20for%20Sequence%20Generation%202018"
        },
        {
            "id": "Sriram_et+al_2018_a",
            "entry": "Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. Cold fusion: Training seq2seq models together with language models. In INTERSPEECH, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriram%2C%20Anuroop%20Jun%2C%20Heewoo%20Satheesh%2C%20Sanjeev%20Coates%2C%20Adam%20Cold%20fusion%3A%20Training%20seq2seq%20models%20together%20with%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriram%2C%20Anuroop%20Jun%2C%20Heewoo%20Satheesh%2C%20Sanjeev%20Coates%2C%20Adam%20Cold%20fusion%3A%20Training%20seq2seq%20models%20together%20with%20language%20models%202018"
        },
        {
            "id": "Sun_et+al_2017_a",
            "entry": "Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. arXiv preprint arXiv:1703.01030, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01030"
        },
        {
            "id": "Wen_2018_a",
            "entry": "Wen Sun, J Andrew Bagnell, and Byron Boots. Truncated horizon policy search: Combining reinforcement learning & imitation learning. arXiv preprint arXiv:1805.11240, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11240"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to Sequence Learning with Neural Networks. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%202014"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "Tjandra_et+al_2018_a",
            "entry": "Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Sequence-to-Sequence ASR Optimization via Reinforcement Learning. ICASSP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tjandra%2C%20Andros%20Sakti%2C%20Sakriani%20Nakamura%2C%20Satoshi%20Sequence-to-Sequence%20ASR%20Optimization%20via%20Reinforcement%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tjandra%2C%20Andros%20Sakti%2C%20Sakriani%20Nakamura%2C%20Satoshi%20Sequence-to-Sequence%20ASR%20Optimization%20via%20Reinforcement%20Learning%202018"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaswani%2C%20Ashish%20Shazeer%2C%20Noam%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20and%20Illia%20Polosukhin%202017"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a Foreign Language. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Kaiser%2C%20Lukasz%20Koo%2C%20Terry%20Petrov%2C%20Slav%20Grammar%20as%20a%20Foreign%20Language%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Kaiser%2C%20Lukasz%20Koo%2C%20Terry%20Petrov%2C%20Slav%20Grammar%20as%20a%20Foreign%20Language%202015"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. Switchout: an efficient data augmentation algorithm for neural machine translation. EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xinyi%20Pham%2C%20Hieu%20Dai%2C%20Zihang%20Neubig%2C%20Graham%20Switchout%3A%20an%20efficient%20data%20augmentation%20algorithm%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xinyi%20Pham%2C%20Hieu%20Dai%2C%20Zihang%20Neubig%2C%20Graham%20Switchout%3A%20an%20efficient%20data%20augmentation%20algorithm%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "Wiseman_2016_a",
            "entry": "Sam Wiseman and Alexander M. Rush. Sequence-to-Sequence Learning as Beam-Search Optimization. EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-Sequence%20Learning%20as%20Beam-Search%20Optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-Sequence%20Learning%20as%20Beam-Search%20Optimization%202016"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20Attend%20and%20Tell%3A%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20Attend%20and%20Tell%3A%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%202015"
        },
        {
            "id": "Zeyer_et+al_2018_a",
            "entry": "Albert Zeyer, Kazuki Irie, Ralf Schl\u00fcter, and Hermann Ney. Improved training of end-to-end attention models for speech recognition. arXiv preprint arXiv:1805.03294, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.03294"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Yu Zhang, William Chan, and Navdeep Jaitly. Very Deep Convolutional Networks for End-to-End Speech Recognition. ICASSP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yu%20Chan%2C%20William%20Jaitly%2C%20Navdeep%20Very%20Deep%20Convolutional%20Networks%20for%20End-to-End%20Speech%20Recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yu%20Chan%2C%20William%20Jaitly%2C%20Navdeep%20Very%20Deep%20Convolutional%20Networks%20for%20End-to-End%20Speech%20Recognition%202017"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning. arXiv:1709.00103, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.00103"
        }
    ]
}
