{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING",
        "author": "Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Hk4fpoA5Km"
        },
        "abstract": "Algorithms for imitation learning based on adversarial optimization, such as generative adversarial imitation learning (GAIL) and adversarial inverse reinforcement learning (AIRL), can effectively mimic demonstrated behaviours by employing both reward and reinforcement learning (RL). However, applications of such algorithms are challenged by the inherent instability and poor sample efficiency of on-policy RL. In particular, the inadequate handling of absorbing states in canonical implementations of RL environments causes an implicit bias in reward functions used by these algorithms. While these biases might work well for some environments, they lead to sub-optimal behaviors in others. Moreover, despite the ability of these algorithms to learn from a few demonstrations, they require a prohibitively large number of the environment interactions for many real-world applications. To address these issues, we first propose to extend the environment MDP with absorbing states which leads to task-independent, and more importantly, unbiased rewards. Secondly, we introduce an off-policy learning algorithm, which we refer to as Discriminator-Actor-Critic. We demonstrate the effectiveness of proper handling of absorbing states, while empirically improving the sample efficiency by an average factor of 10. Our implementation is available online 1."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "Virtual Reality",
            "url": "https://en.wikipedia.org/wiki/Virtual_Reality"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "actor critic",
            "url": "https://en.wikipedia.org/wiki/actor_critic"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "robotics",
            "url": "https://en.wikipedia.org/wiki/robotics"
        }
    ],
    "abbreviations": {
        "GAIL": "Generative Adversarial Imitation Learning",
        "AIRL": "adversarial inverse reinforcement learning",
        "RL": "reinforcement learning",
        "MDP": "Markov Decision Process",
        "BC": "Behavioral Cloning",
        "IRL": "Inverse Reinforcement Learning",
        "SAM": "Sample-efficient Adversarial Mimic",
        "DDPG": "Deep Deterministic Policy Gradients",
        "GANs": "Generative Adversarial Networks",
        "VR": "Virtual Reality"
    },
    "highlights": [
        "The Adversarial Imitation Learning (AIL) class of algorithms learns a policy that robustly imitates an expert\u2019s actions via a collection of expert demonstrations, an adversarial discriminator and a reinforcement learning method",
        "First we propose a new algorithm, which we call Discriminator-Actor-Critic (DAC) (Figure 1), that is compatible with the Generative Adversarial Imitation Learning and adversarial inverse reinforcement learning frameworks by extending them with an off-policy discriminator and an off-policy actor-critic reinforcement learning algorithm",
        "We experimentally demonstrate that this removes the bias due to incorrect absorbing state handling in both Generative Adversarial Imitation Learning-like and adversarial inverse reinforcement learning-like variants of our DAC algorithm",
        "Following Syed et al (2008), and by treating imitation learning as an occupancy matching problem, <a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon (2016</a>) proposed a Generative Adversarial Imitation Learning (GAIL) framework for learning a policy from demonstrations, which bypasses the need to recover the expert\u2019s reward function",
        "In this work we address several important issues associated with the popular Generative Adversarial Imitation Learning framework",
        "We show that our algorithm reaches state-of-theart performance for an imitation learning algorithm on several standard reinforcement learning benchmarks, and is able to recover the expert policy given a significantly smaller number of samples than in recent Generative Adversarial Imitation Learning work"
    ],
    "key_statements": [
        "The Adversarial Imitation Learning (AIL) class of algorithms learns a policy that robustly imitates an expert\u2019s actions via a collection of expert demonstrations, an adversarial discriminator and a reinforcement learning method",
        "The Adversarial Inverse Reinforcement Learning (AIRL) algorithm (<a class=\"ref-link\" id=\"cFu_et+al_2017_a\" href=\"#rFu_et+al_2017_a\">Fu et al, 2017</a>) makes use of a modified Generative Adversarial Imitation Learning discriminator to recover a reward function to perform Inverse Reinforcement Learning (IRL) (Abbeel & Ng, 2004). This subsequent dense reward is robust to changes in dynamics or environment properties",
        "While Generative Adversarial Imitation Learning requires as little as 200 expert frame transitions to learn a robust reward function on most MuJoCo (<a class=\"ref-link\" id=\"cTodorov_et+al_2012_a\" href=\"#rTodorov_et+al_2012_a\">Todorov et al, 2012</a>) tasks, the number of policy frame transitions sampled from the environment can be as high as 25 million in order to reach convergence",
        "In this work we address this issue by incorporating an off-policy reinforcement learning algorithm (TD3 (<a class=\"ref-link\" id=\"cFujimoto_et+al_2018_a\" href=\"#rFujimoto_et+al_2018_a\">Fujimoto et al, 2018</a>)) and an off-policy discriminator to dramatically decrease the sample complexity by orders of magnitude",
        "First we propose a new algorithm, which we call Discriminator-Actor-Critic (DAC) (Figure 1), that is compatible with the Generative Adversarial Imitation Learning and adversarial inverse reinforcement learning frameworks by extending them with an off-policy discriminator and an off-policy actor-critic reinforcement learning algorithm",
        "We experimentally demonstrate that this removes the bias due to incorrect absorbing state handling in both Generative Adversarial Imitation Learning-like and adversarial inverse reinforcement learning-like variants of our DAC algorithm",
        "We demonstrate that DAC achieves state-of-the-art AIL performance for a number of difficult imitation learning tasks, where proper handling of terminal states is crucial for matching expert performance in the presence of absorbing states",
        "Following Syed et al (2008), and by treating imitation learning as an occupancy matching problem, <a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon (2016</a>) proposed a Generative Adversarial Imitation Learning (GAIL) framework for learning a policy from demonstrations, which bypasses the need to recover the expert\u2019s reward function",
        "We consider problems that satisfy the definition of a Markov Decision Process (MDP), formalized by the tuple: (S, A, p(s), p(s |s, a), r(s, a, s ), \u03b3)",
        "In order to use the same notation for tasks with absorbing states, whose finite length episodes end when reaching a terminal state, we can define a set of absorbing states sa (Sutton et al, 1998) that an agent enters after the end of episode, has zero reward and transitions to itself for all agent actions: sa \u223c p(\u00b7|sT , aT ), r = 0 and sa \u223c p(\u00b7|sa, \u00b7)",
        "In the top-left plot, we show DAC is an order of magnitude more sample efficent than TRPO and PPO based Generative Adversarial Imitation Learning baselines",
        "In this work we address several important issues associated with the popular Generative Adversarial Imitation Learning framework",
        "We show that our algorithm reaches state-of-theart performance for an imitation learning algorithm on several standard reinforcement learning benchmarks, and is able to recover the expert policy given a significantly smaller number of samples than in recent Generative Adversarial Imitation Learning work"
    ],
    "summary": [
        "The Adversarial Imitation Learning (AIL) class of algorithms learns a policy that robustly imitates an expert\u2019s actions via a collection of expert demonstrations, an adversarial discriminator and a reinforcement learning method.",
        "The Generative Adversarial Imitation Learning (GAIL) algorithm (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon, 2016</a>) uses a discriminator reward and a policy gradient algorithm to imitate an expert RL policy.",
        "While GAIL requires as little as 200 expert frame transitions to learn a robust reward function on most MuJoCo (<a class=\"ref-link\" id=\"cTodorov_et+al_2012_a\" href=\"#rTodorov_et+al_2012_a\">Todorov et al, 2012</a>) tasks, the number of policy frame transitions sampled from the environment can be as high as 25 million in order to reach convergence.",
        "Following Syed et al (2008), and by treating imitation learning as an occupancy matching problem, <a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho & Ermon (2016</a>) proposed a Generative Adversarial Imitation Learning (GAIL) framework for learning a policy from demonstrations, which bypasses the need to recover the expert\u2019s reward function.",
        "We present examples of bias present in implementations of different AIL algorithms as they assign zero rewards to absorbing states:",
        "Under the implicit assumption of zero rewards for absorbing states in the MDP implementation, this strictly positive estimator cannot recover the true reward function for environments where an agent is required to solve the task as quickly as possible.",
        "We propose a method to handle absorbing states of the standard benchmark MDPs in such a way that AIL algorithms are able to recover different reward functions without adjusting the form of reward estimator.",
        "In order to resolve the issues described in Section 4.1, we suggest explicitly learning rewards for absorbing states for expert demonstrations and trajectories produced by a policy.",
        "When using a fixed and untrained GAIL discriminator that outputs 0.5 for every state-action pair, we were able to reach episode rewards of around 1000 on the Hopper environment, corresponding to approximately one third of the expert performance.",
        "Alternative reward functions do not have this positive bias but still require proper handling of the absorbing states as well in order to avoid early termination due to incorrectly assigned per-frame penalty.",
        "We address 1) sample inefficiency with respect to policy transitions in the environment and 2) we demonstrate a number of reward biases that can either implicitly impose prior knowledge about the true reward, or alternatively, prevent the policy from imitating the optimal expert.",
        "We show that our algorithm reaches state-of-theart performance for an imitation learning algorithm on several standard RL benchmarks, and is able to recover the expert policy given a significantly smaller number of samples than in recent GAIL work."
    ],
    "headline": "We introduce an off-policy learning algorithm, which we refer to as Discriminator-Actor-Critic",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20Proceedings%20of%20the%20twentyfirst%20international%20conference%20on%20Machine%20learning%20pp%201%20ACM%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pieter%20Abbeel%20and%20Andrew%20Y%20Ng%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%20In%20Proceedings%20of%20the%20twentyfirst%20international%20conference%20on%20Machine%20learning%20pp%201%20ACM%202004"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Bain_1999_a",
            "entry": "Michael Bain and Claude Sommut. A framework for behavioural cloning. Machine intelligence, 15 (15):103, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bain%2C%20Michael%20Sommut%2C%20Claude%20A%20framework%20for%20behavioural%20cloning.%20Machine%20intelligence%2C%2015%201999"
        },
        {
            "id": "Baram_et+al_2017_a",
            "entry": "Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end differentiable adversarial imitation learning. In International Conference on Machine Learning, pp. 390\u2013399, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baram%2C%20Nir%20Anschel%2C%20Oron%20Caspi%2C%20Itai%20Mannor%2C%20Shie%20End-to-end%20differentiable%20adversarial%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baram%2C%20Nir%20Anschel%2C%20Oron%20Caspi%2C%20Itai%20Mannor%2C%20Shie%20End-to-end%20differentiable%20adversarial%20imitation%20learning%202017"
        },
        {
            "id": "Kalousis_2018_a",
            "entry": "Lionel Blondeand Alexandros Kalousis. Sample-efficient imitation learning via generative adversarial nets. arXiv preprint arXiv:1809.02064, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.02064"
        },
        {
            "id": "Coumans_2016_a",
            "entry": "E Coumans and Y Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. GitHub repository, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coumans%2C%20E.%20Bai%2C%20Y.%20Pybullet%2C%20a%20python%20module%20for%20physics%20simulation%20for%20games%2C%20robotics%20and%20machine%20learning.%20GitHub%20repository%202016"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. NIPS Workshop on Adversarial Training, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Christiano%2C%20Paul%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20A%20connection%20between%20generative%20adversarial%20networks%2C%20inverse%20reinforcement%20learning%2C%20and%20energy-based%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Christiano%2C%20Paul%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20A%20connection%20between%20generative%20adversarial%20networks%2C%20inverse%20reinforcement%20learning%2C%20and%20energy-based%20models%202016"
        },
        {
            "id": "Finn_et+al_0000_a",
            "entry": "Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pp. 49\u201358, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "Fujimoto_et+al_2018_a",
            "entry": "Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09477"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767\u20135777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Haarnoja_et+al_0000_a",
            "entry": "Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02808"
        },
        {
            "id": "Haarnoja_et+al_0000_b",
            "entry": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "Henderson_et+al_0000_a",
            "entry": "Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, and Doina Precup. Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning. arXiv preprint arXiv:1709.06683, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06683"
        },
        {
            "id": "Henderson_et+al_0000_b",
            "entry": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06560"
        },
        {
            "id": "Hester_et+al_2017_a",
            "entry": "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, et al. Deep q-learning from demonstrations. arXiv preprint arXiv:1704.03732, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03732"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Kang_et+al_2018_a",
            "entry": "Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2469\u20132478. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kang%2C%20Bingyi%20Jie%2C%20Zequn%20Feng%2C%20Jiashi%20Policy%20optimization%20with%20demonstrations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kang%2C%20Bingyi%20Jie%2C%20Zequn%20Feng%2C%20Jiashi%20Policy%20optimization%20with%20demonstrations%202018"
        },
        {
            "id": "Kee-Eung_2018_a",
            "entry": "Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KeeEung%20Kim%20and%20Hyun%20Soo%20Park%20Imitation%20learning%20via%20kernel%20mean%20embedding%20AAAI%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KeeEung%20Kim%20and%20Hyun%20Soo%20Park%20Imitation%20learning%20via%20kernel%20mean%20embedding%20AAAI%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Le_et+al_2018_a",
            "entry": "Hoang M Le, Nan Jiang, Alekh Agarwal, Miroslav Dud\u0131k, Yisong Yue, and Hal Daume III. Hierarchical imitation and reinforcement learning. arXiv preprint arXiv:1803.00590, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00590"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Lucic_et+al_2017_a",
            "entry": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "Nair_et+al_2017_a",
            "entry": "Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10089"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th International Conf. on Machine Learning, pp. 663\u2013670. Morgan Kaufmann, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Pardo_et+al_2017_a",
            "entry": "Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement learning. arXiv preprint arXiv:1712.00378, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00378"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Peters_2008_a",
            "entry": "Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682\u2013697, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008"
        },
        {
            "id": "Ratliff_2006_a",
            "entry": "Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning, pp. 729\u2013736. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nathan%20D%20Ratliff%20J%20Andrew%20Bagnell%20and%20Martin%20A%20Zinkevich%20Maximum%20margin%20planning%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20729736%20ACM%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nathan%20D%20Ratliff%20J%20Andrew%20Bagnell%20and%20Martin%20A%20Zinkevich%20Maximum%20margin%20planning%20In%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%20pp%20729736%20ACM%202006"
        },
        {
            "id": "Reddy_et+al_2019_a",
            "entry": "Siddharth Reddy, Anca D. Dragan, and Sergey Levine. What would pi* do?: Imitation learning via off-policy reinforcement learning, 2019. URL https://openreview.net/forum?id= B1excoAqKQ.",
            "url": "https://openreview.net/forum?id="
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05770"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Sasaki_et+al_2019_a",
            "entry": "Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample efficient imitation learning for continuous control. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=BkN5UoAqF7.",
            "url": "https://openreview.net/forum?id=BkN5UoAqF7",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sasaki%2C%20Fumihiro%20Yohira%2C%20Tetsuya%20Kawaguchi%2C%20Atsuo%20Sample%20efficient%20imitation%20learning%20for%20continuous%20control%202019"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "Syed_1039_a",
            "entry": "Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th international conference on Machine learning, pp. 1032\u2013 1039. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Umar%20Syed%20Michael%20Bowling%20and%20Robert%20E%20Schapire%20Apprenticeship%20learning%20using%20linear%20programming%20In%20Proceedings%20of%20the%2025th%20international%20conference%20on%20Machine%20learning%20pp%201032%201039%20ACM%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Umar%20Syed%20Michael%20Bowling%20and%20Robert%20E%20Schapire%20Apprenticeship%20learning%20using%20linear%20programming%20In%20Proceedings%20of%20the%2025th%20international%20conference%20on%20Machine%20learning%20pp%201032%201039%20ACM%202008"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Torabi_et+al_2018_a",
            "entry": "Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06158"
        },
        {
            "id": "Tucker_et+al_2018_a",
            "entry": "George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E Turner, Zoubin Ghahramani, and Sergey Levine. The mirage of action-dependent baselines in reinforcement learning. arXiv preprint arXiv:1802.10031, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10031"
        },
        {
            "id": "Vecer_et+al_2017_a",
            "entry": "Matej Vecer\u0131k, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothorl, Thomas Lampe, and Martin A Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. CoRR, abs/1707.08817, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08817"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, pp. 5320\u20135329, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017"
        },
        {
            "id": "Zhu_et+al_2018_a",
            "entry": "Published as a conference paper at ICLR 2019 Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, Janos Kramar, Raia Hadsell, Nando de Freitas, et al. Reinforcement and imitation learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433\u20131438.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09564"
        },
        {
            "id": "Chicago_2008_a",
            "entry": "Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chicago%20IL%20USA%202008"
        }
    ]
}
