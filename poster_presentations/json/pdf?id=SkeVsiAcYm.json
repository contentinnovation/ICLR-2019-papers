{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "GENERATIVE PREDECESSOR MODELS FOR SAMPLE-",
        "author": "EFFICIENT IMITATION LEARNING",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkeVsiAcYm"
        },
        "abstract": "We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a stateof-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "stationary distribution",
            "url": "https://en.wikipedia.org/wiki/stationary_distribution"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "IRL": "inverse reinforcement learning",
        "GAIL": "Generative Adversarial Imitation Learning",
        "SAIL": "State Aware Imitation Learning"
    },
    "highlights": [
        "Training or programming agents to act intelligently in unstructured and sequential environments is a difficult and central challenge in the field of artificial intelligence",
        "We propose a novel imitation learning algorithm, Generative Predecessor Models for Imitation Learning (GPRIL), based on a simple core insight: Augmenting the training set with state-action pairs that are likely to eventually lead the agent to states demonstrated by the expert is an effective way to train corrective behavior and to prevent accumulating errors",
        "We report final results after convergence and can see in figure 1b that both Generative Adversarial Imitation Learning and GPRIL outperform behavioral cloning, indicating that generalizing over state-action trajectories requires fewer demonstrations than generalizing over actions alone",
        "While sample efficiency of Generative Adversarial Imitation Learning could be increased by decreasing batch size or increasing various learning rates, we found that this can lead to unstable learning performance while reducing the amount of samples required by only a small amount",
        "We introduced GPRIL, a novel algorithm for imitation learning which uses generative models to model multi-step predecessor distributions and to perform state-action distribution matching",
        "We show that the algorithm compares favorably with state-of-the-art imitation learning methods, achieving higher or equivalent performance while requiring several orders of magnitude fewer environment samples"
    ],
    "key_statements": [
        "Training or programming agents to act intelligently in unstructured and sequential environments is a difficult and central challenge in the field of artificial intelligence",
        "Imitation learning provides an avenue to tackle this challenge by allowing agents to learn from human teachers, which constitutes a natural way for experts to describe the desired behavior and provides an efficient learning signal for the agent",
        "Key challenges in the field are diverse and include questions such as how to learn from observations alone (e.g. <a class=\"ref-link\" id=\"cAytar_et+al_2018_a\" href=\"#rAytar_et+al_2018_a\">Aytar et al (2018</a>)), learning the correspondence between the expert\u2019s demonstrations and the agent\u2019s observations (e.g. <a class=\"ref-link\" id=\"cSermanet_et+al_2017_a\" href=\"#rSermanet_et+al_2017_a\">Sermanet et al (2017</a>)) as well as the question of how to integrate imitation learning with other approaches such as reinforcement learning (e.g. <a class=\"ref-link\" id=\"cVecerik_et+al_2017_a\" href=\"#rVecerik_et+al_2017_a\">Vecerik et al (2017</a>))",
        "At the core of the imitation learning problems lies the challenge of utilizing a given set of demonstrations to match the expert\u2019s behavior as closely as possible. We approach this problem considering the setting where the set of expert demonstrations is given up-front and the dynamics of the environment can only be observed through interaction",
        "We propose a novel imitation learning algorithm, Generative Predecessor Models for Imitation Learning (GPRIL), based on a simple core insight: Augmenting the training set with state-action pairs that are likely to eventually lead the agent to states demonstrated by the expert is an effective way to train corrective behavior and to prevent accumulating errors",
        "Together these properties are sufficient to allow GPRIL to be applied in real-world settings, which we demonstrate in section 4.3",
        "In section 1, we provided an intuitive framework for using predecessor models to augment our training set and achieve robust imitation learning from few samples",
        "We report final results after convergence and can see in figure 1b that both Generative Adversarial Imitation Learning and GPRIL outperform behavioral cloning, indicating that generalizing over state-action trajectories requires fewer demonstrations than generalizing over actions alone",
        "While sample efficiency of Generative Adversarial Imitation Learning could be increased by decreasing batch size or increasing various learning rates, we found that this can lead to unstable learning performance while reducing the amount of samples required by only a small amount",
        "Observation and action space are identical with the exception of the omission of torques from the observation space as they are not necessary to solve this task. We use this task to evaluate the performance of Generative Adversarial Imitation Learning and GPRIL when learning from only a very limited set of demonstrated states",
        "We find that in the first two scenarios, this holds for Generative Adversarial Imitation Learning as well as can been in figure 2b while in the third case, Generative Adversarial Imitation Learning becomes highly unstable and the resulting performance can vary wildly, leading to a low average success rate",
        "We introduced GPRIL, a novel algorithm for imitation learning which uses generative models to model multi-step predecessor distributions and to perform state-action distribution matching",
        "We show that the algorithm compares favorably with state-of-the-art imitation learning methods, achieving higher or equivalent performance while requiring several orders of magnitude fewer environment samples"
    ],
    "summary": [
        "Training or programming agents to act intelligently in unstructured and sequential environments is a difficult and central challenge in the field of artificial intelligence.",
        "We propose a novel imitation learning algorithm, Generative Predecessor Models for Imitation Learning (GPRIL), based on a simple core insight: Augmenting the training set with state-action pairs that are likely to eventually lead the agent to states demonstrated by the expert is an effective way to train corrective behavior and to prevent accumulating errors.",
        "We use predecessor models to derive a principled approach to state-distribution matching and propose the following imitation learning loop: 1.",
        "Methods have been proposed that aim to match state-action distributions directly and achieve state-of-the-art result without learning a reward function first.",
        "We use generative models to model the distribution of long-term predecessor state-action pairs.",
        "We will show that the samples drawn from a long-term predecessor distribution conditioned on s enable us to estimate the gradient of the logarithmic state distribution \u2207\u03b8 log d\u03c0\u03b8 (s) and, later, to match the agent\u2019s state-action-distribution to that of the expert.",
        "We derived the gradient of the logarithm of the stationary state distribution as approximately proportional to the expected gradient of the log policy, evaluated at samples obtained from the long-term predecessor distribution B\u03c0\u03b8 .",
        "We propose to train a model B\u03c9\u03c0\u03b8 to represent B\u03c0\u03b8 and use its samples to estimate \u2207\u03b8 log d\u03c0\u03b8 (s).",
        "State-action distribution matching has been a promising approach to sample-efficient and robust imitation learning.",
        "Where \u2207\u03b8 log \u03c0\u03b8(a|s) can be computed directly by taking the gradient of the policy using the demonstrated state-action pairs and \u2207\u03b8 log d\u03c0\u03b8 (s) can be evaluated using samples drawn from B\u03c9\u03c0\u03b8 (\u00b7, \u00b7|s) according to equation 7.",
        "We use a range of robotic insertion tasks similar to the domains introduced by <a class=\"ref-link\" id=\"cVecerik_et+al_2017_a\" href=\"#rVecerik_et+al_2017_a\">Vecerik et al (2017</a>) but without access to a reward signal or, in some cases, expert actions.",
        "We use this task to evaluate the performance of GAIL and GPRIL when learning from only a very limited set of demonstrated states.",
        "We introduced GPRIL, a novel algorithm for imitation learning which uses generative models to model multi-step predecessor distributions and to perform state-action distribution matching.",
        "We show that the algorithm compares favorably with state-of-the-art imitation learning methods, achieving higher or equivalent performance while requiring several orders of magnitude fewer environment samples.",
        "Stability and sample-efficiency of GPRIL are sufficient to enable experiments on a real robot, which we demonstrated on a peg-insertion task with a variable-position socket."
    ],
    "headline": "We propose Generative Predecessor Models for Imitation Learning, a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states",
    "reference_links": [
        {
            "id": "Akgun_et+al_2012_a",
            "entry": "Akgun, B., M. Cakmak, J. W. Yoo, and A. L. Thomaz 2012. Trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective. In International Conference on Human-Robot Interaction, Pp. 391\u2013398.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akgun%20B%20M%20Cakmak%20J%20W%20Yoo%20and%20A%20L%20Thomaz%202012%20Trajectories%20and%20keyframes%20for%20kinesthetic%20teaching%20a%20humanrobot%20interaction%20perspective%20In%20International%20Conference%20on%20HumanRobot%20Interaction%20Pp%20391398",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akgun%20B%20M%20Cakmak%20J%20W%20Yoo%20and%20A%20L%20Thomaz%202012%20Trajectories%20and%20keyframes%20for%20kinesthetic%20teaching%20a%20humanrobot%20interaction%20perspective%20In%20International%20Conference%20on%20HumanRobot%20Interaction%20Pp%20391398"
        },
        {
            "id": "Aytar_et+al_2018_a",
            "entry": "Aytar, Y., T. Pfaff, D. Budden, T. L. Paine, Z. Wang, and N. de Freitas 2018. Playing hard exploration games by watching YouTube. arXiv preprint arXiv:1805.11592.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11592"
        },
        {
            "id": "Boularias_2011_a",
            "entry": "Boularias, A., J. Kober, and J. Peters 2011. Relative entropy inverse reinforcement learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, Pp. 182\u2013189.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boularias%2C%20A.%2C%20J.%20Kober%202011%2C%20J.%20Peters%20Relative%20entropy%20inverse%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boularias%2C%20A.%2C%20J.%20Kober%202011%2C%20J.%20Peters%20Relative%20entropy%20inverse%20reinforcement%20learning%202011"
        },
        {
            "id": "Chernova_2014_a",
            "entry": "Chernova, S. and A. L. Thomaz 2014. Robot learning from human teachers. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8(3):1\u2013121.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chernova%20S%20and%20A%20L%20Thomaz%202014%20Robot%20learning%20from%20human%20teachers%20Synthesis%20Lectures%20on%20Artificial%20Intelligence%20and%20Machine%20Learning%20831121",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chernova%20S%20and%20A%20L%20Thomaz%202014%20Robot%20learning%20from%20human%20teachers%20Synthesis%20Lectures%20on%20Artificial%20Intelligence%20and%20Machine%20Learning%20831121"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Dinh, L., J. Sohl-Dickstein, and S. Bengio 2016. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Edwards_et+al_2018_a",
            "entry": "Edwards, A. D., L. Downs, and J. C. Davidson 2018. Forward-backward reinforcement learning. arXiv preprint arXiv:1803.10227.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10227"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Finn, C., S. Levine, and P. Abbeel 2016. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, Pp. 49\u201358.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%20C%20S%20Levine%20and%20P%20Abbeel%202016%20Guided%20cost%20learning%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%20In%20International%20Conference%20on%20Machine%20Learning%20Pp%204958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%20C%20S%20Levine%20and%20P%20Abbeel%202016%20Guided%20cost%20learning%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%20In%20International%20Conference%20on%20Machine%20Learning%20Pp%204958"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Fu, J., K. Luo, and S. Levine 2018. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%20J%20K%20Luo%20and%20S%20Levine%202018%20Learning%20robust%20rewards%20with%20adverserial%20inverse%20reinforcement%20learning%20In%20International%20Conference%20on%20Learning%20Representations",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%20J%20K%20Luo%20and%20S%20Levine%202018%20Learning%20robust%20rewards%20with%20adverserial%20inverse%20reinforcement%20learning%20In%20International%20Conference%20on%20Learning%20Representations"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Germain, M., K. Gregor, I. Murray, and H. Larochelle 2015. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, Pp. 881\u2013889.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%20M%20K%20Gregor%20I%20Murray%20and%20H%20Larochelle%202015%20Made%20Masked%20autoencoder%20for%20distribution%20estimation%20In%20International%20Conference%20on%20Machine%20Learning%20Pp%20881889",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%20M%20K%20Gregor%20I%20Murray%20and%20H%20Larochelle%202015%20Made%20Masked%20autoencoder%20for%20distribution%20estimation%20In%20International%20Conference%20on%20Machine%20Learning%20Pp%20881889"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems, Pp. 2672\u20132680.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%20I%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%202014%20Generative%20adversarial%20nets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2026722680",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%20I%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%202014%20Generative%20adversarial%20nets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2026722680"
        },
        {
            "id": "Goyal_et+al_2018_a",
            "entry": "Goyal, A., P. Brakel, W. Fedus, T. Lillicrap, S. Levine, H. Larochelle, and Y. Bengio 2018. Recall traces: Backtracking models for efficient reinforcement learning. arXiv preprint arXiv:1804.00379.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00379"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Ho, J. and S. Ermon 2016. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, Pp. 4565\u20134573.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%20J%20and%20S%20Ermon%202016%20Generative%20adversarial%20imitation%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2045654573",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%20J%20and%20S%20Ermon%202016%20Generative%20adversarial%20imitation%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2045654573"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Kingma, D. P. and M. Welling 2013. Auto-encoding variational bayes. CoRR, abs/1312.6114.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mirza, M. and S. Osindero 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Morimura_et+al_2010_a",
            "entry": "Morimura, T., E. Uchibe, J. Yoshimoto, J. Peters, and K. Doya 2010. Derivatives of logarithmic stationary distributions for policy gradient reinforcement learning. Neural computation, 22(2):342\u2013376.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morimura%20T%20E%20Uchibe%20J%20Yoshimoto%20J%20Peters%20and%20K%20Doya%202010%20Derivatives%20of%20logarithmic%20stationary%20distributions%20for%20policy%20gradient%20reinforcement%20learning%20Neural%20computation%20222342376",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morimura%20T%20E%20Uchibe%20J%20Yoshimoto%20J%20Peters%20and%20K%20Doya%202010%20Derivatives%20of%20logarithmic%20stationary%20distributions%20for%20policy%20gradient%20reinforcement%20learning%20Neural%20computation%20222342376"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Ng, A. and S. Russell 2000. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, Pp. 663\u2013670.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.%202000%2C%20S.%20Russell%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20A.%202000%2C%20S.%20Russell%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Odena_et+al_2017_a",
            "entry": "Odena, A., C. Olah, and J. Shlens 2017. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning, Pp. 2642\u20132651.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20A.%20Olah%2C%20C.%20J.%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20A.%20Olah%2C%20C.%20J.%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "Pan_et+al_2018_a",
            "entry": "Pan, Y., M. Zaheer, A. White, A. Patterson, and M. White 2018. Organizing experience: a deeper look at replay mechanisms for sample-based planning in continuous state domains. In International Joint Conference on Artificial Intelligence, Pp. 4794\u2013 4800.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%20Y%20M%20Zaheer%20A%20White%20A%20Patterson%20and%20M%20White%202018%20Organizing%20experience%20a%20deeper%20look%20at%20replay%20mechanisms%20for%20samplebased%20planning%20in%20continuous%20state%20domains%20In%20International%20Joint%20Conference%20on%20Artificial%20Intelligence%20Pp%204794%204800",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%20Y%20M%20Zaheer%20A%20White%20A%20Patterson%20and%20M%20White%202018%20Organizing%20experience%20a%20deeper%20look%20at%20replay%20mechanisms%20for%20samplebased%20planning%20in%20continuous%20state%20domains%20In%20International%20Joint%20Conference%20on%20Artificial%20Intelligence%20Pp%204794%204800"
        },
        {
            "id": "Papamakarios_et+al_2017_a",
            "entry": "Papamakarios, G., I. Murray, and T. Pavlakou 2017. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, Pp. 2335\u20132344.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20G.%20Murray%2C%20I.%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20G.%20Murray%2C%20I.%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017"
        },
        {
            "id": "Pathak_et+al_2018_a",
            "entry": "Pathak, D., P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell 2018. Zero-shot visual imitation. In International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Mahmoudieh%2C%20P.%20Luo%2C%20G.%20Agrawal%2C%20P.%20Zero-shot%20visual%20imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Mahmoudieh%2C%20P.%20Luo%2C%20G.%20Agrawal%2C%20P.%20Zero-shot%20visual%20imitation%202018"
        },
        {
            "id": "Peng_1993_a",
            "entry": "Peng, J. and R. J. Williams 1993. Efficient learning and planning within the dyna framework. Adaptive Behavior, 1(4):437\u2013 454.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20J.%201993%2C%20R.%20J.%20Williams%20Efficient%20learning%20and%20planning%20within%20the%20dyna%20framework%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20J.%201993%2C%20R.%20J.%20Williams%20Efficient%20learning%20and%20planning%20within%20the%20dyna%20framework%201993"
        },
        {
            "id": "Peng_et+al_2018_b",
            "entry": "Peng, X. B., P. Abbeel, S. Levine, and M. van de Panne 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Pomerleau_1989_a",
            "entry": "Pomerleau, D. A. 1989. Alvinn: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems, Pp. 305\u2013313.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pomerleau%2C%20D.A.%20Alvinn%3A%20An%20autonomous%20land%20vehicle%20in%20a%20neural%20network%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pomerleau%2C%20D.A.%20Alvinn%3A%20An%20autonomous%20land%20vehicle%20in%20a%20neural%20network%201989"
        },
        {
            "id": "Ross_2010_a",
            "entry": "Ross, S. and D. Bagnell 2010. Efficient reductions for imitation learning. In International Conference on Artificial Intelligence and Statistics, Pp. 661\u2013668.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%20S%20and%20D%20Bagnell%202010%20Efficient%20reductions%20for%20imitation%20learning%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20Pp%20661668",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%20S%20and%20D%20Bagnell%202010%20Efficient%20reductions%20for%20imitation%20learning%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20Pp%20661668"
        },
        {
            "id": "Ross_et+al_2011_b",
            "entry": "Ross, S., G. Gordon, and D. Bagnell 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, Pp. 627\u2013635.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%20S%20G%20Gordon%20and%20D%20Bagnell%202011%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20noregret%20online%20learning%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20Pp%20627635",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%20S%20G%20Gordon%20and%20D%20Bagnell%202011%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20noregret%20online%20learning%20In%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20Pp%20627635"
        },
        {
            "id": "Schroecker_et+al_2016_a",
            "entry": "Schroecker, Y., H. Ben Amor, and A. Thomaz 2016. Directing policy search with interactively taught via-points. In International Conference on Autonomous Agents & Multiagent Systems, Pp. 1052\u20131059.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroecker%20Y%20H%20Ben%20Amor%20and%20A%20Thomaz%202016%20Directing%20policy%20search%20with%20interactively%20taught%20viapoints%20In%20International%20Conference%20on%20Autonomous%20Agents%20%20Multiagent%20Systems%20Pp%2010521059",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroecker%20Y%20H%20Ben%20Amor%20and%20A%20Thomaz%202016%20Directing%20policy%20search%20with%20interactively%20taught%20viapoints%20In%20International%20Conference%20on%20Autonomous%20Agents%20%20Multiagent%20Systems%20Pp%2010521059"
        },
        {
            "id": "Schroecker_2017_b",
            "entry": "Schroecker, Y. and C. L. Isbell 2017. State aware imitation learning. In Advances in Neural Information Processing Systems, Pp. 2915\u20132924.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroecker%20Y%20and%20C%20L%20Isbell%202017%20State%20aware%20imitation%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2029152924",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroecker%20Y%20and%20C%20L%20Isbell%202017%20State%20aware%20imitation%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2029152924"
        },
        {
            "id": "Sermanet_et+al_2017_a",
            "entry": "Sermanet, P., C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine 2017. Time-contrastive networks: self-supervised learning from video. arXiv preprint arXiv:1704.06888.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06888"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Sutton, R. S. and A. G. Barto 1998. Reinforcement learning: An introduction. MIT press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%20R%20S%20and%20A%20G%20Barto%201998%20Reinforcement%20learning%20An%20introduction%20MIT%20press"
        },
        {
            "id": "Sutton_et+al_1999_b",
            "entry": "Sutton, R. S., D. A. McAllester, S. P. Singh, Y. Mansour, and Others 1999. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In Advances in Neural Information Processing Systems, Pp. 1057\u20131063.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%20R%20S%20D%20A%20McAllester%20S%20P%20Singh%20Y%20Mansour%20and%20Others%201999%20Policy%20Gradient%20Methods%20for%20Reinforcement%20Learning%20with%20Function%20Approximation%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2010571063",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%20R%20S%20D%20A%20McAllester%20S%20P%20Singh%20Y%20Mansour%20and%20Others%201999%20Policy%20Gradient%20Methods%20for%20Reinforcement%20Learning%20with%20Function%20Approximation%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20Pp%2010571063"
        },
        {
            "id": "Tsitsiklis_1997_a",
            "entry": "Tsitsiklis, J. N. and B. Van Roy 1997. Analysis of temporal-diffference learning with function approximation. In Advances in Neural Information Processing Systems, Pp. 1075\u20131081.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20Analysis%20of%20temporal-diffference%20learning%20with%20function%20approximation%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20Analysis%20of%20temporal-diffference%20learning%20with%20function%20approximation%201997"
        },
        {
            "id": "Van_et+al_0000_a",
            "entry": "Van Den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu 2016a. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Van_et+al_0000_b",
            "entry": "Van Den Oord, A., N. Kalchbrenner, and K. Kavukcuoglu 2016b. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Vecerik_et+al_2017_a",
            "entry": "Vecerik, M., T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Rothorl, T. Lampe, and M. A. Riedmiller 2017. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. CoRR, abs/1707.08817.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08817"
        },
        {
            "id": "Zhu_et+al_2018_a",
            "entry": "Zhu, Y., Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tunyasuvunakool, J. Kramar, R. Hadsell, N. de Freitas, et al. 2018. Reinforcement and imitation learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09564"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Ziebart, B. D., A. L. Maas, J. A. Bagnell, and A. K. Dey 2008. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, Pp. 1433\u20131438.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%20B%20D%20A%20L%20Maas%20J%20A%20Bagnell%20and%20A%20K%20Dey%202008%20Maximum%20entropy%20inverse%20reinforcement%20learning%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%20Pp%2014331438",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%20B%20D%20A%20L%20Maas%20J%20A%20Bagnell%20and%20A%20K%20Dey%202008%20Maximum%20entropy%20inverse%20reinforcement%20learning%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%20Pp%2014331438"
        }
    ]
}
