{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SGD CONVERGES TO GLOBAL MINIMUM IN DEEP LEARNING VIA STAR-CONVEX PATH",
        "author": "Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang\u00a7, Vahid Tarokh, \u2217Duke University, \u2020University of Science and Technology of China \u2021Microsoft Research, Asia, \u00a7The Ohio State University",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BylIciRcYQ"
        },
        "abstract": "Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper. In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent",
        "MLP": "multi-layer perceptron"
    },
    "highlights": [
        "Training neural networks has been proven to be NP-hard decades ago <a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\"><a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\">Blum & Rivest (1988</a></a>)",
        "We focus on the empirical observation that stochastic gradient descent can train various neural networks to achieve zero loss, and validate that stochastic gradient descent follows an epochwise star-convex path in empirical optimization processes",
        "The underlying technical reason is that the common global minimizer structure in Observation 1 suppresses the randomness induced by sampling and reshuffling of stochastic gradient descent, and ensures a common direction along which stochastic gradient descent can approach the global minimum on all individual data samples",
        "We propose an epochwise star-convex property of the optimization path of stochastic gradient descent, which we validate in various experiments",
        "Based on such a property, we show that stochastic gradient descent approaches a global minimum at an epoch level",
        "We further examine the property at an iteration level, and empirically show that it is satisfied in a major part of training processes. Such a more refined property guarantees the convergence of stochastic gradient descent to a global minimum, and the algorithm enjoys a self-variance-reducing effect"
    ],
    "key_statements": [
        "Training neural networks has been proven to be NP-hard decades ago <a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\"><a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\">Blum & Rivest (1988</a></a>)",
        "We focus on the empirical observation that stochastic gradient descent can train various neural networks to achieve zero loss, and validate that stochastic gradient descent follows an epochwise star-convex path in empirical optimization processes",
        "Based on such a property, we prove that the Euclidean distance between the variable sequence generated by stochastic gradient descent and a global minimizer decreases at an epoch level",
        "We validate that stochastic gradient descent follows an iterationwise star-convex path during the major part of the training process. Based on such a property, we prove that the entire variable sequence generated by stochastic gradient descent converges to a global minimizer of the objective function",
        "Our results provide a novel and promising aspect to understand stochastic gradient descent-based optimization in deep learning",
        "We introduce the following fact that is widely observed in training overparameterized deep neural networks, which we assume to hold throughout the paper",
        "We provide empirical observations on the algorithm path of stochastic gradient descent in training neural networks",
        "In the two subsections, we first propose a property that the algorithm path of stochastic gradient descent satisfies, based on which we formally prove that the variable sequence generated by stochastic gradient descent admits the behavior observed in Figure 1",
        "The underlying technical reason is that the common global minimizer structure in Observation 1 suppresses the randomness induced by sampling and reshuffling of stochastic gradient descent, and ensures a common direction along which stochastic gradient descent can approach the global minimum on all individual data samples",
        "We conduct further experiments to demonstrate that stochastic gradient descent follows the iterationwise star-convex path likely only for successful trainings to zero loss value, where a shared global minimum among all individual loss functions is achieved",
        "The results are shown in Figure 4, from which we observe that the training loss converges to a non-zero value when the number of hidden neurons is small, implying that the algorithm likely attains a sub-optimal point which is not a common global minimum shared by all individual loss functions",
        "We propose an epochwise star-convex property of the optimization path of stochastic gradient descent, which we validate in various experiments",
        "Based on such a property, we show that stochastic gradient descent approaches a global minimum at an epoch level",
        "We further examine the property at an iteration level, and empirically show that it is satisfied in a major part of training processes. Such a more refined property guarantees the convergence of stochastic gradient descent to a global minimum, and the algorithm enjoys a self-variance-reducing effect"
    ],
    "summary": [
        "Training neural networks has been proven to be NP-hard decades ago <a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\"><a class=\"ref-link\" id=\"cBlum_1988_a\" href=\"#rBlum_1988_a\">Blum & Rivest (1988</a></a>).",
        "The fact that SGD can train neural networks to zero loss value implies that the non-negative loss functions on all data samples share a common global minimum.",
        "We focus on the empirical observation that SGD can train various neural networks to achieve zero loss, and validate that SGD follows an epochwise star-convex path in empirical optimization processes.",
        "The property of epochwise star-convex path of SGD is sufficient to explain such desirable empirical observations, the loss function can be highly nonconvex and has complex landscape.",
        "We first define a notion of an iterationwise star-convex path for SGD, based on which we formally establish the convergence of SGD to a global minimizer.",
        "Theorem 3 formally establishes the convergence of SGD to a global minimizer along an iterationwise star-convex path.",
        "Theorem 3 shows that SGD converges to a common global minimizer where the gradient of loss function on all data samples vanish, and we obtain the following interesting corollary.",
        "In all subfigures of Figure 3, the red curves denote the training loss and the blue bars denote the fraction of iterations that satisfy the iterationwise star-convexity within such an epoch.",
        "It can be seen from Figure 3 that, for all three networks on the MNIST dataset, the path of SGD satisfies iterationwise star-convexity for most of the iterations, except in the last several epochs where the training loss already well saturates at zero value.",
        "On the CIFAR10 dataset, we observe a strong evidence for the iterationwise star-convex path of SGD after several epochs of the initial phase of training.",
        "We conduct further experiments to demonstrate that SGD follows the iterationwise star-convex path likely only for successful trainings to zero loss value, where a shared global minimum among all individual loss functions is achieved.",
        "The results are shown in Figure 4, from which we observe that the training loss converges to a non-zero value when the number of hidden neurons is small, implying that the algorithm likely attains a sub-optimal point which is not a common global minimum shared by all individual loss functions.",
        "We observe that the corresponding SGD paths have much fewer iterations satisfying the iterationwise star-convexity compared to the successful training instances shown in Figure 3.",
        "Such empirical findings partially suggest that iterationwise star-convex SGD path more likely occurs when SGD can find a common global minimum, e.g., training overparameterized networks to zero loss value.",
        "We believe that our study sheds light on the success of SGD in training neural networks from both empirical aspect and theoretical aspect"
    ],
    "headline": "We focus on the empirical observation that stochastic gradient descent can train various neural networks to achieve zero loss, and validate that stochastic gradient descent follows an epochwise star-convex path in empirical optimization processes",
    "reference_links": [
        {
            "id": "Blum_1988_a",
            "entry": "A. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. In Proc. 1st Annual Workshop on Computational Learning Theory (COLT), pp. 9\u201318, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Rivest%2C%20R.L.%20Training%20a%203-node%20neural%20network%20is%20NP-complete%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Rivest%2C%20R.L.%20Training%20a%203-node%20neural%20network%20is%20NP-complete%201988"
        },
        {
            "id": "Bottou_et+al_1606_a",
            "entry": "L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. ArXiv: 1606.04838, June 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04838"
        },
        {
            "id": "Daneshmand_et+al_2018_a",
            "entry": "H. Daneshmand, J. M. Kohler, A. Lucchi, and T. Hofmann. Escaping saddles with stochastic gradients. In Proc. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daneshmand%2C%20H.%20Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Hofmann%2C%20T.%20Escaping%20saddles%20with%20stochastic%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daneshmand%2C%20H.%20Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Hofmann%2C%20T.%20Escaping%20saddles%20with%20stochastic%20gradients%202018"
        },
        {
            "id": "Defazio_et+al_2014_a",
            "entry": "A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 1646\u20131654. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research (JMLR), 12:2121\u20132159, July 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011-07"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proc. 28th Conference on Learning Theory (COLT), volume 40, pp. 797\u2013842, 03\u201306 Jul 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015-07"
        },
        {
            "id": "Ghadimi_2016_a",
            "entry": "S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1):59\u201399, Mar 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016-03"
        },
        {
            "id": "Ghadimi_et+al_2016_b",
            "entry": "S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267\u2013305, Jan 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016-01"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. In Proc. 34th International Conference on Machine Learning (ICML), volume 70, pp. 1724\u20131732, Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017-08"
        },
        {
            "id": "Johnson_2013_a",
            "entry": "R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Proc. 26th International Conference on Neural Information Processing Systems (NIPS), pp. 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. Kingma and J. Ba. Adam: A method for stochastic optimization. Proc. International Conference on Learning Representations (ICLR), 12 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Kleinberg_et+al_2018_a",
            "entry": "B. Kleinberg, Y. Li, and Y. Yuan. An alternative view: When does SGD escape local minima? In Proc. 35th International Conference on Machine Learning (ICML), volume 80, pp. 2698\u20132707, Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20B.%20Li%2C%20Y.%20Yuan%2C%20Y.%20An%20alternative%20view%3A%20When%20does%20SGD%20escape%20local%20minima%3F%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20B.%20Li%2C%20Y.%20Yuan%2C%20Y.%20An%20alternative%20view%3A%20When%20does%20SGD%20escape%20local%20minima%3F%202018-07"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2017_a",
            "entry": "A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, May 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202017-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202017-05"
        },
        {
            "id": "Lan_2012_a",
            "entry": "G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133 (1):365\u2013397, Jun 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20G.%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012-06-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20G.%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012-06-01"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, Nov 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Q. Li, Y. Zhou, Y. Liang, and P. K. Varshney. Convergence analysis of proximal gradient with momentum for nonconvex optimization. In Proc. 34th International Conference on Machine Learning (ICML), volume 70, pp. 2111\u20132119, Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Q.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Varshney%2C%20P.K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Q.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Varshney%2C%20P.K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017-08"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "X. Li, S. Ling, T. Strohmer, and K. Wei. Rapid, robust, and reliable blind deconvolution via nonconvex optimization. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20X.%20Ling%2C%20S.%20Strohmer%2C%20T.%20Rapid%2C%20K.Wei%20and%20reliable%20blind%20deconvolution%20via%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20X.%20Ling%2C%20S.%20Strohmer%2C%20T.%20Rapid%2C%20K.Wei%20and%20reliable%20blind%20deconvolution%20via%20nonconvex%20optimization%202018"
        },
        {
            "id": "Linnainmaa_1976_a",
            "entry": "S. Linnainmaa. Taylor expansion of the accumulated rounding error. Numerical Mathematics, 16: 146\u2013160, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linnainmaa%2C%20S.%20Taylor%20expansion%20of%20the%20accumulated%20rounding%20error%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linnainmaa%2C%20S.%20Taylor%20expansion%20of%20the%20accumulated%20rounding%20error%201976"
        },
        {
            "id": "Moulines_2011_a",
            "entry": "E. Moulines and F. R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Proc. Advances in Neural Information Processing Systems (NIPS), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moulines%2C%20E.%20Bach%2C%20F.R.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moulines%2C%20E.%20Bach%2C%20F.R.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011"
        },
        {
            "id": "Needell_et+al_2014_a",
            "entry": "D. Needell, R. Ward, and N. Srebro. Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. In Proc. Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Needell%2C%20D.%20Ward%2C%20R.%20Srebro%2C%20N.%20Stochastic%20gradient%20descent%2C%20weighted%20sampling%2C%20and%20the%20randomized%20Kaczmarz%20algorithm%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Needell%2C%20D.%20Ward%2C%20R.%20Srebro%2C%20N.%20Stochastic%20gradient%20descent%2C%20weighted%20sampling%2C%20and%20the%20randomized%20Kaczmarz%20algorithm%202014"
        },
        {
            "id": "Nemirovski_et+al_2009_a",
            "entry": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "Nesterov_2006_a",
            "entry": "Y. Nesterov and B. Polyak. Cubic regularization of Newton\u2019s method and its global performance. Mathematical Programming, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20Newton%E2%80%99s%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20Newton%E2%80%99s%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "Reddi_et+al_2018_a",
            "entry": "S. Reddi, M. Zaheer, S. Sra, B. Poczos, F. Bach, R. Salakhutdinov, and A. Smola. A generic approach for escaping saddle points. In Proc. 21st International Conference on Artificial Intelligence and Statistics (AISTATS), volume 84, pp. 1233\u20131242, Apr 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018-04"
        },
        {
            "id": "Reddi_et+al_2018_b",
            "entry": "S. J. Reddi, S. Kale, and S. Kumar. On the convergence of Adam and beyond. In Proc. International Conference on Learning Representations (ICLR), 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20Adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20Adam%20and%20beyond%202018"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400\u2013407, Sep 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951-09"
        },
        {
            "id": "Schmidt_et+al_2017_a",
            "entry": "M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1):83\u2013112, Mar 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017-03"
        },
        {
            "id": "Tu_et+al_2016_a",
            "entry": "S. Tu, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht. Low-rank solutions of linear matrix equations via Procrustes flow. In Proc. 33rd International Conference on Machine Learning (ICML), pp. 964\u2013973, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20S.%20Boczar%2C%20R.%20Simchowitz%2C%20M.%20Soltanolkotabi%2C%20M.%20Low-rank%20solutions%20of%20linear%20matrix%20equations%20via%20Procrustes%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20S.%20Boczar%2C%20R.%20Simchowitz%2C%20M.%20Soltanolkotabi%2C%20M.%20Low-rank%20solutions%20of%20linear%20matrix%20equations%20via%20Procrustes%20flow%202016"
        },
        {
            "id": "Wang_et+al_1810_a",
            "entry": "Z. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh. SpiderBoost: A class of faster variance-reduced algorithms for nonconvex optimization. ArXiv:1810.10690, October 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1810.10690"
        },
        {
            "id": "Wang_et+al_1802_a",
            "entry": "Z. Wang, Y. Zhou, Y. Liang, and G. Lan. Sample complexity of stochastic variance-reduced cubic regularization for nonconvex optimization. ArXiv:1802.07372v1, February 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07372v1"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In Proc. International Conference on Learning Representations (ICLR), 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "Zhang_et+al_2017_b",
            "entry": "H. Zhang, Y. Zhou, Y. Liang, and Y. Chi. A nonconvex approach for phase retrieval: reshaped Wirtinger flow and incremental algorithms. Journal of Machine Learning Research (JMLR), 18 (141):1\u201335, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Chi%2C%20Y.%20A%20nonconvex%20approach%20for%20phase%20retrieval%3A%20reshaped%20Wirtinger%20flow%20and%20incremental%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Chi%2C%20Y.%20A%20nonconvex%20approach%20for%20phase%20retrieval%3A%20reshaped%20Wirtinger%20flow%20and%20incremental%20algorithms%202017"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "K. Zhong, Z. Song, P. Jain, P. L. Bartlett, and I. S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In Proc. 34th International Conference on Machine Learning (ICML), volume 70, pp. 4140\u20134149, Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20K.%20Song%2C%20Z.%20Jain%2C%20P.%20Bartlett%2C%20P.L.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20K.%20Song%2C%20Z.%20Jain%2C%20P.%20Bartlett%2C%20P.L.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017-08"
        },
        {
            "id": "Zhou_2017_a",
            "entry": "Y. Zhou and Y. Liang. Characterization of gradient dominance and regularity conditions for neural networks. ArXiv:1710.06910v2, Oct 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06910v2"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "Y. Zhou and Y. Liang. Critical points of linear neural networks: Analytical forms and landscape properties. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Liang%2C%20Y.%20Critical%20points%20of%20linear%20neural%20networks%3A%20Analytical%20forms%20and%20landscape%20properties%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Liang%2C%20Y.%20Critical%20points%20of%20linear%20neural%20networks%3A%20Analytical%20forms%20and%20landscape%20properties%202018"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Y. Zhou, H. Zhang, and Y. Liang. Geometrical properties and accelerated gradient solvers of nonconvex phase retrieval. In Proc. 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 331\u2013335, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Zhang%2C%20H.%20Liang%2C%20Y.%20Geometrical%20properties%20and%20accelerated%20gradient%20solvers%20of%20nonconvex%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Zhang%2C%20H.%20Liang%2C%20Y.%20Geometrical%20properties%20and%20accelerated%20gradient%20solvers%20of%20nonconvex%20phase%20retrieval%202016"
        },
        {
            "id": "Zhou_et+al_2018_b",
            "entry": "Y. Zhou, Z. Wang, and Y. Liang. Convergence of cubic regularization for nonconvex optimization under KL property. In Proc. Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Wang%2C%20Z.%20Liang%2C%20Y.%20Convergence%20of%20cubic%20regularization%20for%20nonconvex%20optimization%20under%20KL%20property%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Wang%2C%20Z.%20Liang%2C%20Y.%20Convergence%20of%20cubic%20regularization%20for%20nonconvex%20optimization%20under%20KL%20property%202018"
        }
    ]
}
