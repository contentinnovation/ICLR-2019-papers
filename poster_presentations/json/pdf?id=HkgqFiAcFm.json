{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MARGINAL POLICY GRADIENTS: A UNIFIED FAMILY OF ESTIMATORS FOR BOUNDED ACTION SPACES WITH APPLICATIONS",
        "author": "Carson Eisenach, Haichuan Yang, Ji Liu, and Han Liu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkgqFiAcFm"
        },
        "abstract": "Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over Rd and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced whenever policies over bounded action spaces are modeled using distributions with unbounded support by applying a transformation T to the sampled action before execution in the environment. Recently, the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, but to date no variance reduced methods exist when the action is a direction, something often seen in RTS games. To this end we introduce the angular policy gradient (APG), a stochastic policy gradient method for directional control. With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task show that the APG estimator offers a substantial improvement over the standard policy gradient."
    },
    "keywords": [
        {
            "term": "real-time strategy",
            "url": "https://en.wikipedia.org/wiki/real-time_strategy"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "law of total variance",
            "url": "https://en.wikipedia.org/wiki/law_of_total_variance"
        },
        {
            "term": "discrete action",
            "url": "https://en.wikipedia.org/wiki/discrete_action"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "robotics",
            "url": "https://en.wikipedia.org/wiki/robotics"
        }
    ],
    "abbreviations": {
        "RTS": "real-time strategy",
        "RL": "reinforcement learning",
        "CAPG": "clipped action policy gradient",
        "APG": "angular policy gradient",
        "MOBA": "Multi-player Online Battle Arena",
        "MPG": "marginal policy gradients",
        "LOTV": "law of total variance"
    },
    "highlights": [
        "Recent work in deep reinforcement learning (RL) has achieved human level-control for complex tasks like Atari 2600 games and the ancient game of Go. <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al (2015</a></a>) show that it is possible to learn to play Atari 2600 games using end to end reinforcement learning",
        "Because the agent must learn a continuous parameter for each discrete action, a policy gradient method is a natural approach to an real-time strategy game",
        "Motivated by challenges found in complex control problems, we introduced a general family of variance reduced policy gradients estimators",
        "This view provides the first unified approach to problems where the environment only depends on the action through some transformation T , and we demonstrate that clipped action policy gradient and angular policy gradient are members of this family corresponding to different choices of T",
        "Because thorough experimental work has already been done for the clipped action policy gradient member of the family (<a class=\"ref-link\" id=\"cFujita_2018_a\" href=\"#rFujita_2018_a\">Fujita & Maeda, 2018</a>), confirming the benefits of marginal policy gradients estimators, we do not reproduce those results here",
        "At this time few reinforcement learning environments use directional actions, we anticipate the number will grow as reinforcement learning is applied to newer and increasingly complex tasks like Multi-player Online Battle Arena games where such action spaces are common"
    ],
    "key_statements": [
        "Recent work in deep reinforcement learning (RL) has achieved human level-control for complex tasks like Atari 2600 games and the ancient game of Go. <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al (2015</a></a>) show that it is possible to learn to play Atari 2600 games using end to end reinforcement learning",
        "Because the agent must learn a continuous parameter for each discrete action, a policy gradient method is a natural approach to an real-time strategy game",
        "We introduce the marginal policy gradients (MPG) family of estimators; this general class of estimators contains both angular policy gradient and clipped action policy gradient, and we present a unified analysis of the variance reduction properties of all such methods",
        "Motivated by challenges found in complex control problems, we introduced a general family of variance reduced policy gradients estimators",
        "This view provides the first unified approach to problems where the environment only depends on the action through some transformation T , and we demonstrate that clipped action policy gradient and angular policy gradient are members of this family corresponding to different choices of T",
        "We show that it can be applied to parametrized action spaces",
        "Because thorough experimental work has already been done for the clipped action policy gradient member of the family (<a class=\"ref-link\" id=\"cFujita_2018_a\" href=\"#rFujita_2018_a\">Fujita & Maeda, 2018</a>), confirming the benefits of marginal policy gradients estimators, we do not reproduce those results here",
        "Instead we focus on the case when T (a) = a/||a|| and demonstrate the effectiveness of the angular policy gradient approach on King of Glory and our own Platform2D-v1 environment",
        "At this time few reinforcement learning environments use directional actions, we anticipate the number will grow as reinforcement learning is applied to newer and increasingly complex tasks like Multi-player Online Battle Arena games where such action spaces are common",
        "We envision that our methods can be applied to autonomous vehicle, in particular quadcopter, control"
    ],
    "summary": [
        "Recent work in deep reinforcement learning (RL) has achieved human level-control for complex tasks like Atari 2600 games and the ancient game of Go. <a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al (2015</a></a>) show that it is possible to learn to play Atari 2600 games using end to end reinforcement learning.",
        "Because the agent must learn a continuous parameter for each discrete action, a policy gradient method is a natural approach to an RTS game.",
        "The current practice, despite most continuous control problems having bounded action spaces, is to use a Gaussian distribution to model the policy and apply a transformation T to the action a before execution in the environment.",
        "See Appendix B.5 for rigorous discussion of the construction of a distribution over parametrized action spaces and the corresponding policy gradient algorithms.",
        "In Section 2, we described a general setting in which a stochastic policy gradient theorem holds on a measure space (A, E, \u03bb) for a family of \u03bb-compatible probability measures, \u03a0 = {\u03c0(\u00b7, \u03b8|s) : \u03b8 \u2208 \u0398}.",
        "The implication of Theorem 4.6 is that if there is some information loss via a function T before the action interacts with the dynamics of the environment, one obtains a lower variance estimator of the gradient by replacing the density of \u03c0 with the density of T\u2217\u03c0 in the expression for the policy gradient.",
        "Denoting by fMV (a, \u03b8|s) and fAG(b, \u03b8|s) the corresponding multivariate and angular Gaussian densities, respectively, we state the results for this setting as Corollary 4.8.",
        "See Appendix B.5 for an in depth discussion of policy gradient methods for parametrized action spaces.",
        "A standard policy gradient approach for parametrized action spaces, and 2.",
        "A marginal policy gradient approach, adapted to the parametrized action space where",
        "1 task, the agent is trained to play as the hero Di Ren Jie and training occurs by competing with the game\u2019s internal AI, playing as Di Ren Jie. The bottom row of Figure 2 shows the results, and as before, the angular policy gradient outperforms the standard policy gradient by a significant margin both in terms of win percentage and cumulative discounted reward.",
        "Motivated by challenges found in complex control problems, we introduced a general family of variance reduced policy gradients estimators.",
        "This view provides the first unified approach to problems where the environment only depends on the action through some transformation T , and we demonstrate that CAPG and APG are members of this family corresponding to different choices of T .",
        "Instead we focus on the case when T (a) = a/||a|| and demonstrate the effectiveness of the angular policy gradient approach on King of Glory and our own Platform2D-v1 environment.",
        "At this time few RL environments use directional actions, we anticipate the number will grow as RL is applied to newer and increasingly complex tasks like MOBA games where such action spaces are common."
    ],
    "headline": "To this end we introduce the angular policy gradient, a stochastic policy gradient method for directional control",
    "reference_links": [
        {
            "id": "Asadi_et+al_2017_a",
            "entry": "Kavosh Asadi, Cameron Allen, Melrose Roderick, Abdel-Rahman Mohamed, George Konidaris, and Michael Littman. Mean Actor Critic, 2017. arXiv:1709.00503.",
            "arxiv_url": "https://arxiv.org/pdf/1709.00503"
        },
        {
            "id": "Blackwell_1947_a",
            "entry": "David Blackwell. Conditional expectation and unbiased sequential estimation. Annals of Mathematical Statistics, 18(1):105\u2013110, 1947.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blackwell%2C%20David%20Conditional%20expectation%20and%20unbiased%20sequential%20estimation%201947",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blackwell%2C%20David%20Conditional%20expectation%20and%20unbiased%20sequential%20estimation%201947"
        },
        {
            "id": "Chang_1997_a",
            "entry": "J T Chang and D Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):287\u2013317, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20J.T.%20Pollard%2C%20D.%20Conditioning%20as%20disintegration%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20J.T.%20Pollard%2C%20D.%20Conditioning%20as%20disintegration%201997"
        },
        {
            "id": "Chou_et+al_2017_a",
            "entry": "Po-Wei Chou, Daniel Maturana, and Sebastian Scherer. Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chou%2C%20Po-Wei%20Maturana%2C%20Daniel%20Scherer%2C%20Sebastian%20Improving%20Stochastic%20Policy%20Gradients%20in%20Continuous%20Control%20with%20Deep%20Reinforcement%20Learning%20using%20the%20Beta%20Distribution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chou%2C%20Po-Wei%20Maturana%2C%20Daniel%20Scherer%2C%20Sebastian%20Improving%20Stochastic%20Policy%20Gradients%20in%20Continuous%20Control%20with%20Deep%20Reinforcement%20Learning%20using%20the%20Beta%20Distribution%202017"
        },
        {
            "id": "Ciosek_2018_a",
            "entry": "Kamil Ciosek and Shimon Whiteson. Expected Policy Gradients for Reinforcement Learning, 2018. arXiv:1801.03326.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03326"
        },
        {
            "id": "Fellows_et+al_2018_a",
            "entry": "Matthew Fellows, Kamil Ciosek, and Shimon Whiteson. Fourier Policy Gradients. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fellows%2C%20Matthew%20Ciosek%2C%20Kamil%20Whiteson%2C%20Shimon%20Fourier%20Policy%20Gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fellows%2C%20Matthew%20Ciosek%2C%20Kamil%20Whiteson%2C%20Shimon%20Fourier%20Policy%20Gradients%202018"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic Neural Networks for Hierarchical Reinforcement Learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Florensa%2C%20Carlos%20Duan%2C%20Yan%20Abbeel%2C%20Pieter%20Stochastic%20Neural%20Networks%20for%20Hierarchical%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Florensa%2C%20Carlos%20Duan%2C%20Yan%20Abbeel%2C%20Pieter%20Stochastic%20Neural%20Networks%20for%20Hierarchical%20Reinforcement%20Learning%202017"
        },
        {
            "id": "Foerster_et+al_2016_a",
            "entry": "Jakob N Foerster, Yannis M Assael, Nando De Freitas, and Shimon Whiteson. Learning to Communicate with Deep Multi-Agent Reinforcement Learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Assael%2C%20Yannis%20M.%20Freitas%2C%20Nando%20De%20Whiteson%2C%20Shimon%20Learning%20to%20Communicate%20with%20Deep%20Multi-Agent%20Reinforcement%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Assael%2C%20Yannis%20M.%20Freitas%2C%20Nando%20De%20Whiteson%2C%20Shimon%20Learning%20to%20Communicate%20with%20Deep%20Multi-Agent%20Reinforcement%20Learning%202016"
        },
        {
            "id": "Fujita_2018_a",
            "entry": "Yasuhiro Fujita and Shin-Ichi Maeda. Clipped Action Policy Gradient. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujita%2C%20Yasuhiro%20Maeda%2C%20Shin-Ichi%20Clipped%20Action%20Policy%20Gradient%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fujita%2C%20Yasuhiro%20Maeda%2C%20Shin-Ichi%20Clipped%20Action%20Policy%20Gradient%202018"
        },
        {
            "id": "Greensmith_et+al_2004_a",
            "entry": "Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning. Journal of Machine Learning Research, 5:1471\u20131530, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greensmith%2C%20Evan%20Bartlett%2C%20Peter%20L.%20Baxter%2C%20Jonathan%20Variance%20Reduction%20Techniques%20for%20Gradient%20Estimates%20in%20Reinforcement%20Learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greensmith%2C%20Evan%20Bartlett%2C%20Peter%20L.%20Baxter%2C%20Jonathan%20Variance%20Reduction%20Techniques%20for%20Gradient%20Estimates%20in%20Reinforcement%20Learning%202004"
        },
        {
            "id": "Hausknecht_2016_a",
            "entry": "Matthew Hausknecht and Peter Stone. Deep Reinforcement Learning In Parameterized Action Space. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthew%20Hausknecht%20and%20Peter%20Stone%20Deep%20Reinforcement%20Learning%20In%20Parameterized%20Action%20Space%20In%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthew%20Hausknecht%20and%20Peter%20Stone%20Deep%20Reinforcement%20Learning%20In%20Parameterized%20Action%20Space%20In%20ICLR%202016"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Lei%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015"
        },
        {
            "id": "Klambauer_et+al_2017_a",
            "entry": "Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-Normalizing Neural Networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunter%20Klambauer%20Thomas%20Unterthiner%20Andreas%20Mayr%20and%20Sepp%20Hochreiter%20SelfNormalizing%20Neural%20Networks%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunter%20Klambauer%20Thomas%20Unterthiner%20Andreas%20Mayr%20and%20Sepp%20Hochreiter%20SelfNormalizing%20Neural%20Networks%20In%20NIPS%202017"
        },
        {
            "id": "Masson_et+al_2016_a",
            "entry": "Warwick Masson, Pravesh Ranchod, and George Konidaris. Reinforcement Learning with Parameterized Actions. In AAAI, 2016. ISBN 9781577357605.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masson%2C%20Warwick%20Ranchod%2C%20Pravesh%20Konidaris%2C%20George%20Reinforcement%20Learning%20with%20Parameterized%20Actions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Masson%2C%20Warwick%20Ranchod%2C%20Pravesh%20Konidaris%2C%20George%20Reinforcement%20Learning%20with%20Parameterized%20Actions%202016"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, (518):529\u2013533, 2015. doi: 10.1038/nature14236.",
            "crossref": "https://dx.doi.org/10.1038/nature14236",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14236"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P Lillicrap, David Silver, Koray Kavukcuoglu, Korayk@google Com, and Google Deepmind. Asynchronous Methods for Deep Reinforcement Learning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Paine_et+al_2018_a",
            "entry": "P. J. Paine, S. P. Preston, M. Tsagris, and Andrew T. A. Wood. An elliptically symmetric angular Gaussian distribution. Statistics and Computing, 28:689\u2013697, 2018. doi: 10.1007/s11222-017-9756-4.",
            "crossref": "https://dx.doi.org/10.1007/s11222-017-9756-4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11222-017-9756-4"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Gregory Chanan, Zeming Lin, Sam Gross, Edward Yang, Luca Antiga, and Zachary Devito. Automatic differentiation in PyTorch. In NIPS Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Gregory%20Chanan%20Zeming%20Lin%20Sam%20Gross%20Edward%20Yang%20Luca%20Antiga%20and%20Zachary%20Devito%20Automatic%20differentiation%20in%20PyTorch%20In%20NIPS%20Workshop%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Gregory%20Chanan%20Zeming%20Lin%20Sam%20Gross%20Edward%20Yang%20Luca%20Antiga%20and%20Zachary%20Devito%20Automatic%20differentiation%20in%20PyTorch%20In%20NIPS%20Workshop%202017"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region Policy Optimization. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%20Schulman%20Sergey%20Levine%20Philipp%20Moritz%20Michael%20Jordan%20and%20Pieter%20Abbeel%20Trust%20Region%20Policy%20Optimization%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%20Schulman%20Sergey%20Levine%20Philipp%20Moritz%20Michael%20Jordan%20and%20Pieter%20Abbeel%20Trust%20Region%20Policy%20Optimization%20In%20ICML%202015"
        },
        {
            "id": "Schulman_et+al_2016_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Moritz%2C%20Philipp%20Levine%2C%20Sergey%20Jordan%2C%20Michael%20High-Dimensional%20Continuous%20Control%20Using%20Generalized%20Advantage%20Estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Moritz%2C%20Philipp%20Levine%2C%20Sergey%20Jordan%2C%20Michael%20High-Dimensional%20Continuous%20Control%20Using%20Generalized%20Advantage%20Estimation%202016"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov Openai. Proximal Policy Optimization Algorithms, 2017. arXiv:1707.06347.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Silver_et+al_2014_a",
            "entry": "David Silver, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic Policy Gradient Algorithms. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Wierstra%2C%20Daan%20Deterministic%20Policy%20Gradient%20Algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Wierstra%2C%20Daan%20Deterministic%20Policy%20Gradient%20Algorithms%202014"
        },
        {
            "id": "Sutton_2018_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: an introduction. 2018. ISBN 0262193981. doi: 10.1109/TNN.1998.712192.",
            "crossref": "https://dx.doi.org/10.1109/TNN.1998.712192"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "Richard S Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In NIPS, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Mcallester%2C%20David%20Singh%2C%20Satinder%20Mansour%2C%20Yishay%20Policy%20Gradient%20Methods%20for%20Reinforcement%20Learning%20with%20Function%20Approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Mcallester%2C%20David%20Singh%2C%20Satinder%20Mansour%2C%20Yishay%20Policy%20Gradient%20Methods%20for%20Reinforcement%20Learning%20with%20Function%20Approximation%202000"
        },
        {
            "id": "Tamar_et+al_2012_a",
            "entry": "Aviv Tamar, Dotan Di Castro, and Ron Meir. Integrating a partial model into model free reinforcement learning. Journal of Machine Learning Research, 13:1927\u20131966, 2012. ISSN 15324435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tamar%2C%20Aviv%20Castro%2C%20Dotan%20Di%20Meir%2C%20Ron%20Integrating%20a%20partial%20model%20into%20model%20free%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tamar%2C%20Aviv%20Castro%2C%20Dotan%20Di%20Meir%2C%20Ron%20Integrating%20a%20partial%20model%20into%20model%20free%20reinforcement%20learning%202012"
        },
        {
            "id": "Usunier_et+al_2017_a",
            "entry": "Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Usunier%2C%20Nicolas%20Synnaeve%2C%20Gabriel%20Lin%2C%20Zeming%20Chintala%2C%20Soumith%20Episodic%20Exploration%20for%20Deep%20Deterministic%20Policies%3A%20An%20Application%20to%20StarCraft%20Micromanagement%20Tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Usunier%2C%20Nicolas%20Synnaeve%2C%20Gabriel%20Lin%2C%20Zeming%20Chintala%2C%20Soumith%20Episodic%20Exploration%20for%20Deep%20Deterministic%20Policies%3A%20An%20Application%20to%20StarCraft%20Micromanagement%20Tasks%202017"
        },
        {
            "id": "Vinyals_et+al_2017_a",
            "entry": "Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Uttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado Van Hasselt, David Silver, Timothy Lillicrap, Deepmind Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing Blizzard. StarCraft II: A New Challenge for Reinforcement Learning, 2017. arXiv:1708.04782.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04782"
        },
        {
            "id": "We_2018_a",
            "entry": "We require a stochastic policy gradient theorem that can be applied to distributions on arbitrary measurable spaces in order to rigorously analyze the Marginal Policy Gradients framework. Let the notation be as in Section 2. The first ingredient is Proposition A.1, which gives a very general form of policy gradient, defined for an arbitrary probability measure. Proposition A.1. [Ciosek & Whiteson (2018)] Let \u03c0(\u00b7|s) be a probability measure on (A, E), then",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20require%20a%20stochastic%20policy%20gradient%20theorem%20that%20can%20be%20applied%20to%20distributions%20on%20arbitrary%20measurable%20spaces%20in%20order%20to%20rigorously%20analyze%20the%20Marginal%20Policy%20Gradients%20framework%20Let%20the%20notation%20be%20as%20in%20Section%202%20The%20first%20ingredient%20is%20Proposition%20A1%20which%20gives%20a%20very%20general%20form%20of%20policy%20gradient%20defined%20for%20an%20arbitrary%20probability%20measure%20Proposition%20A1%20Ciosek%20%20Whiteson%202018%20Let%20%CF%80s%20be%20a%20probability%20measure%20on%20A%20E%20then",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20require%20a%20stochastic%20policy%20gradient%20theorem%20that%20can%20be%20applied%20to%20distributions%20on%20arbitrary%20measurable%20spaces%20in%20order%20to%20rigorously%20analyze%20the%20Marginal%20Policy%20Gradients%20framework%20Let%20the%20notation%20be%20as%20in%20Section%202%20The%20first%20ingredient%20is%20Proposition%20A1%20which%20gives%20a%20very%20general%20form%20of%20policy%20gradient%20defined%20for%20an%20arbitrary%20probability%20measure%20Proposition%20A1%20Ciosek%20%20Whiteson%202018%20Let%20%CF%80s%20be%20a%20probability%20measure%20on%20A%20E%20then"
        },
        {
            "id": "This_2018_a",
            "entry": "This is an important step towards the form of stochastic policy gradient theorem we need in order to present our unified analysis that includes measures with uncountable support and also those which do not admit a density with respect to Lebesgue measure \u2013 something frequently encountered in practice. To obtain a stochastic policy gradient theorem from Proposition A.1 we simply need to replace \u2207v\u03c0(s) with an appropriate expression. As in Ciosek & Whiteson (2018), we need to be able to justify an interchange along the lines of",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=This%20is%20an%20important%20step%20towards%20the%20form%20of%20stochastic%20policy%20gradient%20theorem%20we%20need%20in%20order%20to%20present%20our%20unified%20analysis%20that%20includes%20measures%20with%20uncountable%20support%20and%20also%20those%20which%20do%20not%20admit%20a%20density%20with%20respect%20to%20Lebesgue%20measure%20%20something%20frequently%20encountered%20in%20practice%20To%20obtain%20a%20stochastic%20policy%20gradient%20theorem%20from%20Proposition%20A1%20we%20simply%20need%20to%20replace%20v%CF%80s%20with%20an%20appropriate%20expression%20As%20in%20Ciosek%20%20Whiteson%202018%20we%20need%20to%20be%20able%20to%20justify%20an%20interchange%20along%20the%20lines%20of"
        },
        {
            "id": "The_1997_a",
            "entry": "The definitions and propositions below are from Chang & Pollard (1997), which we include here for completeness. Let (A, E, \u03bb) be a measure space and (B, F) a measurable space. Let \u03bb be a \u03c3-finite measure on E and \u03bc be a \u03c3-finite measure on F. Definition A.2 ((T, \u03bc)-disintegration, Chang & Pollard (1997)). The measure \u03bb has a (T, \u03bc)disintegration, denoted {\u03bbb} if for all nonnegative measurable f on A",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20definitions%20and%20propositions%20below%20are%20from%20Chang%20%20Pollard%201997%20which%20we%20include%20here%20for%20completeness%20Let%20A%20E%20%CE%BB%20be%20a%20measure%20space%20and%20B%20F%20a%20measurable%20space%20Let%20%CE%BB%20be%20a%20%CF%83finite%20measure%20on%20E%20and%20%CE%BC%20be%20a%20%CF%83finite%20measure%20on%20F%20Definition%20A2%20T%20%CE%BCdisintegration%20Chang%20%20Pollard%201997%20The%20measure%20%CE%BB%20has%20a%20T%20%CE%BCdisintegration%20denoted%20%CE%BBb%20if%20for%20all%20nonnegative%20measurable%20f%20on%20A",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20definitions%20and%20propositions%20below%20are%20from%20Chang%20%20Pollard%201997%20which%20we%20include%20here%20for%20completeness%20Let%20A%20E%20%CE%BB%20be%20a%20measure%20space%20and%20B%20F%20a%20measurable%20space%20Let%20%CE%BB%20be%20a%20%CF%83finite%20measure%20on%20E%20and%20%CE%BC%20be%20a%20%CF%83finite%20measure%20on%20F%20Definition%20A2%20T%20%CE%BCdisintegration%20Chang%20%20Pollard%201997%20The%20measure%20%CE%BB%20has%20a%20T%20%CE%BCdisintegration%20denoted%20%CE%BBb%20if%20for%20all%20nonnegative%20measurable%20f%20on%20A"
        },
        {
            "id": "Proposition_1997_b",
            "entry": "Proposition A.3 (Existence, Chang & Pollard (1997)). Let A be a metric space, \u03bb be a \u03c3-finite",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proposition%20A3%20Existence%20Chang%20%20Pollard%201997%20Let%20A%20be%20a%20metric%20space%20%CE%BB%20be%20a%20%CF%83finite",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proposition%20A3%20Existence%20Chang%20%20Pollard%201997%20Let%20A%20be%20a%20metric%20space%20%CE%BB%20be%20a%20%CF%83finite"
        },
        {
            "id": "Proposition_1997_c",
            "entry": "Proposition A.4 (Chang & Pollard (1997)). Let \u03bb have a (T, \u03bc)-disintegration {\u03bbb}, and let \u03c1 be absolutely continuous with respect to \u03bb with a finite density r(a), where each of \u03bb, \u03bc and \u03c1 is \u03c3-finite. Then",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proposition%20A4%20Chang%20%20Pollard%201997%20Let%20%CE%BB%20have%20a%20T%20%CE%BCdisintegration%20%CE%BBb%20and%20let%20%CF%81%20be%20absolutely%20continuous%20with%20respect%20to%20%CE%BB%20with%20a%20finite%20density%20ra%20where%20each%20of%20%CE%BB%20%CE%BC%20and%20%CF%81%20is%20%CF%83finite%20Then",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proposition%20A4%20Chang%20%20Pollard%201997%20Let%20%CE%BB%20have%20a%20T%20%CE%BCdisintegration%20%CE%BBb%20and%20let%20%CF%81%20be%20absolutely%20continuous%20with%20respect%20to%20%CE%BB%20with%20a%20finite%20density%20ra%20where%20each%20of%20%CE%BB%20%CE%BC%20and%20%CF%81%20is%20%CF%83finite%20Then"
        },
        {
            "id": "Masson_2016_b",
            "entry": "Masson et al. (2016) gives a definition for a policy over parametrized action spaces, and our definition is the same in spirit, but for our purposes we need to be careful in formalizing the construction. Our construction here is also a bit more general.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masson%20gives%20a%20definition%20for%20a%20policy%20over%20parametrized%20action%20spaces%2C%20and%20our%20definition%20is%20the%20same%20in%20spirit%2C%20but%20for%20our%20purposes%20we%20need%20to%20be%20careful%20in%20formalizing%20the%20construction.%20Our%20construction%20here%20is%20also%20a%20bit%20more%20general%202016"
        },
        {
            "id": "2",
            "entry": "2. For k \u2208 [K]: specify \u03a0k = {\u03c0(\u00b7, \u03b8|s): \u03b8 \u2208 \u0398}, a \u03bck-compatible family of probability measures on (\u03a9k, Ek). Denote the corresponding densities by fk.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20k%20%20K%20specify%20%CE%A0k%20%20%CF%80%20%CE%B8s%20%CE%B8%20%20%CE%98%20a%20%CE%BCkcompatible%20family%20of%20probability%20measures%20on%20%CE%A9k%20Ek%20Denote%20the%20corresponding%20densities%20by%20fk"
        },
        {
            "id": "3",
            "entry": "3. Denote by \u03bc0 the counting measure on (A0, B(A0)) = (R, B(R)), and specify \u03a00 = {\u03c0(\u00b7, \u03b8|s): \u03b8 \u2208 \u0398} a \u03bc0-compatible family of probability measures, parametrized by \u03b80 and supported on [K]. Denote the corresponding density by f0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denote%20by%20%CE%BC0%20the%20counting%20measure%20on%20A0%20BA0%20%20R%20BR%20and%20specify%20%CE%A00%20%20%CF%80%20%CE%B8s%20%CE%B8%20%20%CE%98%20a%20%CE%BC0compatible%20family%20of%20probability%20measures%20parametrized%20by%20%CE%B80%20and%20supported%20on%20K%20Denote%20the%20corresponding%20density%20by%20f0"
        },
        {
            "id": "4",
            "entry": "4. Let \u03b8:= (\u03b8i)i, and define f\u03c0((k, \u03c9), \u03b8|s):= ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Let%20%CE%B8%20%CE%B8ii%20and%20define%20f%CF%80k%20%CF%89%20%CE%B8s",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Let%20%CE%B8%20%CE%B8ii%20and%20define%20f%CF%80k%20%CF%89%20%CE%B8s"
        }
    ]
}
