{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ADVERSARIAL REPROGRAMMING OF NEURAL NETWORKS",
        "author": "Gamaleldin F. Elsayed, Google Brain gamaleldin.elsayed@gmail.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Syx_Ss05tm"
        },
        "abstract": "Deep neural networks are susceptible to adversarial attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker\u2014without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary\u2014even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "specific output",
            "url": "https://en.wikipedia.org/wiki/specific_output"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks"
    },
    "highlights": [
        "The study of adversarial examples is often motivated in terms of the danger posed by an attacker whose goal is to cause model prediction errors with a small change to the model\u2019s input",
        "The majority of adversarial attacks have consisted of untargeted attacks that aim to degrade the performance of a model without necessarily requiring it to produce a specific output, or targeted attacks in which the attacker designs an adversarial perturbation to produce a specific output for that input",
        "We demonstrate the possibility of reprograming adversarial tasks with adversarial data that has no resemblance to original data, demonstrating that results from transfer learning do not fully explain adversarial reprogramming",
        "To demonstrate the feasibility of adversarial reprogramming, we conducted experiments on six architectures trained on ImageNet",
        "We examined whether adversarial training conferred resistance to adversarial reprogramming, and compared the susceptibility of trained networks to random networks",
        "We proposed a new class of adversarial attacks that aim to reprogram neural networks to perform novel adversarial tasks"
    ],
    "key_statements": [
        "The study of adversarial examples is often motivated in terms of the danger posed by an attacker whose goal is to cause model prediction errors with a small change to the model\u2019s input",
        "The majority of adversarial attacks have consisted of untargeted attacks that aim to degrade the performance of a model without necessarily requiring it to produce a specific output, or targeted attacks in which the attacker designs an adversarial perturbation to produce a specific output for that input",
        "We show that an adversary can accomplish this by learning adversarial reprogramming functions hf (\u00b7; \u03b8) and hg(\u00b7; \u03b8) that map between the two tasks",
        "We present the first instances of adversarial reprogramming",
        "In Section 3, we present a training procedure for crafting adversarial programs, which cause a neural network to perform a new task",
        "We demonstrate the possibility of reprograming adversarial tasks with adversarial data that has no resemblance to original data, demonstrating that results from transfer learning do not fully explain adversarial reprogramming",
        "The objective of the adversary is to reprogram the model to perform a new task by crafting an adversarial program to be included within the network input",
        "To demonstrate the feasibility of adversarial reprogramming, we conducted experiments on six architectures trained on ImageNet",
        "We examined whether adversarial training conferred resistance to adversarial reprogramming, and compared the susceptibility of trained networks to random networks",
        "We investigated the possibility of reprogramming the networks when the adversarial data has no resemblance to the original data",
        "Our results show that ImageNet networks can be successfully reprogramed to function as an MNIST classifier by presenting an additive adversarial program",
        "Our results show that adversarial reprogramming is still successful, yet with lower accuracy, even if we use a very small adversarial program",
        "Our results show that adversarial reprogramming is still successful (Figure 3b) even with nearly imperceptible programs",
        "We found that trained neural networks were more susceptible to adversarial reprogramming than random networks",
        "It is unclear whether the reduced performance when targeting random networks, and when reprogramming to perform CIFAR-10 classification, was due to limitations in the expressivity of the adversarial perturbation, or due to the optimization task in Equation 3 being more difficult in these situations",
        "We demonstrated adversarial reprogramming on classification tasks in the image domain",
        "We proposed a new class of adversarial attacks that aim to reprogram neural networks to perform novel adversarial tasks"
    ],
    "summary": [
        "The study of adversarial examples is often motivated in terms of the danger posed by an attacker whose goal is to cause model prediction errors with a small change to the model\u2019s input.",
        "In Section 3, we present a training procedure for crafting adversarial programs, which cause a neural network to perform a new task.",
        "These adversarial programs alter the network function from ImageNet classification to: counting squares in an image, classifying MNIST digits, and classifying CIFAR-10 images.",
        "The objective of the adversary is to reprogram the model to perform a new task by crafting an adversarial program to be included within the network input.",
        "To demonstrate the feasibility of adversarial reprogramming, we conducted experiments on six architectures trained on ImageNet. In each case, we reprogrammed the network to perform three different adversarial tasks: counting squares, MNIST classification, and CIFAR-10 classification.",
        "We trained one adversarial program per ImageNet model, such that the first 10 ImageNet labels represent the number of squares in each image (Figure 1c).",
        "Our results show that ImageNet networks can be successfully reprogramed to function as an MNIST classifier by presenting an additive adversarial program.",
        "We used the same experiment set up and MNIST reprogramming task as in Section 4.2 \u2013 we used the ImageNet models with randomly initialized rather than trained weights.",
        "Our results show that it is possible to reprogram neural networks to classify shuffled CIFAR-10 images (Table Supp.",
        "We used an Inception V3 model pretrained to classify ImageNet. In our first experiment, we adversarially reprogrammed the network to classify MNIST digits while limiting the size of the program (Figure 3a).",
        "Where X , M and W are as described in Section 3, PX is the adversarial data combined with the adversarial program, ix is the shuffling sequence, \u03b1 is a scalar used to limit the perturbation scale, and XImageNet is an image chosen randomly from ImageNet, which is the same for all MNIST examples.",
        "The resulting adversarial images are very similar to normal images from ImageNet, yet the network is successfully reprogrammed to classify MNIST digits, though with lower accuracy.",
        "It is unclear whether the reduced performance when targeting random networks, and when reprogramming to perform CIFAR-10 classification, was due to limitations in the expressivity of the adversarial perturbation, or due to the optimization task in Equation 3 being more difficult in these situations.",
        "We proposed a new class of adversarial attacks that aim to reprogram neural networks to perform novel adversarial tasks."
    ],
    "headline": "We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker\u2014without the attacker needing to specify or compute the desired output for each test-time input",
    "reference_links": [
        {
            "id": "Barabasi_et+al_2001_a",
            "entry": "Albert-Laszlo Barabasi, Vincent W Freeh, Hawoong Jeong, and Jay B Brockman. Parasitic computing. Nature, 412(6850):894, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barabasi%2C%20Albert-Laszlo%20Freeh%2C%20Vincent%20W.%20Jeong%2C%20Hawoong%20Brockman%2C%20Jay%20B.%20Parasitic%20computing%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barabasi%2C%20Albert-Laszlo%20Freeh%2C%20Vincent%20W.%20Jeong%2C%20Hawoong%20Brockman%2C%20Jay%20B.%20Parasitic%20computing%202001"
        },
        {
            "id": "Biggio_et+al_2013_a",
            "entry": "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III, pp. 387\u2013402, 2013. doi: 10.1007/978-3-642-40994-3_25.",
            "crossref": "https://dx.doi.org/10.1007/978-3-642-40994-3_25",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/978-3-642-40994-3_25"
        },
        {
            "id": "Bratus_et+al_2011_a",
            "entry": "Sergey Bratus, Michael Locasto, Meredith Patterson, Len Sassaman, and Anna Shubina. Exploit programming: From buffer overflows to weird machines and theory of computation. {USENIX; login:}, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bratus%2C%20Sergey%20Locasto%2C%20Michael%20Patterson%2C%20Meredith%20Sassaman%2C%20Len%20Exploit%20programming%3A%20From%20buffer%20overflows%20to%20weird%20machines%20and%20theory%20of%20computation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bratus%2C%20Sergey%20Locasto%2C%20Michael%20Patterson%2C%20Meredith%20Sassaman%2C%20Len%20Exploit%20programming%3A%20From%20buffer%20overflows%20to%20weird%20machines%20and%20theory%20of%20computation%202011"
        },
        {
            "id": "Brown_et+al_2017_a",
            "entry": "Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09665"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357, May 2017. doi: 10.1109/SP.2017.49.",
            "crossref": "https://dx.doi.org/10.1109/SP.2017.49",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/SP.2017.49"
        },
        {
            "id": "Donahue_et+al_2014_a",
            "entry": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647\u2013655, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014"
        },
        {
            "id": "Evtimov_et+al_2017_a",
            "entry": "Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945, 1, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "Ghorbani_et+al_2017_a",
            "entry": "Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. arXiv preprint arXiv:1710.10547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10547"
        },
        {
            "id": "Goodfellow_et+al_2017_a",
            "entry": "Ian Goodfellow, Nicolas Papernot, Sandy Huang, Yan Duan, Pieter Abbeel, and Jack Clark. Attacking machine learning with adversarial examples, 2017. URL https://blog.openai.com/adversarial-example-research/.",
            "url": "https://blog.openai.com/adversarial-example-research/"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Grosse_et+al_2017_a",
            "entry": "Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D. McDaniel. Adversarial examples for malware detection. In ESORICS 2017, pp. 62\u201379, 2017. doi: 10.1007/ 978-3-319-66399-9_4. URL https://doi.org/10.1007/978-3-319-66399-9_4.",
            "crossref": "https://dx.doi.org/10.1007/978-3-319-66399-9_4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/978-3-319-66399-9_4"
        },
        {
            "id": "He_2016_a",
            "entry": "Kun He, Yan Wang, and John Hopcroft. A powerful generative model using random weights for the deep image representation. In Advances in Neural Information Processing Systems, pp. 631\u2013639, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kun%20Wang%2C%20Yan%20and%20John%20Hopcroft.%20A%20powerful%20generative%20model%20using%20random%20weights%20for%20the%20deep%20image%20representation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kun%20Wang%2C%20Yan%20and%20John%20Hopcroft.%20A%20powerful%20generative%20model%20using%20random%20weights%20for%20the%20deep%20image%20representation%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "Kannan_et+al_2018_a",
            "entry": "Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06373"
        },
        {
            "id": "Kolter_2017_a",
            "entry": "J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "Kos_et+al_2017_a",
            "entry": "Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06832"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. ArXiv e-prints, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20A.%20Goodfellow%2C%20I.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale.%20ArXiv%20e-prints%202016-11"
        },
        {
            "id": "Le_et+al_2011_a",
            "entry": "Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y Ng. Ica with reconstruction cost for efficient overcomplete feature learning. In Advances in neural information processing systems, pp. 1017\u20131025, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Karpenko%2C%20Alexandre%20Ngiam%2C%20Jiquan%20and%20Andrew%20Y%20Ng.%20Ica%20with%20reconstruction%20cost%20for%20efficient%20overcomplete%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Karpenko%2C%20Alexandre%20Ngiam%2C%20Jiquan%20and%20Andrew%20Y%20Ng.%20Ica%20with%20reconstruction%20cost%20for%20efficient%20overcomplete%20feature%20learning%202011"
        },
        {
            "id": "Honglak_2009_a",
            "entry": "Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th annual international conference on machine learning, pp. 609\u2013616. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Honglak%20Lee%20Roger%20Grosse%20Rajesh%20Ranganath%20and%20Andrew%20Y%20Ng%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%20In%20Proceedings%20of%20the%2026th%20annual%20international%20conference%20on%20machine%20learning%20pp%20609616%20ACM%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Honglak%20Lee%20Roger%20Grosse%20Rajesh%20Ranganath%20and%20Andrew%20Y%20Ng%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%20In%20Proceedings%20of%20the%2026th%20annual%20international%20conference%20on%20machine%20learning%20pp%20609616%20ACM%202009"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00165"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. arXiv preprint arXiv:1804.08838, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08838"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06748"
        },
        {
            "id": "Lin_et+al_2015_a",
            "entry": "Zhouhan Lin, Roland Memisevic, and Kishore Konda. How far can we go without convolution: Improving fully-connected networks. arXiv preprint arXiv:1511.02580, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.02580"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "De_et+al_2018_a",
            "entry": "Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11271"
        },
        {
            "id": "Mesnil_et+al_2011_a",
            "entry": "Gr\u00e9goire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian Goodfellow, Erick Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, et al. Unsupervised and transfer learning challenge: a deep learning approach. In Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning workshop-Volume 27, pp. 97\u2013111. JMLR. org, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mesnil%2C%20Gr%C3%A9goire%20Dauphin%2C%20Yann%20Glorot%2C%20Xavier%20Rifai%2C%20Salah%20Unsupervised%20and%20transfer%20learning%20challenge%3A%20a%20deep%20learning%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mesnil%2C%20Gr%C3%A9goire%20Dauphin%2C%20Yann%20Glorot%2C%20Xavier%20Rifai%2C%20Salah%20Unsupervised%20and%20transfer%20learning%20challenge%3A%20a%20deep%20learning%20approach%202011"
        },
        {
            "id": "Minsky_1961_a",
            "entry": "Marvin L Minsky. Recursive unsolvability of post\u2019s problem of\" tag\" and other topics in theory of turing machines. Annals of Mathematics, pp. 437\u2013455, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minsky%2C%20Marvin%20L.%20Recursive%20unsolvability%20of%20post%E2%80%99s%20problem%20of%22%20tag%22%20and%20other%20topics%20in%20theory%20of%20turing%20machines%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minsky%2C%20Marvin%20L.%20Recursive%20unsolvability%20of%20post%E2%80%99s%20problem%20of%22%20tag%22%20and%20other%20topics%20in%20theory%20of%20turing%20machines%201961"
        },
        {
            "id": "Moosavi-Dezfooli_et+al_2017_a",
            "entry": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 86\u201394. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017"
        },
        {
            "id": "Neelakantan_et+al_2015_a",
            "entry": "Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04834"
        },
        {
            "id": "Novak_et+al_2019_a",
            "entry": "Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Dan Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. In ICLR, 2019. URL https://openreview.net/forum?id= B1g30j0qF7.",
            "url": "https://openreview.net/forum?id=",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20Roman%20Xiao%2C%20Lechao%20Lee%2C%20Jaehoon%20Bahri%2C%20Yasaman%20Bayesian%20deep%20convolutional%20networks%20with%20many%20channels%20are%20gaussian%20processes%202019"
        },
        {
            "id": "Papernot_et+al_2015_a",
            "entry": "Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. CoRR, abs/1511.07528, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07528"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Peresini_2013_a",
            "entry": "Peter Peresini and Dejan Kostic. Is the network capable of computation? In Network Protocols (ICNP), 2013 21st IEEE International Conference on, pp. 1\u20136. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peresini%2C%20Peter%20Kostic%2C%20Dejan%20Is%20the%20network%20capable%20of%20computation%3F%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peresini%2C%20Peter%20Kostic%2C%20Dejan%20Is%20the%20network%20capable%20of%20computation%3F%202013"
        },
        {
            "id": "Raghu_et+al_2016_a",
            "entry": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05336"
        },
        {
            "id": "Rajat_2007_a",
            "entry": "Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y Ng. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the 24th international conference on Machine learning, pp. 759\u2013766. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajat%20Raina%20Alexis%20Battle%20Honglak%20Lee%20Benjamin%20Packer%20and%20Andrew%20Y%20Ng%20Selftaught%20learning%20transfer%20learning%20from%20unlabeled%20data%20In%20Proceedings%20of%20the%2024th%20international%20conference%20on%20Machine%20learning%20pp%20759766%20ACM%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajat%20Raina%20Alexis%20Battle%20Honglak%20Lee%20Benjamin%20Packer%20and%20Andrew%20Y%20Ng%20Selftaught%20learning%20transfer%20learning%20from%20unlabeled%20data%20In%20Proceedings%20of%20the%2024th%20international%20conference%20on%20Machine%20learning%20pp%20759766%20ACM%202007"
        },
        {
            "id": "Razavian_et+al_2014_a",
            "entry": "Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pp. 512\u2013519. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20off-the-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20off-the-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014"
        },
        {
            "id": "Shazeer_et+al_2017_a",
            "entry": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06538"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Tensorflow-Slim_0000_a",
            "entry": "TensorFlow-Slim. Tensorflow-slim image classification model library. https://github.com/tensorflow/models/tree/master/research/slim. Accessed:2018-05-01.",
            "url": "https://github.com/tensorflow/models/tree/master/research/slim"
        },
        {
            "id": "Tram_et+al_2017_a",
            "entry": "F. Tram\u00e8r, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble Adversarial Training: Attacks and Defenses. ArXiv e-prints, May 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Boneh%2C%20D.%20Ensemble%20Adversarial%20Training%3A%20Attacks%20and%20Defenses.%20ArXiv%20e-prints%202017-05"
        },
        {
            "id": "Ustyuzhaninov_et+al_2016_a",
            "entry": "Ivan Ustyuzhaninov, Wieland Brendel, Leon A Gatys, and Matthias Bethge. Texture synthesis using shallow convolutional networks with random filters. arXiv preprint arXiv:1606.00021, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00021"
        },
        {
            "id": "Yosinski_et+al_2014_a",
            "entry": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320\u20133328, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        }
    ]
}
