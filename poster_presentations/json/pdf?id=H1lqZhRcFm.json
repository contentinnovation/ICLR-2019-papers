{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "UNSUPERVISED LEARNING OF THE SET OF LOCAL MAXIMA",
        "author": "Lior Wolf Facebook AI Research & The School of Computer Science Tel Aviv University wolf@fb.com, wolf@cs.tau.ac.il",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1lqZhRcFm"
        },
        "journal": "Darwin",
        "abstract": "This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function v in an unknown subset of the vector space. Two functions are learned: (i) a set indicator c, which is a binary classifier, and (ii) a comparator function h that given two nearby samples, predicts which sample has the higher value of the unknown function v. Loss terms are used to ensure that all training samples x are a local maxima of v, according to h and satisfy c(x) = 1. Therefore, c and h provide training signals to each other: a point x in the vicinity of x satisfies c(x) = \u22121 or is deemed by h to be lower in value than x. We present an algorithm, show an example where it is more efficient to use local maxima as an indicator function than to employ conventional classification, and derive a suitable generalization bound. Our experiments show that the method is able to outperform one-class classification algorithms in the task of anomaly detection and also provide an additional signal that is extracted in a completely unsupervised way."
    },
    "keywords": [
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "anomaly detection",
            "url": "https://en.wikipedia.org/wiki/anomaly_detection"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "world view",
            "url": "https://en.wikipedia.org/wiki/world_view"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "probability density function",
            "url": "https://en.wikipedia.org/wiki/probability_density_function"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "traffic sign",
            "url": "https://en.wikipedia.org/wiki/traffic_sign"
        },
        {
            "term": "local maxima",
            "url": "https://en.wikipedia.org/wiki/local_maxima"
        }
    ],
    "abbreviations": {
        "PDF": "probability density function",
        "FC": "fully connected",
        "ERC": "European Research Council"
    },
    "highlights": [
        "The same idea, of mixing constraints with optimality, holds for man-made objects",
        "The closest computational problem in the literature is one-class classification (<a class=\"ref-link\" id=\"cMoya_et+al_1993_a\" href=\"#rMoya_et+al_1993_a\">Moya et al, 1993</a>), where one learns a classifier c in order to model a set of unlabeled training samples, all from a single class",
        "Despite having the same structure of the input, our method stands out of the one-class classification and anomaly detection methods we are aware of, by optimizing a specific model that disentangles two aspects of the data: one aspect is captured by a class membership function, similar to many one-class approaches; the other aspect compares pairs of samples",
        "We extend the framework of spectral-norm bounds, which were derived in the context of classification, to the case of unsupervised learning using local maxima",
        "In contrast to this curve-based world view, we focus on the cusps",
        "The value function h can be used as a local optimization score in order to search locally for a better molecule"
    ],
    "key_statements": [
        "The same idea, of mixing constraints with optimality, holds for man-made objects",
        "What architects find most challenging, is that this optimization process needs to correspond to a comprehensive set of state and city regulations that regard, for example, the proximity of the built mass of the house to the lot\u2019s boundaries, or the compliance of the egress sizes with current fire codes",
        "Consider the weights of multiple neural networks trained to minimize the same loss on the same training data, each using a different random initialization",
        "By the nature of the problem, the obtained weights are the local optimum of some loss optimization process",
        "The closest computational problem in the literature is one-class classification (<a class=\"ref-link\" id=\"cMoya_et+al_1993_a\" href=\"#rMoya_et+al_1993_a\">Moya et al, 1993</a>), where one learns a classifier c in order to model a set of unlabeled training samples, all from a single class",
        "We show that the value function, which is trained with different losses and structure from those of the classifier, models a different aspect of the training set",
        "An alternative view of the learning problem we introduce considers the value function v as part of a density estimation problem, and not as part of a multi-network game",
        "Despite having the same structure of the input, our method stands out of the one-class classification and anomaly detection methods we are aware of, by optimizing a specific model that disentangles two aspects of the data: one aspect is captured by a class membership function, similar to many one-class approaches; the other aspect compares pairs of samples",
        "Recall that S is the set of unlabeled training samples, and that we learn two functions c and v such that for all x \u2208 S it holds that: (i) c(x) = 1, and x is a local maxima of v",
        "We extend the framework of spectral-norm bounds, which were derived in the context of classification, to the case of unsupervised learning using local maxima",
        "To clarify: there are no negative samples during training. We evaluate both c and h on the one class classification task: positive points are the MNIST test images of the same digit used for training, and negative points are the test images of all other digits",
        "Since we model local maxima, we take each Gaussian to have a standard deviation that is ten times smaller than that of <a class=\"ref-link\" id=\"cBalduzzi_et+al_2018_a\" href=\"#rBalduzzi_et+al_2018_a\">Balduzzi et al (2018</a>): 0.01 instead of 0.1",
        "In contrast to this curve-based world view, we focus on the cusps",
        "The value function h can be used as a local optimization score in order to search locally for a better molecule"
    ],
    "summary": [
        "The same idea, of mixing constraints with optimality, holds for man-made objects.",
        "Considering the weights of each trained neural network as a single vector in a sample domain, fits into the framework of local optimality under constraints.",
        "The closest computational problem in the literature is one-class classification (<a class=\"ref-link\" id=\"cMoya_et+al_1993_a\" href=\"#rMoya_et+al_1993_a\">Moya et al, 1993</a>), where one learns a classifier c in order to model a set of unlabeled training samples, all from a single class.",
        "We show that the value function, which is trained with different losses and structure from those of the classifier, models a different aspect of the training set.",
        "If the samples are images from a certain class, the classifier would capture class membership and the value function would encode image quality.",
        "Despite having the same structure of the input, our method stands out of the one-class classification and anomaly detection methods we are aware of, by optimizing a specific model that disentangles two aspects of the data: one aspect is captured by a class membership function, similar to many one-class approaches; the other aspect compares pairs of samples.",
        "Recall that S is the set of unlabeled training samples, and that we learn two functions c and v such that for all x \u2208 S it holds that: (i) c(x) = 1, and x is a local maxima of v.",
        "These experiments both help to understand the power of our model in capturing a given set of samples, as well as study the properties of the two underlying functions c and h.",
        "We employ CIFAR to perform an ablation analysis comparing the baseline method\u2019s c and h with four alternatives: (i) training c without training h, employing only Gc; training h and Gh without training c nor Gc; training both h and c but using only the Gc generator to obtain negative samples to both networks; and training both h and c but using only the Gh generator for both.",
        "On the conventional one-class task (i), both our c and h neural networks outperform the baseline Deep-SVDD method, with c performing better than h, as in the MNIST and CIFAR experiments.",
        "We test our method on the Gaussian Mixture Model data following <a class=\"ref-link\" id=\"cBalduzzi_et+al_2018_a\" href=\"#rBalduzzi_et+al_2018_a\">Balduzzi et al (2018</a>), who perform a similar experiment in order to study the phenomenon of mode hopping.",
        "We treat the mixture as a single class and sample a training set from it, to which we apply our methods as well as the variants where each network trains separately."
    ],
    "headline": "This paper describes a new form of unsupervised learning, whose input is a set of unlabeled points that are assumed to be local maxima of an unknown value function v in an unknown subset of the vector space",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Basu%2C%20Amitabh%20Mianjy%2C%20Poorya%20Mukherjee%2C%20Anirbit%20Understanding%20deep%20neural%20networks%20with%20rectified%20linear%20units%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Basu%2C%20Amitabh%20Mianjy%2C%20Poorya%20Mukherjee%2C%20Anirbit%20Understanding%20deep%20neural%20networks%20with%20rectified%20linear%20units%202018"
        },
        {
            "id": "Balduzzi_et+al_2018_a",
            "entry": "David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 354\u2013363, Stockholmsmssan, Stockholm Sweden, 10\u2013 15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/balduzzi18a.html.",
            "url": "http://proceedings.mlr.press/v80/balduzzi18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20David%20Racaniere%2C%20Sebastien%20Martens%2C%20James%20Foerster%2C%20Jakob%20The%20mechanics%20of%20n-player%20differentiable%20games%202018-07-10"
        },
        {
            "id": "Blei_et+al_2003_a",
            "entry": "David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. Latent dirichlet allocation. Journal of Machine Learning Research, 3:2003, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Lafferty%2C%20John%20Latent%20dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Lafferty%2C%20John%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "Chandola_et+al_2009_a",
            "entry": "Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):15, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009"
        },
        {
            "id": "Darwin_1859_a",
            "entry": "Charles Darwin. On the Origin of Species by Means of Natural Selection. Murray, London, 1859. or the Preservation of Favored Races in the Struggle for Life.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Darwin%2C%20Charles%20On%20the%20Origin%20of%20Species%20by%20Means%20of%20Natural%20Selection%201859",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Darwin%2C%20Charles%20On%20the%20Origin%20of%20Species%20by%20Means%20of%20Natural%20Selection%201859"
        },
        {
            "id": "Erfani_et+al_2016_a",
            "entry": "Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. Highdimensional and large-scale anomaly detection using a linear one-class svm with deep learning. Pattern Recognition, 58:121\u2013134, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erfani%2C%20Sarah%20M.%20Rajasegarar%2C%20Sutharshan%20Karunasekera%2C%20Shanika%20Leckie%2C%20Christopher%20Highdimensional%20and%20large-scale%20anomaly%20detection%20using%20a%20linear%20one-class%20svm%20with%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erfani%2C%20Sarah%20M.%20Rajasegarar%2C%20Sutharshan%20Karunasekera%2C%20Shanika%20Leckie%2C%20Christopher%20Highdimensional%20and%20large-scale%20anomaly%20detection%20using%20a%20linear%20one-class%20svm%20with%20deep%20learning%202016"
        },
        {
            "id": "Farkas_et+al_2018_a",
            "entry": "Zoltan Farkas, Dorottya Kalapis, Zoltan Bodi, Bela Szamecz, Andreea Daraba, Karola Almasi, Karoly Kovacs, Gabor Boross, Ferenc Pal, Peter Horvath, et al. Hsp70-associated chaperones have a critical role in buffering protein production costs. eLife, 7:e29845, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farkas%2C%20Zoltan%20Kalapis%2C%20Dorottya%20Bodi%2C%20Zoltan%20Szamecz%2C%20Bela%20Hsp70-associated%20chaperones%20have%20a%20critical%20role%20in%20buffering%20protein%20production%20costs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farkas%2C%20Zoltan%20Kalapis%2C%20Dorottya%20Bodi%2C%20Zoltan%20Szamecz%2C%20Bela%20Hsp70-associated%20chaperones%20have%20a%20critical%20role%20in%20buffering%20protein%20production%20costs%202018"
        },
        {
            "id": "Golan_2018_a",
            "entry": "Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9781\u2013 9791. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf.",
            "url": "http://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golan%2C%20Izhak%20El-Yaniv%2C%20Ran%20Deep%20anomaly%20detection%20using%20geometric%20transformations%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Houben_et+al_2013_a",
            "entry": "Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In International Joint Conference on Neural Networks, number 1288, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houben%2C%20Sebastian%20Stallkamp%2C%20Johannes%20Salmen%2C%20Jan%20Schlipsing%2C%20Marc%20Detection%20of%20traffic%20signs%20in%20real-world%20images%3A%20The%20German%20Traffic%20Sign%20Detection%20Benchmark%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houben%2C%20Sebastian%20Stallkamp%2C%20Johannes%20Salmen%2C%20Jan%20Schlipsing%2C%20Marc%20Detection%20of%20traffic%20signs%20in%20real-world%20images%3A%20The%20German%20Traffic%20Sign%20Detection%20Benchmark%202013"
        },
        {
            "id": "Mcallester_2003_a",
            "entry": "David Mcallester. Simplified pac-bayesian margin bounds. In In COLT, pp. 203\u2013215, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mcallester%2C%20David%20Simplified%20pac-bayesian%20margin%20bounds%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mcallester%2C%20David%20Simplified%20pac-bayesian%20margin%20bounds%202003"
        },
        {
            "id": "Moya_et+al_1993_a",
            "entry": "M. M. Moya, M. W. Koch, and L. D. Hostetler. One-class classifier networks for target recognition applications. NASA STI/Recon Technical Report N, 93, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moya%2C%20M.M.%20Koch%2C%20M.W.%20Hostetler%2C%20L.D.%20One-class%20classifier%20networks%20for%20target%20recognition%20applications%201993"
        },
        {
            "id": "Neyshabur_et+al_2018_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In ICML, pp. 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Parzen_0000_a",
            "entry": "Emanuel Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3):1065\u20131076, 09 1962.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parzen%2C%20Emanuel%20On%20estimation%20of%20a%20probability%20density%20function%20and%20mode",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parzen%2C%20Emanuel%20On%20estimation%20of%20a%20probability%20density%20function%20and%20mode"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Ruff_et+al_2018_a",
            "entry": "Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruff%2C%20Lukas%20Vandermeulen%2C%20Robert%20Goernitz%2C%20Nico%20Deecke%2C%20Lucas%20Deep%20one-class%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruff%2C%20Lukas%20Vandermeulen%2C%20Robert%20Goernitz%2C%20Nico%20Deecke%2C%20Lucas%20Deep%20one-class%20classification%202018"
        },
        {
            "id": "Sakurada_2014_a",
            "entry": "Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction. In Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis, pp. 4. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakurada%2C%20Mayu%20Yairi%2C%20Takehisa%20Anomaly%20detection%20using%20autoencoders%20with%20nonlinear%20dimensionality%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakurada%2C%20Mayu%20Yairi%2C%20Takehisa%20Anomaly%20detection%20using%20autoencoders%20with%20nonlinear%20dimensionality%20reduction%202014"
        },
        {
            "id": "Schlegl_2017_a",
            "entry": "Seebock Schlegl. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. IPMI, pp. 146157, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlegl%2C%20Seebock%20Unsupervised%20anomaly%20detection%20with%20generative%20adversarial%20networks%20to%20guide%20marker%20discovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlegl%2C%20Seebock%20Unsupervised%20anomaly%20detection%20with%20generative%20adversarial%20networks%20to%20guide%20marker%20discovery%202017"
        },
        {
            "id": "Scholkopf_et+al_2001_a",
            "entry": "Bernhard Scholkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, and Robert C. Williamson. Estimating the support of a high-dimensional distribution. Neural Computing, 13(7):1443\u20131471, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20Bernhard%20Platt%2C%20John%20C.%20Shawe-Taylor%2C%20John%20C.%20Smola%2C%20Alex%20J.%20Estimating%20the%20support%20of%20a%20high-dimensional%20distribution%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scholkopf%2C%20Bernhard%20Platt%2C%20John%20C.%20Shawe-Taylor%2C%20John%20C.%20Smola%2C%20Alex%20J.%20Estimating%20the%20support%20of%20a%20high-dimensional%20distribution%202001"
        },
        {
            "id": "Xia_et+al_2015_a",
            "entry": "Y. Xia, X. Cao, F. Wen, G. Hua, and J. Sun. Learning discriminative reconstructions for unsupervised outlier removal. In 2015 IEEE International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%2C%20Y.%20Cao%2C%20X.%20Wen%2C%20F.%20Hua%2C%20G.%20Learning%20discriminative%20reconstructions%20for%20unsupervised%20outlier%20removal%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xia%2C%20Y.%20Cao%2C%20X.%20Wen%2C%20F.%20Hua%2C%20G.%20Learning%20discriminative%20reconstructions%20for%20unsupervised%20outlier%20removal%202015"
        },
        {
            "id": "Xu_et+al_2015_a",
            "entry": "Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint arXiv:1510.01553, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.01553"
        },
        {
            "id": "2",
            "entry": "2. Any ReLU neural network v: R \u2192 R of the form v(x) = W2\u03c6(W1x + b) such that any x \u2208 S is a local maxima of v has at least 2m \u2212 1 neurons.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Any%20ReLU%20neural%20network%20v%20R%20%20R%20of%20the%20form%20vx%20%20W2%CF%86W1x%20%20b%20such%20that%20any%20x%20%20S%20is%20a%20local%20maxima%20of%20v%20has%20at%20least%202m%20%201%20neurons"
        },
        {
            "id": "3",
            "entry": "3. Let D = q \u00b7 D0 \u222a (1 \u2212 q) \u00b7 D1 be a distribution that samples at probability q > 0 from D0 and probability 1 \u2212 q from D1, where D0 is a distribution supported by S and D1 is a distribution supported by the segment [x1 \u2212 1, xm + 1]. Then, for a small enough > 0, every ReLU neural network c: R \u2192 R of the form c(x) = W2\u03c6(W1x + b), such that we consider that v is a piece-wise linear function with 2m linear pieces and arg max v = S. By Thm. 2.2 in Arora et al. (2018), this function can be represented as a ReLU neural network of the form v(x) = W2\u03c6(W1x + b), that has 2m hidden neurons.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Let%20D%20%20q%20%20D0%20%201%20%20q%20%20D1%20be%20a%20distribution%20that%20samples%20at%20probability%20q%20%200%20from%20D0%20and%20probability%201%20%20q%20from%20D1%20where%20D0%20is%20a%20distribution%20supported%20by%20S%20and%20D1%20is%20a%20distribution%20supported%20by%20the%20segment%20x1%20%201%20xm%20%201%20Then%20for%20a%20small%20enough%20%200%20every%20ReLU%20neural%20network%20c%20R%20%20R%20of%20the%20form%20cx%20%20W2%CF%86W1x%20%20b%20such%20that%20we%20consider%20that%20v%20is%20a%20piecewise%20linear%20function%20with%202m%20linear%20pieces%20and%20arg%20max%20v%20%20S%20By%20Thm%2022%20in%20Arora%20et%20al%202018%20this%20function%20can%20be%20represented%20as%20a%20ReLU%20neural%20network%20of%20the%20form%20vx%20%20W2%CF%86W1x%20%20b%20that%20has%202m%20hidden%20neurons",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Let%20D%20%20q%20%20D0%20%201%20%20q%20%20D1%20be%20a%20distribution%20that%20samples%20at%20probability%20q%20%200%20from%20D0%20and%20probability%201%20%20q%20from%20D1%20where%20D0%20is%20a%20distribution%20supported%20by%20S%20and%20D1%20is%20a%20distribution%20supported%20by%20the%20segment%20x1%20%201%20xm%20%201%20Then%20for%20a%20small%20enough%20%200%20every%20ReLU%20neural%20network%20c%20R%20%20R%20of%20the%20form%20cx%20%20W2%CF%86W1x%20%20b%20such%20that%20we%20consider%20that%20v%20is%20a%20piecewise%20linear%20function%20with%202m%20linear%20pieces%20and%20arg%20max%20v%20%20S%20By%20Thm%2022%20in%20Arora%20et%20al%202018%20this%20function%20can%20be%20represented%20as%20a%20ReLU%20neural%20network%20of%20the%20form%20vx%20%20W2%CF%86W1x%20%20b%20that%20has%202m%20hidden%20neurons"
        },
        {
            "id": "In_2018_a",
            "entry": "in contradiction. Let i \u2208 {1,..., m \u2212 1}, ai and bi be two points such that xi < ai < bi < xi+1 and c(ai) = c(bi) = 0. Since c is a continuous function and piece-wise linear and the four points (xi, 1), (a, 0), (b, 0), (xi+1, 1) are not co-linear, we conclude that c has at least three linear pieces in the segment [xi, xi+1]. Similarly, c has at least two linear pieces in each of the segments [x1 \u2212 1, x1] and [xm, xm + 1]. We conclude that c has at least 3m + 1 pieces. By Thm. 2.2 in Arora et al. (2018), c has at least 3m hidden neurons.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=in%20contradiction%20Let%20i%20%E2%88%88%20%7B1%2C...%2C%20m%20%E2%88%92%201%7D%2C%20ai%20and%20bi%20be%20two%20points%20such%20that%20xi%20%3C%20ai%20%3C%20bi%20%3C%20xi%2B1%20and%20c%28ai%29%20%3D%20c%28bi%29%20%3D%200.%20Since%20c%20is%20a%20continuous%20function%20and%20piece-wise%20linear%20and%20the%20four%20points%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=in%20contradiction%20Let%20i%20%E2%88%88%20%7B1%2C...%2C%20m%20%E2%88%92%201%7D%2C%20ai%20and%20bi%20be%20two%20points%20such%20that%20xi%20%3C%20ai%20%3C%20bi%20%3C%20xi%2B1%20and%20c%28ai%29%20%3D%20c%28bi%29%20%3D%200.%20Since%20c%20is%20a%20continuous%20function%20and%20piece-wise%20linear%20and%20the%20four%20points%202018"
        },
        {
            "id": "C(v\u03b8)_(multi-class)_b",
            "entry": "C(v\u03b8) + C(f\u03c9) is the sum of the spectral norms of v\u03b8 and f\u03c9. This suggests a tradeoff between the sum of the spectral complexities of v\u03b8 and f\u03c9 and the ability to generalize. The bound is similar asymptotically to the bounds of Neyshabur et al. (2018) and Bartlett et al. for (multi-class) supervised classification. In their bound, the penalty term is of the form O",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cv%CE%B8%20%20Cf%CF%89%20is%20the%20sum%20of%20the%20spectral%20norms%20of%20v%CE%B8%20and%20f%CF%89%20This%20suggests%20a%20tradeoff%20between%20the%20sum%20of%20the%20spectral%20complexities%20of%20v%CE%B8%20and%20f%CF%89%20and%20the%20ability%20to%20generalize%20The%20bound%20is%20similar%20asymptotically%20to%20the%20bounds%20of%20Neyshabur%20et%20al%202018%20and%20Bartlett%20et%20al%20for%20multiclass%20supervised%20classification%20In%20their%20bound%20the%20penalty%20term%20is%20of%20the%20form%20O",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cv%CE%B8%20%20Cf%CF%89%20is%20the%20sum%20of%20the%20spectral%20norms%20of%20v%CE%B8%20and%20f%CF%89%20This%20suggests%20a%20tradeoff%20between%20the%20sum%20of%20the%20spectral%20complexities%20of%20v%CE%B8%20and%20f%CF%89%20and%20the%20ability%20to%20generalize%20The%20bound%20is%20similar%20asymptotically%20to%20the%20bounds%20of%20Neyshabur%20et%20al%202018%20and%20Bartlett%20et%20al%20for%20multiclass%20supervised%20classification%20In%20their%20bound%20the%20penalty%20term%20is%20of%20the%20form%20O"
        },
        {
            "id": "In_2018_b",
            "entry": "In this section, we build upon the theory presented by Neyshabur et al. (2018) and provide a generalization bound that expresses the guarantees of learning c, along with v for a specific setting.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20section%20we%20build%20upon%20the%20theory%20presented%20by%20Neyshabur%20et%20al%202018%20and%20provide%20a%20generalization%20bound%20that%20expresses%20the%20guarantees%20of%20learning%20c%20along%20with%20v%20for%20a%20specific%20setting",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20this%20section%20we%20build%20upon%20the%20theory%20presented%20by%20Neyshabur%20et%20al%202018%20and%20provide%20a%20generalization%20bound%20that%20expresses%20the%20guarantees%20of%20learning%20c%20along%20with%20v%20for%20a%20specific%20setting"
        },
        {
            "id": "We_2018_c",
            "entry": "We modify the proof of Lem. 1 in Neyshabur et al. (2018), such that it will fit our purposes.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20modify%20the%20proof%20of%20Lem%201%20in%20Neyshabur%20et%20al%202018%20such%20that%20it%20will%20fit%20our%20purposes",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20modify%20the%20proof%20of%20Lem%201%20in%20Neyshabur%20et%20al%202018%20such%20that%20it%20will%20fit%20our%20purposes"
        },
        {
            "id": "Now_2003_c",
            "entry": "Now using the above inequalities together with Eq. 6 in Mcallester (2003), with probability 1 \u2212 \u03b4 over the training set we have: L0,0(\u03b8, \u03c9)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Now%20using%20the%20above%20inequalities%20together%20with%20Eq%206%20in%20Mcallester%202003%20with%20probability%201%20%20%CE%B4%20over%20the%20training%20set%20we%20have%20L00%CE%B8%20%CF%89"
        },
        {
            "id": "Proof_2018_d",
            "entry": "Proof of Lem. 2. We apply Lem. 5 with priors P1, P2 and posteriors q\u03b8, q\u03c9, distributions similar the proof of Thm. 1 in (Neyshabur et al., 2018). In their proof, they show that for their selection of prior and posterior distributions: (1) P\u03b8 \u223cq\u03b8 maxx\u2208X |v\u03b8(x)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20of%20Lem%202%20We%20apply%20Lem%205%20with%20priors%20P1%20P2%20and%20posteriors%20q%CE%B8%20q%CF%89%20distributions%20similar%20the%20proof%20of%20Thm%201%20in%20Neyshabur%20et%20al%202018%20In%20their%20proof%20they%20show%20that%20for%20their%20selection%20of%20prior%20and%20posterior%20distributions%201%20P%CE%B8%20q%CE%B8%20maxxX%20v%CE%B8x",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20of%20Lem%202%20We%20apply%20Lem%205%20with%20priors%20P1%20P2%20and%20posteriors%20q%CE%B8%20q%CF%89%20distributions%20similar%20the%20proof%20of%20Thm%201%20in%20Neyshabur%20et%20al%202018%20In%20their%20proof%20they%20show%20that%20for%20their%20selection%20of%20prior%20and%20posterior%20distributions%201%20P%CE%B8%20q%CE%B8%20maxxX%20v%CE%B8x"
        }
    ]
}
