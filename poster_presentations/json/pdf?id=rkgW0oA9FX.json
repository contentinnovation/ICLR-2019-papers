{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GRAPH HYPERNETWORKS FOR NEURAL ARCHITECTURE SEARCH",
        "author": "Chris Zhang, Mengye Ren, & Raquel Urtasun, 1Uber Advanced Technologies Group, 2University of Waterloo, 3University of Toronto cjzhang@edu.uwaterloo.ca, {mren,urtasun}@uber.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkgW0oA9FX"
        },
        "abstract": "Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast \u2013 they can search nearly 10\u00d7 faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs."
    },
    "keywords": [
        {
            "term": "CIFAR-10",
            "url": "https://en.wikipedia.org/wiki/CIFAR-10"
        },
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "backpropagation through time",
            "url": "https://en.wikipedia.org/wiki/backpropagation_through_time"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "random search",
            "url": "https://en.wikipedia.org/wiki/random_search"
        },
        {
            "term": "topology",
            "url": "https://en.wikipedia.org/wiki/topology"
        },
        {
            "term": "multilayer perceptron",
            "url": "https://en.wikipedia.org/wiki/multilayer_perceptron"
        },
        {
            "term": "network performance",
            "url": "https://en.wikipedia.org/wiki/network_performance"
        }
    ],
    "abbreviations": {
        "NAS": "neural architecture search",
        "GHN": "Graph HyperNetwork",
        "RNN": "recurrent neural network",
        "GRU": "gated recurrent unit",
        "BPTT": "backpropagation through time",
        "CG": "computation graph",
        "MLP": "multilayer perceptron"
    },
    "highlights": [
        "The success of deep learning marks the transition from manual feature engineering to automated feature learning",
        "Anytime models have non-trivial differences to classical models, we show the Graph HyperNetwork is amenable to these changes",
        "PREDICTED PERFORMANCE CORRELATION (CIFAR-10) we evaluate whether the parameters generated from Graph HyperNetwork can be indicative of the final performance",
        "We propose the Graph HyperNetwork (GHN), a composition of graph neural networks and hypernetworks that generates the weights of any architecture by operating directly on their computation graph representation",
        "Using our Graph HyperNetwork to form a surrogate search signal, we achieve competitive results on CIFAR-10 and ImageNet mobile with nearly 10\u00d7 faster speed compared to other random search methods",
        "We show that our proposed method can be extended to outperform the best human-designed architectures in setting of anytime prediction, greatly reducing the computation cost of real-time neural networks"
    ],
    "key_statements": [
        "The success of deep learning marks the transition from manual feature engineering to automated feature learning",
        "In this paper we propose the Graph HyperNetwork (GHN), which can aggregate graph level information by directly learning on the graph representation",
        "Anytime models have non-trivial differences to classical models, we show the Graph HyperNetwork is amenable to these changes",
        "We propose Graph HyperNetwork that predicts the parameters of unseen neural networks by directly operating on their computational graph representations.\n2",
        "The accuracy reported for Graph HyperNetwork Top-Best is the average of 5 runs of the same final architecture",
        "PREDICTED PERFORMANCE CORRELATION (CIFAR-10) we evaluate whether the parameters generated from Graph HyperNetwork can be indicative of the final performance",
        "Note that Graph HyperNetwork clearly outperforms the one-shot model, showing the effectiveness of dynamically predicting parameters based on graph topology",
        "Figure 5a shows how Graph HyperNetwork performance varies as a function of the number of nodes employed during training - fewer nodes generally produces better performance",
        "We propose the Graph HyperNetwork (GHN), a composition of graph neural networks and hypernetworks that generates the weights of any architecture by operating directly on their computation graph representation",
        "Using our Graph HyperNetwork to form a surrogate search signal, we achieve competitive results on CIFAR-10 and ImageNet mobile with nearly 10\u00d7 faster speed compared to other random search methods",
        "We show that our proposed method can be extended to outperform the best human-designed architectures in setting of anytime prediction, greatly reducing the computation cost of real-time neural networks"
    ],
    "summary": [
        "The success of deep learning marks the transition from manual feature engineering to automated feature learning.",
        "The computation graph representation allows GHNs to be the first hypernetwork to generate all the weights of arbitrary CNNs rather than a subset (e.g. <a class=\"ref-link\" id=\"cBrock_et+al_2018_a\" href=\"#rBrock_et+al_2018_a\">Brock et al (2018</a>)), achieving stronger correlation and making the search more efficient and accurate.",
        "We propose Graph HyperNetwork that predicts the parameters of unseen neural networks by directly operating on their computational graph representations.",
        "While initial NAS methods train candidate architectures for a brief period with SGD to obtain the search signal, recent approaches have proposed alternatives in the interest of computational cost.",
        "<a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al (2018</a>); <a class=\"ref-link\" id=\"cBender_et+al_2018_a\" href=\"#rBender_et+al_2018_a\">Bender et al (2018</a>); Liu et al (2018c) use parameter sharing, where a \u201cone-shot\u201d model containing all possible architectures in the search space is trained.",
        "After graph message-passing steps, a hypernet uses the node embeddings to generate each node\u2019s associated parameters.",
        "(Zoph et al, 2018; <a class=\"ref-link\" id=\"cPham_et+al_2018_a\" href=\"#rPham_et+al_2018_a\">Pham et al, 2018</a>), where a small graph module with a fewer number Figure 2: Stacked GHN along of computation nodes is searched, and the final architecture is the depth dimension.",
        "From the results of ablations studies in Section 5.4, the GHN is trained with blocks with N = 7 nodes and T = 5 propagations under the forward-backward scheme, using the ADAM optimizer (<a class=\"ref-link\" id=\"cKingma_2015_a\" href=\"#rKingma_2015_a\">Kingma & Ba, 2015</a>).",
        "We randomly sample 1000 architectures, and select the top 10 performing architectures with GHN generated weights, which we refer to as GHN Top. Our reported search cost includes both the GHN training and evaluation phase.",
        "Table 4 shows performance correlation and search cost of SGD, the one-shot model, and our GHN.",
        "Note that GHN clearly outperforms the one-shot model, showing the effectiveness of dynamically predicting parameters based on graph topology.",
        "Figure 5a shows how GHN performance varies as a function of the number of nodes employed during training - fewer nodes generally produces better performance.",
        "We propose the Graph HyperNetwork (GHN), a composition of graph neural networks and hypernetworks that generates the weights of any architecture by operating directly on their computation graph representation.",
        "Using our GHN to form a surrogate search signal, we achieve competitive results on CIFAR-10 and ImageNet mobile with nearly 10\u00d7 faster speed compared to other random search methods.",
        "We show that our proposed method can be extended to outperform the best human-designed architectures in setting of anytime prediction, greatly reducing the computation cost of real-time neural networks."
    ],
    "headline": "We propose the Graph HyperNetwork to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network",
    "reference_links": [
        {
            "id": "Baker_et+al_2017_a",
            "entry": "Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017"
        },
        {
            "id": "Baker_et+al_2017_b",
            "entry": "Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In NIPS Workshop on Meta-Learning, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202017"
        },
        {
            "id": "Bender_et+al_2018_a",
            "entry": "Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018"
        },
        {
            "id": "Brock_et+al_2018_a",
            "entry": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Smash%3A%20one-shot%20model%20architecture%20search%20through%20hypernetworks%202018"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoderdecoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20RNN%20encoderdecoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Deng_et+al_2017_a",
            "entry": "Boyang Deng, Junjie Yan, and Dahua Lin. Peephole: Predicting network performance before training. arXiv preprint arXiv:1712.03351, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03351"
        },
        {
            "id": "Misha_2013_a",
            "entry": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems 26 (NIPS), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misha%20Denil%20Babak%20Shakibi%20Laurent%20Dinh%20MarcAurelio%20Ranzato%20and%20Nando%20de%20Freitas%20Predicting%20parameters%20in%20deep%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2026%20NIPS%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misha%20Denil%20Babak%20Shakibi%20Laurent%20Dinh%20MarcAurelio%20Ranzato%20and%20Nando%20de%20Freitas%20Predicting%20parameters%20in%20deep%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2026%20NIPS%202013"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Dong_et+al_2018_a",
            "entry": "Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Jin-Dong%20Cheng%2C%20An-Chieh%20Juan%2C%20Da-Cheng%20Wei%2C%20Wei%20Dpp-net%3A%20Device-aware%20progressive%20search%20for%20pareto-optimal%20neural%20architectures%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Jin-Dong%20Cheng%2C%20An-Chieh%20Juan%2C%20Da-Cheng%20Wei%2C%20Wei%20Dpp-net%3A%20Device-aware%20progressive%20search%20for%20pareto-optimal%20neural%20architectures%202018"
        },
        {
            "id": "Elsken_et+al_2018_a",
            "entry": "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Multi-objective architecture search for cnns. arXiv preprint arXiv:1804.09081, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09081"
        },
        {
            "id": "Grubb_2012_a",
            "entry": "Alexander Grubb and Drew Bagnell. Speedboost: Anytime prediction with uniform near-optimality. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grubb%2C%20Alexander%20Bagnell%2C%20Drew%20Speedboost%3A%20Anytime%20prediction%20with%20uniform%20near-optimality%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grubb%2C%20Alexander%20Bagnell%2C%20Drew%20Speedboost%3A%20Anytime%20prediction%20with%20uniform%20near-optimality%202012"
        },
        {
            "id": "Ha_et+al_2017_a",
            "entry": "David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Ha%20Andrew%20Dai%20and%20Quoc%20V%20Le%20Hypernetworks%20In%20Proceedings%20of%20the%205th%20International%20Conference%20on%20Learning%20Representations%20ICLR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=David%20Ha%20Andrew%20Dai%20and%20Quoc%20V%20Le%20Hypernetworks%20In%20Proceedings%20of%20the%205th%20International%20Conference%20on%20Learning%20Representations%20ICLR%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "He_et+al_2016_b",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hsu_et+al_2018_a",
            "entry": "Chi-Hung Hsu, Shu-Huan Chang, Da-Cheng Juan, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and ShihChieh Chang. Monas: Multi-objective neural architecture search using reinforcement learning. arXiv preprint arXiv:1806.10332, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10332"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Chen%2C%20Danlu%20Li%2C%20Tianhong%20Wu%2C%20Felix%20Multi-scale%20dense%20networks%20for%20resource%20efficient%20image%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Chen%2C%20Danlu%20Li%2C%20Tianhong%20Wu%2C%20Felix%20Multi-scale%20dense%20networks%20for%20resource%20efficient%20image%20classification%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kipf_2017_a",
            "entry": "Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In Proceedings of the 4th International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20S.%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20S.%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "Liao_et+al_2018_a",
            "entry": "Renjie Liao, Marc Brockschmidt, Daniel Tarlow, Alexander L. Gaunt, Raquel Urtasun, and Richard S. Zemel. Graph partition neural networks for semi-supervised classification. In ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liao%2C%20Renjie%20Brockschmidt%2C%20Marc%20Tarlow%2C%20Daniel%20Gaunt%2C%20Alexander%20L.%20Graph%20partition%20neural%20networks%20for%20semi-supervised%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liao%2C%20Renjie%20Brockschmidt%2C%20Marc%20Tarlow%2C%20Daniel%20Gaunt%2C%20Alexander%20L.%20Graph%20partition%20neural%20networks%20for%20semi-supervised%20classification%202018"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European Conference on Computer Vision (ECCV), 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Chenxi%20Zoph%2C%20Barret%20Shlens%2C%20Jonathon%20Hua%2C%20Wei%20Progressive%20neural%20architecture%20search%202018"
        },
        {
            "id": "Liu_et+al_2018_b",
            "entry": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20representations%20for%20efficient%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20representations%20for%20efficient%20architecture%20search%202018"
        },
        {
            "id": "Liu_et+al_0000_a",
            "entry": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018c.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09055"
        },
        {
            "id": "Luo_et+al_2018_a",
            "entry": "Renqian Luo, Fei Tian, Tao Qin, and Tie-Yan Liu. Neural architecture optimization. arXiv preprint arXiv:1808.07233, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07233"
        },
        {
            "id": "Miikkulainen_et+al_2017_a",
            "entry": "Risto Miikkulainen, Jason Zhi Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks. arXiv preprint arXiv:abs/1703.00548, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miikkulainen%2C%20Risto%20Liang%2C%20Jason%20Zhi%20Meyerson%2C%20Elliot%20Rawal%2C%20Aditya%20Evolving%20deep%20neural%20networks%202017"
        },
        {
            "id": "Pham_et+al_2018_a",
            "entry": "Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pham%2C%20Hieu%20Guan%2C%20Melody%20Zoph%2C%20Barret%20Le%2C%20Quoc%20Efficient%20neural%20architecture%20search%20via%20parameters%20sharing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pham%2C%20Hieu%20Guan%2C%20Melody%20Zoph%2C%20Barret%20Le%2C%20Quoc%20Efficient%20neural%20architecture%20search%20via%20parameters%20sharing%202018"
        },
        {
            "id": "Real_et+al_2017_a",
            "entry": "Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017"
        },
        {
            "id": "Real_et+al_2018_a",
            "entry": "Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992"
        },
        {
            "id": "Schmidhuber_1993_a",
            "entry": "Jurgen Schmidhuber. A \u2018self-referential\u2019weight matrix. In International Conference on Artificial Neural Networks (ICANN). Springer, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20A%20%E2%80%98self-referential%E2%80%99weight%20matrix%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20A%20%E2%80%98self-referential%E2%80%99weight%20matrix%201993"
        },
        {
            "id": "Tan_et+al_2018_a",
            "entry": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platformaware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.11626"
        },
        {
            "id": "Werbos_0000_a",
            "entry": "Published as a conference paper at ICLR 2019 Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20J.%20Published%20as%20a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20J.%20Published%20as%20a"
        },
        {
            "id": "I_1990_a",
            "entry": "IEEE, 78(10):1550\u20131560, 1990. Lingxi Xie and Alan L. Yuille. Genetic CNN. In IEEE International Conference on Computer",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=IEEE%20781015501560%201990%20Lingxi%20Xie%20and%20Alan%20L%20Yuille%20Genetic%20CNN%20In%20IEEE%20International%20Conference%20on%20Computer",
            "oa_query": "https://api.scholarcy.com/oa_version?query=IEEE%20781015501560%201990%20Lingxi%20Xie%20and%20Alan%20L%20Yuille%20Genetic%20CNN%20In%20IEEE%20International%20Conference%20on%20Computer"
        },
        {
            "id": "Vision_0000_a",
            "entry": "Vision (ICCV), 2017. Yanqi Zhou, Siavash Ebrahimi, Sercan O Ar\u0131k, Haonan Yu, Hairong Liu, and Greg Diamos.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vision%20ICCV%202017%20Yanqi%20Zhou%20Siavash%20Ebrahimi%20Sercan%20O%20Ar%C4%B1k%20Haonan%20Yu%20Hairong%20Liu%20and%20Greg%20Diamos"
        },
        {
            "id": "Zoph_2018_a",
            "entry": "Resource-efficient neural architect. arXiv preprint arXiv:1806.07912, 2018. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07912"
        }
    ]
}
