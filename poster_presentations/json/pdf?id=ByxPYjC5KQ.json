{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "IMPROVING GENERALIZATION AND STABILITY OF GENERATIVE ADVERSARIAL NETWORKS",
        "author": "Hoang Thanh-Tung hoangtha@deakin.edu.au",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByxPYjC5KQ"
        },
        "journal": "Table",
        "abstract": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis."
    },
    "keywords": [
        {
            "term": "equilibria",
            "url": "https://en.wikipedia.org/wiki/equilibria"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        }
    ],
    "abbreviations": {
        "GANs": "Generative Adversarial Networks",
        "TTUR": "Two Timescale Update Rule"
    },
    "highlights": [
        "Generative Adversarial Networks (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are one of the most popular tools for modeling high dimensional data",
        "We show that high capacity discriminators trained with the original Generative Adversarial Networks loss tends to overfit to the mislabeled samples in training dataset, guiding the generator toward collapsed equilibria",
        "Our zero-centered gradient penalty pushes the discriminator toward the optimal one, making Generative Adversarial Networks to converge to equilibrium with good generalization capability",
        "We show that discriminators trained with the original Generative Adversarial Networks loss have poor generalization capability",
        "We show that the original Generative Adversarial Networks loss does not guide the discriminator and the generator toward a generalizable equilibrium",
        "We propose a zero-centered gradient penalty which pushes empirical discriminators toward the optimal discriminator with good generalization capability"
    ],
    "key_statements": [
        "Generative Adversarial Networks (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are one of the most popular tools for modeling high dimensional data",
        "The generalization of Generative Adversarial Networks at local equilibria is not discussed in depth in these papers",
        "We show that high capacity discriminators trained with the original Generative Adversarial Networks loss tends to overfit to the mislabeled samples in training dataset, guiding the generator toward collapsed equilibria",
        "We propose a zero-centered gradient penalty for improving the generalization capability of discriminators",
        "Our zero-centered gradient penalty pushes the discriminator toward the optimal one, making Generative Adversarial Networks to converge to equilibrium with good generalization capability",
        "We show that discriminators trained with the original Generative Adversarial Networks loss have poor generalization capability",
        "We show that the original Generative Adversarial Networks objective encourages gradient exploding in the discriminator",
        "We propose a zero-centered gradient penalty (0-GP) for improving the generalization capability of the discriminator",
        "Our work aims to improve the generalization capability of Generative Adversarial Networks via gradient regularization",
        "A discriminator trained on such dataset will overfit to the mislabeled examples and has poor generalization capability",
        "Discriminators trained with the original Generative Adversarial Networks loss tends to focus on the region of the where fake samples are close to real samples, ignoring other regions",
        "We show that the original Generative Adversarial Networks loss does not guide the discriminator and the generator toward a generalizable equilibrium",
        "We propose a zero-centered gradient penalty which pushes empirical discriminators toward the optimal discriminator with good generalization capability",
        "Experiments on diverse datasets verify that our method significantly improves the generalization and stability of Generative Adversarial Networks"
    ],
    "summary": [
        "GANs (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) are one of the most popular tools for modeling high dimensional data.",
        "We show that high capacity discriminators trained with the original GAN loss tends to overfit to the mislabeled samples in training dataset, guiding the generator toward collapsed equilibria.",
        "Our zero-centered gradient penalty pushes the discriminator toward the optimal one, making GAN to converge to equilibrium with good generalization capability.",
        "We show that discriminators trained with the original GAN loss have poor generalization capability.",
        "3. We propose a zero-centered gradient penalty (0-GP) for improving the generalization capability of the discriminator.",
        "A discriminator trained on such dataset will overfit to the mislabeled examples and has poor generalization capability.",
        "The discriminator in Fig. 1b was trained on a larger dataset which is sufficient to characterize the two distributions, it still overfits to the data and its value surface is very different from that of the theoretically optimal discriminator in Fig. 1f.",
        "The original GAN loss makes D to maximize its discriminative power, encouraging gradient exploding to occur.",
        "Because TTUR can help the discriminator to maintain its optimality, gradient exploding happens and persists throughout the training process.",
        "Without TTUR, the discriminator cannot maintain its optimality so gradient exploding can happen sometimes during the training but does not persist (Fig. 2a and 2b).",
        "<a class=\"ref-link\" id=\"cMescheder_et+al_2018_a\" href=\"#rMescheder_et+al_2018_a\">Mescheder et al (2018</a>) proposed to force the gradient w.r.t. datapoints in the real and/or fake dataset(s) to be 0 to make the training of GANs convergent.",
        "In section 4, we showed that for discrete training dataset, an empirically optimal discriminator D \u2217 always exists and could be found by gradient descent.",
        "Zero-centered gradient penalty on samples from pr and pg only cannot help improving the generalization of the discriminator.",
        "Discriminators trained with the original GAN loss tends to focus on the region of the where fake samples are close to real samples, ignoring other regions.",
        "To test the effectiveness of gradient penalties in preventing overfitting, we designed a dataset with real and fake samples coming from two Gaussian distributions and trained a MLP based discriminator on that dataset.",
        "The discriminator trained with our 0-GP has the best generalization capability, with a value surface which is the most similar to that of the theoretically optimal one.",
        "GANs with other gradient penalties all fail to learn the distribution and exhibit mode collapse problem to different extents.",
        "We propose a zero-centered gradient penalty which pushes empirical discriminators toward the optimal discriminator with good generalization capability."
    ],
    "headline": "We show that discriminators trained on discrete datasets with the original Generative Adversarial Networks loss have poor generalization capability and do not approximate the theoretically optimal discriminator",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "M. Arjovsky and L. Bottou. Towards Principled Methods for Training Generative Adversarial Networks. ArXiv e-prints, January 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Bottou%2C%20L.%20Towards%20Principled%20Methods%20for%20Training%20Generative%20Adversarial%20Networks.%20ArXiv%20e-prints%202017-01"
        },
        {
            "id": "Arjovsky_et+al_2017_b",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214\u2013223, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017-08"
        },
        {
            "id": "Arora_et+al_2017_a",
            "entry": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 224\u2013232, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017-08"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Fedus_et+al_2018_a",
            "entry": "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672\u20132680. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5767\u20135777. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6626\u20136637. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Bach_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448\u2013456, Lille, France, 07\u201309 Jul 2015. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Francis%20Bach%20and%20David%20Blei%20eds%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20volume%2037%20of%20Proceedings%20of%20Machine%20Learning%20Research%20pp%20448456%20Lille%20France%200709%20Jul%202015%20PMLR",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Francis%20Bach%20and%20David%20Blei%20eds%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20Machine%20Learning%20volume%2037%20of%20Proceedings%20of%20Machine%20Learning%20Research%20pp%20448456%20Lille%20France%200709%20Jul%202015%20PMLR"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Lucas_et+al_2018_a",
            "entry": "Thomas Lucas, Corentin Tallec, Yann Ollivier, and Jakob Verbeek. Mixed batches and symmetric discriminators for GAN training. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2844\u20132853, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20Thomas%20Tallec%2C%20Corentin%20Ollivier%2C%20Yann%20Verbeek%2C%20Jakob%20Mixed%20batches%20and%20symmetric%20discriminators%20for%20GAN%20training%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20Thomas%20Tallec%2C%20Corentin%20Ollivier%2C%20Yann%20Verbeek%2C%20Jakob%20Mixed%20batches%20and%20symmetric%20discriminators%20for%20GAN%20training%202018-07"
        },
        {
            "id": "Mescheder_et+al_2018_a",
            "entry": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3478\u20133487, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018-07"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Mroueh_et+al_2017_a",
            "entry": "Youssef Mroueh and Tom Sercu. Fisher gan. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2513\u20132523. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Youssef%20Mroueh%20and%20Tom%20Sercu%20Fisher%20gan%20In%20I%20Guyon%20U%20V%20Luxburg%20S%20Bengio%20H%20Wallach%20R%20Fergus%20S%20Vishwanathan%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20pp%2025132523%20Curran%20Associates%20Inc%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Youssef%20Mroueh%20and%20Tom%20Sercu%20Fisher%20gan%20In%20I%20Guyon%20U%20V%20Luxburg%20S%20Bengio%20H%20Wallach%20R%20Fergus%20S%20Vishwanathan%20and%20R%20Garnett%20eds%20Advances%20in%20Neural%20Information%20Processing%20Systems%2030%20pp%2025132523%20Curran%20Associates%20Inc%202017"
        },
        {
            "id": "Mroueh_et+al_2018_a",
            "entry": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev GAN. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Youssef%20Mroueh%20ChunLiang%20Li%20Tom%20Sercu%20Anant%20Raj%20and%20Yu%20Cheng%20Sobolev%20GAN%20In%20International%20Conference%20on%20Learning%20Representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Youssef%20Mroueh%20ChunLiang%20Li%20Tom%20Sercu%20Anant%20Raj%20and%20Yu%20Cheng%20Sobolev%20GAN%20In%20International%20Conference%20on%20Learning%20Representations%202018"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent gan optimization is locally stable. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5585\u20135595. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%202017"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "Petzka_et+al_2018_a",
            "entry": "Henning Petzka, Asja Fischer, and Denis Lukovnikov. On the regularization of wasserstein GANs. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petzka%2C%20Henning%20Fischer%2C%20Asja%20Lukovnikov%2C%20Denis%20On%20the%20regularization%20of%20wasserstein%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petzka%2C%20Henning%20Fischer%2C%20Asja%20Lukovnikov%2C%20Denis%20On%20the%20regularization%20of%20wasserstein%20GANs%202018"
        },
        {
            "id": "Qi_2017_a",
            "entry": "G.-J. Qi. Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities. ArXiv e-prints, January 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20G.-J.%20Loss-Sensitive%20Generative%20Adversarial%20Networks%20on%20Lipschitz%20Densities.%20ArXiv%20e-prints%202017-01"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Roth_et+al_2017_a",
            "entry": "Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2018\u20132028. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202017"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 2234\u20132242. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Srivastava_et+al_2017_a",
            "entry": "Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3308\u20133318. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, and Luc Van Gool. Wasserstein divergence for gans. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision \u2013 ECCV 2018, pp. 673\u2013688, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01228-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiqing%20Huang%2C%20Zhiwu%20Thoma%2C%20Janine%20Acharya%2C%20Dinesh%20Wasserstein%20divergence%20for%20gans%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiqing%20Huang%2C%20Zhiwu%20Thoma%2C%20Janine%20Acharya%2C%20Dinesh%20Wasserstein%20divergence%20for%20gans%202018"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in GANs. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Pengchuan%20Liu%2C%20Qiang%20Zhou%2C%20Dengyong%20Xu%2C%20Tao%20On%20the%20discriminationgeneralization%20tradeoff%20in%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Pengchuan%20Liu%2C%20Qiang%20Zhou%2C%20Dengyong%20Xu%2C%20Tao%20On%20the%20discriminationgeneralization%20tradeoff%20in%20GANs%202018"
        }
    ]
}
