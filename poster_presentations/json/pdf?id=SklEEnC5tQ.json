{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS",
        "author": "Shoichiro Yamaguchi, Masanori Koyama Preferred Networks {guguchi, masomatics}@preferred.jp",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SklEEnC5tQ"
        },
        "abstract": "We propose Distributional Concavity (DC) regularization for Generative Adversarial Networks (GANs), a functional gradient-based method that promotes the entropy of the generator distribution and works against mode collapse. Our DC regularization is an easy-to-implement method that can be used in combination with the current state of the art methods like Spectral Normalization and Wasserstein GAN with gradient penalty to further improve the performance. We will not only show that our DC regularization can achieve highly competitive results on ILSVRC2012 and CIFAR datasets in terms of Inception score and Frechet inception distance, but also provide a mathematical guarantee that our method can always increase the entropy of the generator distribution. We will also show an intimate theoretical connection between our method and the theory of optimal transport."
    },
    "keywords": [
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        }
    ],
    "abbreviations": {
        "DC": "Distributional Concavity",
        "GANs": "Generative Adversarial Networks",
        "FID": "Frechet inception distance",
        "SN": "Spectral Normalization"
    },
    "highlights": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is a model consisting of two adversarial neural networks designed for the training of a generator distribution that mimics a target distribution defined on a high dimensional space, and it has been successful in numerous applications including image and movie generations (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al, 2017</a></a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSaito_et+al_2017_a\" href=\"#rSaito_et+al_2017_a\"><a class=\"ref-link\" id=\"cSaito_et+al_2017_a\" href=\"#rSaito_et+al_2017_a\">Saito et al, 2017</a></a>)",
        "Note that we can automatically make x 2/2 \u2212 L to be convex by making L concave, The connection between our Distributional Concavity regularization and optimal transport is not limited to the property we introduced above",
        "Inception score and Frechet inception distance are performance measures that are commonly used to evaluate the severity of mode collapse",
        "Dilemma of Generative Adversarial Networks update As we have shown above, the experimental results support our claim that the Distributional Concavity regularization promotes the entropy of the generator distribution and stabilizes the training process of Generative Adversarial Networks",
        "The conventional update rule is implicitly targetting a distribution of the form T #\u03bc where T = Id \u2212 \u03b1\u2207L for some \u03b1 and a discriminator L",
        "Optimal transport takes this form only when we are optimizing W2 distance, while the common constraint of |\u2207L| = 1 is a condition required for dual potential when p = 1 (W1) (<a class=\"ref-link\" id=\"cVillani_2008_a\" href=\"#rVillani_2008_a\">Villani, 2008</a>)"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>) is a model consisting of two adversarial neural networks designed for the training of a generator distribution that mimics a target distribution defined on a high dimensional space, and it has been successful in numerous applications including image and movie generations (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al, 2017</a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a>; <a class=\"ref-link\" id=\"cSaito_et+al_2017_a\" href=\"#rSaito_et+al_2017_a\">Saito et al, 2017</a>)",
        "Mode collapse is a persistent central problem for the training of Generative Adversarial Networks, which collectively refers to the lack of diversity in generator distribution",
        "When the entropy of the true distribution is higher than the current generator distribution, the functional gradient that is properly derived from the optimal mass transport always increases the entropy of the distribution",
        "We propose an update for the generator of Generative Adversarial Networks that promotes the entropy of the generator without the need for an explicit estimation of the actual entropy",
        "We show that, when the entropy of the true distribution is larger than that of the current generator distribution, the functional gradient derived from the 2-Wasserstein (W2) optimal transport always increases the entropy of the distribution",
        "We provide a mathematical guarantee that our method increases the entropy of the generator distribution at every step",
        "If L is concave on the support of \u03bc, H(T\u03b1#\u03bc) \u2265 H(\u03bc). Note that this statement is independent of the step size \u03b1, because any positive scalar multiple of concave function is concave. This result ensures that any update with a concave L will always increase the entropy; that is, the target distribution will be more dispersed than the current distribution, and the mode collapse is less likely to happen",
        "Note that we can automatically make x 2/2 \u2212 L to be convex by making L concave, The connection between our Distributional Concavity regularization and optimal transport is not limited to the property we introduced above",
        "Inception score and Frechet inception distance are performance measures that are commonly used to evaluate the severity of mode collapse",
        "Effect of Distributional Concavity regularization on entropy Using a simple Gaussian Mixture Model with five modes as the true distribution, we evaluated the sheer ability of the Distributional Concavity regularization to promote the entropy of the generator distribution",
        "As we show in Table 1, Distributional Concavity regularization outperformed EGAN-Ent-VI on both models",
        "Dilemma of Generative Adversarial Networks update As we have shown above, the experimental results support our claim that the Distributional Concavity regularization promotes the entropy of the generator distribution and stabilizes the training process of Generative Adversarial Networks",
        "The conventional update rule is implicitly targetting a distribution of the form T #\u03bc where T = Id \u2212 \u03b1\u2207L for some \u03b1 and a discriminator L",
        "Optimal transport takes this form only when we are optimizing W2 distance, while the common constraint of |\u2207L| = 1 is a condition required for dual potential when p = 1 (W1) (<a class=\"ref-link\" id=\"cVillani_2008_a\" href=\"#rVillani_2008_a\">Villani, 2008</a>)",
        "The conventional Generative Adversarial Networks are making the discriminator with W1 criteria and updating the generator with W2 criteria",
        "This, is in general a highly complex mathematical problem for which there is a separate field of study (<a class=\"ref-link\" id=\"cSantambrogio_2015_a\" href=\"#rSantambrogio_2015_a\">Santambrogio, 2015</a>)",
        "To the authors\u2019 best knowledge, no studies have provided a solid solution to this dilemma"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) is a model consisting of two adversarial neural networks designed for the training of a generator distribution that mimics a target distribution defined on a high dimensional space, and it has been successful in numerous applications including image and movie generations (<a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\"><a class=\"ref-link\" id=\"cIsola_et+al_2017_a\" href=\"#rIsola_et+al_2017_a\">Isola et al, 2017</a></a>; <a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhu_et+al_2017_a\" href=\"#rZhu_et+al_2017_a\">Zhu et al, 2017</a></a>; <a class=\"ref-link\" id=\"cSaito_et+al_2017_a\" href=\"#rSaito_et+al_2017_a\"><a class=\"ref-link\" id=\"cSaito_et+al_2017_a\" href=\"#rSaito_et+al_2017_a\">Saito et al, 2017</a></a>).",
        "The regularization strategy of monotonically increasing the entropy of the generator distribution over the course of its training is not without theoretical basis.",
        "When the entropy of the true distribution is higher than the current generator distribution, the functional gradient that is properly derived from the optimal mass transport always increases the entropy of the distribution.",
        "The functional update used in our method is always a monotonic mapping, which turns out to be one of the properties satisfied by the update with optimal transport.",
        "We show that, when the entropy of the true distribution is larger than that of the current generator distribution, the functional gradient derived from the 2-Wasserstein (W2) optimal transport always increases the entropy of the distribution.",
        "As hinted a moment ago, this suggests that, by directly regularizing the L in the usual update scheme of GANs, one can realize a functional gradient update of the generator with a controlled target distribution.",
        "This result ensures that any update with a concave L will always increase the entropy; that is, the target distribution will be more dispersed than the current distribution, and the mode collapse is less likely to happen.",
        "By making L concave, we can not only construct a target distribution whose entropy is greater than that of the current generator distribution, but assure that the corresponding transport is monotonic.",
        "As we will further articulate in the discussion section, the challenge remains to conduct faithful W2-based updates in the implementation of GANs. If \u03bc\u03b8 := G\u03b8#\u03bcz is our current parametric generator distribution and \u03bd is the true distribution, our DC regularization method first (i) proposes a target distribution T #\u03bc\u03b8 using a concave L, and seek a measure \u03bc\u03b8new that well approximates the target distribution T #\u03bc\u03b8.",
        "This result (Fig 2 (b)) suggests that, when using the update rule similar to the equation 10, the training proceeds much faster when we choose a target distribution with monotonic mapping.",
        "Dilemma of GANs update As we have shown above, the experimental results support our claim that the DC regularization promotes the entropy of the generator distribution and stabilizes the training process of GANs. As mentioned briefly in the theory section 2, we do not have the guarantee that our update rule consistently reduces the W2 distance between the generator distribution and the true distribution."
    ],
    "headline": "We propose Distributional Concavity regularization for Generative Adversarial Networks, a functional gradient-based method that promotes the entropy of the generator distribution and works against mode collapse",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Bottou%2C%20Leon%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Bottou%2C%20Leon%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Arjovsky_et+al_2017_b",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In ICML, pp. 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Brenier_1991_a",
            "entry": "Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics, 44(4):375\u2013417, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brenier%2C%20Yann%20Polar%20factorization%20and%20monotone%20rearrangement%20of%20vector-valued%20functions%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brenier%2C%20Yann%20Polar%20factorization%20and%20monotone%20rearrangement%20of%20vector-valued%20functions%201991"
        },
        {
            "id": "Cacoullos_1966_a",
            "entry": "Theophilos Cacoullos. Estimation of a multivariate density. Annals of the Institute of Statistical Mathematics, 18(1):179\u2013189, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cacoullos%2C%20Theophilos%20Estimation%20of%20a%20multivariate%20density%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cacoullos%2C%20Theophilos%20Estimation%20of%20a%20multivariate%20density%201966"
        },
        {
            "id": "Dai_et+al_2017_a",
            "entry": "Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville. Calibrating energy-based generative adversarial networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Zihang%20Almahairi%2C%20Amjad%20Bachman%2C%20Philip%20Hovy%2C%20Eduard%20Calibrating%20energy-based%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Zihang%20Almahairi%2C%20Amjad%20Bachman%2C%20Philip%20Hovy%2C%20Eduard%20Calibrating%20energy-based%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Dowson_1982_a",
            "entry": "DC Dowson and BV Landau. The frechet distance between multivariate normal distributions. Journal of Multivariate Analysis, 12(3):450\u2013455, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dowson%2C%20D.C.%20Landau%2C%20B.V.%20The%20frechet%20distance%20between%20multivariate%20normal%20distributions%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dowson%2C%20D.C.%20Landau%2C%20B.V.%20The%20frechet%20distance%20between%20multivariate%20normal%20distributions%201982"
        },
        {
            "id": "Goodfellow_2016_a",
            "entry": "Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein GANs. In NIPS, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium. In NIPS, pp. 6629\u20136640, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20nash%20equilibrium%202017"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pp. 5967\u20135976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Johnson_2018_a",
            "entry": "Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial models. In ICML, pp. 2376\u20132384, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Composite%20functional%20gradient%20learning%20of%20generative%20adversarial%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Composite%20functional%20gradient%20learning%20of%20generative%20adversarial%20models%202018"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Lim_2017_a",
            "entry": "Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv preprint arXiv:1705.02894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02894"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04086"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, pp. 2813\u20132821, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Metz_et+al_2016_a",
            "entry": "Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "Miyato_et+al_2018_a",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Nitanda_2018_a",
            "entry": "Atsushi Nitanda and Taiji Suzuki. Gradient layer: Enhancing the convergence of adversarial training for generative models. In AISTATS, pp. 1008\u20131016, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nitanda%2C%20Atsushi%20Suzuki%2C%20Taiji%20Gradient%20layer%3A%20Enhancing%20the%20convergence%20of%20adversarial%20training%20for%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nitanda%2C%20Atsushi%20Suzuki%2C%20Taiji%20Gradient%20layer%3A%20Enhancing%20the%20convergence%20of%20adversarial%20training%20for%20generative%20models%202018"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Ozakin_2009_a",
            "entry": "Arkadas Ozakin and Alexander G Gray. Submanifold density estimation. In Advances in Neural Information Processing Systems, pp. 1375\u20131382, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ozakin%2C%20Arkadas%20Gray%2C%20Alexander%20G.%20Submanifold%20density%20estimation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ozakin%2C%20Arkadas%20Gray%2C%20Alexander%20G.%20Submanifold%20density%20estimation%202009"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Saito_et+al_2017_a",
            "entry": "Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In ICCV, pp. 2849\u20132858, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saito%2C%20Masaki%20Matsumoto%2C%20Eiichi%20Saito%2C%20Shunta%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saito%2C%20Masaki%20Matsumoto%2C%20Eiichi%20Saito%2C%20Shunta%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping%202017"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, pp. 2226\u20132234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Santambrogio_2015_a",
            "entry": "Filippo Santambrogio. Optimal transport for applied mathematicians. Birkauser, NY, pp. 99\u2013102, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santambrogio%2C%20Filippo%20Optimal%20transport%20for%20applied%20mathematicians%202015"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Torralba_et+al_2008_a",
            "entry": "Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958\u20131970, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torralba%2C%20Antonio%20Fergus%2C%20Rob%20Freeman%2C%20William%20T.%20million%20tiny%20images%3A%20A%20large%20data%20set%20for%20nonparametric%20object%20and%20scene%20recognition%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Torralba%2C%20Antonio%20Fergus%2C%20Rob%20Freeman%2C%20William%20T.%20million%20tiny%20images%3A%20A%20large%20data%20set%20for%20nonparametric%20object%20and%20scene%20recognition%202008"
        },
        {
            "id": "Villani_2003_a",
            "entry": "Cedric Villani. Topics in Optimal Transportation. Graduate studies in mathematics. American Mathematical Society, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20Cedric%20Topics%20in%20Optimal%20Transportation.%20Graduate%20studies%20in%20mathematics%202003"
        },
        {
            "id": "Villani_2008_a",
            "entry": "Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20Cedric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pp. 2242\u20132251, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        }
    ]
}
