{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "COARSE-GRAIN FINE-GRAIN COATTENTION NETWORK FOR MULTI-EVIDENCE QUESTION ANSWERING",
        "author": "Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, and Richard Socher",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Syl7OsRqY7"
        },
        "abstract": "End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and selfattention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new stateof-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "Sony Music Entertainment",
            "url": "https://en.wikipedia.org/wiki/Sony_Music_Entertainment"
        },
        {
            "term": "reading comprehension",
            "url": "https://en.wikipedia.org/wiki/reading_comprehension"
        },
        {
            "term": "coarse grain",
            "url": "https://en.wikipedia.org/wiki/coarse_grain"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "GRAIN",
            "url": "https://en.wikipedia.org/wiki/GRAIN"
        }
    ],
    "abbreviations": {
        "CFC": "Coarse-grain Fine-grain Coattention Network",
        "QA": "question answering",
        "GRUs": "Gated Recurrent Units",
        "MLP": "multi-layer perceptron",
        "SME": "Sony Music Entertainment"
    },
    "highlights": [
        "A requirement of scalable and practical question answering (QA) systems is the ability to reason over multiple documents and combine their information to answer questions",
        "Our multi-evidence question answering model, the Coarse-grain Fine-grain Coattention Network (CFC), selects among a set of candidate answers given a set of support documents and a query",
        "On the whole TriviaQA dev set, reranking using rows respectively correspond the Coarse-grain Fine-grain Coattention Network results in a gain of 3.1% EM and 3.0% F1, which suggests to the removal of coarse-grain that the Coarse-grain Fine-grain Coattention Network can be used to further refine the outputs produced by module, the removal of finespan extraction question answering models",
        "We presented Coarse-grain Fine-grain Coattention Network, a new state-of-the-art model for multi-evidence question answering inspired by coarse-grain reasoning and fine-grain reasoning",
        "We showed in our analysis that the complementary coarse-grain and fine-grain modules of the Coarse-grain Fine-grain Coattention Network focus on different aspects of the input, and are an effective means to represent large collections of long documents"
    ],
    "key_statements": [
        "A requirement of scalable and practical question answering (QA) systems is the ability to reason over multiple documents and combine their information to answer questions",
        "Our multi-evidence question answering model, the Coarse-grain Fine-grain Coattention Network (CFC), selects among a set of candidate answers given a set of support documents and a query",
        "In contrast to the coarse-grain module, the fine-grain module, shown in Figure 3, finds the specific context in which the candidate occurs in the supporting documents using coreference resolution 1",
        "On the whole TriviaQA dev set, reranking using rows respectively correspond the Coarse-grain Fine-grain Coattention Network results in a gain of 3.1% EM and 3.0% F1, which suggests to the removal of coarse-grain that the Coarse-grain Fine-grain Coattention Network can be used to further refine the outputs produced by module, the removal of finespan extraction question answering models",
        "We examine the hierarchical attention maps produced by the Coarse-grain Fine-grain Coattention Network on examples from the WikiHop development set",
        "We find that coattention layers consistently focus on phrases that are similar between the document and the query, while lower level self-attention layers capture phrases that characterize the entity described by the document",
        "We examine 100 errors the Coarse-grain Fine-grain Coattention Network produced on the WikiHop development set and categorize them into four types",
        "We presented Coarse-grain Fine-grain Coattention Network, a new state-of-the-art model for multi-evidence question answering inspired by coarse-grain reasoning and fine-grain reasoning",
        "We showed in our analysis that the complementary coarse-grain and fine-grain modules of the Coarse-grain Fine-grain Coattention Network focus on different aspects of the input, and are an effective means to represent large collections of long documents"
    ],
    "summary": [
        "A requirement of scalable and practical question answering (QA) systems is the ability to reason over multiple documents and combine their information to answer questions.",
        "Our multi-evidence QA model, the Coarse-grain Fine-grain Coattention Network (CFC), selects among a set of candidate answers given a set of support documents and a query.",
        "The coarse-grain module summarizes support documents without knowing the candidates: it builds codependent representations of support documents and the query using coattention, produces a coarse-grain summary using self-attention.",
        "The coarse-grain module of the CFC, shown in Figure 2, builds codependent representations of support documents Es and the query Eq using coattention, and summarizes the coattention context using self-attention to compare it to the candidate Ec. Coattention and similar techniques are crucial to single-document question answering models (Xiong et al, 2017; <a class=\"ref-link\" id=\"cWang_2017_a\" href=\"#rWang_2017_a\">Wang & Jiang, 2017</a>; <a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\">Seo et al, 2017</a>).",
        "We summarize the coattention context \u2014 a codependent encoding of the supporting document and the query \u2014 using hierarchical self-attention.",
        "In contrast to the coarse-grain module, the fine-grain module, shown in Figure 3, finds the specific context in which the candidate occurs in the supporting documents using coreference resolution 1.",
        "This coattention context, which is a codependent encoding of the mentions and the query, is again summarized via self-attention to produce a fine-grain summary to score the candidate.",
        "This enables the CFC to more effectively model the large collection of potentially long documents found in WikiHop. 3.2 RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA",
        "On the whole TriviaQA dev set, reranking using rows respectively correspond the CFC results in a gain of 3.1% EM and 3.0% F1, which suggests to the removal of coarse-grain that the CFC can be used to further refine the outputs produced by module, the removal of finespan extraction question answering models.",
        "Table 3 shows the performance contributions of the coarse-grain GRUs, and the replacement module, the fine-grain module, as well as model decisions such of encoder GRUs with projecas self-attention and bidirectional GRUs. Both the coarse-grain tion over word embeddings.",
        "Coarse-grain summary self-attention, described in equation 15, tends to focus on support documents that present information relevant to the object in the query.",
        "In the CFC, we present a novel way to combine self-attention and coattention in a hierarchy to build effective conditional and codependent representations of a large number of potentially long documents.",
        "We presented CFC, a new state-of-the-art model for multi-evidence question answering inspired by coarse-grain reasoning and fine-grain reasoning.",
        "We showed in our analysis that the complementary coarse-grain and fine-grain modules of the CFC focus on different aspects of the input, and are an effective means to represent large collections of long documents."
    ],
    "headline": "We propose the Coarse-grain Fine-grain Coattention Network, a new question answering model that combines information from evidence across multiple documents",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Cao_et+al_2018_a",
            "entry": "Nicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents with graph convolutional networks. arXiv preprint arXiv:1808.09920, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.09920"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "Danqi Chen, Jason Bolton, and Christopher D. Manning. A thorough examination of the CNN/Daily Mail reading comprehension task. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Danqi%20Bolton%2C%20Jason%20Manning%2C%20Christopher%20D.%20A%20thorough%20examination%20of%20the%20CNN/Daily%20Mail%20reading%20comprehension%20task%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Danqi%20Bolton%2C%20Jason%20Manning%2C%20Christopher%20D.%20A%20thorough%20examination%20of%20the%20CNN/Daily%20Mail%20reading%20comprehension%20task%202015"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Choi_et+al_2017_a",
            "entry": "Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Eunsol%20Hewlett%2C%20Daniel%20Uszkoreit%2C%20Jakob%20Polosukhin%2C%20Illia%20Coarse-to-fine%20question%20answering%20for%20long%20documents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Eunsol%20Hewlett%2C%20Daniel%20Uszkoreit%2C%20Jakob%20Polosukhin%2C%20Illia%20Coarse-to-fine%20question%20answering%20for%20long%20documents%202017"
        },
        {
            "id": "Clark_2018_a",
            "entry": "Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clark%2C%20Christopher%20Gardner%2C%20Matt%20Simple%20and%20effective%20multi-paragraph%20reading%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clark%2C%20Christopher%20Gardner%2C%20Matt%20Simple%20and%20effective%20multi-paragraph%20reading%20comprehension%202018"
        },
        {
            "id": "Dang_2006_a",
            "entry": "Hoa Trang Dang. Overview of DUC 2006. In DUC, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoa%20Trang%20Dang%20Overview%20of%20DUC%202006%20In%20DUC%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoa%20Trang%20Dang%20Overview%20of%20DUC%202006%20In%20DUC%202006"
        },
        {
            "id": "Yoon_2018_a",
            "entry": "SangKeun Lee Deunsol Yoon, Dongbok Lee. Dynamic self-attention: Computing attention over words dynamically for sentence embedding. arXiv preprint arXiv:1808.07383, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07383"
        },
        {
            "id": "Dhingra_et+al_2018_a",
            "entry": "Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05922"
        },
        {
            "id": "Dong_2018_a",
            "entry": "Li Dong and Mirella Lapata. Coarse-to-fine decoding for neural semantic parsing. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Li%20Lapata%2C%20Mirella%20Coarse-to-fine%20decoding%20for%20neural%20semantic%20parsing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Li%20Lapata%2C%20Mirella%20Coarse-to-fine%20decoding%20for%20neural%20semantic%20parsing%202018"
        },
        {
            "id": "Gupta_et+al_2007_a",
            "entry": "Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. Measuring importance and query relevance in topic-focused multi-document summarization. In ACL, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Surabhi%20Nenkova%2C%20Ani%20Jurafsky%2C%20Dan%20Measuring%20importance%20and%20query%20relevance%20in%20topic-focused%20multi-document%20summarization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Surabhi%20Nenkova%2C%20Ani%20Jurafsky%2C%20Dan%20Measuring%20importance%20and%20query%20relevance%20in%20topic-focused%20multi-document%20summarization%202007"
        },
        {
            "id": "Hashimoto_et+al_2017_a",
            "entry": "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple NLP tasks. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hashimoto%2C%20Kazuma%20Xiong%2C%20Caiming%20Tsuruoka%2C%20Yoshimasa%20Socher%2C%20Richard%20A%20joint%20many-task%20model%3A%20Growing%20a%20neural%20network%20for%20multiple%20NLP%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hashimoto%2C%20Kazuma%20Xiong%2C%20Caiming%20Tsuruoka%2C%20Yoshimasa%20Socher%2C%20Richard%20A%20joint%20many-task%20model%3A%20Growing%20a%20neural%20network%20for%20multiple%20NLP%20tasks%202017"
        },
        {
            "id": "Hermann_et+al_2015_a",
            "entry": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karl%20Moritz%20Hermann%20Tomas%20Kocisky%20Edward%20Grefenstette%20Lasse%20Espeholt%20Will%20Kay%20Mustafa%20Suleyman%20and%20Phil%20Blunsom%20Teaching%20machines%20to%20read%20and%20comprehend%20In%20NIPS%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karl%20Moritz%20Hermann%20Tomas%20Kocisky%20Edward%20Grefenstette%20Lasse%20Espeholt%20Will%20Kay%20Mustafa%20Suleyman%20and%20Phil%20Blunsom%20Teaching%20machines%20to%20read%20and%20comprehend%20In%20NIPS%202015"
        },
        {
            "id": "Hewlett_et+al_2016_a",
            "entry": "Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot. WIKIREADING: A novel large-scale language understanding task over Wikipedia. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hewlett%2C%20Daniel%20Lacoste%2C%20Alexandre%20Jones%2C%20Llion%20Polosukhin%2C%20Illia%20WIKIREADING%3A%20A%20novel%20large-scale%20language%20understanding%20task%20over%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hewlett%2C%20Daniel%20Lacoste%2C%20Alexandre%20Jones%2C%20Llion%20Polosukhin%2C%20Illia%20WIKIREADING%3A%20A%20novel%20large-scale%20language%20understanding%20task%20over%202016"
        },
        {
            "id": "Hu_et+al_2018_a",
            "entry": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. Reinforced mnemonic reader for machine comprehension. In IJCAI, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Minghao%20Peng%2C%20Yuxing%20Huang%2C%20Zhen%20Qiu%2C%20Xipeng%20Reinforced%20mnemonic%20reader%20for%20machine%20comprehension%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Minghao%20Peng%2C%20Yuxing%20Huang%2C%20Zhen%20Qiu%2C%20Xipeng%20Reinforced%20mnemonic%20reader%20for%20machine%20comprehension%202018"
        },
        {
            "id": "Hu_et+al_2018_b",
            "entry": "Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Dongsheng Li. Read + verify: Machine reading comprehension with unanswerable questions. In AAAI, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Minghao%20Wei%2C%20Furu%20Peng%2C%20Yuxing%20Huang%2C%20Zhen%20Read%20%2B%20verify%3A%20Machine%20reading%20comprehension%20with%20unanswerable%20questions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Minghao%20Wei%2C%20Furu%20Peng%2C%20Yuxing%20Huang%2C%20Zhen%20Read%20%2B%20verify%3A%20Machine%20reading%20comprehension%20with%20unanswerable%20questions%202018"
        },
        {
            "id": "Iyyer_et+al_2014_a",
            "entry": "Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum III. A neural network for factoid question answering over paragraphs. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyyer%2C%20Mohit%20Boyd-Graber%2C%20Jordan%20Claudino%2C%20Leonardo%20Socher%2C%20Richard%20A%20neural%20network%20for%20factoid%20question%20answering%20over%20paragraphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyyer%2C%20Mohit%20Boyd-Graber%2C%20Jordan%20Claudino%2C%20Leonardo%20Socher%2C%20Richard%20A%20neural%20network%20for%20factoid%20question%20answering%20over%20paragraphs%202014"
        },
        {
            "id": "Joshi_et+al_2017_a",
            "entry": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joshi%2C%20Mandar%20Choi%2C%20Eunsol%20Weld%2C%20Daniel%20S.%20Zettlemoyer%2C%20Luke%20TriviaQA%3A%20A%20large%20scale%20distantly%20supervised%20challenge%20dataset%20for%20reading%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joshi%2C%20Mandar%20Choi%2C%20Eunsol%20Weld%2C%20Daniel%20S.%20Zettlemoyer%2C%20Luke%20TriviaQA%3A%20A%20large%20scale%20distantly%20supervised%20challenge%20dataset%20for%20reading%20comprehension%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kitaev_2018_a",
            "entry": "Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20parsing%20with%20a%20self-attentive%20encoder%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitaev%2C%20Nikita%20Klein%2C%20Dan%20Constituency%20parsing%20with%20a%20self-attentive%20encoder%202018"
        },
        {
            "id": "Kumar_et+al_2016_a",
            "entry": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference resolution. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kenton%20He%2C%20Luheng%20Lewis%2C%20Mike%20Zettlemoyer%2C%20Luke%20End-to-end%20neural%20coreference%20resolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kenton%20He%2C%20Luheng%20Lewis%2C%20Mike%20Zettlemoyer%2C%20Luke%20End-to-end%20neural%20coreference%20resolution%202017"
        },
        {
            "id": "Loshchilov_2017_a",
            "entry": "Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017"
        },
        {
            "id": "Lu_et+al_2016_a",
            "entry": "Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiasen%20Yang%2C%20Jianwei%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Hierarchical%20question-image%20co-attention%20for%20visual%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Jiasen%20Yang%2C%20Jianwei%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Hierarchical%20question-image%20co-attention%20for%20visual%20question%20answering%202016"
        },
        {
            "id": "Lu_et+al_2013_a",
            "entry": "Wang Lu, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. A sentence compression based framework to query-focused multi-document summarization. In ACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Wang%20Raghavan%2C%20Hema%20Castelli%2C%20Vittorio%20Florian%2C%20Radu%20A%20sentence%20compression%20based%20framework%20to%20query-focused%20multi-document%20summarization%202013"
        },
        {
            "id": "Manning_et+al_2014_a",
            "entry": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The Stanford CoreNLP natural language processing toolkit. In ACL, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%20D%20Manning%20Mihai%20Surdeanu%20John%20Bauer%20Jenny%20Rose%20Finkel%20Steven%20Bethard%20and%20David%20McClosky%20The%20Stanford%20CoreNLP%20natural%20language%20processing%20toolkit%20In%20ACL%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christopher%20D%20Manning%20Mihai%20Surdeanu%20John%20Bauer%20Jenny%20Rose%20Finkel%20Steven%20Bethard%20and%20David%20McClosky%20The%20Stanford%20CoreNLP%20natural%20language%20processing%20toolkit%20In%20ACL%202014"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bryan%20McCann%20James%20Bradbury%20Caiming%20Xiong%20and%20Richard%20Socher%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%202017"
        },
        {
            "id": "Min_et+al_2018_a",
            "entry": "Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. Efficient and robust question answering from minimal context over documents. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Min%2C%20Sewon%20Zhong%2C%20Victor%20Socher%2C%20Richard%20Xiong%2C%20Caiming%20Efficient%20and%20robust%20question%20answering%20from%20minimal%20context%20over%20documents.%20In%20ACL%202018"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Petrov_2009_a",
            "entry": "Slav Orlinov Petrov. Coarse-to-fine Natural Language Processing. PhD thesis, University of California, Berkeley, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrov%2C%20Slav%20Orlinov%20Coarse-to-fine%20Natural%20Language%20Processing%202009"
        },
        {
            "id": "Rajpurkar_et+al_2016_a",
            "entry": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20Pranav%20Zhang%2C%20Jian%20Lopyrev%2C%20Konstantin%20Liang%2C%20Percy%20SQuAD%3A%20100%2C%20000%2B%20questions%20for%20machine%20comprehension%20of%20text%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajpurkar%2C%20Pranav%20Zhang%2C%20Jian%20Lopyrev%2C%20Konstantin%20Liang%2C%20Percy%20SQuAD%3A%20100%2C%20000%2B%20questions%20for%20machine%20comprehension%20of%20text%202016"
        },
        {
            "id": "Rajpurkar_et+al_2018_a",
            "entry": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for SQuAD. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20Pranav%20Jia%2C%20Robin%20Liang%2C%20Percy%20Know%20what%20you%20don%E2%80%99t%20know%3A%20Unanswerable%20questions%20for%20SQuAD%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajpurkar%2C%20Pranav%20Jia%2C%20Robin%20Liang%2C%20Percy%20Know%20what%20you%20don%E2%80%99t%20know%3A%20Unanswerable%20questions%20for%20SQuAD%202018"
        },
        {
            "id": "Richardson_et+al_2013_a",
            "entry": "Matthew Richardson, Christopher J. C. Burges, and Erin Renshaw. MCTest: A challenge dataset for the open-domain. In EMNLP, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richardson%2C%20Matthew%20Burges%2C%20Christopher%20J.C.%20Renshaw%2C%20Erin%20MCTest%3A%20A%20challenge%20dataset%20for%20the%20open-domain%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richardson%2C%20Matthew%20Burges%2C%20Christopher%20J.C.%20Renshaw%2C%20Erin%20MCTest%3A%20A%20challenge%20dataset%20for%20the%20open-domain%202013"
        },
        {
            "id": "Rush_et+al_2015_a",
            "entry": "Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015"
        },
        {
            "id": "Seo_et+al_2017_a",
            "entry": "Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Min%20Joon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Min%20Joon%20Kembhavi%2C%20Aniruddha%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Bidirectional%20attention%20flow%20for%20machine%20comprehension%202017"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi Zhang. Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. In IJCAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Tao%20Zhou%2C%20Tianyi%20Long%2C%20Guodong%20Jiang%2C%20Jing%20Reinforced%20self-attention%20network%3A%20a%20hybrid%20of%20hard%20and%20soft%20attention%20for%20sequence%20modeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Tao%20Zhou%2C%20Tianyi%20Long%2C%20Guodong%20Jiang%2C%20Jing%20Reinforced%20self-attention%20network%3A%20a%20hybrid%20of%20hard%20and%20soft%20attention%20for%20sequence%20modeling%202018"
        },
        {
            "id": "Song_et+al_2018_a",
            "entry": "Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. Exploring graph-structured passage representation for multi-hop reading comprehension with graph neural networks. arXiv preprint arXiv:1809.02040, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.02040"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sukhbaatar_et+al_2015_a",
            "entry": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        },
        {
            "id": "Swayamdipta_et+al_2018_a",
            "entry": "Swabha Swayamdipta, Ankur P. Parikh, and Tom Kwiatkowski. Multi-mention learning for reading comprehension with neural cascades. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Swayamdipta%2C%20Swabha%20Parikh%2C%20Ankur%20P.%20Kwiatkowski%2C%20Tom%20Multi-mention%20learning%20for%20reading%20comprehension%20with%20neural%20cascades%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Swayamdipta%2C%20Swabha%20Parikh%2C%20Ankur%20P.%20Kwiatkowski%2C%20Tom%20Multi-mention%20learning%20for%20reading%20comprehension%20with%20neural%20cascades%202018"
        },
        {
            "id": "Trischler_et+al_2017_a",
            "entry": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In The 2nd Workshop on Representation Learning for NLP. ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trischler%2C%20Adam%20Wang%2C%20Tong%20Yuan%2C%20Xingdi%20Harris%2C%20Justin%20NewsQA%3A%20A%20machine%20comprehension%20dataset.%20In%20The%202nd%20Workshop%20on%20Representation%20Learning%20for%20NLP%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trischler%2C%20Adam%20Wang%2C%20Tong%20Yuan%2C%20Xingdi%20Harris%2C%20Justin%20NewsQA%3A%20A%20machine%20comprehension%20dataset.%20In%20The%202nd%20Workshop%20on%20Representation%20Learning%20for%20NLP%202017"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "Wang_2017_a",
            "entry": "Shuohang Wang and Jing Jiang. Machine comprehension using match-LSTM and answer pointer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shuohang%20Jiang%2C%20Jing%20Machine%20comprehension%20using%20match-LSTM%20and%20answer%20pointer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shuohang%20Jiang%2C%20Jing%20Machine%20comprehension%20using%20match-LSTM%20and%20answer%20pointer%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, and Zhiguo Wang. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shuohang%20Yu%2C%20Mo%20Jiang%2C%20Jing%20Zhang%2C%20Wei%20Evidence%20aggregation%20for%20answer%20re-ranking%20in%20open-domain%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shuohang%20Yu%2C%20Mo%20Jiang%2C%20Jing%20Zhang%2C%20Wei%20Evidence%20aggregation%20for%20answer%20re-ranking%20in%20open-domain%20question%20answering%202018"
        },
        {
            "id": "Welbl_et+al_2018_a",
            "entry": "Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. TACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welbl%2C%20Johannes%20Stenetorp%2C%20Pontus%20Riedel%2C%20Sebastian%20Constructing%20datasets%20for%20multi-hop%20reading%20comprehension%20across%20documents%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welbl%2C%20Johannes%20Stenetorp%2C%20Pontus%20Riedel%2C%20Sebastian%20Constructing%20datasets%20for%20multi-hop%20reading%20comprehension%20across%20documents%202018"
        },
        {
            "id": "Weston_et+al_2015_a",
            "entry": "Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLR, 2015. Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. In ICLR, 2017. Caiming Xiong, Victor Zhong, and Richard Socher. DCN+: Mixed objective and deep residual coattention for question answering. In ICLR, 2018. Yi Yang, Wen tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In ACL, 2015. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR, 2018. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Positionaware attention and supervised data improve slot filling. In EMNLP, 2017. Victor Zhong, Caiming Xiong, and Richard Socher. Global-locally self-attentive dialogue state tracker. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weston%2C%20Jason%20Chopra%2C%20Sumit%20Bordes%2C%20Antoine%20Memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weston%2C%20Jason%20Chopra%2C%20Sumit%20Bordes%2C%20Antoine%20Memory%20networks%202015"
        },
        {
            "id": "For_2015_a",
            "entry": "For the best-performing model, we train the CFC using Adam (Kingma & Ba, 2015) for a maximum of 50 epochs with a batch size of 80 examples. We use an initial learning rate of 10\u22123 with (\u03b21, \u03b22) = (0.9, 0.999) and employ a cosine learning rate decay Loshchilov & Hutter (2017) over the maximum budget. We find this approach to outperform a development set-based annealing heuristic as well as those based on piecewise-constant approximations. We evaluate the accuracy of the model on the development set every epoch, and evaluate the model that obtained the best accuracy on the development set on the held-out test set. We present the convergence plot in Figure 8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20the%20bestperforming%20model%20we%20train%20the%20CFC%20using%20Adam%20Kingma%20%20Ba%202015%20for%20a%20maximum%20of%2050%20epochs%20with%20a%20batch%20size%20of%2080%20examples%20We%20use%20an%20initial%20learning%20rate%20of%20103%20with%20%CE%B21%20%CE%B22%20%2009%200999%20and%20employ%20a%20cosine%20learning%20rate%20decay%20Loshchilov%20%20Hutter%202017%20over%20the%20maximum%20budget%20We%20find%20this%20approach%20to%20outperform%20a%20development%20setbased%20annealing%20heuristic%20as%20well%20as%20those%20based%20on%20piecewiseconstant%20approximations%20We%20evaluate%20the%20accuracy%20of%20the%20model%20on%20the%20development%20set%20every%20epoch%20and%20evaluate%20the%20model%20that%20obtained%20the%20best%20accuracy%20on%20the%20development%20set%20on%20the%20heldout%20test%20set%20We%20present%20the%20convergence%20plot%20in%20Figure%208"
        }
    ]
}
