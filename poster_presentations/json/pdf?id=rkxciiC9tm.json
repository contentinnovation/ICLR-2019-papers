{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "NADPEX: AN ON-POLICY TEMPORALLY CONSISTENT",
        "author": "EXPLORATION METHOD FOR DEEP REINFORCEMENT",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkxciiC9tm"
        },
        "abstract": "Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients\u2019 alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy\u2019s stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control."
    },
    "keywords": [
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "stochastic neural network",
            "url": "https://en.wikipedia.org/wiki/stochastic_neural_network"
        },
        {
            "term": "random variable",
            "url": "https://en.wikipedia.org/wiki/random_variable"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        }
    ],
    "abbreviations": {
        "MDP": "Markov Decision Process",
        "NADPEx": "Neural Adaptive Dropout Policy Exploration",
        "PPO": "Policy Optimization",
        "GAE": "Generalized Advantage Estimation",
        "NADPEX": "NEURAL ADAPTIVE DROPOUT POLICY EXPLORATION",
        "BPTT": "back-propagated through time",
        "PAC": "Probably Approximately Correct"
    },
    "highlights": [
        "Exploration remains a challenge in reinforcement learning, in spite of its recent successes in robotic manipulation and locomotion (<a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\">Schulman et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\"><a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a></a>; Schulman et al, 2017b)",
        "Neural Adaptive Dropout Policy Exploration (NADPEx), models stochasticity at large time scales with a distribution of plausible subnetworks",
        "Episode-wise parameter noise injection (<a class=\"ref-link\" id=\"cPlappert_et+al_2017_a\" href=\"#rPlappert_et+al_2017_a\">Plappert et al, 2017</a>) is another way to introduce the hierarchy of stochasticity, which could be regarded as a special case of Neural Adaptive Dropout Policy Exploration, with Gaussian mulitplicative dropout on connection and a heurstic adaptation of variance",
        "TEMPORALLY CONSISTENT EXPLORATION We evaluate how Neural Adaptive Dropout Policy Exploration with Gaussian dropout performs in environments where reward signals are sparse",
        "Neural Adaptive Dropout Policy Exploration, that models stochasticity at large time scale with a distribution of plausible subnetworks from the same complete network to achieve onpolicy temporally consistent exploration",
        "Our experiments exhibit that Neural Adaptive Dropout Policy Exploration successfully solves continuous control tasks, even with strong sparsity in rewards"
    ],
    "key_statements": [
        "Exploration remains a challenge in reinforcement learning, in spite of its recent successes in robotic manipulation and locomotion (<a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\">Schulman et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\"><a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a></a>; Schulman et al, 2017b)",
        "Neural Adaptive Dropout Policy Exploration (NADPEx), models stochasticity at large time scales with a distribution of plausible subnetworks",
        "Episode-wise parameter noise injection (<a class=\"ref-link\" id=\"cPlappert_et+al_2017_a\" href=\"#rPlappert_et+al_2017_a\">Plappert et al, 2017</a>) is another way to introduce the hierarchy of stochasticity, which could be regarded as a special case of Neural Adaptive Dropout Policy Exploration, with Gaussian mulitplicative dropout on connection and a heurstic adaptation of variance",
        "As we scope our discussion down to on-policy exploration strategy, where z from q\u03c6(z) need to be stored for training, Neural Adaptive Dropout Policy Exploration is more scalable to much larger neural networks with possibly larger mini-batch size for stochastic gradient-based optimizers",
        "We developed Neural Adaptive Dropout Policy Exploration based on openai/baselines (<a class=\"ref-link\" id=\"cDhariwal_et+al_2017_a\" href=\"#rDhariwal_et+al_2017_a\">Dhariwal et al, 2017</a>)",
        "TEMPORALLY CONSISTENT EXPLORATION We evaluate how Neural Adaptive Dropout Policy Exploration with Gaussian dropout performs in environments where reward signals are sparse",
        "Neural Adaptive Dropout Policy Exploration, that models stochasticity at large time scale with a distribution of plausible subnetworks from the same complete network to achieve onpolicy temporally consistent exploration",
        "Our experiments exhibit that Neural Adaptive Dropout Policy Exploration successfully solves continuous control tasks, even with strong sparsity in rewards"
    ],
    "summary": [
        "Exploration remains a challenge in reinforcement learning, in spite of its recent successes in robotic manipulation and locomotion (<a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\"><a class=\"ref-link\" id=\"cSchulman_et+al_2015_b\" href=\"#rSchulman_et+al_2015_b\">Schulman et al, 2015b</a></a>; <a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a></a>; <a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\"><a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016</a></a>; Schulman et al, 2017b).",
        "This work proposes Neural Adaptive Dropout Policy Exploration (NADPEx) as a simple, scalable and improvement-guaranteed method for on-policy temporally consistent exploration, which generalizes <a class=\"ref-link\" id=\"cOsband_et+al_2016_a\" href=\"#rOsband_et+al_2016_a\">Osband et al (2016</a>) and <a class=\"ref-link\" id=\"cPlappert_et+al_2017_a\" href=\"#rPlappert_et+al_2017_a\">Plappert et al (2017</a>).",
        "Neural Adaptive Dropout Policy Exploration (NADPEx), models stochasticity at large time scales with a distribution of plausible subnetworks.",
        "Episode-wise parameter noise injection (<a class=\"ref-link\" id=\"cPlappert_et+al_2017_a\" href=\"#rPlappert_et+al_2017_a\">Plappert et al, 2017</a>) is another way to introduce the hierarchy of stochasticity, which could be regarded as a special case of NADPEx, with Gaussian mulitplicative dropout on connection and a heurstic adaptation of variance.",
        "This reduction in variance could be enhanced if ones choose to drop modules or paths in NADPEx. More importantly, as we scope our discussion down to on-policy exploration strategy, where z from q\u03c6(z) need to be stored for training, NADPEx is more scalable to much larger neural networks with possibly larger mini-batch size for stochastic gradient-based optimizers.",
        "To dropout neurons rather than connections, NADPEx has a complexity of O(N m) for both sampling and memory, comparing to O(N m2) of parameter noise, with m denoting the number of neurons in one layer.",
        "When the initial dropout rate is small, NADPEx agents learn to earn reward faster than agents with only action space noise.",
        "Their higher variance between random seeds indicates that some of them are not exploring as efficiently and the NADPEx policies may not be optimal, normally they will be surpassed by ones with pik = 0.1 in the middle stage of training.",
        "4.2 TEMPORALLY CONSISTENT EXPLORATION We evaluate how NADPEx with Gaussian dropout performs in environments where reward signals are sparse.",
        "Different from clipping PPO, NADPEx with small initial dropout rate performs best, earning much higher score than action noise.",
        "Based on hierarchical reinforcement learning, <a class=\"ref-link\" id=\"cFlorensa_et+al_2017_a\" href=\"#rFlorensa_et+al_2017_a\">Florensa et al (2017</a>) models the stochasticity at large time scales with a random variable - option - and model low-level policies with Stochastic Neural Networks.",
        "In NADPEx, this stochasticity at large time scales is captured with a distribution of plausible neural subnetworks from the same complete network.",
        "NADPEx, that models stochasticity at large time scale with a distribution of plausible subnetworks from the same complete network to achieve onpolicy temporally consistent exploration.",
        "These subnetworks are sampled through dropout at the beginning of episodes, used to explore the environment with diverse and consistent behavioral patterns and updated through simultaneous gradient back-propagation.",
        "Our experiments exhibit that NADPEx successfully solves continuous control tasks, even with strong sparsity in rewards"
    ],
    "headline": "We introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration - for deep reinforcement learning agents",
    "reference_links": [
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329\u20131338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. arXiv preprint arXiv:1704.03012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03012"
        },
        {
            "id": "Fortunato_et+al_2017_a",
            "entry": "Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2574\u20132584, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Co-Reyes%2C%20John%20Levine%2C%20Sergey%20Ex2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Co-Reyes%2C%20John%20Levine%2C%20Sergey%20Ex2%3A%20Exploration%20with%20exemplar%20models%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "Gangwani_2017_a",
            "entry": "Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. arXiv preprint arXiv:1711.01012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01012"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic neural networks. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Levine%2C%20Sergey%20Sutskever%2C%20Ilya%20Mnih%2C%20Andriy%20Muprop%3A%20Unbiased%20backpropagation%20for%20stochastic%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Levine%2C%20Sergey%20Sutskever%2C%20Ilya%20Mnih%2C%20Andriy%20Muprop%3A%20Unbiased%20backpropagation%20for%20stochastic%20neural%20networks%202016"
        },
        {
            "id": "Henderson_et+al_0000_a",
            "entry": "Peter Henderson, Thang Doan, Riashat Islam, and David Meger. Bayesian policy gradients via alpha divergence dropout inference. arXiv preprint arXiv:1712.02037, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02037"
        },
        {
            "id": "Henderson_et+al_0000_b",
            "entry": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06560"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1207.0580"
        },
        {
            "id": "Houthooft_et+al_2016_a",
            "entry": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109\u20131117, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016"
        },
        {
            "id": "Jaksch_et+al_2010_a",
            "entry": "Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563\u20131600, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2015_a",
            "entry": "Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Maddison_2016_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Molchanov_et+al_2017_a",
            "entry": "Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. arXiv preprint arXiv:1701.05369, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05369"
        },
        {
            "id": "Osband_et+al_2014_a",
            "entry": "Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0635"
        },
        {
            "id": "Osband_et+al_2016_a",
            "entry": "Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pp. 4026\u20134034, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016"
        },
        {
            "id": "Osband_et+al_2017_a",
            "entry": "Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07608"
        },
        {
            "id": "Ostrovski_et+al_2017_a",
            "entry": "Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01310"
        },
        {
            "id": "Pathak_et+al_0000_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "Plappert_et+al_2017_a",
            "entry": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "Raiko_et+al_2015_a",
            "entry": "Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary stochastic feedforward neural networks. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raiko%2C%20Tapani%20Berglund%2C%20Mathias%20Alain%2C%20Guillaume%20Dinh%2C%20Laurent%20Techniques%20for%20learning%20binary%20stochastic%20feedforward%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raiko%2C%20Tapani%20Berglund%2C%20Mathias%20Alain%2C%20Guillaume%20Dinh%2C%20Laurent%20Techniques%20for%20learning%20binary%20stochastic%20feedforward%20neural%20networks%202015"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial Intelligence and Statistics, pp. 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528\u2013 3536, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Abbeel%2C%20Pieter%20Gradient%20estimation%20using%20stochastic%20computation%20graphs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Abbeel%2C%20Pieter%20Gradient%20estimation%20using%20stochastic%20computation%20graphs%202015"
        },
        {
            "id": "Schulman_et+al_2015_b",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889\u20131897, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_0000_a",
            "entry": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015c.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "Schulman_et+al_0000_b",
            "entry": "John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft qlearning. arXiv preprint arXiv:1704.06440, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06440"
        },
        {
            "id": "Schulman_et+al_0000_c",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Such_et+al_2017_a",
            "entry": "Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750\u2013 2759, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Wan_et+al_2013_a",
            "entry": "Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning, pp. 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013"
        },
        {
            "id": "Wang_2013_a",
            "entry": "Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine learning, pp. 118\u2013126, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Sida%20Manning%2C%20Christopher%20Fast%20dropout%20training%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Sida%20Manning%2C%20Christopher%20Fast%20dropout%20training%202013"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01224"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient. In International Conference on Machine Learning, pp. 5459\u20135468, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Tianbing%20Liu%2C%20Qiang%20Zhao%2C%20Liang%20Peng%2C%20Jian%20Learning%20to%20explore%20via%20meta-policy%20gradient%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Tianbing%20Liu%2C%20Qiang%20Zhao%2C%20Liang%20Peng%2C%20Jian%20Learning%20to%20explore%20via%20meta-policy%20gradient%202018"
        }
    ]
}
