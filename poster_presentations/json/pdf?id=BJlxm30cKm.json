{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AN EMPIRICAL STUDY OF EXAMPLE FORGETTING DURING DEEP NEURAL NETWORK LEARNING",
        "author": "Mariya Toneva",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BJlxm30cKm"
        },
        "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \u201cforgetting event\u201d to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set\u2019s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "catastrophic forgetting",
            "url": "https://en.wikipedia.org/wiki/Catastrophic_Forgetting"
        },
        {
            "term": "data set",
            "url": "https://en.wikipedia.org/wiki/data_set"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent"
    },
    "highlights": [
        "In particular neural networks, cannot perform continual learning",
        "They have a tendency to forget previously learnt information when trained on new tasks, a phenomenon usually called catastrophic forgetting (Kirkpatrick et al, 2017; <a class=\"ref-link\" id=\"cRitter_et+al_2018_a\" href=\"#rRitter_et+al_2018_a\">Ritter et al, 2018</a>)",
        "One of the hypothesized causes of catastrophic forgetting in neural networks is the shift in the input distribution across different tasks\u2014e.g., a lack of common factors or structure in the inputs of different tasks might lead standard optimization techniques to converge to radically different solutions each time a new task is presented",
        "Stability across seeds To test the stability of our metric with respect to the variance generated by stochastic gradient descent, we compute the number of forgetting events per example for 10 different random seeds and measure their correlation",
        "In this paper, inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks when training on single classification tasks",
        "We show that catastrophic forgetting can occur in the context of what is usually considered to be a single task"
    ],
    "key_statements": [
        "In particular neural networks, cannot perform continual learning",
        "They have a tendency to forget previously learnt information when trained on new tasks, a phenomenon usually called catastrophic forgetting (Kirkpatrick et al, 2017; <a class=\"ref-link\" id=\"cRitter_et+al_2018_a\" href=\"#rRitter_et+al_2018_a\">Ritter et al, 2018</a>)",
        "One of the hypothesized causes of catastrophic forgetting in neural networks is the shift in the input distribution across different tasks\u2014e.g., a lack of common factors or structure in the inputs of different tasks might lead standard optimization techniques to converge to radically different solutions each time a new task is presented",
        "Our experimental findings suggest that: a) there exist a large number of unforgettable examples, i.e., examples that are never forgotten once learnt, those examples are stable across seeds and strongly correlated from one neural architecture to another; b) examples with noisy labels are among the most forgotten examples, along with images with \u201cuncommon\u201d features, visually complicated to classify; c) training a neural network on a dataset where a very large fraction of the least forgotten examples have been removed still results in extremely competitive performance on the test set.\n2 RELATED WORK",
        "Stability across seeds To test the stability of our metric with respect to the variance generated by stochastic gradient descent, we compute the number of forgetting events per example for 10 different random seeds and measure their correlation",
        "In this paper, inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks when training on single classification tasks",
        "We show that catastrophic forgetting can occur in the context of what is usually considered to be a single task"
    ],
    "summary": [
        "In particular neural networks, cannot perform continual learning.",
        "Our experimental findings suggest that: a) there exist a large number of unforgettable examples, i.e., examples that are never forgotten once learnt, those examples are stable across seeds and strongly correlated from one neural architecture to another; b) examples with noisy labels are among the most forgotten examples, along with images with \u201cuncommon\u201d features, visually complicated to classify; c) training a neural network on a dataset where a very large fraction of the least forgotten examples have been removed still results in extremely competitive performance on the test set.",
        "Stability across seeds To test the stability of our metric with respect to the variance generated by stochastic gradient descent, we compute the number of forgetting events per example for 10 different random seeds and measure their correlation.",
        "First learning events We investigate whether unforgettable and forgettable examples need to be presented different numbers of times in order to be learnt for the first time.",
        "We randomly change the labels of 20% of CIFAR-10 and record the number of forgetting events of both the noisy and regular examples through training.",
        "By removing examples ordered by number of forgetting events, 30% of the dataset can be removed while maintaining comparable generalization performance as the base model trained on the full dataset, and up to 35% can be removed with marginal degradation.",
        "The results on the other datasets are similar: a large fraction of training examples can be ignored without hurting the final generalization performance of the classifiers (Figure 6).",
        "In Figure 5 (Right), we show the evolution of the generalization error when we remove from the dataset 5,000 examples with increasing forgetting statistics.",
        "Each point in the figure corresponds to the generalization error of a model trained on the full dataset minus 5,000 examples as a function of the average number of forgetting events in those 5,000 examples.",
        "We see in Figure 6 that performance on those datasets starts to degrade at different fractions of removed examples: the number of support vectors varies from one dataset to the other, based on the complexity of the underlying data distribution.",
        "In this paper, inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks when training on single classification tasks.",
        "The unforgettable examples seem to play little part in the final performance of the classifier as they can be removed from the training set without hurting generalization.",
        "Future work involves understanding forgetting events better from a theoretical perspective, exploring potential applications to other areas of supervised learning, such as speech or text and to reinforcement learning where forgetting is prevalent due to the continual shift of the underlying distribution"
    ],
    "headline": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks",
    "reference_links": [
        {
            "id": "Advani_2017_a",
            "entry": "Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks. CoRR, abs/1710.03667, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03667"
        },
        {
            "id": "Bengio_2007_a",
            "entry": "Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel Machines. MIT Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20LeCun%2C%20Yann%20Scaling%20learning%20algorithms%20towards%20AI.%20In%20Large%20Scale%20Kernel%20Machines%202007"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20Jerome%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20Jerome%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Brodley_1999_a",
            "entry": "Carla E Brodley and Mark A Friedl. Identifying mislabeled training data. Journal of artificial intelligence research, 11:131\u2013167, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brodley%2C%20Carla%20E.%20Friedl%2C%20Mark%20A.%20Identifying%20mislabeled%20training%20data%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brodley%2C%20Carla%20E.%20Friedl%2C%20Mark%20A.%20Identifying%20mislabeled%20training%20data%201999"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. In Advances in Neural Information Processing Systems, pp. 1002\u20131012, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Haw-Shiuan%20Learned-Miller%2C%20Erik%20McCallum%2C%20Andrew%20Active%20Bias%3A%20Training%20More%20Accurate%20Neural%20Networks%20by%20Emphasizing%20High%20Variance%20Samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Haw-Shiuan%20Learned-Miller%2C%20Erik%20McCallum%2C%20Andrew%20Active%20Bias%3A%20Training%20More%20Accurate%20Neural%20Networks%20by%20Emphasizing%20High%20Variance%20Samples%202017"
        },
        {
            "id": "Chaudhari_et+al_2016_a",
            "entry": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing Gradient Descent Into Wide Valleys. ICLR \u201917, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-SGD%3A%20Biasing%20Gradient%20Descent%20Into%20Wide%20Valleys%202016"
        },
        {
            "id": "Devries_2017_a",
            "entry": "Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "Fan_et+al_2017_a",
            "entry": "Yang Fan, Fei Tian, Tao Qin, and Jiang Bian. Learning What Data to Learn. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Yang%20Tian%2C%20Fei%20Qin%2C%20Tao%20Bian%2C%20Jiang%20Learning%20What%20Data%20to%20Learn%202017"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. of ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700\u20134708, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20Connected%20Convolutional%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20Connected%20Convolutional%20Networks%202017"
        },
        {
            "id": "Jiang_et+al_2018_a",
            "entry": "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning datadriven curriculum for very deep neural networks on corrupted labels. In Proceedings of the 35th International Conference on Machine Learning. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Lu%20Zhou%2C%20Zhengyuan%20Leung%2C%20Thomas%20Li%2C%20Li-Jia%20MentorNet%3A%20Learning%20datadriven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Lu%20Zhou%2C%20Zhengyuan%20Leung%2C%20Thomas%20Li%2C%20Li-Jia%20MentorNet%3A%20Learning%20datadriven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018"
        },
        {
            "id": "John_1995_a",
            "entry": "George H John. Robust decision trees: removing outliers from databases. In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, pp. 174\u2013179. AAAI Press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%2C%20George%20H.%20Robust%20decision%20trees%3A%20removing%20outliers%20from%20databases%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%2C%20George%20H.%20Robust%20decision%20trees%3A%20removing%20outliers%20from%20databases%201995"
        },
        {
            "id": "Dy_2018_a",
            "entry": "Angelos Katharopoulos and Franois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In Jennifer G. Dy and Andreas Krause (eds.), ICML, volume 80 of JMLR Workshop and Conference Proceedings, pp. 2530\u20132539. JMLR.org, 2018. URL http://dblp.uni-trier.de/db/conf/icml/icml2018.html#KatharopoulosF18.",
            "url": "http://dblp.uni-trier.de/db/conf/icml/icml2018.html#KatharopoulosF18",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angelos%20Katharopoulos%20and%20Franois%20Fleuret%20Not%20all%20samples%20are%20created%20equal%20Deep%20learning%20with%20importance%20sampling%20In%20Jennifer%20G%20Dy%20and%20Andreas%20Krause%20eds%20ICML%20volume%2080%20of%20JMLR%20Workshop%20and%20Conference%20Proceedings%20pp%2025302539%20JMLRorg%202018%20URL%20httpdblpunitrierdedbconficmlicml2018htmlKatharopoulosF18"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "Kim_2018_a",
            "entry": "Tae-Hoon Kim and Jonghyun Choi. Screenernet: Learning curriculum for neural networks. CoRR, abs/1801.00904, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1801.html#abs-1801-00904.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1801.html#abs-1801-00904",
            "arxiv_url": "https://arxiv.org/pdf/1801.00904"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http://arxiv.org/abs/1412.6980.cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.",
            "url": "http://arxiv.org/abs/1412.6980.cite",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kirkpatrick_et+al_2016_a",
            "entry": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and Others. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202016"
        },
        {
            "id": "Kleinberg_et+al_2018_a",
            "entry": "Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local minima? CoRR, abs/1802.06175, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-06175.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-06175",
            "arxiv_url": "https://arxiv.org/pdf/1802.06175"
        },
        {
            "id": "Koh_2017_a",
            "entry": "Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh (eds.), ICML, volume 70 of JMLR Workshop and Conference Proceedings, pp. 1885\u20131894. JMLR.org, 2017. URL http://dblp.uni-trier.de/db/conf/icml/icml2017.html#KohL17.",
            "url": "http://dblp.uni-trier.de/db/conf/icml/icml2017.html#KohL17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20Pang%20Wei%20Liang%2C%20Percy%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https://www.cs.toronto.edu/\u0303kriz/learning-features-2009-TR.pdf.",
            "url": "https://www.cs.toronto.edu/\u0303kriz/learning-features-2009-TR.pdf"
        },
        {
            "id": "Kumar_et+al_2010_a",
            "entry": "M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-Paced Learning for Latent Variable Models. In Proc. of NIPS, pp. 1\u20139, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-Paced%20Learning%20for%20Latent%20Variable%20Models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-Paced%20Learning%20for%20Latent%20Variable%20Models%202010"
        },
        {
            "id": "Lecun_et+al_1999_a",
            "entry": "Y. LeCun, C. Cortes C., and C. Burges. The mnist database of handwritten digits. 1999. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Lee_2011_a",
            "entry": "Yong Jae Lee and Kristen Grauman. Learning the easy things first: Self-paced visual category discovery. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1721\u20131728. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yong%20Jae%20Grauman%2C%20Kristen%20Learning%20the%20easy%20things%20first%3A%20Self-paced%20visual%20category%20discovery%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yong%20Jae%20Grauman%2C%20Kristen%20Learning%20the%20easy%20things%20first%3A%20Self-paced%20visual%20category%20discovery%202011"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. CoRR, abs/1804.08838, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1804.html#abs-1804-08838.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1804.html#abs-1804-08838",
            "arxiv_url": "https://arxiv.org/pdf/1804.08838"
        },
        {
            "id": "Neyshabur_et+al_2014_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. CoRR, abs/1412.6614, 2014. URL http://dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14",
            "arxiv_url": "https://arxiv.org/pdf/1412.6614"
        },
        {
            "id": "Perez_et+al_2018_a",
            "entry": "Guillermo Valle Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. CoRR, abs/1805.08522, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-08522.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-08522",
            "arxiv_url": "https://arxiv.org/pdf/1805.08522"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proc. of ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Ritter_et+al_2018_a",
            "entry": "Hippolyt Ritter, Aleksandar Botev, and David Barber. Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting. 2018. URL http://arxiv.org/abs/1805.07810.",
            "url": "http://arxiv.org/abs/1805.07810",
            "arxiv_url": "https://arxiv.org/pdf/1805.07810"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05952"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The Implicit Bias of Gradient Descent on Separable Data. 2017. URL http://arxiv.org/abs/1710.10345.",
            "url": "http://arxiv.org/abs/1710.10345",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Sukhbaatar_et+al_2014_a",
            "entry": "Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2080"
        },
        {
            "id": "Tachet_et+al_2018_a",
            "entry": "R. Tachet, M. Pezeshki, S. Shabanian, A. Courville, and Y. Bengio. On the learning dynamics of deep neural networks. 2018. doi: arXiv:1809.06848v1. URL https://arxiv.org/abs/1809.06848.",
            "url": "https://arxiv.org/abs/1809.06848",
            "arxiv_url": "https://arxiv.org/pdf/1809.06848v1"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying Generalization Properties in Neural Networks. pp. 1\u201323, 2018. doi: arXiv:1809.07402v1. URL http://arxiv.org/abs/1809.07402.",
            "url": "http://arxiv.org/abs/1809.07402",
            "arxiv_url": "https://arxiv.org/pdf/1809.07402v1"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Tengyu Xu, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Convergence of sgd in learning relu models with separable data. CoRR, abs/1806.04339, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-04339.",
            "url": "http://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-04339",
            "arxiv_url": "https://arxiv.org/pdf/1806.04339"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2016. URL http://arxiv.org/abs/1605.07146.cite arxiv:1605.07146.",
            "url": "http://arxiv.org/abs/1605.07146.cite",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "Zhao_2015_a",
            "entry": "Peilin Zhao and Tong Zhang. Stochastic Optimization with Importance Sampling for Regularized Loss Minimization. In Proc. of ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Peilin%20Zhang%2C%20Tong%20Stochastic%20Optimization%20with%20Importance%20Sampling%20for%20Regularized%20Loss%20Minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Peilin%20Zhang%2C%20Tong%20Stochastic%20Optimization%20with%20Importance%20Sampling%20for%20Regularized%20Loss%20Minimization%202015"
        }
    ]
}
