{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "A Family of Nonparametric Density Estimation Algorithms",
        "author": "E. G. Tabak, Cristina V. Turner",
        "date": 2012,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJed6j0cKX",
            "doi": "10.1002/cpa.21423"
        },
        "journal": "Communications on Pure and Applied Mathematics",
        "volume": "66",
        "abstract": "For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task \u2013 so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.",
        "pages": "145-164"
    },
    "keywords": [
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "inverse problem",
            "url": "https://en.wikipedia.org/wiki/inverse_problem"
        },
        {
            "term": "astrophysics",
            "url": "https://en.wikipedia.org/wiki/astrophysics"
        },
        {
            "term": "inverse process",
            "url": "https://en.wikipedia.org/wiki/inverse_process"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "approximate Bayesian computation",
            "url": "https://en.wikipedia.org/wiki/approximate_Bayesian_computation"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        }
    ],
    "abbreviations": {
        "INNs": "invertible neural networks",
        "ABC": "approximate Bayesian computation",
        "cGANs": "Conditional GANs",
        "cVAEs": "conditional variational autoencoders",
        "MMD": "Maximum Mean Discrepancy",
        "ERC": "European Research Council"
    },
    "highlights": [
        "When analyzing complex physical systems, a common problem is that the system parameters of interest cannot be measured directly",
        "We show that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically in the asymptotic limit of zero loss, and practically on synthetic and real-world data from astrophysics and medicine",
        "Our experiments show that joint bi-directional training of f and g avoids many complications arising in e.g. conditional variational autoencoders or Bayesian neural networks, which have to learn the forward process implicitly",
        "The results produced by the invertible neural networks provide relevant insights: First, we find that the posteriors for layer thickness d and anisotropy g match the shape of their priors, i.e. y\u2217 holds no information about these parameters \u2013 they are unrecoverable",
        "We have shown that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically and practically on problems from medicine and astrophysics",
        "We see the following fundamental advantages of our invertible neural networks-based method compared to alternative approaches: Firstly, one can learn the forward process and obtain the inverse process \u2018for free\u2019, as opposed to e.g. Conditional GANs, which focus on the inverse and learn the forward process only implicitly"
    ],
    "key_statements": [
        "When analyzing complex physical systems, a common problem is that the system parameters of interest cannot be measured directly",
        "We show that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically in the asymptotic limit of zero loss, and practically on synthetic and real-world data from astrophysics and medicine",
        "We consider a common scenario in natural and life sciences: Researchers are interested in a set of variables x \u2208 RD describing some phenomenon of interest, but only variables y \u2208 RM can be observed, for which the theory of the respective research field provides a model y = s(x) for the forward process",
        "We introduce a latent random variable z \u2208 RK drawn from a multi-variate standard normal distribution and reparametrize q(x | y) in terms of a deterministic function g of y and z, represented by a neural network with parameters \u03b8: x = g(y, z; \u03b8) with z \u223c p(z) = N (z; 0, IK )",
        "In contrast to standard methodology, we propose to learn the model g(y, z; \u03b8) of the inverse process jointly with a model f (x; \u03b8) approximating the known forward process s(x):",
        "Our experiments show that joint bi-directional training of f and g avoids many complications arising in e.g. conditional variational autoencoders or Bayesian neural networks, which have to learn the forward process implicitly",
        "We train an invertible neural networks for this problem, along with two ablations, as well as a conditional variational autoencoders with and without IAF (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>) and a network using the method of <a class=\"ref-link\" id=\"cKendall_2017_a\" href=\"#rKendall_2017_a\">Kendall and Gal (2017</a>), with dropout sampling and additional aleatoric error terms for each parameter",
        "The results produced by the invertible neural networks provide relevant insights: First, we find that the posteriors for layer thickness d and anisotropy g match the shape of their priors, i.e. y\u2217 holds no information about these parameters \u2013 they are unrecoverable",
        "In Fig. 10 in the appendix, we show how the invertible neural networks is applied to real multispectral images",
        "We have shown that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically and practically on problems from medicine and astrophysics",
        "We see the following fundamental advantages of our invertible neural networks-based method compared to alternative approaches: Firstly, one can learn the forward process and obtain the inverse process \u2018for free\u2019, as opposed to e.g. Conditional GANs, which focus on the inverse and learn the forward process only implicitly"
    ],
    "summary": [
        "When analyzing complex physical systems, a common problem is that the system parameters of interest cannot be measured directly.",
        "We show that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically in the asymptotic limit of zero loss, and practically on synthetic and real-world data from astrophysics and medicine.",
        "Modeling the conditional posterior of an inverse process is a classical statistical task that can in principle be solved by Bayesian methods.",
        "Neural networks can be trained to predict accurate sufficient statistics for parametric posteriors (<a class=\"ref-link\" id=\"cPapamakarios_2016_a\" href=\"#rPapamakarios_2016_a\">Papamakarios and Murray, 2016</a>; <a class=\"ref-link\" id=\"cSiddharth_et+al_2017_a\" href=\"#rSiddharth_et+al_2017_a\">Siddharth et al, 2017</a>), or can be designed to learn a mean-field distribution for the network\u2019s weights via dropout variational inference (Gal and Ghahramani, 2015; <a class=\"ref-link\" id=\"cKingma_et+al_2015_a\" href=\"#rKingma_et+al_2015_a\">Kingma et al, 2015</a>).",
        "These networks successfully learned unconditional generative distributions for artificial data and standard image sets (e.g. MNIST, CelebA, LSUN bedrooms), and some encouraging results for conditional modeling exist as well (Oord et al, 2016; <a class=\"ref-link\" id=\"cSalimans_et+al_2017_a\" href=\"#rSalimans_et+al_2017_a\">Salimans et al, 2017</a>; <a class=\"ref-link\" id=\"cPapamakarios_et+al_2017_a\" href=\"#rPapamakarios_et+al_2017_a\">Papamakarios et al, 2017</a>; <a class=\"ref-link\" id=\"cUria_et+al_2016_a\" href=\"#rUria_et+al_2016_a\">Uria et al, 2016</a>).",
        "Our experiments show that joint bi-directional training of f and g avoids many complications arising in e.g. cVAEs or Bayesian neural networks, which have to learn the forward process implicitly.",
        "In appendix Sec. 1, we prove the following theorem: Theorem: If an INN f (x) = [y, z] is trained as proposed, and both the supervised loss Ly = E[(y\u2212fy(x))2] and the unsupervised loss Lz = D q(y, z), p(y) p(z) reach zero, sampling according to Eq 1 with g = f \u22121 returns the true posterior p(x | y\u2217) for any measurement y\u2217.",
        "This model constitutes the forward process, and traditional methods to learn point estimates of the inverse (<a class=\"ref-link\" id=\"cWirkert_et+al_2016_a\" href=\"#rWirkert_et+al_2016_a\">Wirkert et al, 2016</a>; 2017; <a class=\"ref-link\" id=\"cClaridge_2013_a\" href=\"#rClaridge_2013_a\">Claridge and Hidovic-Rowe, 2013</a>) are already sufficiently reliable to be used in clinical trials.",
        "We train an INN for this problem, along with two ablations, as well as a cVAE with and without IAF (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>) and a network using the method of <a class=\"ref-link\" id=\"cKendall_2017_a\" href=\"#rKendall_2017_a\">Kendall and Gal (2017</a>), with dropout sampling and additional aleatoric error terms for each parameter.",
        "Fig. 4 shows generated parameter distributions for one fixed measurement y\u2217, comparing the INN to cVAE-IAF, Dropout sampling and ABC.",
        "The results produced by the INN provide relevant insights: First, we find that the posteriors for layer thickness d and anisotropy g match the shape of their priors, i.e. y\u2217 holds no information about these parameters \u2013 they are unrecoverable.",
        "We plan to systematically analyze the properties of different invertible architectures, as well as more flexible models utilizing cycle losses, in the context of representative inverse problem.",
        "We are interested in how our method can be scaled up to higher dimensionalities, where MMD becomes less effective"
    ],
    "headline": "We argue that a particular class of neural networks is well suited for this task \u2013 so-called Invertible Neural Networks",
    "reference_links": [
        {
            "id": "Baldwin_et+al_1981_a",
            "entry": "Jack A. Baldwin, Mark M. Phillips, and Roberto Terlevich. Classification parameters for the emission-line spectra of extragalactic objects. PASP, 93:5\u201319, February 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldwin%2C%20Jack%20A.%20Phillips%2C%20Mark%20M.%20Terlevich%2C%20Roberto%20Classification%20parameters%20for%20the%20emission-line%20spectra%20of%20extragalactic%20objects%201981-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldwin%2C%20Jack%20A.%20Phillips%2C%20Mark%20M.%20Terlevich%2C%20Roberto%20Classification%20parameters%20for%20the%20emission-line%20spectra%20of%20extragalactic%20objects%201981-02"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Rianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester normalizing flows for variational inference. arXiv:1803.05649, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05649"
        },
        {
            "id": "Blei_et+al_2017_a",
            "entry": "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "Claridge_2013_a",
            "entry": "Ela Claridge and Dzena Hidovic-Rowe. Model based inversion for deriving maps of histological parameters characteristic of cancer from ex-vivo multispectral images of the colon. IEEE Trans Med Imaging, November 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Claridge%2C%20Ela%20Hidovic-Rowe%2C%20Dzena%20Model%20based%20inversion%20for%20deriving%20maps%20of%20histological%20parameters%20characteristic%20of%20cancer%20from%20ex-vivo%20multispectral%20images%20of%20the%20colon%202013-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Claridge%2C%20Ela%20Hidovic-Rowe%2C%20Dzena%20Model%20based%20inversion%20for%20deriving%20maps%20of%20histological%20parameters%20characteristic%20of%20cancer%20from%20ex-vivo%20multispectral%20images%20of%20the%20colon%202013-11"
        },
        {
            "id": "Danihelka_et+al_2017_a",
            "entry": "Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Comparison of maximum likelihood and GAN-based training of Real NVPs. arXiv:1705.05263, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05263"
        },
        {
            "id": "Deco_1995_a",
            "entry": "Gustavo Deco and Wilfried Brauer. Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures. Neural Networks, 8(4):525\u2013535, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deco%2C%20Gustavo%20Brauer%2C%20Wilfried%20Nonlinear%20higher-order%20statistical%20decorrelation%20by%20volume-conserving%20neural%20architectures%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deco%2C%20Gustavo%20Brauer%2C%20Wilfried%20Nonlinear%20higher-order%20statistical%20decorrelation%20by%20volume-conserving%20neural%20architectures%201995"
        },
        {
            "id": "Dinh_et+al_2014_a",
            "entry": "Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv:1410.8516, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.8516"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Donahue_et+al_2017_a",
            "entry": "Chris Donahue, Akshay Balsubramani, Julian McAuley, and Zachary C Lipton. Semantically decomposing the latent spaces of generative adversarial networks. arXiv:1705.07904, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07904"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "Gal_0000_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate variational inference. arXiv:1506.02158, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02158"
        },
        {
            "id": "Gamerman_2006_a",
            "entry": "Dani Gamerman and Hedibert F Lopes. Markov Chain Monte Carlo: Stochastic simulation for Bayesian inference. Chapman and Hall/CRC, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gamerman%2C%20Dani%20Lopes%2C%20Hedibert%20F.%20Markov%20Chain%20Monte%20Carlo%3A%20Stochastic%20simulation%20for%20Bayesian%20inference%202006"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881\u2013889, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20MADE%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20MADE%3A%20Masked%20autoencoder%20for%20distribution%20estimation%202015"
        },
        {
            "id": "Gomez_et+al_2017_a",
            "entry": "Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. In Advances in Neural Information Processing Systems, pages 2211\u20132221, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez%2C%20Aidan%20N.%20Ren%2C%20Mengye%20Urtasun%2C%20Raquel%20Grosse%2C%20Roger%20B.%20The%20reversible%20residual%20network%3A%20Backpropagation%20without%20storing%20activations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez%2C%20Aidan%20N.%20Ren%2C%20Mengye%20Urtasun%2C%20Raquel%20Grosse%2C%20Roger%20B.%20The%20reversible%20residual%20network%3A%20Backpropagation%20without%20storing%20activations%202017"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "Grover_et+al_2017_a",
            "entry": "Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Combining maximum likelihood and adversarial learning in generative models. arXiv:1705.08868, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08868"
        },
        {
            "id": "Hanahan_2011_a",
            "entry": "Douglas Hanahan and Robert A. Weinberg. Hallmarks of cancer: The next generation. Cell, 144(5):646\u2013674, March 2011. ISSN 00928674.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanahan%2C%20Douglas%20Weinberg%2C%20Robert%20A.%20Hallmarks%20of%20cancer%3A%20The%20next%20generation%202011-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanahan%2C%20Douglas%20Weinberg%2C%20Robert%20A.%20Hallmarks%20of%20cancer%3A%20The%20next%20generation%202011-03"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive flows. arXiv:1804.00779, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00779"
        },
        {
            "id": "Hyvaerinen_1999_a",
            "entry": "Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429\u2013439, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyv%C3%A4rinen%2C%20Aapo%20Pajunen%2C%20Petteri%20Nonlinear%20independent%20component%20analysis%3A%20Existence%20and%20uniqueness%20results%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyv%C3%A4rinen%2C%20Aapo%20Pajunen%2C%20Petteri%20Nonlinear%20independent%20component%20analysis%3A%20Existence%20and%20uniqueness%20results%201999"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 1125\u20131134, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Jacobsen_et+al_2018_a",
            "entry": "J\u00f6rn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-RevNet: Deep invertible networks. arXiv:1802.07088, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07088"
        },
        {
            "id": "Kendall_2017_a",
            "entry": "Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, pages 5580\u20135590, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%202017"
        },
        {
            "id": "Kewley_et+al_2013_a",
            "entry": "Lisa J. Kewley, Michael A. Dopita, Claus Leitherer, Romeel Dav\u00e9, Tiantian Yuan, Mark Allen, Brent Groves, and Ralph Sutherland. Theoretical evolution of optical strong lines across cosmic time. The Astrophysical Journal, 774(2):100, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kewley%2C%20Lisa%20J.%20Dopita%2C%20Michael%20A.%20Leitherer%2C%20Claus%20Dav%C3%A9%2C%20Romeel%20Theoretical%20evolution%20of%20optical%20strong%20lines%20across%20cosmic%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kewley%2C%20Lisa%20J.%20Dopita%2C%20Michael%20A.%20Leitherer%2C%20Claus%20Dav%C3%A9%2C%20Romeel%20Theoretical%20evolution%20of%20optical%20strong%20lines%20across%20cosmic%20time%202013"
        },
        {
            "id": "Kingma_2018_a",
            "entry": "Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03039"
        },
        {
            "id": "Kingma_et+al_2015_a",
            "entry": "Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pages 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743\u20134751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Kolesnikov_2017_a",
            "entry": "Alexander Kolesnikov and Christoph H. Lampert. PixelCNN models with auxiliary variables for natural image modeling. In International Conference on Machine Learning, pages 1905\u20131914, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolesnikov%2C%20Alexander%20Lampert%2C%20Christoph%20H.%20PixelCNN%20models%20with%20auxiliary%20variables%20for%20natural%20image%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolesnikov%2C%20Alexander%20Lampert%2C%20Christoph%20H.%20PixelCNN%20models%20with%20auxiliary%20variables%20for%20natural%20image%20modeling%202017"
        },
        {
            "id": "Lintusaari_et+al_2017_a",
            "entry": "Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Fundamentals and recent developments in approximate bayesian computation. Systematic Biology, 66(1):e66\u2013e82, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lintusaari%2C%20Jarno%20Gutmann%2C%20Michael%20U.%20Dutta%2C%20Ritabrata%20Kaski%2C%20Samuel%20Fundamentals%20and%20recent%20developments%20in%20approximate%20bayesian%20computation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lintusaari%2C%20Jarno%20Gutmann%2C%20Michael%20U.%20Dutta%2C%20Ritabrata%20Kaski%2C%20Samuel%20Fundamentals%20and%20recent%20developments%20in%20approximate%20bayesian%20computation%202017"
        },
        {
            "id": "Lu_2014_a",
            "entry": "Guolan Lu and Baowei Fei. Medical hyperspectral imaging: a review. Journal of Biomedical Optics, 19(1):10901, January 2014. ISSN 1560-2281.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Guolan%20Fei%2C%20Baowei%20Medical%20hyperspectral%20imaging%3A%20a%20review%202014-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Guolan%20Fei%2C%20Baowei%20Medical%20hyperspectral%20imaging%3A%20a%20review%202014-01"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "A\u00e4ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pages 4797\u20134805, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20A%C3%A4ron%20Kalchbrenner%2C%20Nal%20Vinyals%2C%20Oriol%20Espeholt%2C%20Lasse%20Conditional%20image%20generation%20with%20PixelCNN%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20A%C3%A4ron%20Kalchbrenner%2C%20Nal%20Vinyals%2C%20Oriol%20Espeholt%2C%20Lasse%20Conditional%20image%20generation%20with%20PixelCNN%20decoders%202016"
        },
        {
            "id": "Papamakarios_2016_a",
            "entry": "George Papamakarios and Iain Murray. Fast \u03b5-free inference of simulation models with bayesian conditional density estimation. In Advances in Neural Information Processing Systems, pages 1028\u20131036, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20George%20Murray%2C%20Iain%20Fast%20%CE%B5-free%20inference%20of%20simulation%20models%20with%20bayesian%20conditional%20density%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20George%20Murray%2C%20Iain%20Fast%20%CE%B5-free%20inference%20of%20simulation%20models%20with%20bayesian%20conditional%20density%20estimation%202016"
        },
        {
            "id": "Papamakarios_et+al_2017_a",
            "entry": "George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, pages 2335\u2013 2344, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20George%20Murray%2C%20Iain%20Pavlakou%2C%20Theo%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20George%20Murray%2C%20Iain%20Pavlakou%2C%20Theo%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017"
        },
        {
            "id": "Pellegrini_et+al_2011_a",
            "entry": "Eric W. Pellegrini, Jack A. Baldwin, and Gary J. Ferland. Structure and feedback in 30 Doradus. II. Structure and chemical abundances. The Astrophysical Journal, 738(1):34, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pellegrini%2C%20Eric%20W.%20Baldwin%2C%20Jack%20A.%20Ferland%2C%20Gary%20J.%20Structure%20and%20feedback%20in%2030%20Doradus.%20II.%20Structure%20and%20chemical%20abundances%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pellegrini%2C%20Eric%20W.%20Baldwin%2C%20Jack%20A.%20Ferland%2C%20Gary%20J.%20Structure%20and%20feedback%20in%2030%20Doradus.%20II.%20Structure%20and%20chemical%20abundances%202011"
        },
        {
            "id": "Rahner_et+al_2017_a",
            "entry": "Daniel Rahner, Eric W. Pellegrini, Simon C. O. Glover, and Ralf S. Klessen. Winds and radiation in unison: A new semi-analytic feedback model for cloud dissolution. Monthly Notices of the Royal Astronomical Society, 470:4453\u20134472, 10 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahner%2C%20Daniel%20Pellegrini%2C%20Eric%20W.%20Glover%2C%20Simon%20C.O.%20Klessen%2C%20Ralf%20S.%20Winds%20and%20radiation%20in%20unison%3A%20A%20new%20semi-analytic%20feedback%20model%20for%20cloud%20dissolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahner%2C%20Daniel%20Pellegrini%2C%20Eric%20W.%20Glover%2C%20Simon%20C.O.%20Klessen%2C%20Ralf%20S.%20Winds%20and%20radiation%20in%20unison%3A%20A%20new%20semi-analytic%20feedback%20model%20for%20cloud%20dissolution%202017"
        },
        {
            "id": "Reissl_et+al_2016_a",
            "entry": "Stefan Reissl, Robert Brauer, and Sebastian Wolf. Radiative transfer with polaris: I. analysis of magnetic fields through synthetic dust continuum polarization measurements. Astronomy & Astrophysics, 593, 04 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reissl%2C%20Stefan%20Brauer%2C%20Robert%20Wolf%2C%20Sebastian%20Radiative%20transfer%20with%20polaris%3A%20I.%20analysis%20of%20magnetic%20fields%20through%20synthetic%20dust%20continuum%20polarization%20measurements%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reissl%2C%20Stefan%20Brauer%2C%20Robert%20Wolf%2C%20Sebastian%20Radiative%20transfer%20with%20polaris%3A%20I.%20analysis%20of%20magnetic%20fields%20through%20synthetic%20dust%20continuum%20polarization%20measurements%202016"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530\u20131538, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Rippel_2013_a",
            "entry": "Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density models. arXiv:1302.5125, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1302.5125"
        },
        {
            "id": "Robert_2004_a",
            "entry": "Christian Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christian%20Robert%20and%20George%20Casella%20Monte%20Carlo%20Statistical%20Methods%20Springer%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christian%20Robert%20and%20George%20Casella%20Monte%20Carlo%20Statistical%20Methods%20Springer%202004"
        },
        {
            "id": "Klessen_2016_a",
            "entry": "Ralf S. Klessen and Simon C. O. Glover. Physical processes in the interstellar medium. Saas-Fee Advanced Course, 43:85, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klessen%2C%20Ralf%20S.%20Glover%2C%20Simon%20C.O.%20Physical%20processes%20in%20the%20interstellar%20medium%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klessen%2C%20Ralf%20S.%20Glover%2C%20Simon%20C.O.%20Physical%20processes%20in%20the%20interstellar%20medium%202016"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. arXiv:1701.05517, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05517"
        },
        {
            "id": "Schirrmeister_et+al_2018_a",
            "entry": "R.T. Schirrmeister, P. Chrabaszcz, F. Hutter, and T. Ball. Training generative reversible networks. arXiv:1806.01610, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01610"
        },
        {
            "id": "Siddharth_et+al_2017_a",
            "entry": "N Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, and Philip HS Torr. Learning disentangled representations with semi-supervised deep generative models. In Advances in Neural Information Processing Systems, pages 5925\u20135935, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siddharth%2C%20N.%20Paige%2C%20Brooks%20de%20Meent%2C%20Jan-Willem%20Van%20Desmaison%2C%20Alban%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siddharth%2C%20N.%20Paige%2C%20Brooks%20de%20Meent%2C%20Jan-Willem%20Van%20Desmaison%2C%20Alban%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017"
        },
        {
            "id": "Sohn_et+al_2015_a",
            "entry": "Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Advances in Neural Information Processing Systems, pages 3483\u20133491, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohn%2C%20Kihyuk%20Lee%2C%20Honglak%20Yan%2C%20Xinchen%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015"
        },
        {
            "id": "Sunn_et+al_2013_a",
            "entry": "Mikael Sunn\u00e5ker, Alberto Giovanni Busetto, Elina Numminen, Jukka Corander, Matthieu Foll, and Christophe Dessimoz. Approximate bayesian computation. PLoS computational biology, 9(1):e1002803, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sunn%C3%A5ker%2C%20Mikael%20Busetto%2C%20Alberto%20Giovanni%20Numminen%2C%20Elina%20Corander%2C%20Jukka%20Approximate%20bayesian%20computation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sunn%C3%A5ker%2C%20Mikael%20Busetto%2C%20Alberto%20Giovanni%20Numminen%2C%20Elina%20Corander%2C%20Jukka%20Approximate%20bayesian%20computation%202013"
        },
        {
            "id": "Tabak_2013_a",
            "entry": "E. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. Communications on Pure and Applied Mathematics, 66(2):145\u2013164, 2013. doi: 10.1002/ cpa.21423.",
            "crossref": "https://dx.doi.org/10.1002/cpa.21423",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1002/cpa.21423"
        },
        {
            "id": "Tabak_2010_a",
            "entry": "Esteban G Tabak, Eric Vanden-Eijnden, et al. Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217\u2013233, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tabak%2C%20Esteban%20G.%20Vanden-Eijnden%2C%20Eric%20Density%20estimation%20by%20dual%20ascent%20of%20the%20log-likelihood%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tabak%2C%20Esteban%20G.%20Vanden-Eijnden%2C%20Eric%20Density%20estimation%20by%20dual%20ascent%20of%20the%20log-likelihood%202010"
        },
        {
            "id": "Teng_et+al_2018_a",
            "entry": "Yunfei Teng, Anna Choromanska, and Mariusz Bojarski. Invertible autoencoder for domain adaptation. arXiv:1802.06869, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06869"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. arXiv:1711.01558, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01558"
        },
        {
            "id": "Tomczak_2016_a",
            "entry": "Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv:1611.09630, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09630"
        },
        {
            "id": "Trippe_2018_a",
            "entry": "Brian L Trippe and Richard E Turner. Conditional density estimation with bayesian normalising flows. arXiv:1802.04908, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04908"
        },
        {
            "id": "Uria_et+al_2016_a",
            "entry": "Benigno Uria, Marc-Alexandre C\u00f4t\u00e9, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. Journal of Machine Learning Research, 17(205): 1\u201337, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uria%2C%20Benigno%20C%C3%B4t%C3%A9%2C%20Marc-Alexandre%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Neural%20autoregressive%20distribution%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uria%2C%20Benigno%20C%C3%B4t%C3%A9%2C%20Marc-Alexandre%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Neural%20autoregressive%20distribution%20estimation%202016"
        },
        {
            "id": "Wilkinson_2013_a",
            "entry": "Richard David Wilkinson. Approximate bayesian computation (abc) gives exact results under the assumption of model error. Statistical applications in genetics and molecular biology, 12(2):129\u2013141, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilkinson%2C%20Richard%20David%20Approximate%20bayesian%20computation%20%28abc%29%20gives%20exact%20results%20under%20the%20assumption%20of%20model%20error%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilkinson%2C%20Richard%20David%20Approximate%20bayesian%20computation%20%28abc%29%20gives%20exact%20results%20under%20the%20assumption%20of%20model%20error%202013"
        },
        {
            "id": "Wirkert_et+al_2016_a",
            "entry": "Sebastian J Wirkert, Hannes Kenngott, Benjamin Mayer, Patrick Mietkowski, Martin Wagner, Peter Sauer, Neil T Clancy, Daniel S Elson, and Lena Maier-Hein. Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International journal of computer assisted radiology and surgery, 11(6):909\u2013917, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wirkert%2C%20Sebastian%20J.%20Kenngott%2C%20Hannes%20Mayer%2C%20Benjamin%20Mietkowski%2C%20Patrick%20Robust%20near%20real-time%20estimation%20of%20physiological%20parameters%20from%20megapixel%20multispectral%20images%20with%20inverse%20monte%20carlo%20and%20random%20forest%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wirkert%2C%20Sebastian%20J.%20Kenngott%2C%20Hannes%20Mayer%2C%20Benjamin%20Mietkowski%2C%20Patrick%20Robust%20near%20real-time%20estimation%20of%20physiological%20parameters%20from%20megapixel%20multispectral%20images%20with%20inverse%20monte%20carlo%20and%20random%20forest%20regression%202016"
        },
        {
            "id": "Wirkert_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Sebastian J. Wirkert, Anant S. Vemuri, Hannes G. Kenngott, Sara Moccia, Michael G\u00f6tz, Benjamin F. B. Mayer, Klaus H. Maier-Hein, Daniel S. Elson, and Lena Maier-Hein. Physiological Parameter Estimation from Multispectral Images Unleashed. In Medical Image Computing and Computer-Assisted Intervention, Lecture Notes in Computer Science, pages 134\u2013141.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wirkert%2C%20Sebastian%20J.%20Vemuri%2C%20Anant%20S.%20Kenngott%2C%20Hannes%20G.%20Moccia%2C%20Sara%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wirkert%2C%20Sebastian%20J.%20Vemuri%2C%20Anant%20S.%20Kenngott%2C%20Hannes%20G.%20Moccia%2C%20Sara%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Springer_2017_a",
            "entry": "Springer, Cham, September 2017. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In CVPR, pages 2223\u20132232, 2017a. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465\u2013476, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%2C%20Cham%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%2C%20Cham%20and%20Alexei%20A%20Efros.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017-06"
        }
    ]
}
