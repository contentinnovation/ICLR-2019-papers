{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "On Using Very Large Target Vocabulary for Neural Machine Translation",
        "author": "S\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio",
        "date": 2015,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1e13s05YX",
            "doi": "10.3115/v1/p15-1001"
        },
        "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
        "abstract": "Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing blackbox functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external blackbox non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this \u201cEstimate and Replace\u201d paradigm, we train a neural network, end to end, to compute the input to blackbox functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods."
    },
    "keywords": [
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "Genetic Algorithms",
            "url": "https://en.wikipedia.org/wiki/Genetic_Algorithms"
        },
        {
            "term": "cross entropy",
            "url": "https://en.wikipedia.org/wiki/cross_entropy"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "abbreviations": {
        "DNNs": "deep neural networks",
        "RL": "Reinforcement Learning",
        "GA": "Genetic Algorithms",
        "CE": "cross entropy",
        "NALU": "Neural Arithmetic Logic Unit",
        "LSTM": "Long Short-Term Memory"
    },
    "highlights": [
        "End-to-end supervised learning with deep neural networks (DNNs) has taken the stage in the past few years, achieving state-of-the-art performance in multiple domains including computer vision (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), natural language processing (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\"><a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\">Jean et al, 2015</a></a>), and speech recognition (<a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a>)",
        "This experiment presents the advantage of the Estimate and Replace approach to train a deep neural networks with less data",
        "Results vs. End-to-End Table 2 shows a comparison of EstiNet performance with an end-to-end Neural Arithmetic Logic Unit model",
        "We use labels that we generate from the black-box function during the optimization process to compensate for the lack of intermediate labels",
        "We show that our training process is more stable and has lower sample complexity compared to policy gradient methods",
        "By leveraging the concrete black-box function at inference time, our model generalizes better than end-to-end neural network models"
    ],
    "key_statements": [
        "End-to-end supervised learning with deep neural networks (DNNs) has taken the stage in the past few years, achieving state-of-the-art performance in multiple domains including computer vision (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), natural language processing (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\"><a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\">Jean et al, 2015</a></a>), and speech recognition (<a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a>)",
        "End-to-end training of a solution composed of trainable components and black-box functions poses several challenges we address in this work\u2014coping with non-differentiable black-box functions, fitting the network to call these functions with the correct arguments, and doing so without any intermediate labels",
        "The EstiNet model generalizes better than the baseline, and the accuracy difference between the two training procedures increases as the amount of training data decreases",
        "This experiment presents the advantage of the Estimate and Replace approach to train a deep neural networks with less data",
        "Results vs. End-to-End Table 2 shows a comparison of EstiNet performance with an end-to-end Neural Arithmetic Logic Unit model",
        "Both models were trained on sequences of length k = 10",
        "Results in Figure 2 show that EstiNet significantly outperforms the Reinforcement Learning agent.\n4.3",
        "Results Results are shown in Table 3. Solving this task infers the ability to generalize to the black-box function, which in our case is the ability to replace or update the original lookup table with another at inference time without the need to retrain our model",
        "Results Table 4 summarizes the TLL model performance for the training procedures described in Section 3.1",
        "We hypothesize that fixing the estimator parameters during the end-to-end training process prevents the rest of the model from fitting the train set",
        "Hybrid training further improved upon online training fitting the training set and performance carried to inference mode.\n5 RELATED WORK",
        "We proposed an architecture, termed EstiNet, and a training and deployment process, termed Estimate and Replace, which aim to overcome these limitations",
        "We use labels that we generate from the black-box function during the optimization process to compensate for the lack of intermediate labels",
        "We show that our training process is more stable and has lower sample complexity compared to policy gradient methods",
        "By leveraging the concrete black-box function at inference time, our model generalizes better than end-to-end neural network models",
        "The input of this module must be compatible with the output of the previous layer, be it an estimation function at training time, or its black-box function counterpart at inference time, which belong to different distributions"
    ],
    "summary": [
        "End-to-end supervised learning with deep neural networks (DNNs) has taken the stage in the past few years, achieving state-of-the-art performance in multiple domains including computer vision (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), natural language processing (<a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\"><a class=\"ref-link\" id=\"cSutskever_et+al_2014_a\" href=\"#rSutskever_et+al_2014_a\">Sutskever et al, 2014</a></a>; <a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\"><a class=\"ref-link\" id=\"cJean_et+al_2015_a\" href=\"#rJean_et+al_2015_a\">Jean et al, 2015</a></a>), and speech recognition (<a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a>).",
        "We propose a method for performing end-to-end training of a decomposed solution comprising of a neural network that calls black-box functions.",
        "The modular nature of the EstiNet model presents a unique training challenge: EstiNet is a modular architecture where each of its two modules, namely the argument extractor and the black-box estimator is trained using its own input-label pair samples and loss function.",
        "We fix its parameters and load the trained black-box estimator into the EstiNet model and train the argument extractor with the task\u2019s dataset and target loss function.",
        "We train a network to answer simple greater-than/less-than logical questions on real numbers, such as: \u201cis 7.5 greater than 8.2?\u201d We solve the text-logic task by constructing an EstiNet model with an argument extractor layer that extracts the arguments and operator (7.5, 8.2 and \u201c>\u201d in the above example), and a black-box estimator that performs simple logic operations.",
        "We solve the task by constructing an EstiNet model with an argument extractor layer that classifies the digit in a given MNIST image, and a black-box estimator that performs the sum operation.",
        "RL We compare the EstiNet performance with an AC-based RL agent as an existing solution for training a neural network calling a black-box function without intermediate labels.",
        "We solve the image-lookup task by constructing an EstiNet model with an argument extractor similar to the previous task and a black-box estimator that outputs the classification prediction.",
        "Solving this task infers the ability to generalize to the black-box function, which in our case is the ability to replace or update the original lookup table with another at inference time without the need to retrain our model.",
        "Current solutions for learning using black-box functionality in neural network prediction have critical limitations which manifest themselves in at least one of the following aspects: (i) poor generalization, low learning efficiency, under-utilization of available optimal functions, and the need for intermediate labels.",
        "Solving the general case of this problem requires learning of multiple black-box interfaces, along unbounded successive calls, where the final prediction is a computed function over the output of these calls.",
        "The input of this module must be compatible with the output of the previous layer, be it an estimation function at training time, or its black-box function counterpart at inference time, which belong to different distributions.",
        "Our approach implies a modular neural network that enjoys added interpretability and reusability benefits"
    ],
    "headline": "We propose a method for end-to-end training of a base neural network that integrates calls to existing blackbox functions",
    "reference_links": [
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.01705"
        },
        {
            "id": "Artzi_2013_a",
            "entry": "Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49\u201362, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Branavan_et+al_2009_a",
            "entry": "S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL \u201909, pp. 82\u201390, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics. ISBN 978-1-932432-45-9. URL http://dl.acm.org/citation.cfm?id=1687878.1687892.",
            "url": "http://dl.acm.org/citation.cfm?id=1687878.1687892",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Branavan%2C%20S.R.K.%20Chen%2C%20Harr%20Zettlemoyer%2C%20Luke%20S.%20Barzilay%2C%20Regina%20Reinforcement%20learning%20for%20mapping%20instructions%20to%20actions%202009"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. arXiv preprint arXiv:1704.06611, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06611"
        },
        {
            "id": "Fodor_1988_a",
            "entry": "Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_a",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adria Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538:471 EP \u2013, Oct 2016. URL http://dx.doi.org/10.1038/nature20101. Article.",
            "crossref": "https://dx.doi.org/10.1038/nature20101",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature20101"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, abs/1603.05027, 2016. URL http://arxiv.org/abs/1603.05027.",
            "url": "http://arxiv.org/abs/1603.05027",
            "arxiv_url": "https://arxiv.org/pdf/1603.05027"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jean_et+al_2015_a",
            "entry": "Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1\u201310. Association for Computational Linguistics, 2015. doi: 10.3115/v1/P15-1001. URL http://www.aclweb.org/anthology/ P15-1001.",
            "crossref": "https://dx.doi.org/10.3115/v1/P15-1001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.3115/v1/P15-1001"
        },
        {
            "id": "Johnson_et+al_2017_a",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Fei-Fei Li, C. Lawrence Zitnick, and Ross B. Girshick. Inferring and executing programs for visual reasoning. CoRR, abs/1705.03633, 2017. URL http://arxiv.org/abs/1705.03633.",
            "url": "http://arxiv.org/abs/1705.03633",
            "arxiv_url": "https://arxiv.org/pdf/1705.03633"
        },
        {
            "id": "Kahan_1996_a",
            "entry": "William Kahan. Ieee standard 754 for binary floating-point arithmetic. 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kahan%2C%20William%20Ieee%20standard%20754%20for%20binary%20floating-point%20arithmetic%201996"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc Le, and Ni Lao. Memory augmented policy optimization for program synthesis and semantic parsing. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Chen%20Norouzi%2C%20Mohammad%20Berant%2C%20Jonathan%20Le%2C%20Quoc%20Memory%20augmented%20policy%20optimization%20for%20program%20synthesis%20and%20semantic%20parsing%202018"
        },
        {
            "id": "Liang_2016_a",
            "entry": "Percy Liang. Learning executable semantic parsers for natural language understanding. Commun. ACM, 59(9):68\u201376, August 2016. ISSN 0001-0782. doi: 10.1145/2866568. URL http://doi.acm.org/10.1145/2866568.",
            "crossref": "https://dx.doi.org/10.1145/2866568",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2866568"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Xi Victoria Lin, Chenglong Wang, Deric Pang, Kevin Vu, and Michael D Ernst. Program synthesis from natural language using recurrent neural networks. Technical report, Technical Report UWCSE-17-03-01, University of Washington Department of Computer Science and Engineering, Seattle, WA, USA, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Xi%20Victoria%20Wang%2C%20Chenglong%20Pang%2C%20Deric%20Vu%2C%20Kevin%20Program%20synthesis%20from%20natural%20language%20using%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "Lipton_2016_a",
            "entry": "Zachary Chase Lipton. The mythos of model interpretability. CoRR, abs/1606.03490, 2016. URL http://arxiv.org/abs/1606.03490.",
            "url": "http://arxiv.org/abs/1606.03490",
            "arxiv_url": "https://arxiv.org/pdf/1606.03490"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Neelakantan_et+al_2016_a",
            "entry": "Arvind Neelakantan, Quoc V Le, Martin Abadi, Andrew McCallum, and Dario Amodei. Learning a natural language interface with neural programmer. arXiv preprint arXiv:1611.08945, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.08945"
        },
        {
            "id": "Pasupat_2015_a",
            "entry": "Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.00305"
        },
        {
            "id": "Pereyra_et+al_2017_a",
            "entry": "Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regularizing neural networks by penalizing confident output distributions. CoRR, abs/1701.06548, 2017. URL http://arxiv.org/abs/1701.06548.",
            "url": "http://arxiv.org/abs/1701.06548",
            "arxiv_url": "https://arxiv.org/pdf/1701.06548"
        },
        {
            "id": "Reed_2015_a",
            "entry": "Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06279"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3104\u20133112. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.",
            "url": "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2818\u20132826. IEEE Computer Society, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.308. URL https://doi.org/10.1109/CVPR.2016.308.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2016.308",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2016.308"
        },
        {
            "id": "Szegedy_et+al_2017_a",
            "entry": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inceptionresnet and the impact of residual connections on learning. In AAAI, volume 4, pp. 12, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inceptionresnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inceptionresnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017"
        },
        {
            "id": "Trask_et+al_2018_a",
            "entry": "Andrew Trask, Felix Hill, Scott Reed, Jack W. Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. CoRR, abs/1808.00508, 2018. URL http://arxiv.org/abs/1808.00508.",
            "url": "http://arxiv.org/abs/1808.00508",
            "arxiv_url": "https://arxiv.org/pdf/1808.00508"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03762"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Xiong_et+al_2016_a",
            "entry": "Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05256"
        },
        {
            "id": "Zaremba_2015_a",
            "entry": "Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. CoRR, abs/1505.00521, 2015. URL http://arxiv.org/abs/1505.00521.",
            "url": "http://arxiv.org/abs/1505.00521",
            "arxiv_url": "https://arxiv.org/pdf/1505.00521"
        }
    ]
}
