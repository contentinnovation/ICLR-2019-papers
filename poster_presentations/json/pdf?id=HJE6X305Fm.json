{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DON\u2019T LET YOUR DISCRIMINATOR BE FOOLED",
        "author": "Brady Zhou Department of Computer Science University of Texas brady.zhou@utexas.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJE6X305Fm"
        },
        "abstract": "Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel. Some of the most promising adversarial models today minimize a Wasserstein objective. It is smoother and more stable to optimize. In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties. By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss. We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively. The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions."
    },
    "keywords": [
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "GANs": "Generative adversarial networks",
        "LSGAN": "least squares GANs",
        "GAN": "generative adversarial network",
        "RFM": "Robust Feature Matching",
        "IN": "instance noise",
        "GP": "gradient penalty",
        "AR": "Adversarial Regularization",
        "FID": "Fr\u00e9chet Inception Distance"
    },
    "highlights": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014a</a></a></a>) are at the forefront of generative modeling",
        "We show that Generative adversarial networks objectives significantly stabilize, if the discriminator is robust to adversarial attacks",
        "We extend their analysis and show that any Generative adversarial networks objective can be made smooth as long as the discriminator is robust to adversarial perturbations",
        "The gradient penalty does not nearly perform as well as our robust regularizations (AR and Robust Feature Matching), both of which improve the quality of the synthesized images throughout different Generative adversarial networks objectives",
        "We are the first to show that a robustness regularization guarantees a smooth and robust loss function for any Generative adversarial networks objective",
        "Our results suggest that robust regularization leads to better training and visual results than standard gradient penalties"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014a</a></a></a>) are at the forefront of generative modeling",
        "We show that Generative adversarial networks objectives significantly stabilize, if the discriminator is robust to adversarial attacks",
        "We present two new regularization terms, borrowed from the adversarial training literature",
        "We extend their analysis and show that any Generative adversarial networks objective can be made smooth as long as the discriminator is robust to adversarial perturbations",
        "For simplicity of the analysis, we focus on distance norms | \u00b7 | instead of general distance functions of <a class=\"ref-link\" id=\"cCarlini_2017_a\" href=\"#rCarlini_2017_a\">Carlini & Wagner (2017</a>)",
        "We show that the Generative adversarial networks objective is robust to adversarial perturbations, as long as the discriminator is robust",
        "If the loss of a discriminator D is robust to adversarial perturbations u(z) p < \u03b4 in expectation Ez\u223cPZ [|f (D(G(z))) \u2212 f (D(G(z) + u(z)))|] < \u03b5 the adversarial objective is robust",
        "We show how a regularized discriminator objective can lead to a robust discriminator without the need for any constraints.\n4.1",
        "We present quantitative results in terms of Fr\u00e9chet Inception Distance (FID) (<a class=\"ref-link\" id=\"cHeusel_et+al_2017_a\" href=\"#rHeusel_et+al_2017_a\">Heusel et al, 2017</a>), as well as qualitative results",
        "The gradient penalty does not nearly perform as well as our robust regularizations (AR and Robust Feature Matching), both of which improve the quality of the synthesized images throughout different Generative adversarial networks objectives",
        "More generated images as well as additional results measuring the effect of different adversarial attacks can be found in supplementary material",
        "We are the first to show that a robustness regularization guarantees a smooth and robust loss function for any Generative adversarial networks objective",
        "Our results suggest that robust regularization leads to better training and visual results than standard gradient penalties"
    ],
    "summary": [
        "Generative adversarial networks (GANs) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014a</a></a></a>) are at the forefront of generative modeling.",
        "We show that GAN objectives significantly stabilize, if the discriminator is robust to adversarial attacks.",
        "We extend their analysis and show that any GAN objective can be made smooth as long as the discriminator is robust to adversarial perturbations.",
        "The discriminator only needs to be robust to adversarial attacks in expectation over all generated samples.",
        "A function h, e.g. a discriminator, is robust to adversarial perturbations for a generative distribution G(z) if and only if",
        "We show that the GAN objective is robust to adversarial perturbations, as long as the discriminator is robust.",
        "If the loss of a discriminator D is robust to adversarial perturbations u(z) p < \u03b4 in expectation Ez\u223cPZ [|f (D(G(z))) \u2212 f (D(G(z) + u(z)))|] < \u03b5 the adversarial objective is robust",
        "Theorem 4.1 and Theorem 4.2 have some interesting implications for general GANs. First, any GAN that is trained with a robust discriminator or loss has a robust and smooth objective.",
        "Robust discriminators or losses can be hard to train in practice, as they require constrained optimization or a carefully tuned architecture.",
        "Our goal is to train a discriminator D such that it is robust to adversarial perturbations in expectation.",
        "We do this by augmenting the original discriminator training objective with an additional adversarial regularization \u03c1(D, G) = Ez\u223cPZ",
        "Robust GAN training We compare original Jenson-Shannon divergence loss (GAN) (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014a</a></a>), the least squares loss (LSGAN) (<a class=\"ref-link\" id=\"cMao_et+al_2017_a\" href=\"#rMao_et+al_2017_a\">Mao et al, 2017</a>), and the Wasserstein distance (WGAN) (Arjovsky et al, 2017) on CIFAR10 and CelebA.",
        "We add a regularization method: Instance Noise (IN) (S\u00f8nderby et al, 2016), Gradient Penalty (GP) (<a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al, 2017</a>), Adversarial Regularization (AR) \u03c1, and Robust Feature Matching (RFM) \u03c1r.",
        "The gradient penalty does not nearly perform as well as our robust regularizations (AR and RFM), both of which improve the quality of the synthesized images throughout different GAN objectives.",
        "More generated images as well as additional results measuring the effect of different adversarial attacks can be found in supplementary material.",
        "We established a clear connection between robust discriminators in generative adversarial networks and the overall smoothness of the optimization and the quality of the results.",
        "We are the first to show that a robustness regularization guarantees a smooth and robust loss function for any GAN objective.",
        "Our results suggest that robust regularization leads to better training and visual results than standard gradient penalties"
    ],
    "headline": "We show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties",
    "reference_links": [
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%20Soumith%20Chintala%20and%20L%C3%A9on%20Bottou%20Wasserstein%20gan%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%20Soumith%20Chintala%20and%20L%C3%A9on%20Bottou%20Wasserstein%20gan%202017"
        },
        {
            "id": "Bottou_et+al_2017_a",
            "entry": "Leon Bottou, Martin Arjovsky, David Lopez-Paz, and Maxime Oquab. Geometrical insights for implicit generative modeling. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20Leon%20Arjovsky%2C%20Martin%20Lopez-Paz%2C%20David%20Oquab%2C%20Maxime%20Geometrical%20insights%20for%20implicit%20generative%20modeling%202017"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Denton_et+al_2015_a",
            "entry": "Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, pp. 1486\u20131494, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20L.%20Chintala%2C%20Soumith%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20L.%20Chintala%2C%20Soumith%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goodfellow_et+al_2014_b",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. CoRR, 2014b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20nash%20equilibrium%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. pp. 5967\u20135976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "Kannan_et+al_2018_a",
            "entry": "Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kannan%2C%20Harini%20Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Adversarial%20logit%20pairing%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%202015-12"
        },
        {
            "id": "Mao_et+al_2017_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, pp. 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Mescheder_2018_a",
            "entry": "Lars Mescheder. On the convergence properties of gan training. arXiv preprint arXiv:1801.04406, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04406"
        },
        {
            "id": "Dezfooli_et+al_2016_a",
            "entry": "Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In NIPS, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), IEEE, pp. 372\u2013387. IEEE, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582\u2013597, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "Pathak_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In CVPR, pp. 2536\u20132544, 2016. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, 2015. Casper Kaae S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz\u00e1r. Amortised map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob",
            "arxiv_url": "https://arxiv.org/pdf/1610.04490"
        },
        {
            "id": "Fergus_2013_a",
            "entry": "Fergus. Intriguing properties of neural networks. CoRR, 2013. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Jun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In ECCV, pp. 597\u2013613.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Springer_2017_a",
            "entry": "Springer, 2016. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016%20JunYan%20Zhu%20Taesung%20Park%20Phillip%20Isola%20and%20Alexei%20A%20Efros%20Unpaired%20imagetoimage%20translation%20using%20cycleconsistent%20adversarial%20networks%20In%20ICCV%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%202016%20JunYan%20Zhu%20Taesung%20Park%20Phillip%20Isola%20and%20Alexei%20A%20Efros%20Unpaired%20imagetoimage%20translation%20using%20cycleconsistent%20adversarial%20networks%20In%20ICCV%202017"
        }
    ]
}
