{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HIERARCHICAL RL USING AN ENSEMBLE OF PROPRIOCEPTIVE PERIODIC POLICIES",
        "author": "Kenneth Marino,& Abhinav Gupta",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SJz1x20cFQ"
        },
        "journal": "Still",
        "abstract": "In this work we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a \u201cphase function.\u201d The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods."
    },
    "keywords": [
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "central pattern generators",
            "url": "https://en.wikipedia.org/wiki/central_pattern_generators"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "CPGs": "central pattern generators"
    },
    "highlights": [
        "The problem becomes how to train a variety of controllers which are generally useful for an agent acting in the world and are sufficiently diverse so that the high-level controller will always have access to the building blocks it needs to solve the high-level task",
        "We show impressive results on a variety of difficult Mujoco maze and navigation tasks with sparse rewards and demonstrate that our method compares favorably to other recent hierarchical RL approaches",
        "In this work we present a simple, yet effective way of solving difficult sparse reward RL problems",
        "We show that our method performs well in difficult sparse reward problems in Mujoco locomotion tasks"
    ],
    "key_statements": [
        "The problem becomes how to train a variety of controllers which are generally useful for an agent acting in the world and are sufficiently diverse so that the high-level controller will always have access to the building blocks it needs to solve the high-level task",
        "We show impressive results on a variety of difficult Mujoco maze and navigation tasks with sparse rewards and demonstrate that our method compares favorably to other recent hierarchical RL approaches",
        "In this work we present a simple, yet effective way of solving difficult sparse reward RL problems",
        "We show that our method performs well in difficult sparse reward problems in Mujoco locomotion tasks"
    ],
    "summary": [
        "The problem becomes how to train a variety of controllers which are generally useful for an agent acting in the world and are sufficiently diverse so that the high-level controller will always have access to the building blocks it needs to solve the high-level task.",
        "Using this method of training, we train a number of low-level controllers on Mujoco locomotion tasks and are able to learn a diverse and effective set of low-level policies.",
        "We show impressive results on a variety of difficult Mujoco maze and navigation tasks with sparse rewards and demonstrate that our method compares favorably to other recent hierarchical RL approaches.",
        "Because of the structure of our lowlevel task, we need to ensure that the low-level policies are diverse and controllable is given by the design of the pre-training task), and we do so by using multiple random instantiations of the \u201cchange the sensory response\u201d policies that maximize equation 3.2.",
        "In section 3.2 we describe the reward function which we use to train the low-level policies.",
        "In section 3.4 we describe how we learn a high-level controller by learning to choose when and which of the trained low-level policies should be used.",
        "\u201ccontrollable\u201d means that the low-level policies are easy for the high-level policy to use, for example because they are predictable and their effect is independent of the external state.",
        "For our phase-conditioned policies, we want to encourage the agents actions and internal states to be cyclic during training; at any given time step, the current action and proprioceptive state should match the one from the last cycle.",
        "Once we have learned a variety of low-level policies, we want to learn how and when to use these low-level policies in a way that lets us solve much more complex task with sparse rewards.",
        "4.1 LOW-LEVEL CONTROL As described in Section 3.2 and Section 3.3, we train a number of low-level policies using our simple reward function.",
        "Figure 1 shows the reward curves of our phase-conditioned low-level policies compared to standard networks.",
        "We compare the results of our method on Random Ant Cross Maze when training the high-level policy with PPO, A2C and DQN.",
        "We first train a set of proprioceptive low-level agents on an intuitive reward and combine them by training a high-level policy to select the appropriate sequence of low-level policies."
    ],
    "headline": "In this work we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks",
    "reference_links": [
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pp. 1726\u20131734, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Dayan_1992_a",
            "entry": "Peter Dayan and Geoffrey E. Hinton. Feudal reinforcement learning. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992], pp. 271\u2013278, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201992-11-30",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201992-11-30"
        },
        {
            "id": "Dietterich_1999_a",
            "entry": "Thomas G. Dietterich. State abstraction in MAXQ hierarchical reinforcement learning. In Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999], pp. 994\u20131000, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dietterich%2C%20Thomas%20G.%20State%20abstraction%20in%20MAXQ%20hierarchical%20reinforcement%20learning%201999-11-29",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dietterich%2C%20Thomas%20G.%20State%20abstraction%20in%20MAXQ%20hierarchical%20reinforcement%20learning%201999-11-29"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06070"
        },
        {
            "id": "Florensa_et+al_2017_a",
            "entry": "Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. arXiv preprint arXiv:1704.03012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03012"
        },
        {
            "id": "Frans_et+al_2017_a",
            "entry": "Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. arXiv preprint arXiv:1710.09767, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09767"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07507"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp. 1846\u20131855, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20Tuomas%20Hartikainen%2C%20Kristian%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Latent%20space%20policies%20for%20hierarchical%20reinforcement%20learning%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20Tuomas%20Hartikainen%2C%20Kristian%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Latent%20space%20policies%20for%20hierarchical%20reinforcement%20learning%202018-07"
        },
        {
            "id": "Haarnoja_et+al_0000_a",
            "entry": "Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02808"
        },
        {
            "id": "Hausman_et+al_2018_a",
            "entry": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20Karol%20Springenberg%2C%20Jost%20Tobias%20Wang%2C%20Ziyu%20Heess%2C%20Nicolas%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "Heess_et+al_2016_a",
            "entry": "Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05182"
        },
        {
            "id": "Holden_et+al_2017_a",
            "entry": "Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4):42, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kostrikov_2018_a",
            "entry": "Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github.com/ikostrikov/pytorch-a2c-ppo-acktr, 2018.",
            "url": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"
        },
        {
            "id": "Marder_2001_a",
            "entry": "Eve Marder and Dirk Bucher. Central pattern generators and the control of rhythmic movements. Current biology, 11(23):R986\u2013R996, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marder%2C%20Eve%20Bucher%2C%20Dirk%20Central%20pattern%20generators%20and%20the%20control%20of%20rhythmic%20movements%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marder%2C%20Eve%20Bucher%2C%20Dirk%20Central%20pattern%20generators%20and%20the%20control%20of%20rhythmic%20movements%202001"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Mohamed_2015_a",
            "entry": "Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 2125\u20132133, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. arXiv preprint arXiv:1805.08296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Peng_et+al_2016_a",
            "entry": "Xue Bin Peng, Glen Berseth, and Michiel Van de Panne. Terrain-adaptive locomotion skills using deep reinforcement learning. ACM Transactions on Graphics (TOG), 35(4):81, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20de%20Panne%2C%20Michiel%20Van%20Terrain-adaptive%20locomotion%20skills%20using%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20de%20Panne%2C%20Michiel%20Van%20Terrain-adaptive%20locomotion%20skills%20using%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (TOG), 36(4):41, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xue%20Bin%20Berseth%2C%20Glen%20Yin%2C%20KangKang%20Panne%2C%20Michiel%20Van%20De%20Deeploco%3A%20Dynamic%20locomotion%20skills%20using%20hierarchical%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Peng_et+al_2018_a",
            "entry": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Sharma_2018_a",
            "entry": "Arjun Sharma and Kris M Kitani. Phase-parametric policies for reinforcement learning in cyclic environments. In AAAI Conference on Artificial Intelligence, Pittsburgh, PA, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharma%2C%20Arjun%20Kitani%2C%20Kris%20M.%20Phase-parametric%20policies%20for%20reinforcement%20learning%20in%20cyclic%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharma%2C%20Arjun%20Kitani%2C%20Kris%20M.%20Phase-parametric%20policies%20for%20reinforcement%20learning%20in%20cyclic%20environments%202018"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013 211, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3540\u20133549, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Watkins_1989_a",
            "entry": "Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King\u2019s College, Cambridge, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watkins%2C%20Christopher%20John%20Cornish%20Hellaby%20Learning%20from%20delayed%20rewards%201989"
        },
        {
            "id": "For_2018_a",
            "entry": "For all RL algorithms in the paper (except for DQN in Figure 7b and the values from other works in Figure 3 and Figure 7a), we used the implementation of Kostrikov (2018). For DQN, we wrote our own simple implementation. The hyperparameters for these three algorithms are shown in Tables 1, 2 and 3 We use the ADAM (Kingma & Ba, 2014) optimizer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=For%20all%20RL%20algorithms%20in%20the%20paper%20except%20for%20DQN%20in%20Figure%207b%20and%20the%20values%20from%20other%20works%20in%20Figure%203%20and%20Figure%207a%20we%20used%20the%20implementation%20of%20Kostrikov%202018%20For%20DQN%20we%20wrote%20our%20own%20simple%20implementation%20The%20hyperparameters%20for%20these%20three%20algorithms%20are%20shown%20in%20Tables%201%202%20and%203%20We%20use%20the%20ADAM%20Kingma%20%20Ba%202014%20optimizer",
            "oa_query": "https://api.scholarcy.com/oa_version?query=For%20all%20RL%20algorithms%20in%20the%20paper%20except%20for%20DQN%20in%20Figure%207b%20and%20the%20values%20from%20other%20works%20in%20Figure%203%20and%20Figure%207a%20we%20used%20the%20implementation%20of%20Kostrikov%202018%20For%20DQN%20we%20wrote%20our%20own%20simple%20implementation%20The%20hyperparameters%20for%20these%20three%20algorithms%20are%20shown%20in%20Tables%201%202%20and%203%20We%20use%20the%20ADAM%20Kingma%20%20Ba%202014%20optimizer"
        }
    ]
}
