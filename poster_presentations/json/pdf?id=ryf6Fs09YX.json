{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GO GRADIENT FOR EXPECTATION-BASED OBJECTIVES",
        "author": "Yulai Cong, Miaoyun Zhao, Ke Bai Lawrence Carin Department of Electrical and Computer Engineering, Duke University",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryf6Fs09YX"
        },
        "abstract": "Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters \u03b3 for expectation-based objectives Eq\u03b3(y)[f (y)]. Most existing methods either (i) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or (ii) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that (i) applies to many distributions associated with non-reparameterizable continuous or discrete random variables, and (ii) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "cumulative distribution function",
            "url": "https://en.wikipedia.org/wiki/cumulative_distribution_function"
        },
        {
            "term": "discrete random variable",
            "url": "https://en.wikipedia.org/wiki/discrete_random_variable"
        },
        {
            "term": "Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Monte_Carlo"
        },
        {
            "term": "continuous random variable",
            "url": "https://en.wikipedia.org/wiki/continuous_random_variable"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "random variable",
            "url": "https://en.wikipedia.org/wiki/random_variable"
        }
    ],
    "abbreviations": {
        "GO": "General and One-sample",
        "BBVI": "black-box variational inference",
        "GANs": "generative adversarial networks",
        "MC": "Monte Carlo",
        "Rep": "reparameterization trick",
        "ELBO": "evidence lower bound",
        "CDF": "cumulative distribution function",
        "HVI": "hierarchical variational inference",
        "GRep": "Generalized Rep",
        "RSVI": "Rejection sampling variational inference",
        "VAE": "variational autoencoder",
        "BGAN": "boundary-seeking GAN",
        "DEF": "deep exponential families",
        "DLDA": "deep latent Dirichlet allocation"
    },
    "highlights": [
        "Typically trained using back-propagation for parameter optimization, have recently demonstrated significant success across a wide range of applications",
        "We make the following main contributions. (i) We propose a new General and One-sample (GO) gradient in Section 3, that principally generalizes reparameterization trick to many non-reparameterizable distributions and justifies two recent methods (<a class=\"ref-link\" id=\"cFigurnov_et+al_2018_a\" href=\"#rFigurnov_et+al_2018_a\">Figurnov et al, 2018</a>; <a class=\"ref-link\" id=\"cJankowiak_2018_a\" href=\"#rJankowiak_2018_a\">Jankowiak & Obermeyer, 2018</a>); the \u201cOne sample\u201d motivating the name General and One-sample is meant to highlight the low variance of the proposed method, one may use more than one sample if desired. We find that the core of the General and One-sample gradient is something we term a variable-nabla, which can be interpreted as the gradient of a random variable wrt a parameter. Utilizing variablenablas to propagate the chain rule through distributions, we broaden the applicability of the General and One-sample gradient in Sections 4-5 and present statistical back-propagation, a statistical generalization of classic back-propagation (<a class=\"ref-link\" id=\"cRumelhart_1986_a\" href=\"#rRumelhart_1986_a\">Rumelhart & Hinton, 1986</a>)",
        "We examine the proposed General and One-sample gradients and statistical back-propagation with four experiments: (i) simple one-dimensional examples are presented to verify the General and One-sample gradient in Theorem 1, corresponding to nonnegative and discrete random variables; the discrete variational autoencoder experiment from Tucker et al (2017) and <a class=\"ref-link\" id=\"cGrathwohl_et+al_2017_a\" href=\"#rGrathwohl_et+al_2017_a\">Grathwohl et al (2017</a>) is reproduced to compare General and One-sample with the state-of-the-art variance-reduction methods; a multinomial generative adversarial networks, generating discrete observations, is constructed to demonstrate the deep General and One-sample gradient in Theorem 2; hierarchical variational inference (HVI) for two deep non-conjugate Bayesian models is developed to verify statistical back-propagation in Theorem 3",
        "For expectation-based objectives, we propose a General and One-sample (GO) gradient that applies to continuous and discrete random variables",
        "We further generalize the General and One-sample gradient to cases for which the underlying model is deep and has a marginal distribution corresponding to the latent variables of interest, and to cases for which the latent variables are hierarchical"
    ],
    "key_statements": [
        "Typically trained using back-propagation for parameter optimization, have recently demonstrated significant success across a wide range of applications",
        "We make the following main contributions. (i) We propose a new General and One-sample (GO) gradient in Section 3, that principally generalizes reparameterization trick to many non-reparameterizable distributions and justifies two recent methods (<a class=\"ref-link\" id=\"cFigurnov_et+al_2018_a\" href=\"#rFigurnov_et+al_2018_a\">Figurnov et al, 2018</a>; <a class=\"ref-link\" id=\"cJankowiak_2018_a\" href=\"#rJankowiak_2018_a\">Jankowiak & Obermeyer, 2018</a>); the \u201cOne sample\u201d motivating the name General and One-sample is meant to highlight the low variance of the proposed method, one may use more than one sample if desired. We find that the core of the General and One-sample gradient is something we term a variable-nabla, which can be interpreted as the gradient of a random variable wrt a parameter. Utilizing variablenablas to propagate the chain rule through distributions, we broaden the applicability of the General and One-sample gradient in Sections 4-5 and present statistical back-propagation, a statistical generalization of classic back-propagation (<a class=\"ref-link\" id=\"cRumelhart_1986_a\" href=\"#rRumelhart_1986_a\">Rumelhart & Hinton, 1986</a>)",
        "We examine the proposed General and One-sample gradients and statistical back-propagation with four experiments: (i) simple one-dimensional examples are presented to verify the General and One-sample gradient in Theorem 1, corresponding to nonnegative and discrete random variables; the discrete variational autoencoder experiment from Tucker et al (2017) and <a class=\"ref-link\" id=\"cGrathwohl_et+al_2017_a\" href=\"#rGrathwohl_et+al_2017_a\">Grathwohl et al (2017</a>) is reproduced to compare General and One-sample with the state-of-the-art variance-reduction methods; a multinomial generative adversarial networks, generating discrete observations, is constructed to demonstrate the deep General and One-sample gradient in Theorem 2; hierarchical variational inference (HVI) for two deep non-conjugate Bayesian models is developed to verify statistical back-propagation in Theorem 3",
        "For expectation-based objectives, we propose a General and One-sample (GO) gradient that applies to continuous and discrete random variables",
        "We further generalize the General and One-sample gradient to cases for which the underlying model is deep and has a marginal distribution corresponding to the latent variables of interest, and to cases for which the latent variables are hierarchical"
    ],
    "summary": [
        "Typically trained using back-propagation for parameter optimization, have recently demonstrated significant success across a wide range of applications.",
        "For expectation objectives Eq\u03b3(y)[f (y)], where q\u03b3 (y) satisfies (i) q\u03b3 (y) = v q\u03b3; the corresponding CDF Q\u03b3 is differentiable wrt parameters \u03b3; and one can calculate \u2207\u03b3 Q\u03b3, the General and One-sample (GO) gradient is defined as",
        "Statistical back-propagation in Theorem 3 is relevant to hierarchical variational inference (HVI) (Ranganath et al, 2016b; <a class=\"ref-link\" id=\"cHoffman_2015_a\" href=\"#rHoffman_2015_a\">Hoffman & Blei, 2015</a>; <a class=\"ref-link\" id=\"cMnih_2014_a\" href=\"#rMnih_2014_a\">Mnih & Gregor, 2014</a>), greatly generalizing GO gradients to the inference of directed acyclic probabilistic graphical models.",
        "Implicit Rep gradients (<a class=\"ref-link\" id=\"cFigurnov_et+al_2018_a\" href=\"#rFigurnov_et+al_2018_a\">Figurnov et al, 2018</a>) and pathwise derivatives (<a class=\"ref-link\" id=\"cJankowiak_2018_a\" href=\"#rJankowiak_2018_a\">Jankowiak & Obermeyer, 2018</a>) are recent low-variance methods that exploit the gradient of the expected function; they are special cases of GO in the single-layer continuous settings.",
        "Stochastic back-propagation (<a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al, 2014</a>; <a class=\"ref-link\" id=\"cFan_et+al_2015_a\" href=\"#rFan_et+al_2015_a\">Fan et al, 2015</a>), focusing mainly on reparameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables.",
        "The proposed statistical back-propagation based on the GO gradient is applicable to most distributions for continuous random variables.",
        "We examine the proposed GO gradients and statistical back-propagation with four experiments: (i) simple one-dimensional examples are presented to verify the GO gradient in Theorem 1, corresponding to nonnegative and discrete random variables; the discrete variational autoencoder experiment from Tucker et al (2017) and <a class=\"ref-link\" id=\"cGrathwohl_et+al_2017_a\" href=\"#rGrathwohl_et+al_2017_a\">Grathwohl et al (2017</a>) is reproduced to compare GO with the state-of-the-art variance-reduction methods; a multinomial GAN, generating discrete observations, is constructed to demonstrate the deep GO gradient in Theorem 2; hierarchical variational inference (HVI) for two deep non-conjugate Bayesian models is developed to verify statistical back-propagation in Theorem 3.",
        "To demonstrate the low variance of the proposed GO gradient, we consider the discrete variational autoencoder (VAE) experiment from REBAR (Tucker et al, 2017) and RELAX (<a class=\"ref-link\" id=\"cGrathwohl_et+al_2017_a\" href=\"#rGrathwohl_et+al_2017_a\">Grathwohl et al, 2017</a>), to make a direct comparison with state-of-the-art variance-reduction methods.",
        "To demonstrate the deep GO gradient in Theorem 2, we adopt multinomial leaf variables x and construct a new multinomial GAN for generating discrete observations with a finite alphabet.",
        "To demonstrate statistical back-propagation in Theorem 3, we design variational inference nets for two nonconjugate hierarchical Bayesian models, i.e., deep exponential families (DEF) (<a class=\"ref-link\" id=\"cRanganath_et+al_2015_a\" href=\"#rRanganath_et+al_2015_a\">Ranganath et al, 2015</a>) and deep latent Dirichlet allocation (DLDA) (<a class=\"ref-link\" id=\"cZhou_et+al_2015_a\" href=\"#rZhou_et+al_2015_a\">Zhou et al, 2015</a>; 2016; <a class=\"ref-link\" id=\"cCong_et+al_2017_a\" href=\"#rCong_et+al_2017_a\">Cong et al, 2017</a>).",
        "For expectation-based objectives, we propose a General and One-sample (GO) gradient that applies to continuous and discrete random variables.",
        "The GO-gradient setup is demonstrated to yield the same low-variance estimation as the reparameterization trick, which is only applicable to reparameterizable continuous random variables.",
        "We present statistical back-propagation, to flexibly integrate deep neural networks with general classes of random variables"
    ],
    "headline": "We propose a General and One-sample gradient that applies to many distributions associated with non-reparameterizable continuous or discrete random variables, and has the same low-variance as the reparameterization trick",
    "reference_links": [
        {
            "id": "Abadi_et+al_0000_a",
            "entry": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. TensorFlow: Large-scale machine learning on heterogeneous systems. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Arjovsky%20S%20Chintala%20and%20L%20Bottou%20Wasserstein%20GAN%20In%20ICLR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Arjovsky%20S%20Chintala%20and%20L%20Bottou%20Wasserstein%20GAN%20In%20ICLR%202017"
        },
        {
            "id": "Bishop_2006_a",
            "entry": "C. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.%20Pattern%20Recognition%20and%20Machine%20Learning%202006"
        },
        {
            "id": "Cong_et+al_2017_a",
            "entry": "Y. Cong, B. Chen, H. Liu, and M. Zhou. Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cong%2C%20Y.%20Chen%2C%20B.%20Liu%2C%20H.%20Zhou%2C%20M.%20Deep%20latent%20Dirichlet%20allocation%20with%20topic-layer-adaptive%20stochastic%20gradient%20Riemannian%20MCMC%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cong%2C%20Y.%20Chen%2C%20B.%20Liu%2C%20H.%20Zhou%2C%20M.%20Deep%20latent%20Dirichlet%20allocation%20with%20topic-layer-adaptive%20stochastic%20gradient%20Riemannian%20MCMC%202017"
        },
        {
            "id": "Fan_et+al_2015_a",
            "entry": "K. Fan, Z. Wang, J. Beck, J. Kwok, and K. Heller. Fast second order stochastic backpropagation for variational inference. In NIPS, pp. 1387\u20131395, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20K.%20Wang%2C%20Z.%20Beck%2C%20J.%20Kwok%2C%20J.%20Fast%20second%20order%20stochastic%20backpropagation%20for%20variational%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20K.%20Wang%2C%20Z.%20Beck%2C%20J.%20Kwok%2C%20J.%20Fast%20second%20order%20stochastic%20backpropagation%20for%20variational%20inference%202015"
        },
        {
            "id": "Figurnov_et+al_2018_a",
            "entry": "M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. arXiv preprint arXiv:1805.08498, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08498"
        },
        {
            "id": "Gan_et+al_2017_a",
            "entry": "Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li, and L. Carin. Triangle generative adversarial networks. In NIPS, pp. 5253\u20135262, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Z.%20Chen%2C%20L.%20Wang%2C%20W.%20Pu%2C%20Y.%20Triangle%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Z.%20Chen%2C%20L.%20Wang%2C%20W.%20Pu%2C%20Y.%20Triangle%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Geddes_et+al_1990_a",
            "entry": "K. Geddes, M. Glasser, R. Moore, and T. Scott. Evaluation of classes of definite integrals involving elementary functions via differentiation of special functions. Applicable Algebra in Engineering, Communication and Computing, 1(2):149\u2013165, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geddes%2C%20K.%20Glasser%2C%20M.%20Moore%2C%20R.%20Scott%2C%20T.%20Evaluation%20of%20classes%20of%20definite%20integrals%20involving%20elementary%20functions%20via%20differentiation%20of%20special%20functions%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geddes%2C%20K.%20Glasser%2C%20M.%20Moore%2C%20R.%20Scott%2C%20T.%20Evaluation%20of%20classes%20of%20definite%20integrals%20involving%20elementary%20functions%20via%20differentiation%20of%20special%20functions%201990"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Grathwohl_et+al_2017_a",
            "entry": "W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. arXiv:1711.00123, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00123"
        },
        {
            "id": "Gu_et+al_0000_a",
            "entry": "S. Gu, S. Levine, I. Sutskever, and A. Mnih. MuProp: Unbiased backpropagation for stochastic neural networks. arXiv:1511.05176, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05176"
        },
        {
            "id": "J_2016_a",
            "entry": "J. M. Hern\u00e1ndez-Lobato, Y. Li, M. Rowland, D. Hern\u00e1ndez-Lobato, T. Bui, and R. E. Turner. Black-box \u03b1-divergence minimization. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20M%20Hern%C3%A1ndezLobato%20Y%20Li%20M%20Rowland%20D%20Hern%C3%A1ndezLobato%20T%20Bui%20and%20R%20E%20Turner%20Blackbox%20%CE%B1divergence%20minimization%20In%20ICML%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20M%20Hern%C3%A1ndezLobato%20Y%20Li%20M%20Rowland%20D%20Hern%C3%A1ndezLobato%20T%20Bui%20and%20R%20E%20Turner%20Blackbox%20%CE%B1divergence%20minimization%20In%20ICML%202016"
        },
        {
            "id": "Hjelm_et+al_2018_a",
            "entry": "R. Hjelm, A. Jacob, T. Che, A. Trischler, K. Cho, and Y. Bengio. Boundary seeking GANs. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hjelm%2C%20R.%20Jacob%2C%20A.%20Che%2C%20T.%20Trischler%2C%20A.%20Boundary%20seeking%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hjelm%2C%20R.%20Jacob%2C%20A.%20Che%2C%20T.%20Trischler%2C%20A.%20Boundary%20seeking%20GANs%202018"
        },
        {
            "id": "Hoffman_2015_a",
            "entry": "M. Hoffman and D. Blei. Stochastic structured variational inference. In AISTATS, pp. 361\u2013369, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.%20Blei%2C%20D.%20Stochastic%20structured%20variational%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.%20Blei%2C%20D.%20Stochastic%20structured%20variational%20inference%202015"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Jankowiak_2018_a",
            "entry": "M. Jankowiak and F. Obermeyer. Pathwise derivatives beyond the reparameterization trick. arXiv preprint arXiv:1806.01851, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01851"
        },
        {
            "id": "Jordan_et+al_1999_a",
            "entry": "M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.%20Saul%2C%20L.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20M.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.%20Saul%2C%20L.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In NIPS, pp. 5501\u20135509, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20Alice%3A%20Towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20Alice%3A%20Towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "C. Li, J. Li, G. Wang, and L. Carin. Learning to sample with adversarially learned likelihood-ratio. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Li%2C%20J.%20Wang%2C%20G.%20Carin%2C%20L.%20Learning%20to%20sample%20with%20adversarially%20learned%20likelihood-ratio%202018"
        },
        {
            "id": "Li_2016_a",
            "entry": "Y. Li and R. E. Turner. R\u00e9nyi divergence variational inference. In NIPS, pp. 1073\u20131081, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Turner%2C%20R.E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Turner%2C%20R.E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016"
        },
        {
            "id": "Maddison_et+al_2016_a",
            "entry": "C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Mnih_2014_a",
            "entry": "A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, pp. 1791\u20131799, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20A.%20Gregor%2C%20K.%20Neural%20variational%20inference%20and%20learning%20in%20belief%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20A.%20Gregor%2C%20K.%20Neural%20variational%20inference%20and%20learning%20in%20belief%20networks%202014"
        },
        {
            "id": "Mnih_2016_a",
            "entry": "A. Mnih and D. Rezende. Variational inference for monte carlo objectives. In ICML, pp. 2188\u20132196, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20A.%20Rezende%2C%20D.%20Variational%20inference%20for%20monte%20carlo%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20A.%20Rezende%2C%20D.%20Variational%20inference%20for%20monte%20carlo%20objectives%202016"
        },
        {
            "id": "Naesseth_et+al_2016_a",
            "entry": "C. A. Naesseth, F. J. R. Ruiz, S. W. Linderman, and D. M. Blei. Rejection sampling variational inference. arXiv:1610.05683, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05683"
        },
        {
            "id": "Neal_1992_a",
            "entry": "R. M. Neal. Connectionist learning of belief networks. Artif. Intell., 56(1):71\u2013113, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20Connectionist%20learning%20of%20belief%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20R.M.%20Connectionist%20learning%20of%20belief%20networks%201992"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20PyTorch%202017"
        },
        {
            "id": "Radford_et+al_0000_a",
            "entry": "A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In AISTATS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.M.%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.M.%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Ranganath_et+al_2015_a",
            "entry": "R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tang%2C%20L.%20Charlin%2C%20L.%20Blei%2C%20D.M.%20Deep%20exponential%20families%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tang%2C%20L.%20Charlin%2C%20L.%20Blei%2C%20D.M.%20Deep%20exponential%20families%202015"
        },
        {
            "id": "Ranganath_et+al_0000_a",
            "entry": "R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. In NIPS, pp. 496\u2013504, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tran%2C%20D.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Operator%20variational%20inference",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tran%2C%20D.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Operator%20variational%20inference"
        },
        {
            "id": "Ranganath_et+al_0000_b",
            "entry": "R. Ranganath, D. Tran, and D. Blei. Hierarchical variational models. In ICML, pp. 324\u2013333, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Roeder_et+al_2017_a",
            "entry": "G. Roeder, Y. Wu, and D. K. Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In NIPS, pp. 6928\u20136937, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roeder%2C%20G.%20Wu%2C%20Y.%20Duvenaud%2C%20D.K.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roeder%2C%20G.%20Wu%2C%20Y.%20Duvenaud%2C%20D.K.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017"
        },
        {
            "id": "Ruiz_et+al_2016_a",
            "entry": "F. J. R. Ruiz, M. K. Titsias, and D. Blei. The generalized reparameterization gradient. In NIPS, pp. 460\u2013468, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruiz%2C%20F.J.R.%20Titsias%2C%20M.K.%20Blei%2C%20D.%20The%20generalized%20reparameterization%20gradient%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruiz%2C%20F.J.R.%20Titsias%2C%20M.K.%20Blei%2C%20D.%20The%20generalized%20reparameterization%20gradient%202016"
        },
        {
            "id": "Rumelhart_1986_a",
            "entry": "D. Rumelhart and G. Hinton. Learning representations by back-propagating errors. Nature, 323(9), 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.%20Hinton%2C%20G.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.%20Hinton%2C%20G.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "Salakhutdinov_2008_a",
            "entry": "R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In ICML, pp. 872\u2013879. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008"
        },
        {
            "id": "Salimans_2013_a",
            "entry": "T. Salimans, D. A. Knowles, et al. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4):837\u2013882, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Knowles%2C%20D.A.%20Fixed-form%20variational%20posterior%20approximation%20through%20stochastic%20linear%20regression%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Knowles%2C%20D.A.%20Fixed-form%20variational%20posterior%20approximation%20through%20stochastic%20linear%20regression%202013"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NIPS, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "J. Schulman, N. Heess, T. Weber, and P. Abbeel. Gradient estimation using stochastic computation graphs. In NIPS, pp. 3528\u20133536, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Heess%2C%20N.%20Weber%2C%20T.%20Abbeel%2C%20P.%20Gradient%20estimation%20using%20stochastic%20computation%20graphs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Heess%2C%20N.%20Weber%2C%20T.%20Abbeel%2C%20P.%20Gradient%20estimation%20using%20stochastic%20computation%20graphs%202015"
        },
        {
            "id": "Titsias_2015_a",
            "entry": "M. Titsias. Local expectation gradients for doubly stochastic variational inference. arXiv preprint arXiv:1503.01494, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.01494"
        },
        {
            "id": "Titsias_2015_b",
            "entry": "M. Titsias and M. L\u00e1zaro-Gredilla. Local expectation gradients for black box variational inference. In NIPS, pp. 2638\u20132646, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.%20L%C3%A1zaro-Gredilla%2C%20M.%20Local%20expectation%20gradients%20for%20black%20box%20variational%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.%20L%C3%A1zaro-Gredilla%2C%20M.%20Local%20expectation%20gradients%20for%20black%20box%20variational%20inference%202015"
        },
        {
            "id": "Tucker_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 G. Tucker, A. Mnih, C. Maddison, J. Lawson, and J. Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS, pp. 2624\u20132633, 2017. R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.%20Lawson%2C%20J.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.%20Lawson%2C%20J.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Learning_1992_a",
            "entry": "Machine learning, 8(3-4):229\u2013256, 1992. M. Yin and M. Zhou. ARM: Augment-REINFORCE-merge gradient for discrete latent variable models. arXiv preprint arXiv:1807.11143, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.11143"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "H. Zhang, B. Chen, D. Guo, and M. Zhou. WHAI: Weibull hybrid autoencoding inference for deep topic modeling. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Chen%2C%20B.%20Guo%2C%20D.%20Zhou%2C%20M.%20WHAI%3A%20Weibull%20hybrid%20autoencoding%20inference%20for%20deep%20topic%20modeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Chen%2C%20B.%20Guo%2C%20D.%20Zhou%2C%20M.%20WHAI%3A%20Weibull%20hybrid%20autoencoding%20inference%20for%20deep%20topic%20modeling%202018"
        },
        {
            "id": "Zhao_et+al_2016_a",
            "entry": "J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20J.%20Mathieu%2C%20M.%20LeCun%2C%20Y.%20Energy-based%20generative%20adversarial%20network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20J.%20Mathieu%2C%20M.%20LeCun%2C%20Y.%20Energy-based%20generative%20adversarial%20network%202016"
        },
        {
            "id": "Zhou_et+al_2015_a",
            "entry": "M. Zhou, Y. Cong, and B. Chen. The Poisson gamma belief network. In NIPS, pp. 3025\u20133033, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20The%20Poisson%20gamma%20belief%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20The%20Poisson%20gamma%20belief%20network%202015"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "M. Zhou, Y. Cong, and B. Chen. Augmentable gamma belief networks. JMLR, 17(1):5656\u20135699, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%202016"
        },
        {
            "id": "Similar_2016_a",
            "entry": "Similar proof in one-dimension is also given in the supplemental materials of Ranganath et al. (2016b).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Similar%20proof%20in%20onedimension%20is%20also%20given%20in%20the%20supplemental%20materials%20of%20Ranganath%20et%20al%202016b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Similar%20proof%20in%20onedimension%20is%20also%20given%20in%20the%20supplemental%20materials%20of%20Ranganath%20et%20al%202016b"
        }
    ]
}
