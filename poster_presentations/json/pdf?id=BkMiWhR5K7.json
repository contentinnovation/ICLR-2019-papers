{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PRIOR CONVICTIONS: BLACK-BOX ADVERSARIAL ATTACKS WITH BANDITS AND PRIORS",
        "author": "Andrew Ilyas, Logan Engstrom, Aleksander Madry {ailyas, engstrom, madry}@mit.edu MIT CSAIL",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BkMiWhR5K7"
        },
        "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and we demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art. 1"
    },
    "keywords": [
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "threat model",
            "url": "https://en.wikipedia.org/wiki/threat_model"
        },
        {
            "term": "natural evolution strategies",
            "url": "https://en.wikipedia.org/wiki/natural_evolution_strategies"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "PGD": "projected gradient descent",
        "NES": "natural evolution strategies",
        "MVU": "minimum-variance unbiased",
        "EG": "exponentiated gradients"
    },
    "highlights": [
        "Recent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction",
        "We propose a new approach for generating black-box adversarial examples, using bandit optimization in order to exploit prior information about the gradient, which we show is necessary to break through the optimality of current methods",
        "We evaluate our bandit approach described in Section 3 and the natural evolutionary strategies (NES) approach of <a class=\"ref-link\" id=\"cIlyas_et+al_2017_a\" href=\"#rIlyas_et+al_2017_a\">Ilyas et al (2017</a>) on their effectiveness in generating untargeted adversarial examples",
        "For ImageNet, we record the effectiveness of the different approaches in both threat models in Table 1 ( 2 and \u221e perturbation constraints), where we show the attack success rate and the mean number of queries needed to generate an adversarial example for the Inception-v3 classifier",
        "When restricted to the inputs on which natural evolution strategies is successful in generating adversarial examples, our attacks are 2.5 and 5 times as query-efficient for the \u221e and 2 settings, respectively",
        "We develop a new, unifying perspective on black-box adversarial attacks"
    ],
    "key_statements": [
        "Recent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction",
        "This vulnerability is present in a wide range of settings, from situations in which inputs are fed directly to classifiers (<a class=\"ref-link\" id=\"cSzegedy_et+al_2013_a\" href=\"#rSzegedy_et+al_2013_a\">Szegedy et al, 2013</a>; <a class=\"ref-link\" id=\"cCarlini_et+al_2016_a\" href=\"#rCarlini_et+al_2016_a\">Carlini et al, 2016</a>) to highly variable real-world environments (<a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>; <a class=\"ref-link\" id=\"cAthalye_et+al_2017_a\" href=\"#rAthalye_et+al_2017_a\">Athalye et al, 2017</a>)",
        "We propose a new approach for generating black-box adversarial examples, using bandit optimization in order to exploit prior information about the gradient, which we show is necessary to break through the optimality of current methods",
        "We develop a bandit optimization framework for generating black-box adversarial examples which allows for the seamless integration of priors",
        "A natural idea here would be to avoid fully estimating the gradient and rely instead only on its imperfect estimators. This gives rise to the following question: How accurate of an gradient estimate is necessary to execute a successful projected gradient descent attack?. We examine this question first in the simplest possible setting: one in which we only take a single projected gradient descent step",
        "We describe our bandit framework for adversarial example generation in more detail",
        "Note that the algorithm is general and can be used to construct black-box adversarial examples where the perturbation is constrained to any convex set ( p-norm constraints being a special case)",
        "We evaluate our bandit approach described in Section 3 and the natural evolutionary strategies (NES) approach of <a class=\"ref-link\" id=\"cIlyas_et+al_2017_a\" href=\"#rIlyas_et+al_2017_a\">Ilyas et al (2017</a>) on their effectiveness in generating untargeted adversarial examples",
        "For ImageNet, we record the effectiveness of the different approaches in both threat models in Table 1 ( 2 and \u221e perturbation constraints), where we show the attack success rate and the mean number of queries needed to generate an adversarial example for the Inception-v3 classifier",
        "When restricted to the inputs on which natural evolution strategies is successful in generating adversarial examples, our attacks are 2.5 and 5 times as query-efficient for the \u221e and 2 settings, respectively",
        "We develop a new, unifying perspective on black-box adversarial attacks",
        "We prove that a standard least-squares estimator both captures the existing state-of-the-art approaches to black-box adversarial attacks, and is, in a certain natural sense, an optimal solution to the problem",
        "We break the barrier posed by this optimality by considering a previously unexplored aspect of the problem: the fact that there exists plenty of extra prior information about the gradient that one can exploit to mount a successful adversarial attack"
    ],
    "summary": [
        "Recent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction.",
        "We propose a new approach for generating black-box adversarial examples, using bandit optimization in order to exploit prior information about the gradient, which we show is necessary to break through the optimality of current methods.",
        "Given the availability of these informative gradient priors, we need a framework that enables us to incorporate these priors into our construction of black-box adversarial attacks.",
        "Just as this choice of the loss function t allows us to quantify performance on the gradient estimation problem, the latent vector vt will allow us to algorithmically incorporate prior information into our predictions.",
        "Note that the algorithm is general and can be used to construct black-box adversarial examples where the perturbation is constrained to any convex set ( p-norm constraints being a special case).",
        "In order to translate our gradient estimation algorithm into an efficient method for constructing black-box adversarial examples, we interleave our iterative gradient estimation algorithm with an iterative update of the image itself, using the boundary projection of gt in place of the gradient (c.f.",
        "This results in a general, efficient, prior-exploiting algorithm for constructing black-box adversarial examples.",
        "For ImageNet, we record the effectiveness of the different approaches in both threat models in Table 1 ( 2 and \u221e perturbation constraints), where we show the attack success rate and the mean number of queries needed to generate an adversarial example for the Inception-v3 classifier.",
        "When restricted to the inputs on which NES is successful in generating adversarial examples, our attacks are 2.5 and 5 times as query-efficient for the \u221e and 2 settings, respectively.",
        "Building on the work of <a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al (2017</a>), <a class=\"ref-link\" id=\"cIlyas_et+al_2017_a\" href=\"#rIlyas_et+al_2017_a\">Ilyas et al (2017</a>) designed a black-box attack strategy that uses finite differences but via natural evolution strategies (NES) to estimate the gradients.",
        "That for attacking single inputs, the overall query efficiency of this type of methods tends to be worse than that of the gradient estimation based ones.",
        "We prove that a standard least-squares estimator both captures the existing state-of-the-art approaches to black-box adversarial attacks, and is, in a certain natural sense, an optimal solution to the problem.",
        "We break the barrier posed by this optimality by considering a previously unexplored aspect of the problem: the fact that there exists plenty of extra prior information about the gradient that one can exploit to mount a successful adversarial attack.",
        "We identify two examples of such priors: a \u201ctime-dependent\u201d prior that corresponds to similarity of the gradients evaluated at similar inputs, and a \u201cdata-dependent\u201d prior derived from the latent structure present in the input space"
    ],
    "headline": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available",
    "reference_links": [
        {
            "id": "Athalye_et+al_2017_a",
            "entry": "Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. CoRR, abs/1707.07397, 2017. URL http://arxiv.org/abs/1707.07397.",
            "url": "http://arxiv.org/abs/1707.07397",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "Bhagoji_et+al_2017_a",
            "entry": "Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. arXiv preprint arXiv:1712.09491, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09491"
        },
        {
            "id": "Carlini_2017_a",
            "entry": "Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "Carlini_et+al_2016_a",
            "entry": "Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security Symposium, pp. 513\u2013530, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%202016"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15\u201326. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017"
        },
        {
            "id": "Foucart_2013_a",
            "entry": "Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing, volume 1. Birkh\u00e4user Basel, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20Simon%20Rauhut%2C%20Holger%20A%20mathematical%20introduction%20to%20compressive%20sensing%2C%20volume%201%202013"
        },
        {
            "id": "Gittens_2011_a",
            "entry": "A. Gittens and J. A. Tropp. Tail bounds for all eigenvalues of a sum of random matrices. ArXiv e-prints, apr 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gittens%2C%20A.%20Tropp%2C%20J.A.%20Tail%20bounds%20for%20all%20eigenvalues%20of%20a%20sum%20of%20random%20matrices.%20ArXiv%20e-prints%202011-04"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Gorban_et+al_2016_a",
            "entry": "Alexander N Gorban, Ivan Yu Tyukin, Danil V Prokhorov, and Konstantin I Sofeikov. Approximation with random bases: Pro et contra. Information Sciences, 364:129\u2013145, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gorban%2C%20Alexander%20N.%20Tyukin%2C%20Ivan%20Yu%20Prokhorov%2C%20Danil%20V.%20and%20Konstantin%20I%20Sofeikov.%20Approximation%20with%20random%20bases%3A%20Pro%20et%20contra%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gorban%2C%20Alexander%20N.%20Tyukin%2C%20Ivan%20Yu%20Prokhorov%2C%20Danil%20V.%20and%20Konstantin%20I%20Sofeikov.%20Approximation%20with%20random%20bases%3A%20Pro%20et%20contra%202016"
        },
        {
            "id": "Hazan_2016_a",
            "entry": "Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157\u2013325, 2016. ISSN 2167-3888. doi: 10.1561/2400000013. URL http://dx.doi.org/10.1561/2400000013.",
            "crossref": "https://dx.doi.org/10.1561/2400000013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1561/2400000013"
        },
        {
            "id": "Ilyas_et+al_2017_a",
            "entry": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. arXiv preprint arXiv:1712.07113, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07113"
        },
        {
            "id": "Johnson_1984_a",
            "entry": "William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "Laurent_2000_a",
            "entry": "B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302\u20131338, 10 2000. doi: 10.1214/aos/1015957395. URL https://doi.org/10.1214/aos/1015957395.",
            "crossref": "https://dx.doi.org/10.1214/aos/1015957395",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/aos/1015957395"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Bias_1994_a",
            "entry": "Ron Meir. Bias, variance and the combination of least squares estimators. In NIPS, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bias%2C%20Ron%20Meir%20variance%20and%20the%20combination%20of%20least%20squares%20estimators%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bias%2C%20Ron%20Meir%20variance%20and%20the%20combination%20of%20least%20squares%20estimators%201994"
        },
        {
            "id": "Moosavi-Dezfooli_et+al_2015_a",
            "entry": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015. URL http://arxiv.org/abs/1511.04599.",
            "url": "http://arxiv.org/abs/1511.04599",
            "arxiv_url": "https://arxiv.org/pdf/1511.04599"
        },
        {
            "id": "Narodytska_2017_a",
            "entry": "Nina Narodytska and Shiva Kasiviswanathan. Simple black-box adversarial attacks on deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 1310\u20131318. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narodytska%2C%20Nina%20Kasiviswanathan%2C%20Shiva%20Simple%20black-box%20adversarial%20attacks%20on%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narodytska%2C%20Nina%20Kasiviswanathan%2C%20Shiva%20Simple%20black-box%20adversarial%20attacks%20on%20deep%20neural%20networks%202017"
        },
        {
            "id": "Nelson_et+al_2012_a",
            "entry": "Blaine Nelson, Benjamin IP Rubinstein, Ling Huang, Anthony D Joseph, Steven J Lee, Satish Rao, and JD Tygar. Query strategies for evading convex-inducing classifiers. Journal of Machine Learning Research, 13(May):1293\u20131332, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Query%20strategies%20for%20evading%20convex-inducing%20classifiers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Query%20strategies%20for%20evading%20convex-inducing%20classifiers%202012"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Rigollet_2015_a",
            "entry": "Phillipe Rigollet. Mathematics of machine learning course notes: Projected gradient descent, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Phillipe%20Mathematics%20of%20machine%20learning%20course%20notes%3A%20Projected%20gradient%20descent%202015"
        },
        {
            "id": "URL_0000_a",
            "entry": "URL https://ocw.mit.edu/courses/mathematics/",
            "url": "https://ocw.mit.edu/courses/mathematics/"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Spall_2005_a",
            "entry": "James C Spall. Introduction to stochastic search and optimization: estimation, simulation, and control, volume 65. John Wiley & Sons, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spall%2C%20James%20C.%20Introduction%20to%20stochastic%20search%20and%20optimization%3A%20estimation%2C%20simulation%2C%20and%20control%2C%20volume%2065%202005"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Szegedy_et+al_0000_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. corr abs/1512.00567 (2015), 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.00567"
        },
        {
            "id": "Tu_et+al_2018_a",
            "entry": "Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. CoRR, abs/1805.11770, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11770"
        },
        {
            "id": "Xu_et+al_2016_a",
            "entry": "Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Weilin%20Qi%2C%20Yanjun%20Evans%2C%20David%20Automatically%20evading%20classifiers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Weilin%20Qi%2C%20Yanjun%20Evans%2C%20David%20Automatically%20evading%20classifiers%202016"
        },
        {
            "id": "I_2011_a",
            "entry": "I )), and thus, by the covariance estimation theorem of Gittens and Tropp Gittens & Tropp (2011) (see",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20%20and%20thus%20by%20the%20covariance%20estimation%20theorem%20of%20Gittens%20and%20Tropp%20Gittens%20%20Tropp%202011%20see",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20%20and%20thus%20by%20the%20covariance%20estimation%20theorem%20of%20Gittens%20and%20Tropp%20Gittens%20%20Tropp%202011%20see"
        },
        {
            "id": "To_2016_a",
            "entry": "To bound the second term in (12), we note that all the vectors \u03b4i are chosen independently of the vector g and each other. So, if we consider the set {g, \u03b41,..., \u03b4k} of k + 1 corresponding normalized directions, we have (see, e.g., (Gorban et al., 2016)) that the probability that any two of them have the (absolute value of) their inner product be larger than some \u03b5 =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=To%20bound%20the%20second%20term%20in%2012%20we%20note%20that%20all%20the%20vectors%20%CE%B4i%20are%20chosen%20independently%20of%20the%20vector%20g%20and%20each%20other%20So%20if%20we%20consider%20the%20set%20g%20%CE%B41%20%CE%B4k%20of%20k%20%201%20corresponding%20normalized%20directions%20we%20have%20see%20eg%20Gorban%20et%20al%202016%20that%20the%20probability%20that%20any%20two%20of%20them%20have%20the%20absolute%20value%20of%20their%20inner%20product%20be%20larger%20than%20some%20%CE%B5",
            "oa_query": "https://api.scholarcy.com/oa_version?query=To%20bound%20the%20second%20term%20in%2012%20we%20note%20that%20all%20the%20vectors%20%CE%B4i%20are%20chosen%20independently%20of%20the%20vector%20g%20and%20each%20other%20So%20if%20we%20consider%20the%20set%20g%20%CE%B41%20%CE%B4k%20of%20k%20%201%20corresponding%20normalized%20directions%20we%20have%20see%20eg%20Gorban%20et%20al%202016%20that%20the%20probability%20that%20any%20two%20of%20them%20have%20the%20absolute%20value%20of%20their%20inner%20product%20be%20larger%20than%20some%20%CE%B5"
        },
        {
            "id": "Id),_2000_b",
            "entry": "Id), so we have that (see, e.g., Lemma 1 in (Laurent & Massart, 2000)), for any 1 \u2264 i \u2264 k and any \u03b5 > 0, Pr ||\u03b4i||2 \u2265 1 + \u03b5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Id%20so%20we%20have%20that%20see%20eg%20Lemma%201%20in%20Laurent%20%20Massart%202000%20for%20any%201%20%20i%20%20k%20and%20any%20%CE%B5%20%200%20Pr%20%CE%B4i2%20%201%20%20%CE%B5"
        }
    ]
}
