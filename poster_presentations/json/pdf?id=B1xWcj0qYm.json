{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE MINIMAL SUPERVISION FOR TRAINING ANY BINARY CLASSIFIER FROM ONLY UNLABELED DATA",
        "author": "Nan Lu, Gang Niu, Aditya Krishna Menon",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1xWcj0qYm"
        },
        "abstract": "Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question\u2014what the minimal supervision is for training any binary classifier from only U data. Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent. Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data."
    },
    "keywords": [
        {
            "term": "different class",
            "url": "https://en.wikipedia.org/wiki/Different_Class"
        },
        {
            "term": "binary classifier",
            "url": "https://en.wikipedia.org/wiki/binary_classifier"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "semi supervised learning",
            "url": "https://en.wikipedia.org/wiki/semi_supervised_learning"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "ERM": "empirical risk minimization"
    },
    "highlights": [
        "With some properly chosen loss function (e.g., <a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\">Bartlett et al, 2006</a></a>; Tewari & Bartlett, 2007; <a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\"><a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\">Reid & Williamson, 2010</a></a>) and regularization (e.g., Tikhonov, 1943; <a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>), empirical risk minimization (ERM) is the common practice of supervised classification (Vapnik, 1998)",
        "It is easy to estimate the risk from only L data in order to carry out empirical risk minimization, and U data are needed exclusively in regularization",
        "We review empirical risk minimization (Vapnik, 1998) by imaging that we are given Xp = {x1, . . . , xn} \u223c pp(x) and Xn = {x1, . . . , xn } \u223c pn(x)",
        "We focused on training arbitrary binary classifier, ranging from linear to deep models, from only U data by empirical risk minimization",
        "We proved that risk rewrite as the core of empirical risk minimization is impossible given a single set of U data, but it becomes possible given two sets of U data with different class priors, after we assumed that all necessary class priors are given",
        "This possibility led to an unbiased risk estimator, and with the help of this risk estimator we proposed UU learning, the first empirical risk minimization-based learning method from two sets of U data"
    ],
    "key_statements": [
        "With some properly chosen loss function (e.g., <a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\">Bartlett et al, 2006</a></a>; Tewari & Bartlett, 2007; <a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\"><a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\">Reid & Williamson, 2010</a></a>) and regularization (e.g., Tikhonov, 1943; <a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>), empirical risk minimization (ERM) is the common practice of supervised classification (Vapnik, 1998)",
        "It is easy to estimate the risk from only L data in order to carry out empirical risk minimization, and U data are needed exclusively in regularization",
        "We review empirical risk minimization (Vapnik, 1998) by imaging that we are given Xp = {x1, . . . , xn} \u223c pp(x) and Xn = {x1, . . . , xn } \u223c pn(x)",
        "In order to try different \u03c0p, we first subsample the original datasets to match the desired \u03c0p and calculate the sample sizes n and n according to how many P and N data there are in the subsampled datasets, where \u03b8 and \u03b8 are set as close to 0.9 and 0.1 as possible",
        "We focused on training arbitrary binary classifier, ranging from linear to deep models, from only U data by empirical risk minimization",
        "We proved that risk rewrite as the core of empirical risk minimization is impossible given a single set of U data, but it becomes possible given two sets of U data with different class priors, after we assumed that all necessary class priors are given",
        "This possibility led to an unbiased risk estimator, and with the help of this risk estimator we proposed UU learning, the first empirical risk minimization-based learning method from two sets of U data"
    ],
    "summary": [
        "With some properly chosen loss function (e.g., <a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\"><a class=\"ref-link\" id=\"cBartlett_et+al_2006_a\" href=\"#rBartlett_et+al_2006_a\">Bartlett et al, 2006</a></a>; Tewari & Bartlett, 2007; <a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\"><a class=\"ref-link\" id=\"cReid_2010_a\" href=\"#rReid_2010_a\">Reid & Williamson, 2010</a></a>) and regularization (e.g., Tikhonov, 1943; <a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\"><a class=\"ref-link\" id=\"cSrivastava_et+al_2014_a\" href=\"#rSrivastava_et+al_2014_a\">Srivastava et al, 2014</a></a>), empirical risk minimization (ERM) is the common practice of supervised classification (Vapnik, 1998).",
        "The risk is estimated from both L and U data, and the resulted empirical training risk is minimized.",
        "We raise a fundamental question in weakly-supervised learning\u2014how many sets of U data with different class priors are necessary for rewriting the risk?",
        "With the help of this risk estimator, we propose an ERM-based learning method from two sets of U data.",
        "We have proven that R(g) is not rewritable given ptr, and <a class=\"ref-link\" id=\"cQuadrianto_et+al_2009_a\" href=\"#rQuadrianto_et+al_2009_a\">Quadrianto et al (2009</a>) has proven that R(g) can be estimated from Xtr and Xtr, where g is a linear model and is the logistic loss.",
        "We experimentally analyze the proposed method in training deep networks and subsequently experimentally compare it with state-of-the-art methods for learning from two sets of U data.",
        "Let and be real numbers around 1, \u03b8 = \u03b8 and \u03b8 = \u03b8 be perturbed \u03b8 and \u03b8 , and we test UU on MNIST and CIFAR-10 by drawing data using \u03b8 and \u03b8 but training models using \u03b8 and \u03b8 instead.",
        "We compare UU with two state-of-the-art methods for dealing with two sets of U data:10 proportion-SVM that is the best in learning from label proportions; balanced error minimization (BER, Menon et al, 2015) that is the most related work to UU.",
        "In order to try different \u03c0p, we first subsample the original datasets to match the desired \u03c0p and calculate the sample sizes n and n according to how many P and N data there are in the subsampled datasets, where \u03b8 and \u03b8 are set as close to 0.9 and 0.1 as possible.",
        "We focused on training arbitrary binary classifier, ranging from linear to deep models, from only U data by ERM.",
        "We proved that risk rewrite as the core of ERM is impossible given a single set of U data, but it becomes possible given two sets of U data with different class priors, after we assumed that all necessary class priors are given.",
        "This possibility led to an unbiased risk estimator, and with the help of this risk estimator we proposed UU learning, the first ERM-based learning method from two sets of U data.",
        "Experiments demonstrated that UU learning could successfully train fully connected, all convolutional and residual networks, and it compared favorably with state-of-the-art methods for learning from two sets of U data."
    ],
    "headline": "We study training arbitrary binary classifier from only unlabeled data by empirical risk minimization",
    "reference_links": [
        {
            "id": "Angluin_1988_a",
            "entry": "D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2(4):343\u2013370, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988"
        },
        {
            "id": "Bao_et+al_2018_a",
            "entry": "H. Bao, G. Niu, and M. Sugiyama. Classification from pairwise similarity and unlabeled data. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bao%2C%20H.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bao%2C%20H.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018"
        },
        {
            "id": "Bartlett_et+al_2006_a",
            "entry": "P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "Belkin_et+al_2006_a",
            "entry": "M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: a geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399\u20132434, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20a%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20a%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006"
        },
        {
            "id": "Brodersen_et+al_2010_a",
            "entry": "K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann. The balanced accuracy and its posterior distribution. In ICPR, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brodersen%2C%20K.H.%20Ong%2C%20C.S.%20Stephan%2C%20K.E.%20Buhmann%2C%20J.M.%20The%20balanced%20accuracy%20and%20its%20posterior%20distribution%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brodersen%2C%20K.H.%20Ong%2C%20C.S.%20Stephan%2C%20K.E.%20Buhmann%2C%20J.M.%20The%20balanced%20accuracy%20and%20its%20posterior%20distribution%202010"
        },
        {
            "id": "Chapelle_et+al_2002_a",
            "entry": "O. Chapelle, J. Weston, and B. Sch\u00f6lkopf. Cluster kernels for semi-supervised learning. In NeurIPS, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Weston%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Cluster%20kernels%20for%20semi-supervised%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Weston%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Cluster%20kernels%20for%20semi-supervised%20learning%202002"
        },
        {
            "id": "Chapelle_et+al_2006_a",
            "entry": "O. Chapelle, B. Sch\u00f6lkopf, and A. Zien (eds.). Semi-Supervised Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Sch%C3%B6lkopf%2C%20B.%20A.%20Zien%20%28eds.%29.%20Semi-Supervised%20Learning%202006"
        },
        {
            "id": "Cheng_et+al_2017_a",
            "entry": "J. Cheng, T. Liu, K. Ramamohanarao, and D. Tao. Learning with bounded instance- and labeldependent label noise. arXiv preprint arXiv:1709.03768, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03768"
        },
        {
            "id": "Du_et+al_2013_a",
            "entry": "M. C. du Plessis, G. Niu, and M. Sugiyama. Clustering unclustered data: Unsupervised binary labeling of two datasets having different class balances. In TAAI, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Clustering%20unclustered%20data%3A%20Unsupervised%20binary%20labeling%20of%20two%20datasets%20having%20different%20class%20balances%202013"
        },
        {
            "id": "Du_et+al_2014_a",
            "entry": "M. C. du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled data. In NeurIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Analysis%20of%20learning%20from%20positive%20and%20unlabeled%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Analysis%20of%20learning%20from%20positive%20and%20unlabeled%20data%202014"
        },
        {
            "id": "Du_et+al_2015_a",
            "entry": "M. C. du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and unlabeled data. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Convex%20formulation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Convex%20formulation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202015"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Elkan_2008_a",
            "entry": "C. Elkan and K. Noto. Learning classifiers from only positive and unlabeled data. In KDD, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elkan%2C%20C.%20Noto%2C%20K.%20Learning%20classifiers%20from%20only%20positive%20and%20unlabeled%20data%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elkan%2C%20C.%20Noto%2C%20K.%20Learning%20classifiers%20from%20only%20positive%20and%20unlabeled%20data%202008"
        },
        {
            "id": "Goldberger_2017_a",
            "entry": "J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017"
        },
        {
            "id": "Gomes_et+al_2010_a",
            "entry": "R. Gomes, A. Krause, and P. Perona. Discriminative clustering by regularized information maximization. In NeurIPS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomes%2C%20R.%20Krause%2C%20A.%20Perona%2C%20P.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomes%2C%20R.%20Krause%2C%20A.%20Perona%2C%20P.%20Discriminative%20clustering%20by%20regularized%20information%20maximization%202010"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "Grandvalet_2004_a",
            "entry": "Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grandvalet%2C%20Y.%20Bengio%2C%20Y.%20Semi-supervised%20learning%20by%20entropy%20minimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grandvalet%2C%20Y.%20Bengio%2C%20Y.%20Semi-supervised%20learning%20by%20entropy%20minimization%202004"
        },
        {
            "id": "Han_et+al_2018_a",
            "entry": "B. Han, J. Yao, G. Niu, M. Zhou, I. W. Tsang, Y. Zhang, and M. Sugiyama. Masking: A new perspective of noisy supervision. In NeurIPS, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20B.%20Yao%2C%20J.%20Niu%2C%20G.%20Zhou%2C%20M.%20Masking%3A%20A%20new%20perspective%20of%20noisy%20supervision%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20B.%20Yao%2C%20J.%20Niu%2C%20G.%20Zhou%2C%20M.%20Masking%3A%20A%20new%20perspective%20of%20noisy%20supervision%202018"
        },
        {
            "id": "Han_et+al_2018_b",
            "entry": "B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama. Co-teaching: Robust training deep neural networks with extremely noisy labels. In NeurIPS, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20B.%20Yao%2C%20Q.%20Yu%2C%20X.%20Niu%2C%20G.%20Co-teaching%3A%20Robust%20training%20deep%20neural%20networks%20with%20extremely%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20B.%20Yao%2C%20Q.%20Yu%2C%20X.%20Niu%2C%20G.%20Co-teaching%3A%20Robust%20training%20deep%20neural%20networks%20with%20extremely%20noisy%20labels%202018"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama. Learning discrete representations via information maximizing self augmented training. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20W.%20Miyato%2C%20T.%20Tokui%2C%20S.%20Matsumoto%2C%20E.%20Learning%20discrete%20representations%20via%20information%20maximizing%20self%20augmented%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20W.%20Miyato%2C%20T.%20Tokui%2C%20S.%20Matsumoto%2C%20E.%20Learning%20discrete%20representations%20via%20information%20maximizing%20self%20augmented%20training%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jiang_et+al_2018_a",
            "entry": "L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and F.-F. Li. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.-J.%20MentorNet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.-J.%20MentorNet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018"
        },
        {
            "id": "Kamnitsas_et+al_2018_a",
            "entry": "K. Kamnitsas, D. C. Castro, L. L. Folgoc, I. Walker, R. Tanno, D. Rueckert, B. Glocker, A. Criminisi, and A. Nori. Semi-supervised learning via compact latent space clustering. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamnitsas%2C%20K.%20Castro%2C%20D.C.%20Folgoc%2C%20L.L.%20Walker%2C%20I.%20Semi-supervised%20learning%20via%20compact%20latent%20space%20clustering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamnitsas%2C%20K.%20Castro%2C%20D.C.%20Folgoc%2C%20L.L.%20Walker%2C%20I.%20Semi-supervised%20learning%20via%20compact%20latent%20space%20clustering%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kiryo_et+al_2017_a",
            "entry": "R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In NeurIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiryo%2C%20R.%20Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20non-negative%20risk%20estimator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiryo%2C%20R.%20Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20non-negative%20risk%20estimator%202017"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Laine_2017_a",
            "entry": "S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Li_et+al_2009_a",
            "entry": "Y.-F. Li, I. W. Tsang, J. T. Kwok, and Z.-H. Zhou. Tighter and convex maximum margin clustering. In AISTATS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.-F.%20Tsang%2C%20I.W.%20Kwok%2C%20J.T.%20Zhou%2C%20Z.-H.%20Tighter%20and%20convex%20maximum%20margin%20clustering%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.-F.%20Tsang%2C%20I.W.%20Kwok%2C%20J.T.%20Zhou%2C%20Z.-H.%20Tighter%20and%20convex%20maximum%20margin%20clustering%202009"
        },
        {
            "id": "Liu_2016_a",
            "entry": "T. Liu and D. Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):447\u2013461, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016"
        },
        {
            "id": "Luo_et+al_2018_a",
            "entry": "Y. Luo, J. Zhu, M. Li, Y. Ren, and B. Zhang. Smooth neighbors on teacher graphs for semisupervised learning. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Y.%20Zhu%2C%20J.%20Li%2C%20M.%20Ren%2C%20Y.%20Smooth%20neighbors%20on%20teacher%20graphs%20for%20semisupervised%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Y.%20Zhu%2C%20J.%20Li%2C%20M.%20Ren%2C%20Y.%20Smooth%20neighbors%20on%20teacher%20graphs%20for%20semisupervised%20learning%202018"
        },
        {
            "id": "Mann_et+al_2007_a",
            "entry": "G. S. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning via expectation regularization. In ICML, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mann%2C%20G.S.%20Simple%2C%20A.McCallum%20robust%20scalable%20semi-supervised%20learning%20via%20expectation%20regularization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mann%2C%20G.S.%20Simple%2C%20A.McCallum%20robust%20scalable%20semi-supervised%20learning%20via%20expectation%20regularization%202007"
        },
        {
            "id": "Mcdiarmid_1989_a",
            "entry": "C. McDiarmid. On the method of bounded differences. In J. Siemons (ed.), Surveys in Combinatorics, pp. 148\u2013188. Cambridge University Press, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989"
        },
        {
            "id": "Menon_et+al_2015_a",
            "entry": "A. K. Menon, B. van Rooyen, C. S. Ong, and R. C. Williamson. Learning from corrupted binary labels via class-probability estimation. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.K.%20van%20Rooyen%2C%20B.%20Ong%2C%20C.S.%20Williamson%2C%20R.C.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.K.%20van%20Rooyen%2C%20B.%20Ong%2C%20C.S.%20Williamson%2C%20R.C.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015"
        },
        {
            "id": "Menon_et+al_2016_a",
            "entry": "A. K. Menon, B. van Rooyen, and N. Natarajan. Learning from binary labels with instancedependent corruption. arXiv preprint arXiv:1605.00751, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.00751"
        },
        {
            "id": "Miyato_et+al_2016_a",
            "entry": "T. Miyato, S. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual adversarial training. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016"
        },
        {
            "id": "Mohri_et+al_2012_a",
            "entry": "M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20M.%20Rostamizadeh%2C%20A.%20Talwalkar%2C%20A.%20Foundations%20of%20Machine%20Learning%202012"
        },
        {
            "id": "Natarajan_et+al_2013_a",
            "entry": "N. Natarajan, I. S. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. In NeurIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20N.%20Dhillon%2C%20I.S.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20N.%20Dhillon%2C%20I.S.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013"
        },
        {
            "id": "Netzer_et+al_2011_a",
            "entry": "Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Niu_et+al_2013_a",
            "entry": "G. Niu, W. Jitkrittum, B. Dai, H. Hachiya, and M. Sugiyama. Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niu%2C%20G.%20Jitkrittum%2C%20W.%20Dai%2C%20B.%20Hachiya%2C%20H.%20Squared-loss%20mutual%20information%20regularization%3A%20A%20novel%20information-theoretic%20approach%20to%20semi-supervised%20learning%202013"
        },
        {
            "id": "Niu_et+al_2016_a",
            "entry": "G. Niu, M. C. du Plessis, T. Sakai, Y. Ma, and M. Sugiyama. Theoretical comparisons of positiveunlabeled learning against positive-negative learning. In NeurIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sakai%2C%20T.%20Ma%2C%20Y.%20Theoretical%20comparisons%20of%20positiveunlabeled%20learning%20against%20positive-negative%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sakai%2C%20T.%20Ma%2C%20Y.%20Theoretical%20comparisons%20of%20positiveunlabeled%20learning%20against%20positive-negative%20learning%202016"
        },
        {
            "id": "Patrini_et+al_2017_a",
            "entry": "G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.K.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.K.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017"
        },
        {
            "id": "Quadrianto_et+al_2009_a",
            "entry": "N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le. Estimating labels from label proportions. Journal of Machine Learning Research, 10:2349\u20132374, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quadrianto%2C%20N.%20Smola%2C%20A.J.%20Caetano%2C%20T.S.%20Le%2C%20Q.V.%20Estimating%20labels%20from%20label%20proportions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quadrianto%2C%20N.%20Smola%2C%20A.J.%20Caetano%2C%20T.S.%20Le%2C%20Q.V.%20Estimating%20labels%20from%20label%20proportions%202009"
        },
        {
            "id": "Qui_et+al_2009_a",
            "entry": "J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning. MIT Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qui%C3%B1onero-Candela%2C%20J.%20Sugiyama%2C%20M.%20Schwaighofer%2C%20A.%20Lawrence%2C%20N.D.%20Dataset%20Shift%20in%20Machine%20Learning%202009"
        },
        {
            "id": "Reed_et+al_2015_a",
            "entry": "S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015"
        },
        {
            "id": "Reid_2010_a",
            "entry": "M. D. Reid and R. C. Williamson. Composite binary losses. Journal of Machine Learning Research, 11:2387\u20132422, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reid%2C%20M.D.%20Williamson%2C%20R.C.%20Composite%20binary%20losses%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reid%2C%20M.D.%20Williamson%2C%20R.C.%20Composite%20binary%20losses%202010"
        },
        {
            "id": "Ren_et+al_2018_a",
            "entry": "M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "Schoelkopf_2001_a",
            "entry": "B. Sch\u00f6lkopf and A. Smola. Learning with Kernels. MIT Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Smola%2C%20A.%20Learning%20with%20Kernels%202001"
        },
        {
            "id": "Scott_et+al_2014_a",
            "entry": "C. Scott, G. Blanchard, and G. Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In COLT, 2013. S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20C.%20Blanchard%2C%20G.%20Handy%2C%20G.%20Classification%20with%20asymmetric%20label%20noise%3A%20Consistency%20and%20maximal%20denoising%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%2C%20C.%20Blanchard%2C%20G.%20Handy%2C%20G.%20Classification%20with%20asymmetric%20label%20noise%3A%20Consistency%20and%20maximal%20denoising%202014"
        },
        {
            "id": "Springenberg_et+al_2015_a",
            "entry": "J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15: 1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sugiyama_et+al_2014_a",
            "entry": "M. Sugiyama, G. Niu, M. Yamada, M. Kimura, and H. Hachiya. Information-maximization clustering based on squared-loss mutual information. Neural Computation, 26(1):84\u2013131, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Niu%2C%20G.%20Yamada%2C%20M.%20Kimura%2C%20M.%20Information-maximization%20clustering%20based%20on%20squared-loss%20mutual%20information%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sugiyama%2C%20M.%20Niu%2C%20G.%20Yamada%2C%20M.%20Kimura%2C%20M.%20Information-maximization%20clustering%20based%20on%20squared-loss%20mutual%20information%202014"
        },
        {
            "id": "Sukhbaatar_et+al_2015_a",
            "entry": "S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015. A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. A. Tewari and P. L. Bartlett. On the consistency of multi-class classification methods. Journal of Machine Learning Research, 8:1007\u20131025, 2007. A. N. Tikhonov. On the stability of inverse problems (in Russian). Doklady Akademii Nauk SSSR, 39(5):195\u2013198, 1943. H. Valizadegan and R. Jin. Generalized maximum margin clustering and unsupervised kernel learning. In NeurIPS, 2006. B. van Rooyen and R. C. Williamson. A theory of learning with corrupted labels. Journal of Machine Learning Research, 18(228):1\u201350, 2018. V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20S.%20Bruna%2C%20J.%20Paluri%2C%20M.%20Bourdev%2C%20L.%20Training%20convolutional%20networks%20with%20noisy%20labels%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20S.%20Bruna%2C%20J.%20Paluri%2C%20M.%20Bourdev%2C%20L.%20Training%20convolutional%20networks%20with%20noisy%20labels%202015"
        },
        {
            "id": "Ward_et+al_2009_a",
            "entry": "G. Ward, T. Hastie, S. Barry, J. Elith, and J. Leathwick. Presence-only data and the EM algorithm. Biometrics, 65(2):554\u2013563, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ward%2C%20G.%20Hastie%2C%20T.%20Barry%2C%20S.%20Elith%2C%20J.%20Presence-only%20data%20and%20the%20EM%20algorithm%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ward%2C%20G.%20Hastie%2C%20T.%20Barry%2C%20S.%20Elith%2C%20J.%20Presence-only%20data%20and%20the%20EM%20algorithm%202009"
        },
        {
            "id": "Xiao_et+al_2017_a",
            "entry": "H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "Xu_et+al_2004_a",
            "entry": "L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In NeurIPS, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20L.%20Neufeld%2C%20J.%20Larson%2C%20B.%20Schuurmans%2C%20D.%20Maximum%20margin%20clustering%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20L.%20Neufeld%2C%20J.%20Larson%2C%20B.%20Schuurmans%2C%20D.%20Maximum%20margin%20clustering%202004"
        },
        {
            "id": "Yu_et+al_2013_a",
            "entry": "F. X. Yu, D. Liu, S. Kumar, T. Jebara, and S.-F. Chang. \u221dSVM for learning with label proportions. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.X.%20Liu%2C%20D.%20Kumar%2C%20S.%20Jebara%2C%20T.%20%E2%88%9DSVM%20for%20learning%20with%20label%20proportions%202013"
        },
        {
            "id": "Proof_1989_a",
            "entry": "Proof. Consider the one-side uniform deviation supg\u2208G Ruu(g) \u2212 R(g). Since 0 \u2264 (z) \u2264 C, the change of it will be no more than C \u03b1/n if some xi is replaced, or no more than C \u03b1 /n if some xj is replaced. Subsequently, McDiarmid\u2019s inequality (McDiarmid, 1989) tells us that",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20Consider%20the%20one-side%20uniform%20deviation%20supg%E2%88%88G%20Ruu%28g%29%20%E2%88%92%20R%28g%29.%20Since%200%20%E2%89%A4%20%28z%29%20%E2%89%A4%20C%2C%20the%20change%20of%20it%20will%20be%20no%20more%20than%20C%20%CE%B1/n%20if%20some%20xi%20is%20replaced%2C%20or%20no%20more%20than%20C%20%CE%B1%20/n%20if%20some%20xj%20is%20replaced%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20Consider%20the%20one-side%20uniform%20deviation%20supg%E2%88%88G%20Ruu%28g%29%20%E2%88%92%20R%28g%29.%20Since%200%20%E2%89%A4%20%28z%29%20%E2%89%A4%20C%2C%20the%20change%20of%20it%20will%20be%20no%20more%20than%20C%20%CE%B1/n%20if%20some%20xi%20is%20replaced%2C%20or%20no%20more%20than%20C%20%CE%B1%20/n%20if%20some%20xj%20is%20replaced%201989"
        },
        {
            "id": "By_0000_a",
            "entry": "By symmetrization (Vapnik, 1998), it is a routine work to show that",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=By%20symmetrization%20Vapnik%201998%20it%20is%20a%20routine%20work%20to%20show%20that"
        },
        {
            "id": "E[supg\u2208G_0000_b",
            "entry": "E[supg\u2208G Ruu(g) \u2212 R(g)] \u2264 2\u03b1Rn( \u25e6 G) + 2\u03b1 Rn ( \u25e6 G), and according to Talagrand\u2019s contraction lemma (Shalev-Shwartz & Ben-David, 2014), Rn( \u25e6 G) \u2264 L Rn(G), Rn ( \u25e6 G) \u2264 L Rn (G).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=EsupgG%20Ruug%20%20Rg%20%202%CE%B1Rn%20%20G%20%202%CE%B1%20Rn%20%20%20G%20and%20according%20to%20Talagrands%20contraction%20lemma%20ShalevShwartz%20%20BenDavid%202014%20Rn%20%20G%20%20L%20RnG%20Rn%20%20%20G%20%20L%20Rn%20G"
        },
        {
            "id": "In_2000_c",
            "entry": "In the introduction, we illustrated the learning problem and the proposed method using a Gaussian mixture of two components. The details of this illustrative example are presented here. The P component pp(x) and N component pn(x) are both two-dimensional Gaussian distributions. Their means are \u03bc+ = [+1, +1], \u03bc\u2212 = [\u22121, \u22121], and their covariance is the identity matrix. The two training distributions are created following (1) with class priors \u03b8 = 0.9 and \u03b8 = 0.4. Subsequently, the two sets of U training data were sampled from those distributions with sample sizes n = 2000 and n = 1000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20the%20introduction%20we%20illustrated%20the%20learning%20problem%20and%20the%20proposed%20method%20using%20a%20Gaussian%20mixture%20of%20two%20components%20The%20details%20of%20this%20illustrative%20example%20are%20presented%20here%20The%20P%20component%20ppx%20and%20N%20component%20pnx%20are%20both%20twodimensional%20Gaussian%20distributions%20Their%20means%20are%20%CE%BC%20%201%201%20%CE%BC%20%201%201%20and%20their%20covariance%20is%20the%20identity%20matrix%20The%20two%20training%20distributions%20are%20created%20following%201%20with%20class%20priors%20%CE%B8%20%2009%20and%20%CE%B8%20%2004%20Subsequently%20the%20two%20sets%20of%20U%20training%20data%20were%20sampled%20from%20those%20distributions%20with%20sample%20sizes%20n%20%202000%20and%20n%20%201000"
        },
        {
            "id": "Moreover_2013_a",
            "entry": "Moreover, pp(x) and pn(x) are combined to form the test distribution p(x, y) with weights 0.3 and 0.7, so \u03c0p = 0.3. Note that p(x) changes between training and test distributions (which can be seen from Figure 1 by comparing (c) and (d) in the left panel and the right panel). This is the key difference between UU and CCN (Natarajan et al., 2013). For training, a linear (-in-input) model g(x) = \u03c9 x + b where \u03c9 \u2208 R2 and b \u2208 R, and a sigmoid loss sig(z) = 1/(1 + exp(z)) were used. SGD was employed for optimization, where the learning rate was 0.01 and the batch size was 128. The model just has three parameters, so for the sake of a clear comparison of different risk estimators, we did not add any regularization. For every method, the model was trained 500 epochs. The final models are plotted in Figure 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreover%20pp%28x%29%20and%20pn%28x%29%20are%20combined%20to%20form%20the%20test%20distribution%20p%28x%2C%20y%29%20with%20weights%200.3%20and%200.7%2C%20so%20%CF%80p%20%3D%200.3.%20Note%20that%20p%28x%29%20changes%20between%20training%20and%20test%20distributions%20%28which%20can%20be%20seen%20from%20Figure%201%20by%20comparing%20%28c%29%20and%20%28d%29%20in%20the%20left%20panel%20and%20the%20right%20panel%29.%20This%20is%20the%20key%20difference%20between%20UU%20and%20CCN%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreover%20pp%28x%29%20and%20pn%28x%29%20are%20combined%20to%20form%20the%20test%20distribution%20p%28x%2C%20y%29%20with%20weights%200.3%20and%200.7%2C%20so%20%CF%80p%20%3D%200.3.%20Note%20that%20p%28x%29%20changes%20between%20training%20and%20test%20distributions%20%28which%20can%20be%20seen%20from%20Figure%201%20by%20comparing%20%28c%29%20and%20%28d%29%20in%20the%20left%20panel%20and%20the%20right%20panel%29.%20This%20is%20the%20key%20difference%20between%20UU%20and%20CCN%202013"
        },
        {
            "id": "MNIST_2015_a",
            "entry": "MNIST This is a grayscale image dataset of handwritten digits from 0 to 9 where the size of the images is 28*28. It contains 60,000 training images and 10,000 test images. Since it has 10 classes originally, we used the even digits as the P class and the odd digits as the N class, respectively. The model was FC with ReLU as the activation function: d-300-300-300-300-1. Batch normalization (Ioffe & Szegedy, 2015) was applied before hidden layers. An 2-regularization was added, where the regularization parameter was fixed to 1e-4. The model was trained by SGD with an initial learning rate 1e-3 and a batch size 128. In addition, the learning rate was decreased by",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MNIST%20This%20is%20a%20grayscale%20image%20dataset%20of%20handwritten%20digits%20from%200%20to%209%20where%20the%20size%20of%20the%20images%20is%202828%20It%20contains%2060000%20training%20images%20and%2010000%20test%20images%20Since%20it%20has%2010%20classes%20originally%20we%20used%20the%20even%20digits%20as%20the%20P%20class%20and%20the%20odd%20digits%20as%20the%20N%20class%20respectively%20The%20model%20was%20FC%20with%20ReLU%20as%20the%20activation%20function%20d3003003003001%20Batch%20normalization%20Ioffe%20%20Szegedy%202015%20was%20applied%20before%20hidden%20layers%20An%202regularization%20was%20added%20where%20the%20regularization%20parameter%20was%20fixed%20to%201e4%20The%20model%20was%20trained%20by%20SGD%20with%20an%20initial%20learning%20rate%201e3%20and%20a%20batch%20size%20128%20In%20addition%20the%20learning%20rate%20was%20decreased%20by",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MNIST%20This%20is%20a%20grayscale%20image%20dataset%20of%20handwritten%20digits%20from%200%20to%209%20where%20the%20size%20of%20the%20images%20is%202828%20It%20contains%2060000%20training%20images%20and%2010000%20test%20images%20Since%20it%20has%2010%20classes%20originally%20we%20used%20the%20even%20digits%20as%20the%20P%20class%20and%20the%20odd%20digits%20as%20the%20N%20class%20respectively%20The%20model%20was%20FC%20with%20ReLU%20as%20the%20activation%20function%20d3003003003001%20Batch%20normalization%20Ioffe%20%20Szegedy%202015%20was%20applied%20before%20hidden%20layers%20An%202regularization%20was%20added%20where%20the%20regularization%20parameter%20was%20fixed%20to%201e4%20The%20model%20was%20trained%20by%20SGD%20with%20an%20initial%20learning%20rate%201e3%20and%20a%20batch%20size%20128%20In%20addition%20the%20learning%20rate%20was%20decreased%20by"
        },
        {
            "id": "SVHN_2015_b",
            "entry": "SVHN This is a 32*32 color image dataset of street view house numbers from 0 to 9. It consists of 73,257 training data, 26,032 test data, and 531,131 extra training data. We sampled 100,000 data for training from the concatenation of training data and extra training data\u2014the extra training data were used to ensure enough training data so as to perform class-prior changes. For SVHN dataset, \u20180\u2019, \u20186\u2019, \u20188\u2019, \u20189\u2019 made up the P class, and \u20181\u2019, \u20182\u2019, \u20183\u2019, \u20184\u2019, \u20185\u2019, \u20187\u2019 made up the N class. The model was AllConvNet (Springenberg et al., 2015) as follows.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SVHN%20This%20is%20a%203232%20color%20image%20dataset%20of%20street%20view%20house%20numbers%20from%200%20to%209%20It%20consists%20of%2073257%20training%20data%2026032%20test%20data%20and%20531131%20extra%20training%20data%20We%20sampled%20100000%20data%20for%20training%20from%20the%20concatenation%20of%20training%20data%20and%20extra%20training%20datathe%20extra%20training%20data%20were%20used%20to%20ensure%20enough%20training%20data%20so%20as%20to%20perform%20classprior%20changes%20For%20SVHN%20dataset%200%206%208%209%20made%20up%20the%20P%20class%20and%201%202%203%204%205%207%20made%20up%20the%20N%20class%20The%20model%20was%20AllConvNet%20Springenberg%20et%20al%202015%20as%20follows",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SVHN%20This%20is%20a%203232%20color%20image%20dataset%20of%20street%20view%20house%20numbers%20from%200%20to%209%20It%20consists%20of%2073257%20training%20data%2026032%20test%20data%20and%20531131%20extra%20training%20data%20We%20sampled%20100000%20data%20for%20training%20from%20the%20concatenation%20of%20training%20data%20and%20extra%20training%20datathe%20extra%20training%20data%20were%20used%20to%20ensure%20enough%20training%20data%20so%20as%20to%20perform%20classprior%20changes%20For%20SVHN%20dataset%200%206%208%209%20made%20up%20the%20P%20class%20and%201%202%203%204%205%207%20made%20up%20the%20N%20class%20The%20model%20was%20AllConvNet%20Springenberg%20et%20al%202015%20as%20follows"
        }
    ]
}
