{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "AUTOMATICALLY COMPOSING REPRESENTATION TRANSFORMATIONS AS A MEANS FOR GENERALIZATION",
        "author": "Michael B. Chang Electrical Engineering and Computer Science University of California, Berkeley, USA mbchang@berkeley.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1ffQnRcKX"
        },
        "journal": "SIMON",
        "abstract": "A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning \u2013 either training a separate learner per task or training a single learner for all tasks \u2013 both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domaingeneral framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not."
    },
    "keywords": [
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "physics",
            "url": "https://en.wikipedia.org/wiki/physics"
        },
        {
            "term": "Defence Advanced Research Projects Agency",
            "url": "https://en.wikipedia.org/wiki/Defence_Advanced_Research_Projects_Agency"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "connectionism",
            "url": "https://en.wikipedia.org/wiki/connectionism"
        }
    ],
    "abbreviations": {
        "CRL": "compositional recursive learner",
        "MDP": "Markov decision process",
        "RNN": "recurrent neural network",
        "STN": "Spatial Transformer Network",
        "DARPA": "Defence Advanced Research Projects Agency",
        "NSF": "National Science Foundation"
    },
    "highlights": [
        "This paper seeks to tackle the question of how to build machines that leverage prior experience to solve more complex problems than they have previously encountered",
        "Representing and leveraging prior problem-solving experience amounts to learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution",
        "This paper introduces the compositional recursive learner (CRL), a framework for learning both these transformations and their composition together with sparse supervision, taking a step beyond other approaches that have assumed either pre-specified transformation or composition rules (Sec. 5)",
        "We formalized the compositional problem graph as a language for studying compositionally-structured problems of different complexity that can be applied on various problems in machine learning",
        "We presented the compositional recursive learner, a domain-general framework for learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution",
        "Because problems in supervised, unsupervised, and reinforcement learning can all be expressed under the framework of transformations between representations in the compositional problem graph, we hope that our work motivates further research for tackling the compositional generalization problem in many other domains to accelerate the long-range generalization capabilities that are characteristic of general-purpose learning machines"
    ],
    "key_statements": [
        "This paper seeks to tackle the question of how to build machines that leverage prior experience to solve more complex problems than they have previously encountered",
        "Representing and leveraging prior problem-solving experience amounts to learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution",
        "This paper introduces the compositional recursive learner (CRL), a framework for learning both these transformations and their composition together with sparse supervision, taking a step beyond other approaches that have assumed either pre-specified transformation or composition rules (Sec. 5)",
        "This section introduces the compositional recursive learner (CRL), a framework for training modules to capture primitive subproblems and for composing together these modules as subproblem solutions to form a path between nodes of the compositional problem graph.\n3.1",
        "Training compositional recursive learner on a diverse compositional problem distribution produces a modular recursive program that is trained to transform the input Xin into its output Xout, the corresponding samples of which are drawn from pairs of nodes in the compositional problem graph",
        "We formalized the compositional problem graph as a language for studying compositionally-structured problems of different complexity that can be applied on various problems in machine learning",
        "We presented the compositional recursive learner, a domain-general framework for learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution",
        "Because problems in supervised, unsupervised, and reinforcement learning can all be expressed under the framework of transformations between representations in the compositional problem graph, we hope that our work motivates further research for tackling the compositional generalization problem in many other domains to accelerate the long-range generalization capabilities that are characteristic of general-purpose learning machines"
    ],
    "summary": [
        "This paper seeks to tackle the question of how to build machines that leverage prior experience to solve more complex problems than they have previously encountered.",
        "The central contributions of this paper are to frame the shared structure across multiple tasks in terms of a compositional problem graph, propose compositional generalization as an evaluation scheme to test the degree a learner can apply previously learned knowledge to solve new problems, and introduce the compositional recursive learner, a domain-general framework1 for sequentially composing representation transformations that each solve a subproblem of a larger problem.",
        "Representing and leveraging prior problem-solving experience amounts to learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution.",
        "This section introduces the compositional recursive learner (CRL), a framework for training modules to capture primitive subproblems and for composing together these modules as subproblem solutions to form a path between nodes of the compositional problem graph.",
        "Training CRL on a diverse compositional problem distribution produces a modular recursive program that is trained to transform the input Xin into its output Xout, the corresponding samples of which are drawn from pairs of nodes in the compositional problem graph.",
        "Because the the space of subproblem compositions is combinatorially large, we use a curriculum to encourage solutions for the simpler subproblems to converge somewhat before introducing more complex problems, for which CRL can learn to solve by composing together the modules that had been trained on simpler problems.",
        "Our experiments show that this analogy-making ability helps with compositional generalization because the controller solves new or more complex subproblem combinations by re-using modules that it learned during training.",
        "We presented the compositional recursive learner, a domain-general framework for learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution.",
        "<a class=\"ref-link\" id=\"cGriffiths_et+al_2019_a\" href=\"#rGriffiths_et+al_2019_a\">Griffiths et al (2019</a>) argued that the efficient use cognitive resources in humans may explain their ability to generalize, and this paper provides evidence that reasoning about what computation to execute by making analogies to previously seen problems achieves significantly higher compositional generalization than non-compositional monolithic learners.",
        "Because problems in supervised, unsupervised, and reinforcement learning can all be expressed under the framework of transformations between representations in the compositional problem graph, we hope that our work motivates further research for tackling the compositional generalization problem in many other domains to accelerate the long-range generalization capabilities that are characteristic of general-purpose learning machines."
    ],
    "headline": "This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems",
    "reference_links": [
        {
            "id": "Alet_et+al_2018_a",
            "entry": "Ferran Alet, Tomas Lozano-Perez, and Leslie P Kaelbling. Modular meta-learning. arXiv preprint arXiv:1806.10166, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10166"
        },
        {
            "id": "Anderson_2014_a",
            "entry": "James A Anderson and Geoffrey E Hinton. Models of information processing in the brain. In Parallel models of associative memory, pp. 33\u201374. Psychology Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20James%20A.%20Hinton%2C%20Geoffrey%20E.%20Models%20of%20information%20processing%20in%20the%20brain.%20In%20Parallel%20models%20of%20associative%20memory%202014"
        },
        {
            "id": "Anderson_1990_a",
            "entry": "John Robert Anderson. The adaptive character of thought. chapter 5, pp. 191\u2013230. Psychology Press, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20John%20Robert%20The%20adaptive%20character%20of%20thought%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20John%20Robert%20The%20adaptive%20character%20of%20thought%201990"
        },
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016"
        },
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems, pp. 4331\u20134339, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016"
        },
        {
            "id": "Bacon_et+al_2017_a",
            "entry": "Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pp. 1726\u2013 1734, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "Bahdanau_et+al_2018_a",
            "entry": "Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? arXiv preprint arXiv:1811.12889, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.12889"
        },
        {
            "id": "Barto_2003_a",
            "entry": "Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems, 13(1-2):41\u201377, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barto%2C%20Andrew%20G.%20Mahadevan%2C%20Sridhar%20Recent%20advances%20in%20hierarchical%20reinforcement%20learning.%20Discrete%20event%20dynamic%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barto%2C%20Andrew%20G.%20Mahadevan%2C%20Sridhar%20Recent%20advances%20in%20hierarchical%20reinforcement%20learning.%20Discrete%20event%20dynamic%202003"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pp. 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "Battaglia_et+al_2018_a",
            "entry": "Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01261"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Botvinick_et+al_2009_a",
            "entry": "Matthew M Botvinick, Yael Niv, and Andrew C Barto. Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. Cognition, 113(3):262\u2013280, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Botvinick%2C%20Matthew%20M.%20Niv%2C%20Yael%20Barto%2C%20Andrew%20C.%20Hierarchically%20organized%20behavior%20and%20its%20neural%20foundations%3A%20a%20reinforcement%20learning%20perspective%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Botvinick%2C%20Matthew%20M.%20Niv%2C%20Yael%20Barto%2C%20Andrew%20C.%20Hierarchically%20organized%20behavior%20and%20its%20neural%20foundations%3A%20a%20reinforcement%20learning%20perspective%202009"
        },
        {
            "id": "Bunel_et+al_2018_a",
            "entry": "Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. arXiv preprint arXiv:1805.04276, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.04276"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. arXiv preprint arXiv:1704.06611, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06611"
        },
        {
            "id": "Callaway_et+al_2017_a",
            "entry": "Frederick Callaway, Falk Lieder, Paul Krueger, and Thomas L Griffiths. Mouselab-mdp: A new paradigm for tracing how people plan. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Callaway%2C%20Frederick%20Lieder%2C%20Falk%20Krueger%2C%20Paul%20Griffiths%2C%20Thomas%20L.%20Mouselab-mdp%3A%20A%20new%20paradigm%20for%20tracing%20how%20people%20plan%202017"
        },
        {
            "id": "Calvo_2014_a",
            "entry": "Paco Calvo and John Symons. The Architecture of Cognition: Rethinking Fodor and Pylyshyn\u2019s Systematicity Challenge. MIT Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calvo%2C%20Paco%20Symons%2C%20John%20The%20Architecture%20of%20Cognition%3A%20Rethinking%20Fodor%20and%20Pylyshyn%E2%80%99s%20Systematicity%20Challenge%202014"
        },
        {
            "id": "Carey_2015_a",
            "entry": "Susan Carey. 15 why theories of concepts should not ignore the problem of acquisition. The conceptual mind: New directions in the study of concepts, pp. 415, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carey%2C%20Susan%2015%20why%20theories%20of%20concepts%20should%20not%20ignore%20the%20problem%20of%20acquisition.%20The%20conceptual%20mind%3A%20New%20directions%20in%20the%20study%20of%20concepts%202015"
        },
        {
            "id": "Chang_et+al_2016_a",
            "entry": "Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00341"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Xinyun Chen, Chang Liu, and Dawn Song. Learning neural programs to parse programs. arXiv preprint arXiv:1706.01284, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01284"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Dechter_et+al_2013_a",
            "entry": "Eyal Dechter, Jonathan Malmaud, Ryan P Adams, and Joshua B Tenenbaum. Bootstrap learning via modular concept discovery. In IJCAI, pp. 1302\u20131309, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dechter%2C%20Eyal%20Malmaud%2C%20Jonathan%20Adams%2C%20Ryan%20P.%20Tenenbaum%2C%20Joshua%20B.%20Bootstrap%20learning%20via%20modular%20concept%20discovery%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dechter%2C%20Eyal%20Malmaud%2C%20Jonathan%20Adams%2C%20Ryan%20P.%20Tenenbaum%2C%20Joshua%20B.%20Bootstrap%20learning%20via%20modular%20concept%20discovery%202013"
        },
        {
            "id": "Devin_et+al_2017_a",
            "entry": "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 2169\u20132176. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devin%2C%20Coline%20Gupta%2C%20Abhishek%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devin%2C%20Coline%20Gupta%2C%20Abhishek%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017"
        },
        {
            "id": "Dzeroski_et+al_2001_a",
            "entry": "Saso Dzeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine learning, 43(1-2):7\u201352, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dzeroski%2C%20Saso%20Raedt%2C%20Luc%20De%20Driessens%2C%20Kurt%20Relational%20reinforcement%20learning%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dzeroski%2C%20Saso%20Raedt%2C%20Luc%20De%20Driessens%2C%20Kurt%20Relational%20reinforcement%20learning%202001"
        },
        {
            "id": "Ellis_et+al_2018_a",
            "entry": "Kevin Ellis, Lucas Morales, Mathias Sable-Meyer, Armando Solar-Lezama, and Josh Tenenbaum. Learning libraries of subroutines for neurally\u2013guided bayesian program induction. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 7805\u2013 7815. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8006-learning-libraries-of-subroutines-for-neurallyguided-bayesian-program-inducti pdf.",
            "url": "http://papers.nips.cc/paper/8006-learning-libraries-of-subroutines-for-neurallyguided-bayesian-program-inducti",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ellis%2C%20Kevin%20Morales%2C%20Lucas%20Sable-Meyer%2C%20Mathias%20Solar-Lezama%2C%20Armando%20Learning%20libraries%20of%20subroutines%20for%20neurally%E2%80%93guided%20bayesian%20program%20induction%202018"
        },
        {
            "id": "Eslami_et+al_2016_a",
            "entry": "SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in Neural Information Processing Systems, pp. 3225\u20133233, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eslami%2C%20S.M.Ali%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Tassa%2C%20Yuval%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eslami%2C%20S.M.Ali%20Heess%2C%20Nicolas%20Weber%2C%20Theophane%20Tassa%2C%20Yuval%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016"
        },
        {
            "id": "Fernando_et+al_2017_a",
            "entry": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "Feser_et+al_2016_a",
            "entry": "John K Feser, Marc Brockschmidt, Alexander L Gaunt, and Daniel Tarlow. Differentiable functional program interpreters. arXiv preprint arXiv:1611.01988, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01988"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "Fodor_2002_a",
            "entry": "Jerry A Fodor and Ernest Lepore. The compositionality papers. Oxford University Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Lepore%2C%20Ernest%20The%20compositionality%20papers%202002"
        },
        {
            "id": "Fodor_1988_a",
            "entry": "Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988"
        },
        {
            "id": "Frans_et+al_2017_a",
            "entry": "Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. arXiv preprint arXiv:1710.09767, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09767"
        },
        {
            "id": "Ganin_et+al_2018_a",
            "entry": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M. Ali Eslami, and Orial Vinyals. Synthesizing programs for images using reinforced adversarial learning. arXiv preprint arXiv:1804.01118, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01118"
        },
        {
            "id": "Gaunt_et+al_2016_a",
            "entry": "Alexander L Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Differentiable programs with neural libraries. arXiv preprint arXiv:1611.02109, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02109"
        },
        {
            "id": "Gopnik_2012_a",
            "entry": "Alison Gopnik and Henry M Wellman. Reconstructing constructivism: Causal models, bayesian learning mechanisms, and the theory theory. Psychological bulletin, 138(6):1085, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20Reconstructing%20constructivism%3A%20Causal%20models%2C%20bayesian%20learning%20mechanisms%2C%20and%20the%20theory%20theory%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20Reconstructing%20constructivism%3A%20Causal%20models%2C%20bayesian%20learning%20mechanisms%2C%20and%20the%20theory%20theory%202012"
        },
        {
            "id": "Grant_et+al_2018_a",
            "entry": "Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08930"
        },
        {
            "id": "Graves_2016_a",
            "entry": "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08983"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_b",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "Grefenstette_et+al_2015_a",
            "entry": "Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp. 1828\u20131836, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015"
        },
        {
            "id": "Griffiths_et+al_2015_a",
            "entry": "Thomas L Griffiths, Falk Lieder, and Noah D Goodman. Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in cognitive science, 7(2): 217\u2013229, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20use%20of%20cognitive%20resources%3A%20Levels%20of%20analysis%20between%20the%20computational%20and%20the%20algorithmic%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20use%20of%20cognitive%20resources%3A%20Levels%20of%20analysis%20between%20the%20computational%20and%20the%20algorithmic%202015"
        },
        {
            "id": "Griffiths_et+al_2019_a",
            "entry": "Thomas L Griffiths, Fred Callaway, Michael B Chang, Erin Grant, Paul M Krueger, and Falk Lieder. Doing more with less: Meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, 01 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20Thomas%20L.%20Callaway%2C%20Fred%20Chang%2C%20Michael%20B.%20Grant%2C%20Erin%20Doing%20more%20with%20less%3A%20Meta-reasoning%20and%20meta-learning%20in%20humans%20and%20machines%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20Thomas%20L.%20Callaway%2C%20Fred%20Chang%2C%20Michael%20B.%20Grant%2C%20Erin%20Doing%20more%20with%20less%3A%20Meta-reasoning%20and%20meta-learning%20in%20humans%20and%20machines%202019"
        },
        {
            "id": "Gupta_et+al_0000_a",
            "entry": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised metalearning for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04640"
        },
        {
            "id": "Gupta_et+al_0000_b",
            "entry": "Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07245"
        },
        {
            "id": "Ha_2018_a",
            "entry": "David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10122"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "Hamrick_et+al_2017_a",
            "entry": "Jessica B Hamrick, Andrew J Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, and Peter W Battaglia. Metacontrol for adaptive imagination-based optimization. arXiv preprint arXiv:1705.02670, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02670"
        },
        {
            "id": "Hay_et+al_2014_a",
            "entry": "Nicholas Hay, Stuart Russell, David Tolpin, and Solomon Eyal Shimony. Selecting computations: Theory and applications. arXiv preprint arXiv:1408.2048, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.2048"
        },
        {
            "id": "Higgins_et+al_2018_a",
            "entry": "Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1812.02230"
        },
        {
            "id": "Hinton_et+al_2018_a",
            "entry": "Geoffrey Hinton, Nicholas Frosst, and Sara Sabour. Matrix capsules with em routing. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Frosst%2C%20Nicholas%20Sabour%2C%20Sara%20Matrix%20capsules%20with%20em%20routing%202018"
        },
        {
            "id": "Holte_2003_a",
            "entry": "Robert C Holte and Berthe Y Choueiry. Abstraction and reformulation in artificial intelligence. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 358(1435): 1197\u20131204, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holte%2C%20Robert%20C.%20Choueiry%2C%20Berthe%20Y.%20Abstraction%20and%20reformulation%20in%20artificial%20intelligence%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holte%2C%20Robert%20C.%20Choueiry%2C%20Berthe%20Y.%20Abstraction%20and%20reformulation%20in%20artificial%20intelligence%202003"
        },
        {
            "id": "Hopfield_1982_a",
            "entry": "John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopfield%2C%20John%20J.%20Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hopfield%2C%20John%20J.%20Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities%201982"
        },
        {
            "id": "Jacobs_et+al_1991_a",
            "entry": "Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pp. 2017\u20132025, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017"
        },
        {
            "id": "Janner_et+al_2018_a",
            "entry": "Michael Janner, Sergey Levine, William T Freeman, Joshua B Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about physical interactions with object-oriented prediction and planning. arXiv preprint arXiv:1812.10972, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1812.10972"
        },
        {
            "id": "Joulin_2015_a",
            "entry": "Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural information processing systems, pp. 190\u2013198, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015"
        },
        {
            "id": "Kaiser_2015_a",
            "entry": "\u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08228"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kirsch_et+al_2018_a",
            "entry": "Louis Kirsch, Julius Kunze, and David Barber. Modular networks: Learning to decompose neural computation. In Advances in Neural Information Processing Systems, pp. 2414\u20132423, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirsch%2C%20Louis%20Kunze%2C%20Julius%20Barber%2C%20David%20Modular%20networks%3A%20Learning%20to%20decompose%20neural%20computation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirsch%2C%20Louis%20Kunze%2C%20Julius%20Barber%2C%20David%20Modular%20networks%3A%20Learning%20to%20decompose%20neural%20computation%202018"
        },
        {
            "id": "Kohonen_1972_a",
            "entry": "Teuvo Kohonen. Correlation matrix memories. IEEE transactions on computers, 100(4):353\u2013359, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohonen%2C%20Teuvo%20Correlation%20matrix%20memories%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohonen%2C%20Teuvo%20Correlation%20matrix%20memories%201972"
        },
        {
            "id": "Kulkarni_et+al_2015_a",
            "entry": "Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems, pp. 2539\u20132547, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015"
        },
        {
            "id": "Kulkarni_et+al_2016_a",
            "entry": "Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pp. 3675\u20133683, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Kurach_et+al_2015_a",
            "entry": "Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. arXiv preprint arXiv:1511.06392, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06392"
        },
        {
            "id": "Lake_2017_a",
            "entry": "BM Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. arXiv preprint arXiv:1711.00350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00350"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Lieder_et+al_2017_a",
            "entry": "Falk Lieder, Frederick Callaway, Sayan Gul, Paul M Krueger, and Thomas L Griffiths. Learning to select computations. arXiv preprint arXiv:1711.06892, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06892"
        },
        {
            "id": "Lin_2017_a",
            "entry": "Chen-Hsuan Lin and Simon Lucey. Inverse compositional spatial transformer networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2568\u20132576, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Chen-Hsuan%20Lucey%2C%20Simon%20Inverse%20compositional%20spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Chen-Hsuan%20Lucey%2C%20Simon%20Inverse%20compositional%20spatial%20transformer%20networks%202017"
        },
        {
            "id": "Liska_et+al_2018_a",
            "entry": "Adam Liska, German Kruszewski, and Marco Baroni. Memorize or generalize? searching for a compositional rnn in a haystack. arXiv preprint arXiv:1802.06467, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06467"
        },
        {
            "id": "Lopez-Paz_2017_a",
            "entry": "David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467\u20136476, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017"
        },
        {
            "id": "Loula_et+al_2018_a",
            "entry": "Joao Loula, Marco Baroni, and Brenden M Lake. Rearranging the familiar: Testing compositional generalization in recurrent networks. arXiv preprint arXiv:1807.07545, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.07545"
        },
        {
            "id": "Marcus_1998_a",
            "entry": "Gary F Marcus. Rethinking eliminative connectionism. Cognitive psychology, 37(3):243\u2013282, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Gary%20F.%20Rethinking%20eliminative%20connectionism%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Gary%20F.%20Rethinking%20eliminative%20connectionism%201998"
        },
        {
            "id": "Marcus_2018_a",
            "entry": "Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Gary%20F.%20The%20algebraic%20mind%3A%20Integrating%20connectionism%20and%20cognitive%20science%202018"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive metalearner. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20metalearner%202018"
        },
        {
            "id": "Mnih_et+al_2014_a",
            "entry": "Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances in neural information processing systems, pp. 2204\u20132212, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "Nachum_et+al_2018_a",
            "entry": "Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. arXiv preprint arXiv:1805.08296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08296"
        },
        {
            "id": "Nair_et+al_2018_a",
            "entry": "Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, pp. 9209\u20139220, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Ashvin%20V.%20Pong%2C%20Vitchyr%20Dalal%2C%20Murtaza%20Bahl%2C%20Shikhar%20Visual%20reinforcement%20learning%20with%20imagined%20goals%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Ashvin%20V.%20Pong%2C%20Vitchyr%20Dalal%2C%20Murtaza%20Bahl%2C%20Shikhar%20Visual%20reinforcement%20learning%20with%20imagined%20goals%202018"
        },
        {
            "id": "Nair_2010_a",
            "entry": "Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Nichol_et+al_2018_a",
            "entry": "Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. CoRR, abs/1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. arXiv preprint arXiv:1706.05064, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05064"
        },
        {
            "id": "Parascandolo_et+al_2017_a",
            "entry": "Giambattista Parascandolo, Mateo Rojas-Carulla, Niki Kilbertus, and Bernhard Scholkopf. Learning independent causal mechanisms. arXiv preprint arXiv:1712.00961, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00961"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Pathak_et+al_2019_a",
            "entry": "Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, and Alexei A Efros. Learning to control self-assembling morphologies: a study of generalization via modularity. arXiv preprint arXiv:1902.05546, 2019.",
            "arxiv_url": "https://arxiv.org/pdf/1902.05546"
        },
        {
            "id": "Pinker_1994_a",
            "entry": "S Pinker. Baby born talkingdescribes heaven. The language instinct: How the mind creates language, pp. 265\u2013301, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinker%2C%20S.%20Baby%20born%20talkingdescribes%20heaven.%20The%20language%20instinct%3A%20How%20the%20mind%20creates%20language%201994"
        },
        {
            "id": "Ravi_2016_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202016"
        },
        {
            "id": "Reed_2015_a",
            "entry": "Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06279"
        },
        {
            "id": "Riedel_et+al_2016_a",
            "entry": "Sebastian Riedel, Matko Bosnjak, and Tim Rocktaschel. Programming with a differentiable forth interpreter. CoRR, abs/1605.06640, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06640"
        },
        {
            "id": "Rosenbaum_et+al_2018_a",
            "entry": "Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenbaum%2C%20Clemens%20Klinger%2C%20Tim%20Riemer%2C%20Matthew%20Routing%20networks%3A%20Adaptive%20selection%20of%20non-linear%20functions%20for%20multi-task%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenbaum%2C%20Clemens%20Klinger%2C%20Tim%20Riemer%2C%20Matthew%20Routing%20networks%3A%20Adaptive%20selection%20of%20non-linear%20functions%20for%20multi-task%20learning%202018"
        },
        {
            "id": "Stuart_1991_a",
            "entry": "Stuart Russell and Eric Wefald. Principles of metareasoning. Artificial intelligence, 49(1-3):361\u2013 395, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stuart%20Russell%20and%20Eric%20Wefald%20Principles%20of%20metareasoning%20Artificial%20intelligence%204913361%20395%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stuart%20Russell%20and%20Eric%20Wefald%20Principles%20of%20metareasoning%20Artificial%20intelligence%204913361%20395%201991"
        },
        {
            "id": "Sanborn_et+al_2018_a",
            "entry": "S Sanborn, D Bourgin, M Chang, and T Griffiths. Representational efficiency outweighs action efficiency in human program induction. In Proceedings of the 40th annual Cognitive Science Society, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sanborn%2C%20S.%20Bourgin%2C%20D.%20Chang%2C%20M.%20Griffiths%2C%20T.%20Representational%20efficiency%20outweighs%20action%20efficiency%20in%20human%20program%20induction%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sanborn%2C%20S.%20Bourgin%2C%20D.%20Chang%2C%20M.%20Griffiths%2C%20T.%20Representational%20efficiency%20outweighs%20action%20efficiency%20in%20human%20program%20induction%202018-07"
        },
        {
            "id": "Sanchez-Gonzalez_et+al_2018_a",
            "entry": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control. arXiv preprint arXiv:1806.01242, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01242"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv preprint arXiv:1706.01427, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Schlag_2017_a",
            "entry": "Imanol Schlag and Jurgen Schmidhuber. Gated fast weights for on-the-fly neural program generation. In NIPS Metalearning Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Gated%20fast%20weights%20for%20on-the-fly%20neural%20program%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Gated%20fast%20weights%20for%20on-the-fly%20neural%20program%20generation%202017"
        },
        {
            "id": "Schlag_2018_a",
            "entry": "Imanol Schlag and Jurgen Schmidhuber. Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems, pp. 10003\u201310014, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Learning%20to%20reason%20with%20third%20order%20tensor%20products%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlag%2C%20Imanol%20Schmidhuber%2C%20Jurgen%20Learning%20to%20reason%20with%20third%20order%20tensor%20products%202018"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "Jurgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universitat Munchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Evolutionary%20principles%20in%20self-referential%20learning%2C%20or%20on%20learning%20how%20to%20learn%3A%20the%20meta-meta-%201987"
        },
        {
            "id": "Schmidhuber_1990_a",
            "entry": "Jurgen Schmidhuber. Towards compositional learning with dynamic neural networks. Inst. fur Informatik, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Towards%20compositional%20learning%20with%20dynamic%20neural%20networks%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Towards%20compositional%20learning%20with%20dynamic%20neural%20networks%201990"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992"
        },
        {
            "id": "Schmidhuber_2009_a",
            "entry": "Jurgen Schmidhuber. Ultimate cognition ala godel. Cognitive Computation, 1(2):177\u2013193, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Ultimate%20cognition%20ala%20godel%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Ultimate%20cognition%20ala%20godel%202009"
        },
        {
            "id": "Schmidhuber_2012_a",
            "entry": "Jurgen Schmidhuber. Self-delimiting neural networks. arXiv preprint arXiv:1210.0118, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1210.0118"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Simon_1969_a",
            "entry": "Herbert A Simon. The sciences of the artificial. Cambridge, MA, pp. 132, 1969. Herbert A Simon. The science of design: Creating the artificial. Design Issues, pp. 67\u201382, 1988. Alec Solway, Carlos Diuk, Natalia Cordova, Debbie Yee, Andrew G Barto, Yael Niv, and Matthew M",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%2C%20Herbert%20A.%20The%20sciences%20of%20the%20artificial%201969",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simon%2C%20Herbert%20A.%20The%20sciences%20of%20the%20artificial%201969"
        },
        {
            "id": "Botvinick_2014_a",
            "entry": "Botvinick. Optimal behavioral hierarchy. PLoS computational biology, 10(8):e1003779, 2014. Elizabeth S Spelke. Principles of object perception. Cognitive science, 14(1):29\u201356, 1990. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440\u20132448, 2015. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104\u20133112, 2014. Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean",
            "arxiv_url": "https://arxiv.org/pdf/1412.6806"
        }
    ]
}
