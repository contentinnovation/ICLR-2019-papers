{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "MARGINALIZED AVERAGE ATTENTIONAL NETWORK FOR WEAKLY-SUPERVISED LEARNING",
        "author": "Yuan Yuan, Yueming Lyu, Xi Shen, Ivor W. Tsang, & Dit-Yan Yeung, 1Hong Kong University of Science and Technology, 2Alibaba Group 3University of Technology Sydney, 4Ecole des Ponts ParisTech",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkljioCcFQ"
        },
        "abstract": "In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from O(2T ) to O(T 2). Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization."
    },
    "keywords": [
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "mean average precision",
            "url": "https://en.wikipedia.org/wiki/mean_average_precision"
        },
        {
            "term": "Action recognition",
            "url": "https://en.wikipedia.org/wiki/Action_recognition"
        }
    ],
    "abbreviations": {
        "MAAN": "marginalized average attentional network",
        "MAA": "marginalized average aggregation",
        "CAS": "class activation sequence",
        "CAM": "class activation map",
        "CNNs": "convolutional neural networks",
        "STPN": "sparse temporal pooling network",
        "mAP": "mean average precision",
        "IoU": "intersection over union"
    },
    "highlights": [
        "Weakly-supervised temporal action localization has been of interest to the community recently",
        "We propose the marginalized average attentional network (MAAN) to alleviate the issue raised by the domination of the most salient region in an end-to-end fashion for weakly-supervised action localization",
        "Extensive experiments on two large-scale video datasets show that marginalized average attentional network consistently outperforms the baseline models and achieves superior performance on weakly-supervised temporal action localization",
        "Our main contributions include: (1) a novel end-to-end trainable marginalized average attentional network (MAAN) with a marginalized average aggregation (MAA) module in the weaklysupervised setting; (2) theoretical analysis of the properties of marginalized average aggregation and an explanation of the reasons marginalized average attentional network alleviates the issue raised by the domination of the most salient regions; (3) a fast iterative algorithm that can effectively reduce the computational complexity of marginalized average aggregation; and (4) a superior performance on two benchmark video datasets, THUMOS14 and ActivityNet1.3, on the weakly-supervised temporal action localization.\n2 MARGINALIZED AVERAGE ATTENTIONAL NETWORK",
        "We have proposed the marginalized average attentional network (MAAN) for weakly-supervised temporal action localization",
        "Our proposed marginalized average attentional network achieves superior performance on both the THUMOS14 and the ActivityNet1.3 datasets on weakly-supervised temporal action localization tasks compared to current state-of-the-art methods"
    ],
    "key_statements": [
        "Weakly-supervised temporal action localization has been of interest to the community recently",
        "We propose the marginalized average attentional network (MAAN) to alleviate the issue raised by the domination of the most salient region in an end-to-end fashion for weakly-supervised action localization",
        "With the marginalized average aggregation, the learned latent discriminative probability reduces the difference of response between the most salient regions and the others",
        "Extensive experiments on two large-scale video datasets show that marginalized average attentional network consistently outperforms the baseline models and achieves superior performance on weakly-supervised temporal action localization",
        "Our main contributions include: (1) a novel end-to-end trainable marginalized average attentional network (MAAN) with a marginalized average aggregation (MAA) module in the weaklysupervised setting; (2) theoretical analysis of the properties of marginalized average aggregation and an explanation of the reasons marginalized average attentional network alleviates the issue raised by the domination of the most salient regions; (3) a fast iterative algorithm that can effectively reduce the computational complexity of marginalized average aggregation; and (4) a superior performance on two benchmark video datasets, THUMOS14 and ActivityNet1.3, on the weakly-supervised temporal action localization.\n2 MARGINALIZED AVERAGE ATTENTIONAL NETWORK",
        "Network Architecture: We describe the network architecture that employs the marginalized average aggregation module described above for weakly-supervised temporal action localization",
        "Our marginalized average attentional network uses the attention module to generate the latent discriminative probability pt and replaces the feature aggregator from the weighted sum aggregation by the proposed marginalized average aggregation, which is demonstrated on the right in Figure 4",
        "We evaluate marginalized average attentional network on two popular action localization benchmark datasets, THUMOS14 (<a class=\"ref-link\" id=\"cJiang_et+al_2014_a\" href=\"#rJiang_et+al_2014_a\">Jiang et al, 2014</a>) and ActivityNet1.3 (<a class=\"ref-link\" id=\"cHeilbron_et+al_2015_a\" href=\"#rHeilbron_et+al_2015_a\">Heilbron et al, 2015</a>)",
        "We have proposed the marginalized average attentional network (MAAN) for weakly-supervised temporal action localization",
        "We have proposed a fast algorithm to reduce the computation complexity of marginalized average aggregation",
        "Our proposed marginalized average attentional network achieves superior performance on both the THUMOS14 and the ActivityNet1.3 datasets on weakly-supervised temporal action localization tasks compared to current state-of-the-art methods"
    ],
    "summary": [
        "Weakly-supervised temporal action localization has been of interest to the community recently.",
        "We propose the marginalized average attentional network (MAAN) to alleviate the issue raised by the domination of the most salient region in an end-to-end fashion for weakly-supervised action localization.",
        "MAAN suppresses the action prediction response of the most salient regions by employing marginalized average aggregation (MAA) and learning the latent discriminative probability in a principled manner.",
        "Extensive experiments on two large-scale video datasets show that MAAN consistently outperforms the baseline models and achieves superior performance on weakly-supervised temporal action localization.",
        "Our main contributions include: (1) a novel end-to-end trainable marginalized average attentional network (MAAN) with a marginalized average aggregation (MAA) module in the weaklysupervised setting; (2) theoretical analysis of the properties of MAA and an explanation of the reasons MAAN alleviates the issue raised by the domination of the most salient regions; (3) a fast iterative algorithm that can effectively reduce the computational complexity of MAA; and (4) a superior performance on two benchmark video datasets, THUMOS14 and ActivityNet1.3, on the weakly-supervised temporal action localization.",
        "By employing the latent discriminative probabilities for prediction instead of the attention weights, our method can alleviate the dominant effect of the most salient region in weakly-supervised temporal localization.",
        "STPN uses a feature aggregator to calculate a weighted sum of the snippet-level features with these class-agnostic attention weights to create a video-level representation, as shown on the left in Figure 4.",
        "Our MAAN uses the attention module to generate the latent discriminative probability pt and replaces the feature aggregator from the weighted sum aggregation by the proposed marginalized average aggregation, which is demonstrated on the right in Figure 4.",
        "MAAN implicitly factorizes the attention weight into ctpt, where pt learns the latent discriminative probability of the current snippet, and ct captures the contextual information and regularizes the network to learn a more informative aggregation.",
        "We have proposed the marginalized average attentional network (MAAN) for weakly-supervised temporal action localization.",
        "MAAN employs a novel marginalized average aggregation (MAA) operation to encourage the network to identify the dense and integral action segments and is trained in an end-to-end fashion.",
        "We have proved that MAA reduces the gap between the most discriminant regions in the video to the others, and MAAN generates better class activation sequences to infer the action locations.",
        "Our proposed MAAN achieves superior performance on both the THUMOS14 and the ActivityNet1.3 datasets on weakly-supervised temporal action localization tasks compared to current state-of-the-art methods."
    ],
    "headline": "We propose a marginalized average attentional network to suppress the dominant response of the most salient regions in a principled manner",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Bengio_et+al_2009_a",
            "entry": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "Bilen_2016_a",
            "entry": "Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Weakly%20supervised%20deep%20detection%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Weakly%20supervised%20deep%20detection%20networks%202016"
        },
        {
            "id": "Carreira_2017_a",
            "entry": "J. Carreira and A Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "Cinbis_et+al_2017_a",
            "entry": "Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid. Weakly supervised object localization with multi-fold multiple instance learning. IEEE transactions on pattern analysis and machine intelligence, 39(1):189\u2013203, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cinbis%2C%20Ramazan%20Gokberk%20Verbeek%2C%20Jakob%20Schmid%2C%20Cordelia%20Weakly%20supervised%20object%20localization%20with%20multi-fold%20multiple%20instance%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cinbis%2C%20Ramazan%20Gokberk%20Verbeek%2C%20Jakob%20Schmid%2C%20Cordelia%20Weakly%20supervised%20object%20localization%20with%20multi-fold%20multiple%20instance%20learning%202017"
        },
        {
            "id": "Girdhar_2017_a",
            "entry": "Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition. In Advances in Neural Information Processing Systems, pp. 33\u201344, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girdhar%2C%20Rohit%20Ramanan%2C%20Deva%20Attentional%20pooling%20for%20action%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girdhar%2C%20Rohit%20Ramanan%2C%20Deva%20Attentional%20pooling%20for%20action%20recognition%202017"
        },
        {
            "id": "Gkioxari_et+al_2015_a",
            "entry": "Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action recognition with r* cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1080\u20131088, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gkioxari%2C%20Georgia%20Girshick%2C%20Ross%20Malik%2C%20Jitendra%20Contextual%20action%20recognition%20with%20r%2A%20cnn%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gkioxari%2C%20Georgia%20Girshick%2C%20Ross%20Malik%2C%20Jitendra%20Contextual%20action%20recognition%20with%20r%2A%20cnn%202015"
        },
        {
            "id": "Hamilton_et+al_2017_a",
            "entry": "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1025\u20131035, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "Heilbron_et+al_2015_a",
            "entry": "F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heilbron%2C%20F.C.%20Escorcia%2C%20V.%20Ghanem%2C%20B.%20Niebles%2C%20J.C.%20Activitynet%3A%20A%20large-scale%20video%20benchmark%20for%20human%20activity%20understanding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heilbron%2C%20F.C.%20Escorcia%2C%20V.%20Ghanem%2C%20B.%20Niebles%2C%20J.C.%20Activitynet%3A%20A%20large-scale%20video%20benchmark%20for%20human%20activity%20understanding%202015"
        },
        {
            "id": "Hermann_et+al_2015_a",
            "entry": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pp. 1693\u20131701, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hermann%2C%20Karl%20Moritz%20Kocisky%2C%20Tomas%20Grefenstette%2C%20Edward%20Espeholt%2C%20Lasse%20Teaching%20machines%20to%20read%20and%20comprehend%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hermann%2C%20Karl%20Moritz%20Kocisky%2C%20Tomas%20Grefenstette%2C%20Edward%20Espeholt%2C%20Lasse%20Teaching%20machines%20to%20read%20and%20comprehend%202015"
        },
        {
            "id": "Jiang_et+al_2014_a",
            "entry": "Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. THUMOS challenge: Action recognition with a large number of classes. http://crcv.ucf.edu/ THUMOS14/, 2014.",
            "url": "http://crcv.ucf.edu/THUMOS14/"
        },
        {
            "id": "Kantorov_et+al_2016_a",
            "entry": "Vadim Kantorov, Maxime Oquab, Minsu Cho, and Ivan Laptev. ContextLocNet: Context-aware deep network models for weakly supervised localization. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kantorov%2C%20Vadim%20Oquab%2C%20Maxime%20Cho%2C%20Minsu%20Laptev%2C%20Ivan%20ContextLocNet%3A%20Context-aware%20deep%20network%20models%20for%20weakly%20supervised%20localization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kantorov%2C%20Vadim%20Oquab%2C%20Maxime%20Cho%2C%20Minsu%20Laptev%2C%20Ivan%20ContextLocNet%3A%20Context-aware%20deep%20network%20models%20for%20weakly%20supervised%20localization%202016"
        },
        {
            "id": "Kay_et+al_2017_a",
            "entry": "W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Greem, T. Back, P. Natsev, M. Suleyman, and A. Zisserman. The kinetics human action video dataset. In arXiv:1705.06950v1, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06950v1"
        },
        {
            "id": "Kim_et+al_2017_a",
            "entry": "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks. arXiv preprint arXiv:1702.00887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00887"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. Kingma and J. Ba. Adam: A method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kong_2017_a",
            "entry": "Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for fine-grained classification. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7025\u20137034. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%20Shu%20Fowlkes%2C%20Charless%20Low-rank%20bilinear%20pooling%20for%20fine-grained%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%20Shu%20Fowlkes%2C%20Charless%20Low-rank%20bilinear%20pooling%20for%20fine-grained%20classification%202017"
        },
        {
            "id": "Mensch_2018_a",
            "entry": "Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming for structured prediction and attention. arXiv preprint arXiv:1802.03676, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03676"
        },
        {
            "id": "Nguyen_et+al_2018_a",
            "entry": "Phuc Nguyen, Ting Liu, Gautam Prasad, and Bohyung Han. Weakly supervised action localization by sparse temporal pooling network. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Phuc%20Liu%2C%20Ting%20Prasad%2C%20Gautam%20Han%2C%20Bohyung%20Weakly%20supervised%20action%20localization%20by%20sparse%20temporal%20pooling%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Phuc%20Liu%2C%20Ting%20Prasad%2C%20Gautam%20Han%2C%20Bohyung%20Weakly%20supervised%20action%20localization%20by%20sparse%20temporal%20pooling%20network%202018"
        },
        {
            "id": "Oquab_et+al_2015_a",
            "entry": "Maxime Oquab, L\u00e9on Bottou, Ivan Laptev, and Josef Sivic. Is object localization for free?-weaklysupervised learning with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 685\u2013694, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oquab%2C%20Maxime%20Bottou%2C%20L%C3%A9on%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Is%20object%20localization%20for%20free%3F-weaklysupervised%20learning%20with%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oquab%2C%20Maxime%20Bottou%2C%20L%C3%A9on%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Is%20object%20localization%20for%20free%3F-weaklysupervised%20learning%20with%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Pinheiro_2015_a",
            "entry": "Pedro O Pinheiro and Ronan Collobert. From image-level to pixel-level labeling with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1713\u20131721, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinheiro%2C%20Pedro%20O.%20Collobert%2C%20Ronan%20From%20image-level%20to%20pixel-level%20labeling%20with%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinheiro%2C%20Pedro%20O.%20Collobert%2C%20Ronan%20From%20image-level%20to%20pixel-level%20labeling%20with%20convolutional%20networks%202015"
        },
        {
            "id": "Richard_2016_a",
            "entry": "Alexander Richard and Juergen Gall. Temporal action detection using a statistical language model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3131\u20133140, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%2C%20Alexander%20Gall%2C%20Juergen%20Temporal%20action%20detection%20using%20a%20statistical%20language%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richard%2C%20Alexander%20Gall%2C%20Juergen%20Temporal%20action%20detection%20using%20a%20statistical%20language%20model%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Sharma_et+al_2015_a",
            "entry": "Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual attention. arXiv preprint arXiv:1511.04119, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04119"
        },
        {
            "id": "Shou_2016_a",
            "entry": "Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1049\u20131058, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shou%2C%20Zheng%20Dongang%20Wang%2C%20and%20Shih-Fu%20Chang.%20Temporal%20action%20localization%20in%20untrimmed%20videos%20via%20multi-stage%20cnns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shou%2C%20Zheng%20Dongang%20Wang%2C%20and%20Shih-Fu%20Chang.%20Temporal%20action%20localization%20in%20untrimmed%20videos%20via%20multi-stage%20cnns%202016"
        },
        {
            "id": "Shou_et+al_2017_a",
            "entry": "Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. CDC: convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shou%2C%20Zheng%20Chan%2C%20Jonathan%20Zareian%2C%20Alireza%20Miyazawa%2C%20Kazuyuki%20CDC%3A%20convolutional-de-convolutional%20networks%20for%20precise%20temporal%20action%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shou%2C%20Zheng%20Chan%2C%20Jonathan%20Zareian%2C%20Alireza%20Miyazawa%2C%20Kazuyuki%20CDC%3A%20convolutional-de-convolutional%20networks%20for%20precise%20temporal%20action%20localization%202017"
        },
        {
            "id": "Shou_et+al_2018_a",
            "entry": "Zheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa, and Shih-Fu Chang. Autoloc: Weaklysupervised temporal action localization in untrimmed videos. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 154\u2013171, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shou%2C%20Zheng%20Gao%2C%20Hang%20Zhang%2C%20Lei%20Miyazawa%2C%20Kazuyuki%20Autoloc%3A%20Weaklysupervised%20temporal%20action%20localization%20in%20untrimmed%20videos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shou%2C%20Zheng%20Gao%2C%20Hang%20Zhang%2C%20Lei%20Miyazawa%2C%20Kazuyuki%20Autoloc%3A%20Weaklysupervised%20temporal%20action%20localization%20in%20untrimmed%20videos%202018"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pp. 568\u2013576, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014"
        },
        {
            "id": "Singh_2016_a",
            "entry": "Gurkirt Singh and Fabio Cuzzolin. Untrimmed video classification for activity detection: submission to activitynet challenge. arXiv preprint arXiv:1607.01979, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.01979"
        },
        {
            "id": "Singh_2017_a",
            "entry": "Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In The IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Krishna%20Kumar%20Lee%2C%20Yong%20Jae%20Hide-and-seek%3A%20Forcing%20a%20network%20to%20be%20meticulous%20for%20weakly-supervised%20object%20and%20action%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Krishna%20Kumar%20Lee%2C%20Yong%20Jae%20Hide-and-seek%3A%20Forcing%20a%20network%20to%20be%20meticulous%20for%20weakly-supervised%20object%20and%20action%20localization%202017"
        },
        {
            "id": "Tran_et+al_2015_a",
            "entry": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp. 4489\u20134497, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017"
        },
        {
            "id": "Wah_et+al_2011_a",
            "entry": "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20C.%20Branson%2C%20S.%20Welinder%2C%20P.%20Perona%2C%20P.%20The%20Caltech-UCSD%20Birds-200-2011%20Dataset%202011"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. Untrimmednets for weakly supervised action recognition and detection. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Gool%2C%20Luc%20Van%20Untrimmednets%20for%20weakly%20supervised%20action%20recognition%20and%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Gool%2C%20Luc%20Van%20Untrimmednets%20for%20weakly%20supervised%20action%20recognition%20and%20detection%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "R. Wang and D. Tao. Acitivitynet large scale activity recognition challenge. UTS at Activitynet, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20R.%20Tao%2C%20D.%20Acitivitynet%20large%20scale%20activity%20recognition%20challenge%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20R.%20Tao%2C%20D.%20Acitivitynet%20large%20scale%20activity%20recognition%20challenge%202016"
        },
        {
            "id": "Wei_et+al_2017_a",
            "entry": "Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classification to semantic segmentation approach. In IEEE CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Yunchao%20Feng%2C%20Jiashi%20Liang%2C%20Xiaodan%20Cheng%2C%20Ming-Ming%20Object%20region%20mining%20with%20adversarial%20erasing%3A%20A%20simple%20classification%20to%20semantic%20segmentation%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Yunchao%20Feng%2C%20Jiashi%20Liang%2C%20Xiaodan%20Cheng%2C%20Ming-Ming%20Object%20region%20mining%20with%20adversarial%20erasing%3A%20A%20simple%20classification%20to%20semantic%20segmentation%20approach%202017"
        },
        {
            "id": "Xiong_et+al_2017_a",
            "entry": "Yuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, and Xiaoou Tang. A pursuit of temporal accuracy in general activity detection. arXiv preprint arXiv:1703.02716, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02716"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "H. A. Xu, A. Das, and K. Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20H.A.%20Das%2C%20A.%20Saenko%2C%20K.%20R-c3d%3A%20Region%20convolutional%203d%20network%20for%20temporal%20activity%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20H.A.%20Das%2C%20A.%20Saenko%2C%20K.%20R-c3d%3A%20Region%20convolutional%203d%20network%20for%20temporal%20activity%20detection%202017"
        },
        {
            "id": "Yeung_et+al_2016_a",
            "entry": "Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end learning of action detection from frame glimpses in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2678\u20132687, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeung%2C%20Serena%20Russakovsky%2C%20Olga%20Mori%2C%20Greg%20Fei-Fei%2C%20Li%20End-to-end%20learning%20of%20action%20detection%20from%20frame%20glimpses%20in%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeung%2C%20Serena%20Russakovsky%2C%20Olga%20Mori%2C%20Greg%20Fei-Fei%2C%20Li%20End-to-end%20learning%20of%20action%20detection%20from%20frame%20glimpses%20in%20videos%202016"
        },
        {
            "id": "Yuan_et+al_2016_a",
            "entry": "J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal action localization with pyramid of score distribution features. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20J.%20Ni%2C%20B.%20Yang%2C%20X.%20Kassim%2C%20A.A.%20Temporal%20action%20localization%20with%20pyramid%20of%20score%20distribution%20features%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20J.%20Ni%2C%20B.%20Yang%2C%20X.%20Kassim%2C%20A.A.%20Temporal%20action%20localization%20with%20pyramid%20of%20score%20distribution%20features%202016"
        },
        {
            "id": "Yuan_et+al_2017_a",
            "entry": "Yuan Yuan, Xiaodan Liang, Xiaolong Wang, Dit-Yan Yeung, and Abhinav Gupta. Temporal dynamic graph LSTM for action-driven video object detection. In ICCV, pp. 1819\u20131828, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Yuan%20Liang%2C%20Xiaodan%20Wang%2C%20Xiaolong%20Yeung%2C%20Dit-Yan%20Temporal%20dynamic%20graph%20LSTM%20for%20action-driven%20video%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Yuan%20Liang%2C%20Xiaodan%20Wang%2C%20Xiaolong%20Yeung%2C%20Dit-Yan%20Temporal%20dynamic%20graph%20LSTM%20for%20action-driven%20video%20object%20detection%202017"
        },
        {
            "id": "Yuan_et+al_2017_b",
            "entry": "Z. Yuan, J. Stroud, T. Lu, and J. Deng. Temporal action localization by structured maximal sums. In CVPR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Z.%20Stroud%2C%20J.%20Lu%2C%20T.%20Deng%2C%20J.%20Temporal%20action%20localization%20by%20structured%20maximal%20sums%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Z.%20Stroud%2C%20J.%20Lu%2C%20T.%20Deng%2C%20J.%20Temporal%20action%20localization%20by%20structured%20maximal%20sums%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Dingwen Zhang, Deyu Meng, and Junwei Han. Co-saliency detection via a self-paced multipleinstance learning framework. IEEE transactions on pattern analysis and machine intelligence, 39 (5):865\u2013878, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Dingwen%20Meng%2C%20Deyu%20Han%2C%20Junwei%20Co-saliency%20detection%20via%20a%20self-paced%20multipleinstance%20learning%20framework.%20IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence%2C%2039%202017"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07294"
        },
        {
            "id": "Zhang_et+al_0000_b",
            "entry": "Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and Thomas Huang. Adversarial complementary learning for weakly supervised object localization. arXiv preprint arXiv:1804.06962, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06962"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin. Temporal action detection with structured segment networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Y.%20Xiong%2C%20Y.%20Wang%2C%20L.%20Wu%2C%20Z.%20Temporal%20action%20detection%20with%20structured%20segment%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Y.%20Xiong%2C%20Y.%20Wang%2C%20L.%20Wu%2C%20Z.%20Temporal%20action%20detection%20with%20structured%20segment%20networks%202017"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "B. Zhou, A. Khosla, Lapedriza. A., A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Khosla%2C%20A.%20A.%2C%20Lapedriza%20Oliva%2C%20A.%20Learning%20Deep%20Features%20for%20Discriminative%20Localization%202016"
        },
        {
            "id": "Zhou_et+al_2014_a",
            "entry": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6856"
        },
        {
            "id": "Zhou_et+al_2016_b",
            "entry": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Computer Vision and Pattern Recognition, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20Agata%20Oliva%2C%20Aude%20Learning%20deep%20features%20for%20discriminative%20localization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Lapedriza%2C%20Agata%20Oliva%2C%20Aude%20Learning%20deep%20features%20for%20discriminative%20localization%202016"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Soft proposal networks for weakly supervised object localization. arXiv preprint arXiv:1709.01829, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01829"
        },
        {
            "id": "1",
            "entry": "1. Thus, we obtain the set Video Action Analysis. Researchers have developed quite a few deep network models for video action analysis. Two-stream networks (Simonyan & Zisserman, 2014) and 3D convolutional neural networks (C3D) (Tran et al., 2015) are popular solutions to learn video representations and these techniques, including their variations, are extensively used for video action analysis. Recently, a combination of two-stream networks and 3D convolutions, referred to as I3D (Carreira & Zisserman, 2017), was proposed as a generic video representation learning method, and served as an effective backbone network in various video analysis tasks such as recognition (Wang et al., 2016), localization (Shou et al., 2016), and weakly-supervised learning (Wang et al., 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thus%20we%20obtain%20the%20set%20Video%20Action%20Analysis%20Researchers%20have%20developed%20quite%20a%20few%20deep%20network%20models%20for%20video%20action%20analysis%20Twostream%20networks%20Simonyan%20%20Zisserman%202014%20and%203D%20convolutional%20neural%20networks%20C3D%20Tran%20et%20al%202015%20are%20popular%20solutions%20to%20learn%20video%20representations%20and%20these%20techniques%20including%20their%20variations%20are%20extensively%20used%20for%20video%20action%20analysis%20Recently%20a%20combination%20of%20twostream%20networks%20and%203D%20convolutions%20referred%20to%20as%20I3D%20Carreira%20%20Zisserman%202017%20was%20proposed%20as%20a%20generic%20video%20representation%20learning%20method%20and%20served%20as%20an%20effective%20backbone%20network%20in%20various%20video%20analysis%20tasks%20such%20as%20recognition%20Wang%20et%20al%202016%20localization%20Shou%20et%20al%202016%20and%20weaklysupervised%20learning%20Wang%20et%20al%202017"
        },
        {
            "id": "Weakly-Supervised_2017_b",
            "entry": "Weakly-Supervised Temporal Action Localization. There are only a few approaches based on weakly-supervised learning that rely solely on video-level class labels to localize actions in the temporal domain. Wang et al. (Wang et al., 2017) proposed a UntrimmedNet framework, where two softmax functions are applied across class labels and proposals to perform action classification and detect important temporal segments, respectively. However, using the softmax function across proposals may not be effective for identifying multiple instances. Singh et al. (Singh & Lee, 2017)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=WeaklySupervised%20Temporal%20Action%20Localization%20There%20are%20only%20a%20few%20approaches%20based%20on%20weaklysupervised%20learning%20that%20rely%20solely%20on%20videolevel%20class%20labels%20to%20localize%20actions%20in%20the%20temporal%20domain%20Wang%20et%20al%20Wang%20et%20al%202017%20proposed%20a%20UntrimmedNet%20framework%20where%20two%20softmax%20functions%20are%20applied%20across%20class%20labels%20and%20proposals%20to%20perform%20action%20classification%20and%20detect%20important%20temporal%20segments%20respectively%20However%20using%20the%20softmax%20function%20across%20proposals%20may%20not%20be%20effective%20for%20identifying%20multiple%20instances%20Singh%20et%20al%20Singh%20%20Lee%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=WeaklySupervised%20Temporal%20Action%20Localization%20There%20are%20only%20a%20few%20approaches%20based%20on%20weaklysupervised%20learning%20that%20rely%20solely%20on%20videolevel%20class%20labels%20to%20localize%20actions%20in%20the%20temporal%20domain%20Wang%20et%20al%20Wang%20et%20al%202017%20proposed%20a%20UntrimmedNet%20framework%20where%20two%20softmax%20functions%20are%20applied%20across%20class%20labels%20and%20proposals%20to%20perform%20action%20classification%20and%20detect%20important%20temporal%20segments%20respectively%20However%20using%20the%20softmax%20function%20across%20proposals%20may%20not%20be%20effective%20for%20identifying%20multiple%20instances%20Singh%20et%20al%20Singh%20%20Lee%202017"
        },
        {
            "id": "designed_2018_a",
            "entry": "designed a Hide-and-Seek model to randomly hide some regions in a video during training and force the network to seek other relevant regions. However, the randomly hiding operation, as a data augmentation, cannot guarantee whether it is the action region or the background region that is hidden during training, especially when the dropout probabilities for all the regions are the same. Nguyen et al. (Nguyen et al., 2018) proposed a sparse temporal pooling network (STPN) to identify a sparse set of key segments associated with the actions through attention-based temporal pooling of video segments. However, the sparse constraint may force the network to focus on very few segments and lead to incomplete detection. In order to prevent the model from focusing only on the most salient regions, we are inspired to propose the MAAN model to explicitly take the expectation with respect to the average aggregated features of all the sampled subsets from the video.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=designed%20a%20HideandSeek%20model%20to%20randomly%20hide%20some%20regions%20in%20a%20video%20during%20training%20and%20force%20the%20network%20to%20seek%20other%20relevant%20regions%20However%20the%20randomly%20hiding%20operation%20as%20a%20data%20augmentation%20cannot%20guarantee%20whether%20it%20is%20the%20action%20region%20or%20the%20background%20region%20that%20is%20hidden%20during%20training%20especially%20when%20the%20dropout%20probabilities%20for%20all%20the%20regions%20are%20the%20same%20Nguyen%20et%20al%20Nguyen%20et%20al%202018%20proposed%20a%20sparse%20temporal%20pooling%20network%20STPN%20to%20identify%20a%20sparse%20set%20of%20key%20segments%20associated%20with%20the%20actions%20through%20attentionbased%20temporal%20pooling%20of%20video%20segments%20However%20the%20sparse%20constraint%20may%20force%20the%20network%20to%20focus%20on%20very%20few%20segments%20and%20lead%20to%20incomplete%20detection%20In%20order%20to%20prevent%20the%20model%20from%20focusing%20only%20on%20the%20most%20salient%20regions%20we%20are%20inspired%20to%20propose%20the%20MAAN%20model%20to%20explicitly%20take%20the%20expectation%20with%20respect%20to%20the%20average%20aggregated%20features%20of%20all%20the%20sampled%20subsets%20from%20the%20video",
            "oa_query": "https://api.scholarcy.com/oa_version?query=designed%20a%20HideandSeek%20model%20to%20randomly%20hide%20some%20regions%20in%20a%20video%20during%20training%20and%20force%20the%20network%20to%20seek%20other%20relevant%20regions%20However%20the%20randomly%20hiding%20operation%20as%20a%20data%20augmentation%20cannot%20guarantee%20whether%20it%20is%20the%20action%20region%20or%20the%20background%20region%20that%20is%20hidden%20during%20training%20especially%20when%20the%20dropout%20probabilities%20for%20all%20the%20regions%20are%20the%20same%20Nguyen%20et%20al%20Nguyen%20et%20al%202018%20proposed%20a%20sparse%20temporal%20pooling%20network%20STPN%20to%20identify%20a%20sparse%20set%20of%20key%20segments%20associated%20with%20the%20actions%20through%20attentionbased%20temporal%20pooling%20of%20video%20segments%20However%20the%20sparse%20constraint%20may%20force%20the%20network%20to%20focus%20on%20very%20few%20segments%20and%20lead%20to%20incomplete%20detection%20In%20order%20to%20prevent%20the%20model%20from%20focusing%20only%20on%20the%20most%20salient%20regions%20we%20are%20inspired%20to%20propose%20the%20MAAN%20model%20to%20explicitly%20take%20the%20expectation%20with%20respect%20to%20the%20average%20aggregated%20features%20of%20all%20the%20sampled%20subsets%20from%20the%20video"
        }
    ]
}
