{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION",
        "author": "Nuwan Ferdinand, Haider Al-Lawati, & Stark Draper Department of Electrical and Computer Engineering, University of Toronto {nuwan.ferdinand@,haider.al.lawati@mail.,stark.draper@}utoronto.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkzDIiA5YQ"
        },
        "abstract": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance."
    },
    "keywords": [
        {
            "term": "amazon ec2",
            "url": "https://en.wikipedia.org/wiki/amazon_ec2"
        },
        {
            "term": "Amazon Machine Image",
            "url": "https://en.wikipedia.org/wiki/Amazon_Machine_Image"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "statistics",
            "url": "https://en.wikipedia.org/wiki/statistics"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "topology",
            "url": "https://en.wikipedia.org/wiki/topology"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "Message Passing Interface",
            "url": "https://en.wikipedia.org/wiki/Message_Passing_Interface"
        }
    ],
    "abbreviations": {
        "AMB": "Anytime MiniBatch",
        "AMI": "Amazon Machine Image",
        "MPI": "Message Passing Interface",
        "NSF": "National Science Foundation"
    },
    "highlights": [
        "The advent of massive data sets has resulted in demand for solutions to optimization problems that are too large for a single processor to solve in a reasonable time",
        "We propose an approach that we term Anytime MiniBatch (AMB)",
        "We further show a\u221an upper bound that, in terms of the expected wall time needed to attain a specified regret, Anytime MiniBatch is O( n \u2212 1) faster than methods that use a fixed minibatch size under the assumption that the computation time follows an arbitrary distribution where n is the number of nodes",
        "In Sec. 4 we provide a convergence analysis that accounts for the variability in the amount of work completed by each node",
        "In Sec. 5, we presents a wall time analysis based on random local minibatch sizes",
        "We proposed a distributed optimization method called Anytime MiniBatch"
    ],
    "key_statements": [
        "The advent of massive data sets has resulted in demand for solutions to optimization problems that are too large for a single processor to solve in a reasonable time",
        "We propose an approach that we term Anytime MiniBatch (AMB)",
        "We further show a\u221an upper bound that, in terms of the expected wall time needed to attain a specified regret, Anytime MiniBatch is O( n \u2212 1) faster than methods that use a fixed minibatch size under the assumption that the computation time follows an arbitrary distribution where n is the number of nodes",
        "In Sec. 4 we provide a convergence analysis that accounts for the variability in the amount of work completed by each node",
        "In Sec. 5, we presents a wall time analysis based on random local minibatch sizes",
        "We proposed a distributed optimization method called Anytime MiniBatch"
    ],
    "summary": [
        "The advent of massive data sets has resulted in demand for solutions to optimization problems that are too large for a single processor to solve in a reasonable time.",
        "On the other hand, fixing the computation time means that each node process a different amount of data in each epoch.",
        "All workers get fixed communication time (Tc) to share their gradient information via averaging consensus on their dual variables, accounting for the variable number of data samples processed at each node.",
        "We further show a\u221an upper bound that, in terms of the expected wall time needed to attain a specified regret, AMB is O( n \u2212 1) faster than methods that use a fixed minibatch size under the assumption that the computation time follows an arbitrary distribution where n is the number of nodes.",
        "Consensus Phase: Between computational epochs each node is given a fixed amount of time, Tc, to communicate with neighboring nodes.",
        "As the performance is sensitive to the specific distribution of the processing times of the computing platform used, we first present a generic analysis in terms of the number of epochs processed and the size of the minibatches processed by each node in each epoch.",
        "We characterize the regret after \u03c4 epochs, averaging over the data distribution but keeping a fixed \u201csample path\u201d of per-node minibatch sizes bi(t).",
        "Let ai(t) denote the number of additional gradients that node i could have computed had there been no consensus phase.",
        "The impact of consensus error, which depends on the communication speed relative to the processing speed of each node, is summarized in the assumption of uniform accuracy on the distributed averaging mechanism.",
        "We consider an FMB method in which each node computes computes b/n gradients, where b is the size of the global minibatch in each epoch.",
        "Let Ti(t) denote the amount of time taken by node i to compute b/n gradients for FMB method.",
        "Lemma 6 is proved in App. G and it shows that the expected minibatch size of AMB is at least as big as FMB if we fix T = (1 + n/b)\u03bc.",
        "Let SA and SF be the total compute time across \u03c4 epochs of AMB and FMB, respectively, \u03c3\u221a",
        "The per-node fixed minibatch in FMB is b/n = 800 while the fixed compute time in AMB is T = 12 sec.",
        "A key property of our scheme is that we fix the computation time of each distributed node instead of minibatch size.",
        "We performed numerical experiments using Amazon EC2 and showed our scheme offers significant improvements over fixed minibatch schemes"
    ],
    "headline": "To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch",
    "reference_links": [
        {
            "id": "Arnold_1979_a",
            "entry": "Barry C. Arnold and Richard A. Groeneveld. Bounds on expectations of linear systematic statistics based on dependent samples. Ann. Statist., 7(1):220\u2013223, 01 1979. doi: 10.1214/aos/1176344567.",
            "crossref": "https://dx.doi.org/10.1214/aos/1176344567",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/aos/1176344567"
        },
        {
            "id": "Bertsimas_et+al_2006_a",
            "entry": "Dimitris Bertsimas, Karthik Natarajan, and Chung-Piaw Teo. Tight bounds on expected order statistics. Probab. Eng. Inf. Sci., 20(4):667\u2013686, October 2006. ISSN 0269-9648.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Natarajan%2C%20Karthik%20Teo%2C%20Chung-Piaw%20Tight%20bounds%20on%20expected%20order%20statistics%202006-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20Dimitris%20Natarajan%2C%20Karthik%20Teo%2C%20Chung-Piaw%20Tight%20bounds%20on%20expected%20order%20statistics%202006-10"
        },
        {
            "id": "Dean_et+al_2012_a",
            "entry": "Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems, NIPS\u201912, pp. 1223\u20131231, USA, 2012. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20S.%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20S.%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012"
        },
        {
            "id": "Dekel_et+al_2012_a",
            "entry": "Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(Jan):165\u2013202, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012"
        },
        {
            "id": "Duchi_et+al_2012_a",
            "entry": "John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3): 592\u2013606, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Dual%20averaging%20for%20distributed%20optimization%3A%20Convergence%20analysis%20and%20network%20scaling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Dual%20averaging%20for%20distributed%20optimization%3A%20Convergence%20analysis%20and%20network%20scaling%202012"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran. Speeding up distributed machine learning using codes. IEEE Trans. on Inf. Theory, 64(3):1514\u20131529, March 2018. ISSN 0018-9448. doi: 10.1109/TIT.2017.2736066.",
            "crossref": "https://dx.doi.org/10.1109/TIT.2017.2736066",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TIT.2017.2736066"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ji Liu, Stephen J. Wright, Christopher R\u00e9, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. J. Mach. Learn. Res., 16(1):285\u2013322, January 2015. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015-01"
        },
        {
            "id": "Nedic_2009_a",
            "entry": "Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "Nesterov_2009_a",
            "entry": "Yurii Nesterov. Primal-dual subgradient methods for convex problems. Math. Program., 120(1): 221\u2013259, April 2009. ISSN 0025-5610.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Primal-dual%20subgradient%20methods%20for%20convex%20problems%202009-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Primal-dual%20subgradient%20methods%20for%20convex%20problems%202009-04"
        },
        {
            "id": "Nokleby_2017_a",
            "entry": "M. Nokleby and W. U. Bajwa. Distributed mirror descent for stochastic learning over rate-limited networks. In 2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pp. 1\u20135, Dec 2017. doi: 10.1109/CAMSAP.2017.8313171.",
            "crossref": "https://dx.doi.org/10.1109/CAMSAP.2017.8313171",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CAMSAP.2017.8313171"
        },
        {
            "id": "Pan_et+al_2017_a",
            "entry": "Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal J\u00f3zefowicz. Revisiting distributed synchronous sgd. In Int. Conf. on Learning Representations Workshop Track\u201917, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Xinghao%20Chen%2C%20Jianmin%20Monga%2C%20Rajat%20Bengio%2C%20Samy%20Revisiting%20distributed%20synchronous%20sgd%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Xinghao%20Chen%2C%20Jianmin%20Monga%2C%20Rajat%20Bengio%2C%20Samy%20Revisiting%20distributed%20synchronous%20sgd%202017"
        },
        {
            "id": "Recht_et+al_2011_a",
            "entry": "Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pp. 693\u2013701. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "Dutta_et+al_2018_a",
            "entry": "S. Ghosh P. Dube S. Dutta, G. Joshi and P. Nagpurkar. Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In in Proc. of the Int. Conf. on Artificial Intelligence and Statistics (AISTATS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dutta%2C%20S.Ghosh%20P.Dube%20S.%20Joshi%2C%20G.%20Nagpurkar%2C%20P.%20Slow%20and%20stale%20gradients%20can%20win%20the%20race%3A%20Error-runtime%20trade-offs%20in%20distributed%20SGD%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dutta%2C%20S.Ghosh%20P.Dube%20S.%20Joshi%2C%20G.%20Nagpurkar%2C%20P.%20Slow%20and%20stale%20gradients%20can%20win%20the%20race%3A%20Error-runtime%20trade-offs%20in%20distributed%20SGD%202018"
        },
        {
            "id": "Shi_et+al_2015_a",
            "entry": "Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944\u2013966, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015"
        },
        {
            "id": "Tandon_et+al_2017_a",
            "entry": "R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis. Gradient Coding: Avoiding Stragglers in Distributed Learning. In in Proc. of the Int. Conf. on Int. Conf. on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tandon%2C%20R.%20Lei%2C%20Q.%20Dimakis%2C%20A.G.%20Karampatziakis%2C%20N.%20Gradient%20Coding%3A%20Avoiding%20Stragglers%20in%20Distributed%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tandon%2C%20R.%20Lei%2C%20Q.%20Dimakis%2C%20A.G.%20Karampatziakis%2C%20N.%20Gradient%20Coding%3A%20Avoiding%20Stragglers%20in%20Distributed%20Learning%202017"
        },
        {
            "id": "Tsianos_2016_a",
            "entry": "Konstantinos I Tsianos and Michael G Rabbat. Efficient distributed online prediction and stochastic optimization with approximate distributed averaging. IEEE Transactions on Signal and Information Processing over Networks, 2(4):489\u2013506, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsianos%2C%20Konstantinos%20I.%20Rabbat%2C%20Michael%20G.%20Efficient%20distributed%20online%20prediction%20and%20stochastic%20optimization%20with%20approximate%20distributed%20averaging%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsianos%2C%20Konstantinos%20I.%20Rabbat%2C%20Michael%20G.%20Efficient%20distributed%20online%20prediction%20and%20stochastic%20optimization%20with%20approximate%20distributed%20averaging%202016"
        },
        {
            "id": "Tsianos_et+al_2012_a",
            "entry": "Konstantinos I Tsianos, Sean Lawlor, and Michael G Rabbat. Push-sum distributed dual averaging for convex optimization. In Decision and Control (CDC), 2012 IEEE 51st Annual Conference on, pp. 5453\u20135458. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsianos%2C%20Konstantinos%20I.%20Lawlor%2C%20Sean%20Rabbat%2C%20Michael%20G.%20Push-sum%20distributed%20dual%20averaging%20for%20convex%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsianos%2C%20Konstantinos%20I.%20Lawlor%2C%20Sean%20Rabbat%2C%20Michael%20G.%20Push-sum%20distributed%20dual%20averaging%20for%20convex%20optimization%202012"
        },
        {
            "id": "Tsitsiklis_et+al_1986_a",
            "entry": "John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9): 803\u2013812, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20John%20Bertsekas%2C%20Dimitri%20Athans%2C%20Michael%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20John%20Bertsekas%2C%20Dimitri%20Athans%2C%20Michael%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986"
        },
        {
            "id": "Chenguang_2017_a",
            "entry": "Chenguang Xi and Usman A Khan. Dextra: A fast algorithm for optimization over directed graphs. IEEE Transactions on Automatic Control, 62(10):4980\u20134993, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chenguang%20Xi%20and%20Usman%20A%20Khan%20Dextra%20A%20fast%20algorithm%20for%20optimization%20over%20directed%20graphs%20IEEE%20Transactions%20on%20Automatic%20Control%20621049804993%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chenguang%20Xi%20and%20Usman%20A%20Khan%20Dextra%20A%20fast%20algorithm%20for%20optimization%20over%20directed%20graphs%20IEEE%20Transactions%20on%20Automatic%20Control%20621049804993%202017"
        },
        {
            "id": "Xiao_2010_a",
            "entry": "Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543\u20132596, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Q. Yu, M. Maddah-Ali, and S. Avestimehr. Polynomial codes: An optimal design for high-dimensional coded matrix multiplication. In Advances Neural Inf. Proc. Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Q.%20Maddah-Ali%2C%20M.%20Avestimehr%2C%20S.%20Polynomial%20codes%3A%20An%20optimal%20design%20for%20high-dimensional%20coded%20matrix%20multiplication%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Q.%20Maddah-Ali%2C%20M.%20Avestimehr%2C%20S.%20Polynomial%20codes%3A%20An%20optimal%20design%20for%20high-dimensional%20coded%20matrix%20multiplication%202017"
        },
        {
            "id": "Zinkevich_et+al_2010_a",
            "entry": "Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems 23, pp. 2595\u20132603. Curran Associates, Inc., 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010"
        },
        {
            "id": "From_2016_a",
            "entry": "From (Tsianos & Rabbat, 2016, Lemma 2), we have wi(t) \u2212 w(t)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabbat%2C%20From%20%28Tsianos%20%26%20Lemma%202%29%202016"
        }
    ]
}
