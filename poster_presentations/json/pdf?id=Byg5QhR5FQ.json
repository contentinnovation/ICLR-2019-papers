{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "author": "Kai Sheng Tai, Richard Socher, Christopher D. Manning",
        "date": 2015,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Byg5QhR5FQ",
            "doi": "10.3115/v1/p15-1150"
        },
        "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
        "abstract": "We present a simple neural model that given a formula and a property tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a topdown manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of our model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
    },
    "keywords": [
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "formula",
            "url": "https://en.wikipedia.org/wiki/formula"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        }
    ],
    "abbreviations": {
        "RNN": "recurrent neural networks",
        "GRU": "gated recurrent unit",
        "GRUs": "gated recurrent units",
        "LSTM": "long short-term memory"
    },
    "highlights": [
        "In real-world situations a very successful approach, popularized in Kahneman (2011), to problem solving is based on a clever combination of fast instinctive reasoning and slow logical reasoning",
        "The latter is exemplified by abstract logical formulae where only structural properties matter",
        "This paper presents, as far as we know, a novel neural representation of propositional formulae that makes it possible to test whether a given formula has a given property, e.g., whether the formula is always true or not",
        "We continue by propagating this knowledge to subformulae in a top-down manner, e.g., a binary connective \u25e6 takes a vector v \u2208 Rd and produces two new vectors v1 \u2208 Rd and v2 \u2208 Rd, \u25e6 : Rn \u2192 Rn \u00d7 Rn",
        "All formulae in the dataset are of the form A \u2192 B, see Section 2.1, propagating w, which is supposed to mean false, through \u2192, denoted c\u2192(w), produces two vectors v\u21921 and v\u21922 that correspond to A and B, respectively",
        "We have presented a novel top-down approach to represent formulae by neural networks and showed some preliminary experiments"
    ],
    "key_statements": [
        "In real-world situations a very successful approach, popularized in Kahneman (2011), to problem solving is based on a clever combination of fast instinctive reasoning and slow logical reasoning",
        "The latter is exemplified by abstract logical formulae where only structural properties matter",
        "This paper presents, as far as we know, a novel neural representation of propositional formulae that makes it possible to test whether a given formula has a given property, e.g., whether the formula is always true or not",
        "It follows from the former that p is true and q is false, but from the latter we obtain that q is true and p is false. Because such an assignment is impossible, we showed that the formula cannot be false and it is always true",
        "We start with a vector w \u2208 Rd, where d is a parameter of the model, that roughly represents a property of formulae we want to check.4",
        "We continue by propagating this knowledge to subformulae in a top-down manner, e.g., a binary connective \u25e6 takes a vector v \u2208 Rd and produces two new vectors v1 \u2208 Rd and v2 \u2208 Rd, \u25e6 : Rn \u2192 Rn \u00d7 Rn",
        "Note that our model ignores the names of atoms this cannot be exploited directly, because validation and test sets do not contain pairs of formulae that would result from pairs in the training set by renaming atoms",
        "It beats standard tree recursive models (TreeNet Encoders and TreeLSTM Encoders) and PossibleWorldNet, which performs similar to testing random truth-values, on the massive set and keeps pace on examples from textbooks",
        "We present our model as an alternative to bottom-up approaches, where a possible interpretation of such models is that they combine random assignments and produce the value of the whole formula",
        "All formulae in the dataset are of the form A \u2192 B, see Section 2.1, propagating w, which is supposed to mean false, through \u2192, denoted c\u2192(w), produces two vectors v\u21921 and v\u21922 that correspond to A and B, respectively",
        "We have presented a novel top-down approach to represent formulae by neural networks and showed some preliminary experiments"
    ],
    "summary": [
        "In real-world situations a very successful approach, popularized in Kahneman (2011), to problem solving is based on a clever combination of fast instinctive reasoning and slow logical reasoning.",
        "We start with a vector w \u2208 Rd, where d is a parameter of the model, that roughly represents a property of formulae we want to check.4 We continue by propagating this knowledge to subformulae in a top-down manner, e.g., a binary connective \u25e6 takes a vector v \u2208 Rd and produces two new vectors v1 \u2208 Rd and v2 \u2208 Rd, \u25e6 : Rn \u2192 Rn \u00d7 Rn. Since all the vectors have the same length d, it is possible to connect individual building blocks together and produce a recursive neural network.",
        "Note that our model ignores the names of atoms this cannot be exploited directly, because validation and test sets do not contain pairs of formulae that would result from pairs in the training set by renaming atoms.",
        "Our various experiments suggest that whether w is chosen randomly or learned makes little to no difference.8 and more importantly, connectives ci composed of a single linear layer seem to perform almost and sometimes even better, to more complicated versions containing non-linearities as in our standard TopDownNet. In RNN-Var and RNN-All we use gated recurrent units (GRUs) which in our experiments perform to long short-term memory (LSTM) RNNs. to results obtained in <a class=\"ref-link\" id=\"cEvans_et+al_2018_a\" href=\"#rEvans_et+al_2018_a\">Evans et al (2018</a>), we have seen no real advantage of using their bidirectional variants for our model and in many cases they produce worse results.",
        "It beats standard tree recursive models (TreeNet Encoders and TreeLSTM Encoders) and PossibleWorldNet, which performs similar to testing random truth-values, on the massive set and keeps pace on examples from textbooks.",
        "We present our model as an alternative to bottom-up approaches, where a possible interpretation of such models is that they combine random assignments and produce the value of the whole formula.",
        "All formulae in the dataset are of the form A \u2192 B, see Section 2.1, propagating w, which is supposed to mean false, through \u2192, denoted c\u2192(w), produces two vectors v\u21921 and v\u21922 that correspond to A and B, respectively.",
        "It should be noted that we use our model for deciding whether a formula is a tautology, we can try to enforce other properties by a similar process, e.g., we can test whether a formula is satisfiable, i.e., is true under some assignment of truth-values.",
        "The model deals only with the structure of formulae and ignores completely for example names of individual atoms, only whether they are the same or distinct matters"
    ],
    "headline": "We present a simple neural model that given a formula and a property tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true",
    "reference_links": [
        {
            "id": "Allamanis_et+al_2017_a",
            "entry": "Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles A. Sutton. Learning continuous semantic representations of symbolic expressions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 80\u201388, 2017. URL http://proceedings.mlr.press/v70/allamanis17a.html.",
            "url": "http://proceedings.mlr.press/v70/allamanis17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allamanis%2C%20Miltiadis%20Chanthirasegaran%2C%20Pankajan%20Kohli%2C%20Pushmeet%20Sutton%2C%20Charles%20A.%20Learning%20continuous%20semantic%20representations%20of%20symbolic%20expressions%202017-08-06"
        },
        {
            "id": "Evans_et+al_2018_a",
            "entry": "Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SkZxCk-0Z.",
            "url": "https://openreview.net/forum?id=SkZxCk-0Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evans%2C%20Richard%20Saxton%2C%20David%20Amos%2C%20David%20Kohli%2C%20Pushmeet%20Can%20neural%20networks%20understand%20logical%20entailment%3F%202018"
        },
        {
            "id": "Thinking_et+al_2011_a",
            "entry": "Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011. ISBN 9781429969352.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%20Kahneman%20Thinking%20Fast%20and%20Slow%20Farrar%20Straus%20and%20Giroux%202011%20ISBN%209781429969352"
        },
        {
            "id": "John_2001_a",
            "entry": "John Alan Robinson and Andrei Voronkov (eds.). Handbook of Automated Reasoning (in 2 volumes). Elsevier and MIT Press, 2001. ISBN 0-444-50813-9.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%2C%20John%20Alan%20Robinson%20Andrei%20Voronkov%20%28eds.%29.%20Handbook%20of%20Automated%20Reasoning%20%28in%202%20volumes%29%202001"
        },
        {
            "id": "Rocktaschel_et+al_2016_a",
            "entry": "Tim Rocktaschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom. Reasoning about entailment with neural attention. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocktaschel%2C%20Tim%20Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Kocisky%2C%20Tomas%20Reasoning%20about%20entailment%20with%20neural%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rocktaschel%2C%20Tim%20Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Kocisky%2C%20Tomas%20Reasoning%20about%20entailment%20with%20neural%20attention%202016"
        },
        {
            "id": "Socher_et+al_2012_a",
            "entry": "Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju Island, Korea, pp. 1201\u20131211, 2012. URL http://www.aclweb.org/anthology/D12-1110.",
            "url": "http://www.aclweb.org/anthology/D12-1110",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Huval%2C%20Brody%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Y.%20Semantic%20compositionality%20through%20recursive%20matrix-vector%20spaces%202012-07-12"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured Long Short-Term Memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1556\u20131566. Association for Computational Linguistics, 2015. doi: 10.3115/v1/P15-1150. URL http://www.aclweb.org/anthology/P15-1150.",
            "crossref": "https://dx.doi.org/10.3115/v1/P15-1150",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.3115/v1/P15-1150"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2786\u20132796. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6871-premise-selection-for-theorem-proving-by-deep-graph-embedding.pdf.",
            "url": "http://papers.nips.cc/paper/6871-premise-selection-for-theorem-proving-by-deep-graph-embedding.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Mingzhe%20Tang%2C%20Yihe%20Wang%2C%20Jian%20Deng%2C%20Jia%20Premise%20selection%20for%20theorem%20proving%20by%20deep%20graph%20embedding%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Shuohang Wang and Jing Jiang. Learning natural language inference with LSTM. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1442\u20131451. Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1170. URL http://www.aclweb.org/anthology/N16-1170.",
            "crossref": "https://dx.doi.org/10.18653/v1/N16-1170",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.18653/v1/N16-1170"
        },
        {
            "id": "Zaremba_et+al_2014_a",
            "entry": "Wojciech Zaremba, Karol Kurach, and Rob Fergus. Learning to discover efficient mathematical identities. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1278\u20131286, 2014. URL http://papers.nips.cc/paper/5350-learning-to-discover-efficient-mathematical-identities.",
            "url": "http://papers.nips.cc/paper/5350-learning-to-discover-efficient-mathematical-identities",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaremba%2C%20Wojciech%20Kurach%2C%20Karol%20Fergus%2C%20Rob%20Learning%20to%20discover%20efficient%20mathematical%20identities%202014-12-08"
        }
    ]
}
