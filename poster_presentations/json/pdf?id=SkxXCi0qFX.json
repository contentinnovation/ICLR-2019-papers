{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SOPHIE velocimetry ofKeplertransit candidates",
        "author": "A. Santerne, C. Moutou, M. Tsantaki, F. Bouchy, G. H\u00e9brard, V. Adibekyan, J.-M. Almenara, L. Amard, S. C. C. Barros, I. Boisse, A. S. Bonomo, G. Bruno, B. Courcol, M. Deleuil, O. Demangeon, R. F. D\u00edaz, T. Guillot, M. Havel, G. Montagnier, A. S. Rajpurohit, J. Rey, N. C. Santos",
        "date": 2015,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SkxXCi0qFX",
            "doi": "10.1051/0004-6361/201527329"
        },
        "journal": "Astronomy & Astrophysics",
        "volume": "587",
        "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during metatraining as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.",
        "pages": "A64"
    },
    "keywords": [
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "Fisher information matrix",
            "url": "https://en.wikipedia.org/wiki/Fisher_information_matrix"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "abbreviations": {
        "Meta-RL": "Meta-reinforcement learning",
        "MDP": "Markov decision process",
        "RL": "reinforcement learning",
        "LVC": "low variance curvature",
        "ProMP": "proximal meta-policy search",
        "VPG": "vanilla policy gradient",
        "FIM": "Fisher information matrix"
    },
    "highlights": [
        "A remarkable trait of human intelligence is the ability to adapt to new situations in the face of limited experience",
        "Building on the previous sections, we develop a novel meta-policy search method based on the low variance curvature objective which aims to solve the following optimization problem: max \u03b8",
        "We propose Proximal Meta-Policy Search (ProMP) which incorporates both the benefits of proximal policy optimization and the low variance curvature objective In order to comply with requirement",
        "In this paper we propose a novel Meta-reinforcement learning algorithm, proximal meta-policy search (ProMP), which fully optimizes for the pre-update sampling distribution leading to effective task identification",
        "Our method is the result of a theoretical analysis of gradient-based Meta-reinforcement learning formulations, based on which we develop the low variance curvature (LVC) surrogate objective that produces low variance meta-policy gradient estimates",
        "Experimental results demonstrate that our approach surpasses previous meta-reinforcement learning approaches in a diverse set of continuous control tasks"
    ],
    "key_statements": [
        "A remarkable trait of human intelligence is the ability to adapt to new situations in the face of limited experience",
        "We provide the first formal in-depth analysis of credit assignment w.r.t. preadaptation sampling distribution in Meta-reinforcement learning",
        "We develop a novel Meta-reinforcement learning algorithm",
        "We show that the recent formulation introduced by <a class=\"ref-link\" id=\"cAl-Shedivat_et+al_2018_a\" href=\"#rAl-Shedivat_et+al_2018_a\">Al-Shedivat et al (2018</a>) and <a class=\"ref-link\" id=\"cStadie_et+al_2018_a\" href=\"#rStadie_et+al_2018_a\">Stadie et al (2018</a>) leads to poor credit assignment, while the MAML formulation (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>) potentially yields superior meta-policy updates",
        "We propose the low variance curvature (LVC) surrogate objective which yields gradient estimates with a favorable bias-variance trade-off",
        "To facilitate a sample efficient meta-learning, we introduce the low variance curvature (LVC) estimator: J low variance curvature(\u03c4 ) =",
        "Building on the previous sections, we develop a novel meta-policy search method based on the low variance curvature objective which aims to solve the following optimization problem: max \u03b8",
        "We propose Proximal Meta-Policy Search (ProMP) which incorporates both the benefits of proximal policy optimization and the low variance curvature objective In order to comply with requirement",
        "To answer the posed questions, we evaluate our approach on six continuous control Meta-reinforcement learning benchmark environments based on OpenAI Gym and the Mujoco simulator (<a class=\"ref-link\" id=\"cBrockman_et+al_2016_a\" href=\"#rBrockman_et+al_2016_a\">Brockman et al, 2016</a>; Todorov et al, 2012)",
        "The trajectory level dependencies inherent in the DiCE estimator leads to a meta-gradient standard deviation that is on average 60% higher when compared to low variance curvature",
        "Meta-policy search based on the low variance curvature estimator leads to substantially better sample-efficiency and asymptotic performance",
        "In this paper we propose a novel Meta-reinforcement learning algorithm, proximal meta-policy search (ProMP), which fully optimizes for the pre-update sampling distribution leading to effective task identification",
        "Our method is the result of a theoretical analysis of gradient-based Meta-reinforcement learning formulations, based on which we develop the low variance curvature (LVC) surrogate objective that produces low variance meta-policy gradient estimates",
        "Experimental results demonstrate that our approach surpasses previous meta-reinforcement learning approaches in a diverse set of continuous control tasks"
    ],
    "summary": [
        "A remarkable trait of human intelligence is the ability to adapt to new situations in the face of limited experience.",
        "Its objective is to learn an initialization such that after one or few steps of policy gradients the agent attains full performance on a new task.",
        "As we show, previous gradient-based Meta-RL methods either neglect or perform poor credit assignment w.r.t. the pre-update sampling distribution.",
        "We motivate and introduce the low variance curvature estimator (LVC): an improved estimator for the hessian of the RL-objective which promotes better meta-policy gradient updates.",
        "In section 7.2 we empirically show that the high variance estimates of the DiCE objective lead to noisy meta-policy gradients and poor learning performance.",
        "DiCE] The experiments in section 7.2 underpin the theoretical findings, showing that the low variance hessian estimates obtained through JLVC improve the sample-efficiency of meta-learning by a significant margin when compared to JDiCE.",
        "Building on the previous sections, we develop a novel meta-policy search method based on the low variance curvature objective which aims to solve the following optimization problem: max \u03b8",
        "In order to safely perform multiple meta-gradient steps based on the same sampled data from a recent policy \u03c0\u03b8o , we need to 1) account for changes in the pre-update action distribution \u03c0\u03b8, and 2) bound changes in the pre-update state visitation distribution (<a class=\"ref-link\" id=\"cKakade_2002_a\" href=\"#rKakade_2002_a\">Kakade & Langford, 2002</a>).",
        "We compare the LVC, DiCE, MAML and E-MAML estimators while optimizing meta-learning objective with vanilla policy gradient (VPG) ascent.",
        "The trajectory level dependencies inherent in the DiCE estimator leads to a meta-gradient standard deviation that is on average 60% higher when compared to LVC.",
        "Meta-policy search based on the LVC estimator leads to substantially better sample-efficiency and asymptotic performance.",
        "Since the MAML implementation does not assign credit to the pre-update sampling trajectory, it is unable to learn a sound exploration strategy for task identification and fails to accomplish the task.",
        "E-MAML, which corresponds to formulation II, learns to explore in long but random paths: because it can only assign credit to batches of pre-update trajectories, there is no notion of which actions in particular facilitate good task adaptation.",
        "In this paper we propose a novel Meta-RL algorithm, proximal meta-policy search (ProMP), which fully optimizes for the pre-update sampling distribution leading to effective task identification.",
        "Our method is the result of a theoretical analysis of gradient-based Meta-RL formulations, based on which we develop the low variance curvature (LVC) surrogate objective that produces low variance meta-policy gradient estimates.",
        "We underpin our theoretical contributions with illustrative examples which further justify the soundness and effectiveness of our method"
    ],
    "headline": "This paper provides a theoretical analysis of credit assignment in gradient-based Meta-reinforcement learning",
    "reference_links": [
        {
            "id": "Achiam_et+al_2017_a",
            "entry": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained Policy Optimization. Technical report, 2017. URL https://arxiv.org/pdf/1705.10528.pdf.",
            "url": "https://arxiv.org/pdf/1705.10528.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1705.10528"
        },
        {
            "id": "Al-Shedivat_et+al_2018_a",
            "entry": "Maruan Al-Shedivat, Trapit Bansal, Umass Amherst, Yura Burda, Openai Ilya, Sutskever Openai, Igor Mordatch Openai, and Pieter Abbeel. Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments. In ICLR, 2018. URL https://goo.gl/tboqaN.",
            "url": "https://goo.gl/tboqaN",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Shedivat%2C%20Maruan%20Bansal%2C%20Trapit%20Amherst%2C%20Umass%20Burda%2C%20Yura%20Continuous%20Adaptation%20via%20Meta-Learning%20in%20Nonstationary%20and%20Competitive%20Environments%202018"
        },
        {
            "id": "Alet_et+al_2018_a",
            "entry": "Ferran Alet, Toms Lozano-Perez, and Leslie P. Kaelbling. Modular meta-learning. Technical report, 6 2018. URL http://arxiv.org/abs/1806.10166.",
            "url": "http://arxiv.org/abs/1806.10166",
            "arxiv_url": "https://arxiv.org/pdf/1806.10166"
        },
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gmez Colmenarejo, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. Technical report, 2016. URL https://arxiv.org/pdf/1606.04474.pdf.",
            "url": "https://arxiv.org/pdf/1606.04474.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1606.04474"
        },
        {
            "id": "Baxter_2001_a",
            "entry": "Jonathan Baxter and Peter L Bartlett. Infinite-Horizon Policy-Gradient Estimation. Technical report, 2001. URL https://arxiv.org/pdf/1106.0665.pdf.",
            "url": "https://arxiv.org/pdf/1106.0665.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1106.0665"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. Technical report, 6 2016. URL http://arxiv.org/abs/1606.01540.",
            "url": "http://arxiv.org/abs/1606.01540",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Yutian Chen, Matthew W Hoffman, Sergio Gmez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick, and Nando De Freitas. Learning to Learn without Gradient Descent by Gradient Descent. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yutian%20Hoffman%2C%20Matthew%20W.%20Colmenarejo%2C%20Sergio%20Gmez%20Denil%2C%20Misha%20Learning%20to%20Learn%20without%20Gradient%20Descent%20by%20Gradient%20Descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yutian%20Hoffman%2C%20Matthew%20W.%20Colmenarejo%2C%20Sergio%20Gmez%20Denil%2C%20Misha%20Learning%20to%20Learn%20without%20Gradient%20Descent%20by%20Gradient%20Descent%202017"
        },
        {
            "id": "Clavera_et+al_2018_a",
            "entry": "Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization. In CoRL, 2018. URL http://arxiv.org/abs/1809.05214.",
            "url": "http://arxiv.org/abs/1809.05214",
            "arxiv_url": "https://arxiv.org/pdf/1809.05214"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL$\u02c62$: Fast Reinforcement Learning via Slow Reinforcement Learning. CoRR, abs/1611.0:1\u201314, 2016. ISSN 0004-6361. doi: 10.1051/0004-6361/201527329. URL http://arxiv.org/abs/1611.02779.",
            "crossref": "https://dx.doi.org/10.1051/0004-6361/201527329",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017"
        },
        {
            "id": "Foerster_et+al_2018_a",
            "entry": "Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktaschel, Eric P Xing, and Shimon Whiteson. DiCE: The Infinitely Differentiable Monte Carlo Estimator. In ICML, 2018. URL https://goo.gl/xkkGxN.",
            "url": "https://goo.gl/xkkGxN",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jakob%20Foerster%20Gregory%20Farquhar%20Maruan%20AlShedivat%20Tim%20Rocktaschel%20Eric%20P%20Xing%20and%20Shimon%20Whiteson%20DiCE%20The%20Infinitely%20Differentiable%20Monte%20Carlo%20Estimator%20In%20ICML%202018%20URL%20httpsgooglxkkGxN"
        },
        {
            "id": "Frans_et+al_2018_a",
            "entry": "Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta Learning Shared Hierarchies. In ICLR, 10 2018. URL http://arxiv.org/abs/1710.09767.",
            "url": "http://arxiv.org/abs/1710.09767",
            "arxiv_url": "https://arxiv.org/pdf/1710.09767"
        },
        {
            "id": "Furmston_et+al_2016_a",
            "entry": "Thomas Furmston, Guy Lever, David Barber, and Joelle Pineau. Approximate Newton Methods for Policy Search in Markov Decision Processes. Technical report, 2016. URL http://jmlr.org/papers/volume17/15-414/15-414.pdf.",
            "url": "http://jmlr.org/papers/volume17/15-414/15-414.pdf"
        },
        {
            "id": "Gupta_et+al_2018_a",
            "entry": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised MetaLearning for Reinforcement Learning. In ICML, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Abhishek%20Eysenbach%2C%20Benjamin%20Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Unsupervised%20MetaLearning%20for%20Reinforcement%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Abhishek%20Eysenbach%2C%20Benjamin%20Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Unsupervised%20MetaLearning%20for%20Reinforcement%20Learning%202018"
        },
        {
            "id": "Gupta_et+al_2018_b",
            "entry": "Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. MetaReinforcement Learning of Structured Exploration Strategies. In ICML, 2018b. URL https://arxiv.org/pdf/1802.07245.pdf.",
            "url": "https://arxiv.org/pdf/1802.07245.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1802.07245"
        },
        {
            "id": "Sepp_2001_a",
            "entry": "Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning To Learn Using Gradient Descent. In ICANN, pp. 87\u201394, 2001. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.323.",
            "url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.323",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sepp%20Hochreiter%2C%20A.Steven%20Younger%20Conwell%2C%20Peter%20R.%20Learning%20To%20Learn%20Using%20Gradient%20Descent%202001"
        },
        {
            "id": "Husken_2000_a",
            "entry": "Michael Husken and Christian Goerick. Fast learning for problem classes using knowledge based network initialization. In IJCNN. IEEE Computer Society Press, 2000. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.9720&rep=rep1&type=pdf.",
            "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.9720&rep=rep1&type=pdf"
        },
        {
            "id": "Kakade_2002_a",
            "entry": "Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement Learning. In ICML, 2002. URL https://people.eecs.berkeley.edu/\u0303pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf.",
            "url": "https://people.eecs.berkeley.edu/\u0303pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20Optimal%20Approximate%20Reinforcement%20Learning%202002"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to Generalize: MetaLearning for Domain Generalization. In AAAI, 2017. URL www.aaai.org.",
            "url": "http://www.aaai.org",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Da%20Yang%2C%20Yongxin%20Song%2C%20Yi-Zhe%20Hospedales%2C%20Timothy%20M.%20Learning%20to%20Generalize%3A%20MetaLearning%20for%20Domain%20Generalization%202017"
        },
        {
            "id": "Miconi_et+al_2018_a",
            "entry": "Thomas Miconi, Jeff Clune, and Kenneth O. Stanley. Differentiable plasticity: training plastic neural networks with backpropagation. In ICML, 4 2018. URL https://arxiv.org/abs/1804.02464.",
            "url": "https://arxiv.org/abs/1804.02464",
            "arxiv_url": "https://arxiv.org/pdf/1804.02464"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A Simple Neural Attentive MetaLearner. In ICLR, 7 2018. URL http://arxiv.org/abs/1707.03141.",
            "url": "http://arxiv.org/abs/1707.03141",
            "arxiv_url": "https://arxiv.org/pdf/1707.03141"
        },
        {
            "id": "Nichol_et+al_2018_a",
            "entry": "Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. Technical report, 2018. URL http://arxiv.org/abs/1803.02999.",
            "url": "http://arxiv.org/abs/1803.02999",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Peters_2006_a",
            "entry": "Jan Peters and Stefan Schaal. Policy Gradient Methods for Robotics. In 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2219\u20132225. IEEE, 10 2006. ISBN 1-4244-0258-1. doi: 10.1109/IROS.2006.282564. URL http://ieeexplore.ieee.org/document/4058714/.",
            "crossref": "https://dx.doi.org/10.1109/IROS.2006.282564",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IROS.2006.282564"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a Model for Few-Shot Learning. In ICLR, 11 2017. URL https://openreview.net/forum?id=rJY0-Kcll.",
            "url": "https://openreview.net/forum?id=rJY0-Kcll",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20Model%20for%20Few-Shot%20Learning%202017"
        },
        {
            "id": "Saemundsson_et+al_2018_a",
            "entry": "Steindr Saemundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta Reinforcement Learning with Latent Variable Gaussian Processes. In UAI, 2018. URL https://arxiv.org/pdf/1803.07551.pdf.",
            "url": "https://arxiv.org/pdf/1803.07551.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1803.07551"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap, and Google Deepmind. Meta-Learning with Memory-Augmented Neural Networks. In ICML, 2016. URL http://proceedings.mlr.press/v48/santoro16.pdf.",
            "url": "http://proceedings.mlr.press/v48/santoro16.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Meta-Learning%20with%20Memory-Augmented%20Neural%20Networks%202016"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "Juergen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook. PhD thesis, Technische Universitaet Munchen, 1987. URL http://people.idsia.ch/\u0303juergen/diploma.html.",
            "url": "http://people.idsia.ch/\u0303juergen/diploma.html"
        },
        {
            "id": "Schmidhuber_et+al_1997_a",
            "entry": "Jrgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Shifting Inductive Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement. Machine Learning, 28 (1):105\u2013130, 1997. ISSN 08856125. doi: 10.1023/A:1007383707642. URL http://link.springer.com/10.1023/A:1007383707642.",
            "crossref": "https://dx.doi.org/10.1023/A:1007383707642"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient Estimation Using Stochastic Computation Graphs. In NIPS, 2015a. URL https://arxiv.org/pdf/1506.05254.pdf.",
            "url": "https://arxiv.org/pdf/1506.05254.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1506.05254"
        },
        {
            "id": "Schulman_et+al_2015_b",
            "entry": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust Region Policy Optimization. ICML, 2015b. ISSN 2158-3226. doi: 10.1063/1.4927398. URL http://arxiv.org/abs/1502.05477.",
            "crossref": "https://dx.doi.org/10.1063/1.4927398",
            "arxiv_url": "https://arxiv.org/pdf/1502.05477"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov Openai. Proximal Policy Optimization Algorithms. CoRR, 2017. URL https://arxiv.org/pdf/1707.06347.pdf.",
            "url": "https://arxiv.org/pdf/1707.06347.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Stadie_et+al_2018_a",
            "entry": "Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some Considerations on Learning to Explore via Meta-Reinforcement Learning. Technical report, 2018. URL https://arxiv.org/pdf/1803.01118.pdf.",
            "url": "https://arxiv.org/pdf/1803.01118.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1803.01118"
        },
        {
            "id": "Sung_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to Learn: Meta-Critic Networks for Sample Efficient Learning. Technical report, 6 2017. URL http://arxiv.org/abs/1706.09529. Richard S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In NIPS, 2000. ISBN 0-262-19450-3.doi:10.1.1.37.9714. Sebastian Thrun and Lorien Pratt. Learning to learn.1998. ISBN 0792380479. URL https://dl.acm.org/citation.cfm?id=296639. Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In IROS, pp.5026\u20135033. IEEE, 10 2012. ISBN 978-1-4673-1736-8.doi:10.1109/IROS.2012.6386109. URL http://ieeexplore.ieee.org/document/6386109/. Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-Gradient Reinforcement Learning. Technical report, 5 2018. URL http://arxiv.org/abs/1805.09801.",
            "crossref": "https://dx.doi.org/10.1109/IROS.2012.6386109",
            "arxiv_url": "https://arxiv.org/pdf/1706.09529"
        },
        {
            "id": "The_2017_a",
            "entry": "The first meta-learning formulation, known as MAML (Finn et al., 2017), views the inner update rule U (\u03b8, T ) as a mapping from the pre-update parameter \u03b8 and the task T to an adapted policy parameter \u03b8. The update function can be viewed as stand-alone procedure that encapsulates sampling from the task-specific trajectory distribution PT (\u03c4 |\u03c0\u03b8) and updating the policy parameters. Building on this concept, the meta-objective can be written as",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20first%20metalearning%20formulation%20known%20as%20MAML%20Finn%20et%20al%202017%20views%20the%20inner%20update%20rule%20U%20%CE%B8%20T%20%20as%20a%20mapping%20from%20the%20preupdate%20parameter%20%CE%B8%20and%20the%20task%20T%20to%20an%20adapted%20policy%20parameter%20%CE%B8%20The%20update%20function%20can%20be%20viewed%20as%20standalone%20procedure%20that%20encapsulates%20sampling%20from%20the%20taskspecific%20trajectory%20distribution%20PT%20%CF%84%20%CF%80%CE%B8%20and%20updating%20the%20policy%20parameters%20Building%20on%20this%20concept%20the%20metaobjective%20can%20be%20written%20as"
        },
        {
            "id": "The_2018_a",
            "entry": "The second meta-reinforcement learning formulation views the the inner update \u03b8 = U (\u03b8, \u03c4 1:N ) as a deterministic function of the pre-update policy parameters \u03b8 and N trajectories \u03c4 1:N \u223c PT (\u03c4 1:N |\u03b8) sampled from the pre-update trajectory distribution. This formulation was introduced in Al-Shedivat et al. (2018) and further discussed with respect to its exploration properties in Stadie et al. (2018).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=The%20second%20metareinforcement%20learning%20formulation%20views%20the%20the%20inner%20update%20%CE%B8%20%20U%20%CE%B8%20%CF%84%201N%20%20as%20a%20deterministic%20function%20of%20the%20preupdate%20policy%20parameters%20%CE%B8%20and%20N%20trajectories%20%CF%84%201N%20%20PT%20%CF%84%201N%20%CE%B8%20sampled%20from%20the%20preupdate%20trajectory%20distribution%20This%20formulation%20was%20introduced%20in%20AlShedivat%20et%20al%202018%20and%20further%20discussed%20with%20respect%20to%20its%20exploration%20properties%20in%20Stadie%20et%20al%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=The%20second%20metareinforcement%20learning%20formulation%20views%20the%20the%20inner%20update%20%CE%B8%20%20U%20%CE%B8%20%CF%84%201N%20%20as%20a%20deterministic%20function%20of%20the%20preupdate%20policy%20parameters%20%CE%B8%20and%20N%20trajectories%20%CF%84%201N%20%20PT%20%CF%84%201N%20%CE%B8%20sampled%20from%20the%20preupdate%20trajectory%20distribution%20This%20formulation%20was%20introduced%20in%20AlShedivat%20et%20al%202018%20and%20further%20discussed%20with%20respect%20to%20its%20exploration%20properties%20in%20Stadie%20et%20al%202018"
        },
        {
            "id": "wherein_2018_b",
            "entry": "wherein \u03b4 denotes the angle between the the inner and outer pre-update and post-update policy gradients. Hence, \u2207\u03b8JpIost steers the pre-update policy towards not only towards larger post-updates returns but also towards larger adaptation steps \u03b1\u2207\u03b8Jinner, and better alignment of pre- and postupdate policy gradients. This directly optimizes for maximal improvement / adaptation for the respective task. See Li et al. (2017); Nichol et al. (2018) for a comparable analysis in case of domain generalization and supervised meta-learning. Also note that (38) allows formulation I to perform credit assignment on the trajectory level whereas formulation II can only assign credit to entire batches of N pre-update trajectories \u03c4 1:N.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=wherein%20%CE%B4%20denotes%20the%20angle%20between%20the%20the%20inner%20and%20outer%20preupdate%20and%20postupdate%20policy%20gradients%20Hence%20%CE%B8JpIost%20steers%20the%20preupdate%20policy%20towards%20not%20only%20towards%20larger%20postupdates%20returns%20but%20also%20towards%20larger%20adaptation%20steps%20%CE%B1%CE%B8Jinner%20and%20better%20alignment%20of%20pre%20and%20postupdate%20policy%20gradients%20This%20directly%20optimizes%20for%20maximal%20improvement%20%20adaptation%20for%20the%20respective%20task%20See%20Li%20et%20al%202017%20Nichol%20et%20al%202018%20for%20a%20comparable%20analysis%20in%20case%20of%20domain%20generalization%20and%20supervised%20metalearning%20Also%20note%20that%2038%20allows%20formulation%20I%20to%20perform%20credit%20assignment%20on%20the%20trajectory%20level%20whereas%20formulation%20II%20can%20only%20assign%20credit%20to%20entire%20batches%20of%20N%20preupdate%20trajectories%20%CF%84%201N",
            "oa_query": "https://api.scholarcy.com/oa_version?query=wherein%20%CE%B4%20denotes%20the%20angle%20between%20the%20the%20inner%20and%20outer%20preupdate%20and%20postupdate%20policy%20gradients%20Hence%20%CE%B8JpIost%20steers%20the%20preupdate%20policy%20towards%20not%20only%20towards%20larger%20postupdates%20returns%20but%20also%20towards%20larger%20adaptation%20steps%20%CE%B1%CE%B8Jinner%20and%20better%20alignment%20of%20pre%20and%20postupdate%20policy%20gradients%20This%20directly%20optimizes%20for%20maximal%20improvement%20%20adaptation%20for%20the%20respective%20task%20See%20Li%20et%20al%202017%20Nichol%20et%20al%202018%20for%20a%20comparable%20analysis%20in%20case%20of%20domain%20generalization%20and%20supervised%20metalearning%20Also%20note%20that%2038%20allows%20formulation%20I%20to%20perform%20credit%20assignment%20on%20the%20trajectory%20level%20whereas%20formulation%20II%20can%20only%20assign%20credit%20to%20entire%20batches%20of%20N%20preupdate%20trajectories%20%CF%84%201N"
        },
        {
            "id": "Since_2000_a",
            "entry": "Since the expectation over the trajectory distribution PT (\u03c4 |\u03b8) is in general intractable, the score function trick is typically used to used to produce a Monte Carlo estimate of the policy gradients. Although the gradient estimate can be directly defined, when using a automatic-differentiation toolbox it is usually more convenient to use an objective function whose gradients correspond to the policy gradient estimate. Due to the Policy Gradient Theorem (PGT) Sutton et al. (2000) such a \u201csurrogate\u201d objective can be written as: JPGT = 1 K",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Since%20the%20expectation%20over%20the%20trajectory%20distribution%20PT%20%CF%84%20%CE%B8%20is%20in%20general%20intractable%20the%20score%20function%20trick%20is%20typically%20used%20to%20used%20to%20produce%20a%20Monte%20Carlo%20estimate%20of%20the%20policy%20gradients%20Although%20the%20gradient%20estimate%20can%20be%20directly%20defined%20when%20using%20a%20automaticdifferentiation%20toolbox%20it%20is%20usually%20more%20convenient%20to%20use%20an%20objective%20function%20whose%20gradients%20correspond%20to%20the%20policy%20gradient%20estimate%20Due%20to%20the%20Policy%20Gradient%20Theorem%20PGT%20Sutton%20et%20al%202000%20such%20a%20surrogate%20objective%20can%20be%20written%20as%20JPGT%20%201%20K",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Since%20the%20expectation%20over%20the%20trajectory%20distribution%20PT%20%CF%84%20%CE%B8%20is%20in%20general%20intractable%20the%20score%20function%20trick%20is%20typically%20used%20to%20used%20to%20produce%20a%20Monte%20Carlo%20estimate%20of%20the%20policy%20gradients%20Although%20the%20gradient%20estimate%20can%20be%20directly%20defined%20when%20using%20a%20automaticdifferentiation%20toolbox%20it%20is%20usually%20more%20convenient%20to%20use%20an%20objective%20function%20whose%20gradients%20correspond%20to%20the%20policy%20gradient%20estimate%20Due%20to%20the%20Policy%20Gradient%20Theorem%20PGT%20Sutton%20et%20al%202000%20such%20a%20surrogate%20objective%20can%20be%20written%20as%20JPGT%20%201%20K"
        },
        {
            "id": "While_2018_c",
            "entry": "While (41) and (42) are equivalent (Peters & Schaal, 2006), the more popular formulation formulation (41) can be seen as forward looking credit assignment while (42) can be interpreted as backward looking credit assignment (Foerster et al., 2018). A generalized procedure for constructing \u201csurrogate\u201d objectives for arbitrary stochastic computation graphs can be found in Schulman et al. (2015a).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=While%2041%20and%2042%20are%20equivalent%20Peters%20%20Schaal%202006%20the%20more%20popular%20formulation%20formulation%2041%20can%20be%20seen%20as%20forward%20looking%20credit%20assignment%20while%2042%20can%20be%20interpreted%20as%20backward%20looking%20credit%20assignment%20Foerster%20et%20al%202018%20A%20generalized%20procedure%20for%20constructing%20surrogate%20objectives%20for%20arbitrary%20stochastic%20computation%20graphs%20can%20be%20found%20in%20Schulman%20et%20al%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=While%2041%20and%2042%20are%20equivalent%20Peters%20%20Schaal%202006%20the%20more%20popular%20formulation%20formulation%2041%20can%20be%20seen%20as%20forward%20looking%20credit%20assignment%20while%2042%20can%20be%20interpreted%20as%20backward%20looking%20credit%20assignment%20Foerster%20et%20al%202018%20A%20generalized%20procedure%20for%20constructing%20surrogate%20objectives%20for%20arbitrary%20stochastic%20computation%20graphs%20can%20be%20found%20in%20Schulman%20et%20al%202015a"
        },
        {
            "id": "Estimating_2016_a",
            "entry": "Estimating the the hessian of the reinforcement learning objective has been discussed in Furmston et al. (2016) and Baxter & Bartlett (2001) with focus on second order policy gradient methods. In the infinite horizon MDP case, Baxter & Bartlett (2001) derive a decomposition of the hessian. In the following, we extend their finding to the finite horizon case.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Estimating%20the%20the%20hessian%20of%20the%20reinforcement%20learning%20objective%20has%20been%20discussed%20in%20Furmston%20et%20al%202016%20and%20Baxter%20%20Bartlett%202001%20with%20focus%20on%20second%20order%20policy%20gradient%20methods%20In%20the%20infinite%20horizon%20MDP%20case%20Baxter%20%20Bartlett%202001%20derive%20a%20decomposition%20of%20the%20hessian%20In%20the%20following%20we%20extend%20their%20finding%20to%20the%20finite%20horizon%20case",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Estimating%20the%20the%20hessian%20of%20the%20reinforcement%20learning%20objective%20has%20been%20discussed%20in%20Furmston%20et%20al%202016%20and%20Baxter%20%20Bartlett%202001%20with%20focus%20on%20second%20order%20policy%20gradient%20methods%20In%20the%20infinite%20horizon%20MDP%20case%20Baxter%20%20Bartlett%202001%20derive%20a%20decomposition%20of%20the%20hessian%20In%20the%20following%20we%20extend%20their%20finding%20to%20the%20finite%20horizon%20case"
        },
        {
            "id": "As_2018_d",
            "entry": "As pointed out by Al-Shedivat et al. (2018); Stadie et al. (2018) and Foerster et al. (2018), simply differentiating through the gradient of surrogate objective JPGT as done in the original MAML version (Finn et al., 2017) leads to biased hessian estimates. Specifically, when compared with the unbiased estimate, as derived in (24) and decomposed in Appendix B.2, both H1 and H12 + H12 are missing. Thus, \u2207\u03b8Jpre does not appear in the gradients of the meta-objective (i.e. \u2207\u03b8J = \u2207\u03b8Jpost). Only performing gradient descent with \u2207\u03b8Jpost entirely neglects influences of the pre-update sampling distribution. This issue was overseen in the RL-MAML implementation of Finn et al. (2017). As discussed in Stadie et al. (2018) this leads to poor performance in meta-learning problems that require exploration during the pre-update sampling.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=As%20pointed%20out%20by%20AlShedivat%20et%20al%202018%20Stadie%20et%20al%202018%20and%20Foerster%20et%20al%202018%20simply%20differentiating%20through%20the%20gradient%20of%20surrogate%20objective%20JPGT%20as%20done%20in%20the%20original%20MAML%20version%20Finn%20et%20al%202017%20leads%20to%20biased%20hessian%20estimates%20Specifically%20when%20compared%20with%20the%20unbiased%20estimate%20as%20derived%20in%2024%20and%20decomposed%20in%20Appendix%20B2%20both%20H1%20and%20H12%20%20H12%20are%20missing%20Thus%20%CE%B8Jpre%20does%20not%20appear%20in%20the%20gradients%20of%20the%20metaobjective%20ie%20%CE%B8J%20%20%CE%B8Jpost%20Only%20performing%20gradient%20descent%20with%20%CE%B8Jpost%20entirely%20neglects%20influences%20of%20the%20preupdate%20sampling%20distribution%20This%20issue%20was%20overseen%20in%20the%20RLMAML%20implementation%20of%20Finn%20et%20al%202017%20As%20discussed%20in%20Stadie%20et%20al%202018%20this%20leads%20to%20poor%20performance%20in%20metalearning%20problems%20that%20require%20exploration%20during%20the%20preupdate%20sampling",
            "oa_query": "https://api.scholarcy.com/oa_version?query=As%20pointed%20out%20by%20AlShedivat%20et%20al%202018%20Stadie%20et%20al%202018%20and%20Foerster%20et%20al%202018%20simply%20differentiating%20through%20the%20gradient%20of%20surrogate%20objective%20JPGT%20as%20done%20in%20the%20original%20MAML%20version%20Finn%20et%20al%202017%20leads%20to%20biased%20hessian%20estimates%20Specifically%20when%20compared%20with%20the%20unbiased%20estimate%20as%20derived%20in%2024%20and%20decomposed%20in%20Appendix%20B2%20both%20H1%20and%20H12%20%20H12%20are%20missing%20Thus%20%CE%B8Jpre%20does%20not%20appear%20in%20the%20gradients%20of%20the%20metaobjective%20ie%20%CE%B8J%20%20%CE%B8Jpost%20Only%20performing%20gradient%20descent%20with%20%CE%B8Jpost%20entirely%20neglects%20influences%20of%20the%20preupdate%20sampling%20distribution%20This%20issue%20was%20overseen%20in%20the%20RLMAML%20implementation%20of%20Finn%20et%20al%202017%20As%20discussed%20in%20Stadie%20et%20al%202018%20this%20leads%20to%20poor%20performance%20in%20metalearning%20problems%20that%20require%20exploration%20during%20the%20preupdate%20sampling"
        },
        {
            "id": "Addressing_2018_e",
            "entry": "Addressing the issue of incorrect higher-order derivatives of monte-carlo estimators, Foerster et al. (2018) propose DICE which mainly builds upon an newly introduced MagicBox( ) operator. This operator allows to formulate monte-carlo estimators with correct higher-order derivatives. A DICE formulation of a policy gradient estimator reads as: J DICE =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Addressing%20the%20issue%20of%20incorrect%20higherorder%20derivatives%20of%20montecarlo%20estimators%20Foerster%20et%20al%202018%20propose%20DICE%20which%20mainly%20builds%20upon%20an%20newly%20introduced%20MagicBox%20%20operator%20This%20operator%20allows%20to%20formulate%20montecarlo%20estimators%20with%20correct%20higherorder%20derivatives%20A%20DICE%20formulation%20of%20a%20policy%20gradient%20estimator%20reads%20as%20J%20DICE",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Addressing%20the%20issue%20of%20incorrect%20higherorder%20derivatives%20of%20montecarlo%20estimators%20Foerster%20et%20al%202018%20propose%20DICE%20which%20mainly%20builds%20upon%20an%20newly%20introduced%20MagicBox%20%20operator%20This%20operator%20allows%20to%20formulate%20montecarlo%20estimators%20with%20correct%20higherorder%20derivatives%20A%20DICE%20formulation%20of%20a%20policy%20gradient%20estimator%20reads%20as%20J%20DICE"
        },
        {
            "id": "Note_2018_f",
            "entry": "Note that \u2192 denotes a \u201cevaluates to\u201d and does not necessarily imply equality w.r.t. to gradients. Hence, JDICE(\u03b8) evaluates to the sum of rewards at 0th order but produces the unbiased gradients \u2207\u03b8nJDICE(\u03b8) when differentiated n-times (see Foerster et al. (2018) for proof). To shed more light on the maverick DICE formulation, we rewrite (67) as follows: t=0 t \u03c0\u03b8(at |st ) t =0 \u22a5(\u03c0\u03b8(at |st ))",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Note%20that%20%20denotes%20a%20evaluates%20to%20and%20does%20not%20necessarily%20imply%20equality%20wrt%20to%20gradients%20Hence%20JDICE%CE%B8%20evaluates%20to%20the%20sum%20of%20rewards%20at%200th%20order%20but%20produces%20the%20unbiased%20gradients%20%CE%B8nJDICE%CE%B8%20when%20differentiated%20ntimes%20see%20Foerster%20et%20al%202018%20for%20proof%20To%20shed%20more%20light%20on%20the%20maverick%20DICE%20formulation%20we%20rewrite%2067%20as%20follows%20t0%20t%20%CF%80%CE%B8at%20st%20%20t%200%20%CF%80%CE%B8at%20st",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Note%20that%20%20denotes%20a%20evaluates%20to%20and%20does%20not%20necessarily%20imply%20equality%20wrt%20to%20gradients%20Hence%20JDICE%CE%B8%20evaluates%20to%20the%20sum%20of%20rewards%20at%200th%20order%20but%20produces%20the%20unbiased%20gradients%20%CE%B8nJDICE%CE%B8%20when%20differentiated%20ntimes%20see%20Foerster%20et%20al%202018%20for%20proof%20To%20shed%20more%20light%20on%20the%20maverick%20DICE%20formulation%20we%20rewrite%2067%20as%20follows%20t0%20t%20%CF%80%CE%B8at%20st%20%20t%200%20%CF%80%CE%B8at%20st"
        }
    ]
}
