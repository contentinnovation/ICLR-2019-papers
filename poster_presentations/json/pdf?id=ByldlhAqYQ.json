{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "TRANSFER LEARNING FOR SEQUENCES VIA LEARNING TO COLLOCATE",
        "author": "Wanyun Cui\u00a7 Guangyu Zheng, Zhiqiang Shen\u00b6 Sihang Jiang, Wei Wang, cui.wanyun@sufe.edu.cn, {simonzheng, zhiqiangshen0214, tedjiangfdu}@gmail.com",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ByldlhAqYQ"
        },
        "abstract": "Transfer learning aims to solve the data sparsity for a target domain by applying information of the source domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represents the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply represents the whole sentence by a single vector, which is unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain. In this paper, we proposed the aligned recurrent transfer, ART, to achieve celllevel information transfer. ART is under the pre-training framework. Each cell attentively accepts transferred information from a set of positions in the source domain. Therefore, ART learns the cross-domain word collocations in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments."
    },
    "keywords": [
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "open domain",
            "url": "https://en.wikipedia.org/wiki/open_domain"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "information transfer",
            "url": "https://en.wikipedia.org/wiki/information_transfer"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        }
    ],
    "abbreviations": {
        "RNN": "recurrent neural network",
        "LWT": "Layer-Wise Transfer",
        "CCT": "Corresponding Cell Transfer",
        "HRN": "hierarchical recurrent networks",
        "PTB": "Penn Treebank"
    },
    "highlights": [
        "Target: Sometimes I really hate RIBs. Source: Sometimes I really hate RIBs. In this paper, we proposed ART, a novel transfer learning mechanism, to transfer cell-level information by learning to collocate cross-domain words",
        "Most previous NLP studies focus on open domain tasks",
        "Due to the variety and ambiguity of natural language (<a class=\"ref-link\" id=\"cGlorot_et+al_2011_a\" href=\"#rGlorot_et+al_2011_a\">Glorot et al, 2011</a>; <a class=\"ref-link\" id=\"cSong_et+al_2011_a\" href=\"#rSong_et+al_2011_a\">Song et al, 2011</a>), models for one domain usually incur more errors when adapting to another domain",
        "While existing NLP models are usually trained by the open domain, they suffer from severe performance degeneration when adapting to specific domains",
        "Learn to Collocate and Transfer For each word in the target domain, ART learns to incorporate two types of information from the source domain: (a) the hidden state corresponding to the same word, and (b) the hidden states for all words in the sequence",
        "Supervised Domain Adaptation We evaluate ART when the number of training samples for the target domain is much fewer than that of the source domain"
    ],
    "key_statements": [
        "Target: Sometimes I really hate RIBs. Source: Sometimes I really hate RIBs. In this paper, we proposed ART, a novel transfer learning mechanism, to transfer cell-level information by learning to collocate cross-domain words",
        "Most previous NLP studies focus on open domain tasks",
        "Due to the variety and ambiguity of natural language (<a class=\"ref-link\" id=\"cGlorot_et+al_2011_a\" href=\"#rGlorot_et+al_2011_a\">Glorot et al, 2011</a>; <a class=\"ref-link\" id=\"cSong_et+al_2011_a\" href=\"#rSong_et+al_2011_a\">Song et al, 2011</a>), models for one domain usually incur more errors when adapting to another domain",
        "While existing NLP models are usually trained by the open domain, they suffer from severe performance degeneration when adapting to specific domains",
        "Learn to Collocate and Transfer For each word in the target domain, ART learns to incorporate two types of information from the source domain: (a) the hidden state corresponding to the same word, and (b) the hidden states for all words in the sequence",
        "ART provides a more flexible way to transfer a set of positions in the source domain and represent the long-term dependency",
        "Supervised Domain Adaptation We evaluate ART when the number of training samples for the target domain is much fewer than that of the source domain",
        "Pre-trained models ART uses a pre-trained model from the source domain, and fine-tunes the model with additional layers for the target domain"
    ],
    "summary": [
        "Target: Sometimes I really hate RIBs. Source: Sometimes I really hate RIBs. In this paper, we proposed ART, a novel transfer learning mechanism, to transfer cell-level information by learning to collocate cross-domain words.",
        "Learn to Collocate and Transfer For each word in the target domain, ART learns to incorporate two types of information from the source domain: (a) the hidden state corresponding to the same word, and (b) the hidden states for all words in the sequence.",
        "ART learns to incorporate information (b) based on the attention scores (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>) of all words from the source domain.",
        "We first pre-train the neural network of the source domain.",
        "ART precisely learns to collocate words from the source domain and to transfer their cell-level information for the target domain.",
        "Architecture The source domain and the target domain share an RNN layer, from which the common information is transferred.",
        "In order to compute \u03c0i, we use attention (<a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2015_a\" href=\"#rBahdanau_et+al_2015_a\">Bahdanau et al, 2015</a></a>) to incorporate information of all candidate positions in the sequence from the source domain.",
        "We use CCT to verify the effectiveness of collocating and transferring from the source domain.",
        "Model Details: To adapt ART to sentence classification, we use a max pooling layer to merge the states of different words.",
        "ART provides a more flexible way to transfer a set of positions in the source domain and represent the long-term dependency.",
        "This verifies the effectiveness of ART in representing long-term dependency by learning to collocate and transfer.",
        "ART aligns and transfers information from different positions in the source domain.",
        "Pre-trained models ART uses a pre-trained model from the source domain, and fine-tunes the model with additional layers for the target domain.",
        "ART uses attention mechanism in RNN that each cell in the target domain directly access information of all cells in the source domain.",
        "ART aims at transfer learning for one task in different domains, while BERT and ELMo focus on learning general word representations or sentence representations.",
        "We proposed the ART model to collocate and transfer cell-level information.",
        "ART has three advantages: (1) it transfers more fine-grained cell-level information, and can be adapted to seq2seq or sequence labeling tasks; (2) it aligns and transfers a set of collocated words in the source sentence to represent the cross domain long-term dependency; (3) it is general and can be applied to different tasks.",
        "ART verified the effectiveness of pre-training models with the limited but relevant training corpus"
    ],
    "headline": "We proposed the aligned recurrent transfer, ART, to achieve celllevel information transfer",
    "reference_links": [
        {
            "id": "Ajakan_et+al_2014_a",
            "entry": "Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, and Mario Marchand. Domainadversarial neural networks. In NeurIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hana%20Ajakan%20Pascal%20Germain%20Hugo%20Larochelle%20Francois%20Laviolette%20and%20Mario%20Marchand%20Domainadversarial%20neural%20networks%20In%20NeurIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hana%20Ajakan%20Pascal%20Germain%20Hugo%20Larochelle%20Francois%20Laviolette%20and%20Mario%20Marchand%20Domainadversarial%20neural%20networks%20In%20NeurIPS%202014"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Blitzer_et+al_2007_a",
            "entry": "John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, pp. 440\u2013447, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blitzer%2C%20John%20Dredze%2C%20Mark%20Biographies%2C%20Fernando%20Pereira%20bollywood%20boom-boxes%20and%20blenders%3A%20Domain%20adaptation%20for%20sentiment%20classification%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blitzer%2C%20John%20Dredze%2C%20Mark%20Biographies%2C%20Fernando%20Pereira%20bollywood%20boom-boxes%20and%20blenders%3A%20Domain%20adaptation%20for%20sentiment%20classification%202007"
        },
        {
            "id": "Daume_2007_a",
            "entry": "Hal Daume III. Frustratingly easy domain adaptation. In ACL, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daume%2C%20III%2C%20Hal%20Frustratingly%20easy%20domain%20adaptation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daume%2C%20III%2C%20Hal%20Frustratingly%20easy%20domain%20adaptation%202007"
        },
        {
            "id": "Devlin_et+al_2018_a",
            "entry": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.04805"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. In JMLR, volume 12, pp. 2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Ganin_et+al_2016_a",
            "entry": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. In JMLR, volume 17, pp. 2096\u20132030, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, pp. 513\u2013520, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011"
        },
        {
            "id": "Jiang_2007_a",
            "entry": "Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL, volume 7, pp. 264\u2013271, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Jing%20Zhai%2C%20ChengXiang%20Instance%20weighting%20for%20domain%20adaptation%20in%20nlp%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Jing%20Zhai%2C%20ChengXiang%20Instance%20weighting%20for%20domain%20adaptation%20in%20nlp%202007"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Zheng Li, Yu Zhang, Ying Wei, Yuxiang Wu, and Qiang Yang. End-to-end adversarial memory network for cross-domain sentiment classification. In IJCAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zheng%20Zhang%2C%20Yu%20Wei%2C%20Ying%20Wu%2C%20Yuxiang%20End-to-end%20adversarial%20memory%20network%20for%20cross-domain%20sentiment%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zheng%20Zhang%2C%20Yu%20Wei%2C%20Ying%20Wu%2C%20Yuxiang%20End-to-end%20adversarial%20memory%20network%20for%20cross-domain%20sentiment%20classification%202017"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Zheng Li, Ying Wei, Yu Zhang, and Qiang Yang. Hierarchical attention transfer network for crossdomain sentiment classification. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zheng%20Wei%2C%20Ying%20Zhang%2C%20Yu%20Yang%2C%20Qiang%20Hierarchical%20attention%20transfer%20network%20for%20crossdomain%20sentiment%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zheng%20Wei%2C%20Ying%20Zhang%2C%20Yu%20Yang%2C%20Qiang%20Hierarchical%20attention%20transfer%20network%20for%20crossdomain%20sentiment%20classification%202018"
        },
        {
            "id": "Peng_et+al_2015_a",
            "entry": "Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. A comparative study on regularization strategies for embedding-based neural networks. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Hao%20Mou%2C%20Lili%20Li%2C%20Ge%20Chen%2C%20Yunchuan%20A%20comparative%20study%20on%20regularization%20strategies%20for%20embedding-based%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Hao%20Mou%2C%20Lili%20Li%2C%20Ge%20Chen%2C%20Yunchuan%20A%20comparative%20study%20on%20regularization%20strategies%20for%20embedding-based%20neural%20networks%202015"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Matthew%20E.%20Neumann%2C%20Mark%20Iyyer%2C%20Mohit%20Gardner%2C%20Matt%20Deep%20contextualized%20word%20representations%202018"
        },
        {
            "id": "Radford_et+al_2019_a",
            "entry": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical report, Technical report, OpenAI, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Wu%2C%20Jeffrey%20Child%2C%20Rewon%20Luan%2C%20David%20Language%20models%20are%20unsupervised%20multitask%20learners%202019"
        },
        {
            "id": "Ritter_et+al_2011_a",
            "entry": "Alan Ritter, Sam Clark, Oren Etzioni, et al. Named entity recognition in tweets: an experimental study. In EMNLP, pp. 1524\u20131534, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ritter%2C%20Alan%20Clark%2C%20Sam%20Etzioni%2C%20Oren%20Named%20entity%20recognition%20in%20tweets%3A%20an%20experimental%20study%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ritter%2C%20Alan%20Clark%2C%20Sam%20Etzioni%2C%20Oren%20Named%20entity%20recognition%20in%20tweets%3A%20an%20experimental%20study%202011"
        },
        {
            "id": "Schnabel_2014_a",
            "entry": "Tobias Schnabel and Hinrich Schutze. Flors: Fast and simple domain adaptation for part-of-speech tagging. In TACL, volume 2, pp. 15\u201326, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schnabel%2C%20Tobias%20Schutze%2C%20Hinrich%20Flors%3A%20Fast%20and%20simple%20domain%20adaptation%20for%20part-of-speech%20tagging%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schnabel%2C%20Tobias%20Schutze%2C%20Hinrich%20Flors%3A%20Fast%20and%20simple%20domain%20adaptation%20for%20part-of-speech%20tagging%202014"
        },
        {
            "id": "Song_et+al_2011_a",
            "entry": "Yangqiu Song, Haixun Wang, Zhongyuan Wang, Hongsong Li, and Weizhu Chen. Short text conceptualization using a probabilistic knowledgebase. In IJCAI, pp. 2330\u20132336, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Yangqiu%20Wang%2C%20Haixun%20Wang%2C%20Zhongyuan%20Li%2C%20Hongsong%20Short%20text%20conceptualization%20using%20a%20probabilistic%20knowledgebase%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Yangqiu%20Wang%2C%20Haixun%20Wang%2C%20Zhongyuan%20Li%2C%20Hongsong%20Short%20text%20conceptualization%20using%20a%20probabilistic%20knowledgebase%202011"
        },
        {
            "id": "Erik_2003_a",
            "entry": "Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In NAACL, pp. 142\u2013147, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erik%2C%20F.%20Tjong%20Kim%20Sang%20and%20Fien%20De%20Meulder.%20Introduction%20to%20the%20conll-2003%20shared%20task%3A%20Languageindependent%20named%20entity%20recognition%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erik%2C%20F.%20Tjong%20Kim%20Sang%20and%20Fien%20De%20Meulder.%20Introduction%20to%20the%20conll-2003%20shared%20task%3A%20Languageindependent%20named%20entity%20recognition%202003"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz"
        },
        {
            "id": "Kaiser_2017_a",
            "entry": "Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017. Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. Transfer learning for sequence tagging with hierarchical recurrent networks. In ICLR, 2017. Tobias; Schtze Hinrich Yin, Wenpeng; Schnabel. Online updating of word representations for part-of-speech taggging. In EMNLP, 2015. Ding Ying, Jianfei Yu, and Jing Jiang. Recurrent neural networks with auxiliary labels for crossdomain opinion target extraction. In AAAI, 2017. Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, and Tingting He. Bi-transferring deep neural networks for domain adaptation. In ACL, 2016. Yftah Ziser and Roi Reichart. Pivot based language modeling for improved neural domain adaptation. In NAACL, volume 1, pp. 1241\u20131251, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiser%20Polosukhin%2C%20Illia%20Attention%20is%20all%20you%20need%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiser%20Polosukhin%2C%20Illia%20Attention%20is%20all%20you%20need%202017"
        }
    ]
}
