{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS",
        "author": "Bo Chang, University of British Columbia Vancouver, BC, Canada bchang@stat.ubc.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryxepo0cFX"
        },
        "journal": "Moreover",
        "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "dynamical system",
            "url": "https://en.wikipedia.org/wiki/dynamical_system"
        },
        {
            "term": "short term",
            "url": "https://en.wikipedia.org/wiki/short_term"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "ordinary differential equations",
            "url": "https://en.wikipedia.org/wiki/ordinary_differential_equations"
        },
        {
            "term": "isometry",
            "url": "https://en.wikipedia.org/wiki/isometry"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        },
        {
            "term": "differential equation",
            "url": "https://en.wikipedia.org/wiki/differential_equation"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent neural networks",
        "BPTT": "back-propagated through time",
        "LSTM": "Long short-term memory networks",
        "GRU": "gated recurrent units",
        "CFN": "chaos free network",
        "ResNets": "residual networks",
        "ODEs": "ordinary differential equations",
        "W ht\u22121": " ht\u22121 + tanh"
    },
    "highlights": [
        "Recurrent neural networks (RNNs) (Rumelhart et al, 1986; <a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\"><a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\">Elman, 1990</a></a>) have found widespread use across a variety of domains from language modeling (<a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\"><a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\">Mikolov et al, 2010</a></a>; <a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\"><a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\">Kiros et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>) and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\">Bahdanau et al, 2014</a></a>) to speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>) and recommendation systems (<a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\"><a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\">Hidasi et al, 2015</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\"><a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\">Wu et al, 2017</a></a>)",
        "The forward Euler method approximates the solution to the ordinary differential equations iteratively as ht = ht\u22121 + tanh(W ht\u22121), which can be regarded as a recurrent network without input data",
        "Adopting the visualization technique used by Laurent & von Brecht (2017) and <a class=\"ref-link\" id=\"cHaber_2017_a\" href=\"#rHaber_2017_a\">Haber & Ruthotto (2017</a>), we study the behavior of two-dimensional vanilla Recurrent neural networks and Recurrent neural networks with feedback in the absence of input data and bias: vanilla: ht = tanh(W ht\u22121), feedback: ht = ht\u22121 + tanh(W ht\u22121)",
        "We present a new perspective on the trainability of Recurrent neural networks from the dynamical system viewpoint",
        "We propose the AntisymmetricRNN, which is a discretization of ordinary differential equations that satisfy the critical criterion",
        "An important item of future work is to investigate other stable ordinary differential equations and numerical methods that lead to novel and well-conditioned recurrent architectures"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) (Rumelhart et al, 1986; <a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\"><a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\">Elman, 1990</a></a>) have found widespread use across a variety of domains from language modeling (<a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\"><a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\">Mikolov et al, 2010</a></a>; <a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\"><a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\">Kiros et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>) and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\">Bahdanau et al, 2014</a></a>) to speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>) and recommendation systems (<a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\"><a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\">Hidasi et al, 2015</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\"><a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\">Wu et al, 2017</a></a>)",
        "The gates allow information to flow from inputs at any previous time steps to the end of the sequence more partially addressing the vanishing gradient problem (<a class=\"ref-link\" id=\"cCollins_et+al_2016_a\" href=\"#rCollins_et+al_2016_a\">Collins et al, 2016</a>). These models must be paired with techniques such as normalization layers (<a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe & Szegedy, 2015</a>; <a class=\"ref-link\" id=\"cBa_et+al_2016_a\" href=\"#rBa_et+al_2016_a\">Ba et al, 2016</a>) and gradient clipping (<a class=\"ref-link\" id=\"cPascanu_et+al_2013_a\" href=\"#rPascanu_et+al_2013_a\">Pascanu et al, 2013</a>) to achieve good performance",
        "The forward Euler method approximates the solution to the ordinary differential equations iteratively as ht = ht\u22121 + tanh(W ht\u22121), which can be regarded as a recurrent network without input data",
        "Since \u03bbi(Jt), the eigenvalues of the Jacobian matrix, are all imaginary, |1 + \u03bbi(Jt)| is always greater than 1, which makes the AntisymmetricRNN defined in Equation 10 unstable when solved using forward Euler",
        "Gating is commonly employed in Recurrent neural networks",
        "Each gate is often modeled as a single layer network taking the previous hidden state ht\u22121 and data xt as inputs, followed by a sigmoid activation",
        "Adopting the visualization technique used by Laurent & von Brecht (2017) and <a class=\"ref-link\" id=\"cHaber_2017_a\" href=\"#rHaber_2017_a\">Haber & Ruthotto (2017</a>), we study the behavior of two-dimensional vanilla Recurrent neural networks and Recurrent neural networks with feedback in the absence of input data and bias: vanilla: ht = tanh(W ht\u22121), feedback: ht = ht\u22121 + tanh(W ht\u22121)",
        "In the case of Recurrent neural networks with feedback, the trajectory of the hidden states is predictable based on the eigenvalues of the weight matrix W .We consider the following four weight matrices that correspond to Figure 1(e)-(f) respectively: W+ =",
        "To verify that AntisymmetricRNNs mitigate the exploding/vanishing gradient issues, we conduct an additional set of experiments varying the length of noise padding so that the total time steps T \u2208 {100, 200, 400, 800}",
        "We present a new perspective on the trainability of Recurrent neural networks from the dynamical system viewpoint",
        "We propose the AntisymmetricRNN, which is a discretization of ordinary differential equations that satisfy the critical criterion",
        "An important item of future work is to investigate other stable ordinary differential equations and numerical methods that lead to novel and well-conditioned recurrent architectures"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) (Rumelhart et al, 1986; <a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\"><a class=\"ref-link\" id=\"cElman_1990_a\" href=\"#rElman_1990_a\">Elman, 1990</a></a>) have found widespread use across a variety of domains from language modeling (<a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\"><a class=\"ref-link\" id=\"cMikolov_et+al_2010_a\" href=\"#rMikolov_et+al_2010_a\">Mikolov et al, 2010</a></a>; <a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\"><a class=\"ref-link\" id=\"cKiros_et+al_2015_a\" href=\"#rKiros_et+al_2015_a\">Kiros et al, 2015</a></a>; <a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cJozefowicz_et+al_2016_a\" href=\"#rJozefowicz_et+al_2016_a\">Jozefowicz et al, 2016</a></a>) and machine translation (<a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\"><a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\">Bahdanau et al, 2014</a></a>) to speech recognition (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>) and recommendation systems (<a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\"><a class=\"ref-link\" id=\"cHidasi_et+al_2015_a\" href=\"#rHidasi_et+al_2015_a\">Hidasi et al, 2015</a></a>; <a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\"><a class=\"ref-link\" id=\"cWu_et+al_2017_a\" href=\"#rWu_et+al_2017_a\">Wu et al, 2017</a></a>).",
        "The stability of the ODE solutions and the numerical methods for solving ODEs lead us to design a special form of RNNs, which we name AntisymmetricRNN, that can capture longterm dependencies in the inputs.",
        "The forward Euler method approximates the solution to the ODE iteratively as ht = ht\u22121 + tanh(W ht\u22121), which can be regarded as a recurrent network without input data.",
        "We are going to establish the connections between the stability of an ODE and the trainability of the RNNs by discretizing the ODE, and design a new RNN architecture that is stable and capable of capturing long-term dependencies.",
        "A naive forward Euler discretization of the ODE in Equation 8 leads to the following recurrent network we refer to as the AntisymmetricRNN.",
        "The ODE as defined in Equation 8 is incompatible with the stability condition of the forward Euler method.",
        "Since \u03bbi(Jt), the eigenvalues of the Jacobian matrix, are all imaginary, |1 + \u03bbi(Jt)| is always greater than 1, which makes the AntisymmetricRNN defined in Equation 10 unstable when solved using forward Euler.",
        "Gating is commonly employed in RNNs. Each gate is often modeled as a single layer network taking the previous hidden state ht\u22121 and data xt as inputs, followed by a sigmoid activation.",
        "In the case of RNNs with feedback, the trajectory of the hidden states is predictable based on the eigenvalues of the weight matrix W .We consider the following four weight matrices that correspond to Figure 1(e)-(f) respectively: W+ =",
        "The performance of the proposed antisymmetric networks is evaluated on four image classification tasks with long-range dependencies.",
        "To verify that AntisymmetricRNNs mitigate the exploding/vanishing gradient issues, we conduct an additional set of experiments varying the length of noise padding so that the total time steps T \u2208 {100, 200, 400, 800}.",
        "As shown in the figure, the eigenvalues for LSTMs quickly approaches zero as time steps increase, indicating vanishing gradients during back-propagation.",
        "We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs. This new view opens up possibilities to exploit the computational and theoretical success from dynamical systems to understand and improve the trainability of RNNs. We propose the AntisymmetricRNN, which is a discretization of ODEs that satisfy the critical criterion.",
        "An important item of future work is to investigate other stable ODEs and numerical methods that lead to novel and well-conditioned recurrent architectures."
    ],
    "headline": "We propose the following modification to",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2016_a",
            "entry": "Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In ICML, pp. 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Bengio_et+al_1994_a",
            "entry": "Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994"
        },
        {
            "id": "Chang_et+al_2018_a",
            "entry": "Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural networks. In AAAI, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Bo%20Meng%2C%20Lili%20Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Reversible%20architectures%20for%20arbitrarily%20deep%20residual%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Bo%20Meng%2C%20Lili%20Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Reversible%20architectures%20for%20arbitrarily%20deep%20residual%20neural%20networks%202018"
        },
        {
            "id": "Chang_et+al_2018_b",
            "entry": "Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks from dynamical systems view. In ICLR, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Bo%20Meng%2C%20Lili%20Haber%2C%20Eldad%20Tung%2C%20Frederick%20Multi-level%20residual%20networks%20from%20dynamical%20systems%20view%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Bo%20Meng%2C%20Lili%20Haber%2C%20Eldad%20Tung%2C%20Frederick%20Multi-level%20residual%20networks%20from%20dynamical%20systems%20view%202018"
        },
        {
            "id": "Chen_et+al_0000_a",
            "entry": "Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks. In ICML, pp. 873\u2013882, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Minmin%20Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20RNNs%3A%20Gating%20enables%20signal%20propagation%20in%20recurrent%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Minmin%20Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20RNNs%3A%20Gating%20enables%20signal%20propagation%20in%20recurrent%20neural%20networks"
        },
        {
            "id": "Chen_et+al_0000_b",
            "entry": "Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07366"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In EMNLP, pp. 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Collins_et+al_2016_a",
            "entry": "Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent neural networks. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20Jasmine%20Sohl-Dickstein%2C%20Jascha%20Sussillo%2C%20David%20Capacity%20and%20trainability%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20Jasmine%20Sohl-Dickstein%2C%20Jascha%20Sussillo%2C%20David%20Capacity%20and%20trainability%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Cooijmans_et+al_2017_a",
            "entry": "Tim Cooijmans, Nicolas Ballas, Cesar Laurent, Caglar Gulcehre, and Aaron Courville. Recurrent batch normalization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cooijmans%2C%20Tim%20Ballas%2C%20Nicolas%20Laurent%2C%20Cesar%20Gulcehre%2C%20Caglar%20Recurrent%20batch%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cooijmans%2C%20Tim%20Ballas%2C%20Nicolas%20Laurent%2C%20Cesar%20Gulcehre%2C%20Caglar%20Recurrent%20batch%20normalization%202017"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Hihi_1996_a",
            "entry": "Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In NIPS, pp. 493\u2013499, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hihi%2C%20Salah%20El%20Bengio%2C%20Yoshua%20Hierarchical%20recurrent%20neural%20networks%20for%20long-term%20dependencies%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hihi%2C%20Salah%20El%20Bengio%2C%20Yoshua%20Hierarchical%20recurrent%20neural%20networks%20for%20long-term%20dependencies%201996"
        },
        {
            "id": "Elman_1990_a",
            "entry": "Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elman%2C%20Jeffrey%20L.%20Finding%20structure%20in%20time%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elman%2C%20Jeffrey%20L.%20Finding%20structure%20in%20time%201990"
        },
        {
            "id": "Graves_et+al_2013_a",
            "entry": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, pp. 6645\u20136649. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Haber_2017_a",
            "entry": "Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Stable%20architectures%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haber%2C%20Eldad%20Ruthotto%2C%20Lars%20Stable%20architectures%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hidasi_et+al_2015_a",
            "entry": "Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06939"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hyland_2017_a",
            "entry": "Stephanie L Hyland and Gunnar Ratsch. Learning unitary operators with help from u (n). In AAAI, pp. 2050\u20132058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyland%2C%20Stephanie%20L.%20Ratsch%2C%20Gunnar%20Learning%20unitary%20operators%20with%20help%20from%20u%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jose_et+al_2017_a",
            "entry": "Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. arXiv preprint arXiv:1705.10142, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10142"
        },
        {
            "id": "Jozefowicz_et+al_2015_a",
            "entry": "Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In ICML, pp. 2342\u20132350, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jozefowicz%2C%20Rafal%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20An%20empirical%20exploration%20of%20recurrent%20network%20architectures%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jozefowicz%2C%20Rafal%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20An%20empirical%20exploration%20of%20recurrent%20network%20architectures%202015"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "Kanai_et+al_2017_a",
            "entry": "Sekitoshi Kanai, Yasuhiro Fujiwara, and Sotetsu Iwamura. Preventing gradient explosions in gated recurrent units. In NIPS, pp. 435\u2013444, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanai%2C%20Sekitoshi%20Fujiwara%2C%20Yasuhiro%20Iwamura%2C%20Sotetsu%20Preventing%20gradient%20explosions%20in%20gated%20recurrent%20units%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kanai%2C%20Sekitoshi%20Fujiwara%2C%20Yasuhiro%20Iwamura%2C%20Sotetsu%20Preventing%20gradient%20explosions%20in%20gated%20recurrent%20units%202017"
        },
        {
            "id": "Kiros_et+al_2015_a",
            "entry": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In NIPS, pp. 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Laurent_2017_a",
            "entry": "Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Thomas%20von%20Brecht%2C%20James%20A%20recurrent%20neural%20network%20without%20chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Thomas%20von%20Brecht%2C%20James%20A%20recurrent%20neural%20network%20without%20chaos%202017"
        },
        {
            "id": "Le_et+al_2015_a",
            "entry": "Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations. In ICML, pp. 3276\u20133285, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Yiping%20Zhong%2C%20Aoxiao%20Li%2C%20Quanzheng%20Dong%2C%20Bin%20Beyond%20finite%20layer%20neural%20networks%3A%20Bridging%20deep%20architectures%20and%20numerical%20differential%20equations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Yiping%20Zhong%2C%20Aoxiao%20Li%2C%20Quanzheng%20Dong%2C%20Bin%20Beyond%20finite%20layer%20neural%20networks%3A%20Bridging%20deep%20architectures%20and%20numerical%20differential%20equations%202018"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mikolov_et+al_2014_a",
            "entry": "Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.7753"
        },
        {
            "id": "Mishkin_2015_a",
            "entry": "Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06422"
        },
        {
            "id": "Pascanu_et+al_2012_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. CoRR, abs/1211.5063, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1211.5063"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In ICML, pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Pennington_et+al_2017_a",
            "entry": "Jeffrey Pennington, Sam Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Sam%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Sam%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017"
        },
        {
            "id": "Rumelhart_1986_a",
            "entry": "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20E%20Rumelhart%20Geoffrey%20E%20Hinton%20and%20Ronald%20J%20Williams%20Learning%20representations%20by%20backpropagating%20errors%20nature%203236088533%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=David%20E%20Rumelhart%20Geoffrey%20E%20Hinton%20and%20Ronald%20J%20Williams%20Learning%20representations%20by%20backpropagating%20errors%20nature%203236088533%201986"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "Socher_et+al_2011_a",
            "entry": "Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In ICML, pp. 129\u2013136, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Lin%2C%20Cliff%20C.%20Manning%2C%20Chris%20Ng%2C%20Andrew%20Y.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Lin%2C%20Cliff%20C.%20Manning%2C%20Chris%20Ng%2C%20Andrew%20Y.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011"
        },
        {
            "id": "Tallec_2018_a",
            "entry": "Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tallec%2C%20Corentin%20Ollivier%2C%20Yann%20Can%20recurrent%20neural%20networks%20warp%20time%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tallec%2C%20Corentin%20Ollivier%2C%20Yann%20Can%20recurrent%20neural%20networks%20warp%20time%3F%202018"
        },
        {
            "id": "Vorontsov_et+al_2017_a",
            "entry": "Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. In ICML, pp. 3570\u20133578, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vorontsov%2C%20Eugene%20Trabelsi%2C%20Chiheb%20Kadoury%2C%20Samuel%20Pal%2C%20Chris%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vorontsov%2C%20Eugene%20Trabelsi%2C%20Chiheb%20Kadoury%2C%20Samuel%20Pal%2C%20Chris%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017"
        },
        {
            "id": "Wisdom_et+al_2016_a",
            "entry": "Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. In NIPS, pp. 4880\u20134888, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. Recurrent recommender networks. In WSDM, pp. 495\u2013503. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Chao-Yuan%20Ahmed%2C%20Amr%20Beutel%2C%20Alex%20Smola%2C%20Alexander%20J.%20Recurrent%20recommender%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Chao-Yuan%20Ahmed%2C%20Amr%20Beutel%2C%20Alex%20Smola%2C%20Alexander%20J.%20Recurrent%20recommender%20networks%202017"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv preprint arXiv:1703.01827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01827"
        },
        {
            "id": "Yu_2015_a",
            "entry": "Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07122"
        },
        {
            "id": "Yue_et+al_2018_a",
            "entry": "Boxuan Yue, Junwei Fu, and Jun Liang. Residual recurrent neural networks for learning sequential representations. Information, 9(3):56, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Boxuan%20Fu%2C%20Junwei%20Liang%2C%20Jun%20Residual%20recurrent%20neural%20networks%20for%20learning%20sequential%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Boxuan%20Fu%2C%20Junwei%20Liang%2C%20Jun%20Residual%20recurrent%20neural%20networks%20for%20learning%20sequential%20representations%202018"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient SVD parameterization. In ICML, pp. 5806\u20135814, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jiong%20Lei%2C%20Qi%20Dhillon%2C%20Inderjit%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jiong%20Lei%2C%20Qi%20Dhillon%2C%20Inderjit%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018"
        },
        {
            "id": "Zhang_et+al_2018_b",
            "entry": "Jiong Zhang, Yibo Lin, Zhao Song, and Inderjit Dhillon. Learning long term dependencies via Fourier recurrent units. In ICML, pp. 5815\u20135823, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jiong%20Lin%2C%20Yibo%20Song%2C%20Zhao%20Dhillon%2C%20Inderjit%20Learning%20long%20term%20dependencies%20via%20Fourier%20recurrent%20units%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jiong%20Lin%2C%20Yibo%20Song%2C%20Zhao%20Dhillon%2C%20Inderjit%20Learning%20long%20term%20dependencies%20via%20Fourier%20recurrent%20units%202018"
        },
        {
            "id": "In_1998_a",
            "entry": "In this section, we provide a brief overview of the stability theory by examples. Most of the materials are adapted from Ascher & Petzold (1998).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20section%20we%20provide%20a%20brief%20overview%20of%20the%20stability%20theory%20by%20examples%20Most%20of%20the%20materials%20are%20adapted%20from%20Ascher%20%20Petzold%201998"
        },
        {
            "id": "In_2018_a",
            "entry": "In this section, we provide a proof of a proposition which implies that the AntisymmetricRNN and AntisymmetricRNN with gating satisfy the critical criterion, i.e., the eigenvalues of the Jacobian matrix are imaginary. The proof is adapted from Chang et al. (2018a). Proposition 3. If W \u2208 Rn\u00d7n is an antisymmetric matrix and D \u2208 Rn\u00d7n is an invertible diagonal matrix, then the eigenvalues of DW are imaginary.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20this%20section%20we%20provide%20a%20proof%20of%20a%20proposition%20which%20implies%20that%20the%20AntisymmetricRNN%20and%20AntisymmetricRNN%20with%20gating%20satisfy%20the%20critical%20criterion%20ie%20the%20eigenvalues%20of%20the%20Jacobian%20matrix%20are%20imaginary%20The%20proof%20is%20adapted%20from%20Chang%20et%20al%202018a%20Proposition%203%20If%20W%20%20Rnn%20is%20an%20antisymmetric%20matrix%20and%20D%20%20Rnn%20is%20an%20invertible%20diagonal%20matrix%20then%20the%20eigenvalues%20of%20DW%20are%20imaginary",
            "oa_query": "https://api.scholarcy.com/oa_version?query=In%20this%20section%20we%20provide%20a%20proof%20of%20a%20proposition%20which%20implies%20that%20the%20AntisymmetricRNN%20and%20AntisymmetricRNN%20with%20gating%20satisfy%20the%20critical%20criterion%20ie%20the%20eigenvalues%20of%20the%20Jacobian%20matrix%20are%20imaginary%20The%20proof%20is%20adapted%20from%20Chang%20et%20al%202018a%20Proposition%203%20If%20W%20%20Rnn%20is%20an%20antisymmetric%20matrix%20and%20D%20%20Rnn%20is%20an%20invertible%20diagonal%20matrix%20then%20the%20eigenvalues%20of%20DW%20are%20imaginary"
        },
        {
            "id": "M_2015_a",
            "entry": "Let m be the input dimension and n be the number of hidden units. The input to hidden matrices are initialized to N (0, 1/m). The hidden to hidden matrices are initialized to N (0, \u03c3w2 /n), where \u03c3w is chosen from \u03c3w \u2208 {0, 1, 2, 4, 8, 16}. The bias terms are initialized to zero, except the forget gate bias of LSTM is initialized to 1, as suggested by Jozefowicz et al. (2015). For AntisymmetricRNNs, the step size \u2208 {0.01, 0.1, 1} and diffusion \u03b3 \u2208 {0.001, 0.01, 0.1, 1.0}. We use SGD with momentum and Adagrad (Duchi et al., 2011) as optimizers, with batch size of 128 and learning rate chosen from {0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1}. On MNIST and pixel-by-pixel CIFAR-10, all the models are trained for 50,000 iterations. On noise padded CIFAR-10, models are trained for 10,000 iterations. We use the standard train/test split of MNIST and CIFAR-10. The performance measure is the classification accuracy evaluated on the test set.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=m%20be%2C%20Let%20the%20input%20dimension%20and%20n%20be%20the%20number%20of%20hidden%20units.%20The%20input%20to%20hidden%20matrices%20are%20initialized%20to%20N%20%280%2C%201/m%29.%20The%20hidden%20to%20hidden%20matrices%20are%20initialized%20to%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=m%20be%2C%20Let%20the%20input%20dimension%20and%20n%20be%20the%20number%20of%20hidden%20units.%20The%20input%20to%20hidden%20matrices%20are%20initialized%20to%20N%20%280%2C%201/m%29.%20The%20hidden%20to%20hidden%20matrices%20are%20initialized%20to%202015"
        }
    ]
}
