{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "SYNTHETIC DATASETS FOR NEURAL PROGRAM SYNTHESIS",
        "author": "Richard Shin, UC Berkeley",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryeOSnAqYm"
        },
        "abstract": "The goal of program synthesis is to automatically generate programs in a particular language from corresponding specifications, e.g. input-output behavior. Many current approaches achieve impressive results after training on randomly generated I/O examples in limited domain-specific languages (DSLs), as with string transformations in RobustFill. However, we empirically discover that applying test input generation techniques for languages with control flow and rich input space causes deep networks to generalize poorly to certain data distributions; to correct this, we propose a new methodology for controlling and evaluating the bias of synthetic data distributions over both programs and specifications. We demonstrate, using the Karel DSL and a small Calculator DSL, that training deep networks on these distributions leads to improved cross-distribution generalization performance."
    },
    "keywords": [
        {
            "term": "input output",
            "url": "https://en.wikipedia.org/wiki/input_output"
        },
        {
            "term": "domain specific language",
            "url": "https://en.wikipedia.org/wiki/domain_specific_language"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "semantics",
            "url": "https://en.wikipedia.org/wiki/semantics"
        },
        {
            "term": "program synthesis",
            "url": "https://en.wikipedia.org/wiki/program_synthesis"
        },
        {
            "term": "programming language",
            "url": "https://en.wikipedia.org/wiki/programming_language"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "domain-specific language",
            "url": "https://en.wikipedia.org/wiki/domain-specific_language"
        },
        {
            "term": "artificial intelligence",
            "url": "https://en.wikipedia.org/wiki/artificial_intelligence"
        }
    ],
    "abbreviations": {
        "DSLs": "domain-specific languages",
        "DSL": "domain-specific language",
        "DCFG": "direct CFG sampling",
        "T2T": "tensor2tensor sampling",
        "BAL": "balanced sampling"
    },
    "highlights": [
        "Program synthesis is one of the central problems in Artificial Intelligence studied from the early days (<a class=\"ref-link\" id=\"cManna_1971_a\" href=\"#rManna_1971_a\">Manna & Waldinger, 1971</a>; Waldinger & Lee, 1969) and has seen a lot of recent interest in the machine learning and programming languages community (<a class=\"ref-link\" id=\"cAlur_et+al_2013_a\" href=\"#rAlur_et+al_2013_a\">Alur et al, 2013</a>; <a class=\"ref-link\" id=\"cGulwani_et+al_2012_a\" href=\"#rGulwani_et+al_2012_a\">Gulwani et al, 2012</a>; 2015; Muggleton, 1991; <a class=\"ref-link\" id=\"cLin_et+al_2014_a\" href=\"#rLin_et+al_2014_a\">Lin et al, 2014</a>; Solar-Lezama, 2013)",
        "Program induction aims to embed the semantics of a particular algorithm into a differentiable model trained end-to-end, whereas the goal of program synthesis is for a model to learn the semantics of a domain-specific language (DSL) and produce programs defined by corresponding specifications",
        "We propose 4 distributions for calculator tasks: direct CFG sampling (DCFG), tensor2tensor sampling (T2T), \u201cruns\u201d CFG sampling (RCFG), and balanced sampling (BAL)",
        "We note that the Calculator domain is much simpler than Karel when considering both input complexity and output complexity",
        "Deep networks exhibit an increase in cross-distribution test accuracy, at the expense of a minor decrease in on-distribution test accuracy"
    ],
    "key_statements": [
        "Program synthesis is one of the central problems in Artificial Intelligence studied from the early days (<a class=\"ref-link\" id=\"cManna_1971_a\" href=\"#rManna_1971_a\">Manna & Waldinger, 1971</a>; Waldinger & Lee, 1969) and has seen a lot of recent interest in the machine learning and programming languages community (<a class=\"ref-link\" id=\"cAlur_et+al_2013_a\" href=\"#rAlur_et+al_2013_a\">Alur et al, 2013</a>; <a class=\"ref-link\" id=\"cGulwani_et+al_2012_a\" href=\"#rGulwani_et+al_2012_a\">Gulwani et al, 2012</a>; 2015; Muggleton, 1991; <a class=\"ref-link\" id=\"cLin_et+al_2014_a\" href=\"#rLin_et+al_2014_a\">Lin et al, 2014</a>; Solar-Lezama, 2013)",
        "The recent neural approaches can be broadly classified into two categories: program induction and program synthesis",
        "Program induction aims to embed the semantics of a particular algorithm into a differentiable model trained end-to-end, whereas the goal of program synthesis is for a model to learn the semantics of a domain-specific language (DSL) and produce programs defined by corresponding specifications",
        "We identify many distributions of input examples and domain-specific languages programs for which the Karel synthesis model performs poorly",
        "Our methodology involves defining the distribution over domain-specific languages programs and input space using a set of random variables to encode much of the valuable features which describe the data, e.g. in the Karel domain, the amount of control flow nesting in programs or the number of markers present in the inputs",
        "We propose 4 distributions for calculator tasks: direct CFG sampling (DCFG), tensor2tensor sampling (T2T), \u201cruns\u201d CFG sampling (RCFG), and balanced sampling (BAL)",
        "We evaluated each model on a fresh evaluation set sampled from a mixture of the four unhomogenized distributions (T2T, direct CFG sampling, RCFG, balanced sampling).\n6.3",
        "We note that the Calculator domain is much simpler than Karel when considering both input complexity and output complexity",
        "We propose a robust strategy for controlling and evaluating the bias of synthetic data distributions over programs and specifications by defining certain random variables that capture desired features of the program and input spaces, such as the number of parentheses in a calculator expression, and manipulating their distributions",
        "Deep networks exhibit an increase in cross-distribution test accuracy, at the expense of a minor decrease in on-distribution test accuracy",
        "We believe this methodology would lead to more rigorous evaluation of the synthesis techniques and aid them in learning better models that generalize well",
        "There is a natural question of whether the methods developed in this paper will improve out-of-distribution generalization on applications other than program synthesis which use synthetic data\u2014for example, a convolutional neural network that receives renderings of a virtual environment, for the robotics and vision domains mentioned in Section 2.1"
    ],
    "summary": [
        "Program synthesis is one of the central problems in Artificial Intelligence studied from the early days (<a class=\"ref-link\" id=\"cManna_1971_a\" href=\"#rManna_1971_a\"><a class=\"ref-link\" id=\"cManna_1971_a\" href=\"#rManna_1971_a\">Manna & Waldinger, 1971</a></a>; Waldinger & Lee, 1969) and has seen a lot of recent interest in the machine learning and programming languages community (<a class=\"ref-link\" id=\"cAlur_et+al_2013_a\" href=\"#rAlur_et+al_2013_a\"><a class=\"ref-link\" id=\"cAlur_et+al_2013_a\" href=\"#rAlur_et+al_2013_a\">Alur et al, 2013</a></a>; <a class=\"ref-link\" id=\"cGulwani_et+al_2012_a\" href=\"#rGulwani_et+al_2012_a\"><a class=\"ref-link\" id=\"cGulwani_et+al_2012_a\" href=\"#rGulwani_et+al_2012_a\">Gulwani et al, 2012</a></a>; 2015; Muggleton, 1991; <a class=\"ref-link\" id=\"cLin_et+al_2014_a\" href=\"#rLin_et+al_2014_a\"><a class=\"ref-link\" id=\"cLin_et+al_2014_a\" href=\"#rLin_et+al_2014_a\">Lin et al, 2014</a></a>; Solar-Lezama, 2013).",
        "We identify many distributions of input examples and DSL programs for which the Karel synthesis model performs poorly.",
        "Our methodology involves defining the distribution over DSL programs and input space using a set of random variables to encode much of the valuable features which describe the data, e.g. in the Karel domain, the amount of control flow nesting in programs or the number of markers present in the inputs.",
        "We propose a new methodology to generate different desirable distributions over the space of datasets for program induction and synthesis tasks.",
        "By imposing a more uniform distribution over the salient random variables when generating the I/O specifications and target programs which make up the test set, we observe much lower accuracies of the previous Karel synthesis models (<a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al, 2018</a>) compared to the original test set.",
        "To make the \u201cnumber of grids\u201d salient variable uniform, we modify the training procedure by uniformly sampling a number between 1 and 5 for each mini-batch, and using that many I/O grids to specify the program to the model.",
        "The real-world dataset\u2019s I/O distribution was similar to the MSR training datset in terms of the salient random variables we homogenized, so it is unsurprising that the baseline model was able to outperform the uniform model, consistent with the results on the MSR test set in Figure 2.",
        "We introduce uniformity into the length of action-only programs by synthesizing 20,000 programs of each length 1 to 20, by uniformly selecting tokens from the five action choices and generating I/O with other salient random variables as close to the original training set as possible; we append these new programs to the original dataset to train a new model.",
        "We demonstrate that existing sampling methods for randomly generating input-output examples have unintended and overlooked distribution flaws in both the Calculator and the Karel domain.",
        "We propose a robust strategy for controlling and evaluating the bias of synthetic data distributions over programs and specifications by defining certain random variables that capture desired features of the program and input spaces, such as the number of parentheses in a calculator expression, and manipulating their distributions.",
        "There is a natural question of whether the methods developed in this paper will improve out-of-distribution generalization on applications other than program synthesis which use synthetic data\u2014for example, a convolutional neural network that receives renderings of a virtual environment, for the robotics and vision domains mentioned in Section 2.1.",
        "We believe this methodology would lead to more rigorous evaluation of the synthesis techniques and aid them in learning better models that generalize well"
    ],
    "headline": "We empirically discover that applying test input generation techniques for languages with control flow and rich input space causes deep networks to generalize poorly to certain data distributions; to correct this, we propose a new methodology for controlling and evaluating the bias of synthetic data distributions over both programs and specifications",
    "reference_links": [
        {
            "id": "Alur_et+al_2013_a",
            "entry": "Rajeev Alur, Rastislav Bod\u00edk, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntaxguided synthesis. In Formal Methods in Computer-Aided Design, FMCAD 2013, Portland, OR, USA, October 20-23, 2013, pp. 1\u20138, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alur%2C%20Rajeev%20Bod%C3%ADk%2C%20Rastislav%20Juniwal%2C%20Garvit%20Martin%2C%20Milo%20M.K.%20Syntaxguided%20synthesis%202013-10-20",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alur%2C%20Rajeev%20Bod%C3%ADk%2C%20Rastislav%20Juniwal%2C%20Garvit%20Martin%2C%20Milo%20M.K.%20Syntaxguided%20synthesis%202013-10-20"
        },
        {
            "id": "Balog_et+al_2016_a",
            "entry": "Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01989"
        },
        {
            "id": "Bhupatiraju_et+al_2017_a",
            "entry": "Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Deep API programmer: Learning to program with apis. CoRR, abs/1704.04327, 2017. URL http://arxiv.org/abs/1704.04327.",
            "url": "http://arxiv.org/abs/1704.04327",
            "arxiv_url": "https://arxiv.org/pdf/1704.04327"
        },
        {
            "id": "Bousmalis_et+al_2017_a",
            "entry": "Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, and Vincent Vanhoucke. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. CoRR, abs/1709.07857, 2017. URL http://arxiv.org/abs/1709.07857.",
            "url": "http://arxiv.org/abs/1709.07857",
            "arxiv_url": "https://arxiv.org/pdf/1709.07857"
        },
        {
            "id": "Bunel_et+al_2018_a",
            "entry": "Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunel%2C%20Rudy%20Hausknecht%2C%20Matthew%20Devlin%2C%20Jacob%20Singh%2C%20Rishabh%20Leveraging%20grammar%20and%20reinforcement%20learning%20for%20neural%20program%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunel%2C%20Rudy%20Hausknecht%2C%20Matthew%20Devlin%2C%20Jacob%20Singh%2C%20Rishabh%20Leveraging%20grammar%20and%20reinforcement%20learning%20for%20neural%20program%20synthesis%202018"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. CoRR, abs/1704.06611, 2017. URL http://arxiv.org/abs/1704.06611.",
            "url": "http://arxiv.org/abs/1704.06611",
            "arxiv_url": "https://arxiv.org/pdf/1704.06611"
        },
        {
            "id": "Christiano_et+al_2016_a",
            "entry": "Paul F. Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep inverse dynamics model. CoRR, abs/1610.03518, 2016. URL http://arxiv.org/abs/1610.03518.",
            "url": "http://arxiv.org/abs/1610.03518",
            "arxiv_url": "https://arxiv.org/pdf/1610.03518"
        },
        {
            "id": "Devlin_et+al_2017_a",
            "entry": "Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In ICML, pp. 990\u2013998, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017"
        },
        {
            "id": "Gaunt_et+al_2016_a",
            "entry": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. CoRR, abs/1608.04428, 2016. URL http://arxiv.org/abs/1608.04428.",
            "url": "http://arxiv.org/abs/1608.04428",
            "arxiv_url": "https://arxiv.org/pdf/1608.04428"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Gulwani_et+al_2012_a",
            "entry": "Sumit Gulwani, William R. Harris, and Rishabh Singh. Spreadsheet data manipulation using examples. Commun. ACM, 55(8):97\u2013105, 2012. doi: 10.1145/2240236.2240260. URL http://doi.acm.org/10.1145/2240236.2240260.",
            "crossref": "https://dx.doi.org/10.1145/2240236.2240260",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2240236.2240260"
        },
        {
            "id": "Gulwani_et+al_2015_a",
            "entry": "Sumit Gulwani, Jos\u00e9 Hern\u00e1ndez-Orallo, Emanuel Kitzelmann, Stephen H. Muggleton, Ute Schmid, and Benjamin G. Zorn. Inductive programming meets the real world. Commun. ACM, 58(11): 90\u201399, 2015. doi: 10.1145/2736282. URL http://doi.acm.org/10.1145/2736282.",
            "crossref": "https://dx.doi.org/10.1145/2736282",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2736282"
        },
        {
            "id": "Joulin_2015_a",
            "entry": "Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In NIPS, pp. 190\u2013198, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015"
        },
        {
            "id": "Kaiser_2015_a",
            "entry": "Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. CoRR, abs/1511.08228, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08228"
        },
        {
            "id": "Kurach_et+al_2016_a",
            "entry": "Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurach%2C%20Karol%20Andrychowicz%2C%20Marcin%20Sutskever%2C%20Ilya%20Neural%20random-access%20machines%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurach%2C%20Karol%20Andrychowicz%2C%20Marcin%20Sutskever%2C%20Ilya%20Neural%20random-access%20machines%202016"
        },
        {
            "id": "Lin_et+al_2014_a",
            "entry": "Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B. Tenenbaum, and Stephen Muggleton. Bias reformulation for one-shot function induction. In ECAI 2014 - 21st European Conference on Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications of Intelligent Systems (PAIS 2014), pp. 525\u2013530, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dianhuan%20Lin%20Eyal%20Dechter%20Kevin%20Ellis%20Joshua%20B%20Tenenbaum%20and%20Stephen%20Muggleton%20Bias%20reformulation%20for%20oneshot%20function%20induction%20In%20ECAI%202014%20%2021st%20European%20Conference%20on%20Artificial%20Intelligence%201822%20August%202014%20Prague%20Czech%20Republic%20%20Including%20Prestigious%20Applications%20of%20Intelligent%20Systems%20PAIS%202014%20pp%20525530%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dianhuan%20Lin%20Eyal%20Dechter%20Kevin%20Ellis%20Joshua%20B%20Tenenbaum%20and%20Stephen%20Muggleton%20Bias%20reformulation%20for%20oneshot%20function%20induction%20In%20ECAI%202014%20%2021st%20European%20Conference%20on%20Artificial%20Intelligence%201822%20August%202014%20Prague%20Czech%20Republic%20%20Including%20Prestigious%20Applications%20of%20Intelligent%20Systems%20PAIS%202014%20pp%20525530%202014"
        },
        {
            "id": "Manna_1971_a",
            "entry": "Zohar Manna and Richard J. Waldinger. Toward automatic program synthesis. Commun. ACM, 14 (3):151\u2013165, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manna%2C%20Zohar%20Waldinger%2C%20Richard%20J.%20Toward%20automatic%20program%20synthesis.%20Commun%201971"
        }
    ]
}
