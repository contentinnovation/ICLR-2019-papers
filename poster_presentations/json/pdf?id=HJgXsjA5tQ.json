{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON THE LOSS LANDSCAPE OF A CLASS OF DEEP",
        "author": "NEURAL NETWORKS WITH NO BAD LOCAL VALLEYS",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJgXsjA5tQ"
        },
        "abstract": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "analytic",
            "url": "https://en.wikipedia.org/wiki/analytic"
        }
    ],
    "abbreviations": {
        "SGD": "stochastic gradient descent"
    },
    "highlights": [
        "It has been empirically observed in deep learning (<a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\"><a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\">Dauphin et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a></a>) that the training problem of over-parameterized1 deep CNNs (<a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\"><a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\">LeCun et al, 1990</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) does not seem to have a problem with bad local minima",
        "A possible hypothesis is that the loss landscape of these networks is\u201cwellbehaved\u201d so that it becomes amenable to local search algorithms like stochastic gradient descent and its variants",
        "We show in experiments that despite achieving zero training error, the aforementioned class of neural networks generalize well in practice when trained with stochastic gradient descent whereas an alternative training procedure guaranteed to achieve zero training error has significantly worse generalization performance and is overfitting",
        "For the sigmoid activation function the skip connections allow for all models except Densenet121 to get reasonable performance whereas training the original model fails. This effect can be directly related to our result of Theorem 3.4 that the loss landscape of skip-networks has no bad local valley and it is not difficult to reach a solution with zero training error",
        "We have identified a class of deep neural networks whose loss landscape has no bad local valleys",
        "While our networks are over-parameterized and can achieve zero training error, they generalize well in practice when trained with stochastic gradient descent"
    ],
    "key_statements": [
        "It has been empirically observed in deep learning (<a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\"><a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\">Dauphin et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a></a>) that the training problem of over-parameterized1 deep CNNs (<a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\"><a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\">LeCun et al, 1990</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) does not seem to have a problem with bad local minima",
        "A possible hypothesis is that the loss landscape of these networks is\u201cwellbehaved\u201d so that it becomes amenable to local search algorithms like stochastic gradient descent and its variants",
        "We identify a family of deep networks with skip connections to the output layer whose loss landscape has no bad local valleys",
        "We show in experiments that despite achieving zero training error, the aforementioned class of neural networks generalize well in practice when trained with stochastic gradient descent whereas an alternative training procedure guaranteed to achieve zero training error has significantly worse generalization performance and is overfitting",
        "While we conjecture that the result of Lemma 3.2 holds for softplus activation function without the additional condition as mentioned in Assumption 3.1, the proof of this is considerably harder for such a general class of neural networks since one has to control the output of neurons with skip connection from different layers which depend on each other",
        "For the sigmoid activation function the skip connections allow for all models except Densenet121 to get reasonable performance whereas training the original model fails. This effect can be directly related to our result of Theorem 3.4 that the loss landscape of skip-networks has no bad local valley and it is not difficult to reach a solution with zero training error",
        "We have identified a class of deep neural networks whose loss landscape has no bad local valleys",
        "While our networks are over-parameterized and can achieve zero training error, they generalize well in practice when trained with stochastic gradient descent"
    ],
    "summary": [
        "It has been empirically observed in deep learning (<a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\"><a class=\"ref-link\" id=\"cDauphin_et+al_2014_a\" href=\"#rDauphin_et+al_2014_a\">Dauphin et al, 2014</a></a>; <a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al, 2015</a></a>) that the training problem of over-parameterized1 deep CNNs (<a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\"><a class=\"ref-link\" id=\"cLecun_et+al_1990_a\" href=\"#rLecun_et+al_1990_a\">LeCun et al, 1990</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) does not seem to have a problem with bad local minima.",
        "Note that the M neurons which are directly connected to the output units can lie on different hidden layers in the network.",
        "Note that by our construction the value of unit qj is distinct at different training samples, and it follows from the strict monotonic property of activation functions from Assumption 3.1 and the positivity of \u03b1c, .",
        "While we conjecture that the result of Lemma 3.2 holds for softplus activation function without the additional condition as mentioned in Assumption 3.1, the proof of this is considerably harder for such a general class of neural networks since one has to control the output of neurons with skip connection from different layers which depend on each other.",
        "As existing network architectures have a large number of feature maps per layer, the total number of neurons is often very large compared to number of training samples, it is easy to choose from there a subset of N neurons to connect to the output.",
        "This effect can be directly related to our result of Theorem 3.4 that the loss landscape of skip-networks has no bad local valley and it is not difficult to reach a solution with zero training error.",
        "The second interesting observation is that we do not see any sign of overfitting for the SGD version even though we have increased for all models the number of parameters by adding skip connections to the output layer and we know from Theorem 3.4 that for all the skip-models one can achieve zero training error.",
        "Our results confirm that there is an implicit bias as we see a strong contrast to the results obtained by using the network as a random feature generator and just fitting the connections to the output units (i.e. V ) which leads to solutions with zero training error with probability 1 as shown in Lemma 3.2 and the proof of Theorem 3.4.",
        "An interesting recent exception is <a class=\"ref-link\" id=\"cLiang_et+al_2018_a\" href=\"#rLiang_et+al_2018_a\">Liang et al (2018a</a>) where they show that for binary classification one neuron with a skip-connection to the output layer and exponential activation function is enough to eliminate all bad local minima under mild conditions on the loss function.",
        "More closely related in terms of the setting are (<a class=\"ref-link\" id=\"cNguyen_2017_a\" href=\"#rNguyen_2017_a\">Nguyen & Hein, 2017</a>; 2018) where they study the loss surface of fully connected and convolutional networks if one of the layers has more neurons than the number of training samples for the standard multi-class problem."
    ],
    "headline": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero",
    "reference_links": [
        {
            "id": "Andoni_et+al_2014_a",
            "entry": "A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks. ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andoni%2C%20A.%20Panigrahy%2C%20R.%20Valiant%2C%20G.%20Zhang%2C%20L.%20Learning%20polynomials%20with%20neural%20networks%202014"
        },
        {
            "id": "Apostol_1974_a",
            "entry": "T. M. Apostol. Mathematical analysis. Addison Wesley, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Apostol%2C%20T.M.%20Mathematical%20analysis%201974"
        },
        {
            "id": "Auer_et+al_1996_a",
            "entry": "P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single neurons. NIPS, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Herbster%2C%20M.%20Warmuth%2C%20M.K.%20Exponentially%20many%20local%20minima%20for%20single%20neurons%201996"
        },
        {
            "id": "Blum_1989_a",
            "entry": "A. Blum and R. L Rivest. Training a 3-node neural network is np-complete. NIPS, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Rivest%2C%20R.L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Rivest%2C%20R.L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Globally%20optimal%20gradient%20descent%20for%20a%20convnet%20with%20gaussian%20inputs%202017"
        },
        {
            "id": "Brutzkus_et+al_2018_a",
            "entry": "A. Brutzkus, A. Globerson, E. Malach, and S. Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "A. Choromanska, M. Hena, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Hena%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Hena%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Dauphin_et+al_2014_a",
            "entry": "Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "Du_et+al_2018_a",
            "entry": "S. Du, J. Lee, Y. Tian, A. Singh, and B. P\u00f3czos. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20S.%20Lee%2C%20J.%20Tian%2C%20Y.%20Singh%2C%20A.%20Gradient%20descent%20learns%20one-hidden-layer%20cnn%3A%20Don%E2%80%99t%20be%20afraid%20of%20spurious%20local%20minima%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20S.%20Lee%2C%20J.%20Tian%2C%20Y.%20Singh%2C%20A.%20Gradient%20descent%20learns%20one-hidden-layer%20cnn%3A%20Don%E2%80%99t%20be%20afraid%20of%20spurious%20local%20minima%202018"
        },
        {
            "id": "Gautier_et+al_2016_a",
            "entry": "A. Gautier, Q. Nguyen, and M. Hein. Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gautier%2C%20A.%20Nguyen%2C%20Q.%20Hein%2C%20M.%20Globally%20optimal%20training%20of%20generalized%20polynomial%20neural%20networks%20with%20nonlinear%20spectral%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gautier%2C%20A.%20Nguyen%2C%20Q.%20Hein%2C%20M.%20Globally%20optimal%20training%20of%20generalized%20polynomial%20neural%20networks%20with%20nonlinear%20spectral%20methods%202016"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Vinyals%2C%20O.%20Saxe%2C%20A.M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Vinyals%2C%20O.%20Saxe%2C%20A.M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015"
        },
        {
            "id": "Haeffele_2017_a",
            "entry": "B. D. Haeffele and R. Vidal. Global optimality in neural network training. CVPR, 2017. M. Hardt and T. Ma. Identity matters in deep learning. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20B.D.%20Vidal%2C%20R.%20Global%20optimality%20in%20neural%20network%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20B.D.%20Vidal%2C%20R.%20Global%20optimality%20in%20neural%20network%20training%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "G. Huang, Z. Liu, L. Maaten, and K. Weinberger. Densely connected convolutional networks. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20Maaten%2C%20L.%20Weinberger%2C%20K.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20Maaten%2C%20L.%20Weinberger%2C%20K.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Janzamin_et+al_2016_a",
            "entry": "M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv:1506.08473, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08473"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "K. Kawaguchi. Deep learning without poor local minima. NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_1990_a",
            "entry": "Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Handwritten digit recognition with a back-propagation network. NIPS, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Boser%2C%20B.%20Denker%2C%20J.S.%20Henderson%2C%20D.%20Handwritten%20digit%20recognition%20with%20a%20back-propagation%20network%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Boser%2C%20B.%20Denker%2C%20J.S.%20Henderson%2C%20D.%20Handwritten%20digit%20recognition%20with%20a%20back-propagation%20network%201990"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets. In ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20H.%20Xu%2C%20Z.%20Taylor%2C%20G.%20Studer%2C%20C.%20Visualizing%20the%20loss%20landscape%20of%20neural%20nets%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20H.%20Xu%2C%20Z.%20Taylor%2C%20G.%20Studer%2C%20C.%20Visualizing%20the%20loss%20landscape%20of%20neural%20nets%202018"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate all bad local minima. arXiv:1805.08671, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08671"
        },
        {
            "id": "Liang_et+al_2018_b",
            "entry": "S. Liang, R. Sun, Y. Li, and R. Srikant. Understanding the loss surface of neural networks for binary classification. In ICML, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20S.%20Sun%2C%20R.%20Li%2C%20Y.%20Srikant%2C%20R.%20Understanding%20the%20loss%20surface%20of%20neural%20networks%20for%20binary%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20S.%20Sun%2C%20R.%20Li%2C%20Y.%20Srikant%2C%20R.%20Understanding%20the%20loss%20surface%20of%20neural%20networks%20for%20binary%20classification%202018"
        },
        {
            "id": "Livni_et+al_2014_a",
            "entry": "R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural networks. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20R.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20R.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "Lu_1702_a",
            "entry": "H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv:1702.08580, 2017. B. Mityagin. The zero set of a real analytic function. arXiv:1512.07276, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08580"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. ICML, 2017. Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep cnns. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Q.%20Hein%2C%20M.%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Q.%20Hein%2C%20M.%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "Nguyen_0000_a",
            "entry": "V. D. Nguyen. Complex powers of analytic functions and meromorphic renormalization in qft. arXiv:1503.00995, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.00995"
        },
        {
            "id": "Nouiehed_2018_a",
            "entry": "M. Nouiehed and M. Razaviyayn. Learning deep models: Critical points and local openness. ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nouiehed%2C%20M.%20Razaviyayn%2C%20M.%20Learning%20deep%20models%3A%20Critical%20points%20and%20local%20openness%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nouiehed%2C%20M.%20Razaviyayn%2C%20M.%20Learning%20deep%20models%3A%20Critical%20points%20and%20local%20openness%202018"
        },
        {
            "id": "Safran_2016_a",
            "entry": "I. Safran and O. Shamir. On the quality of the initial basin in overspecified networks. ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20I.%20Shamir%2C%20O.%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20I.%20Shamir%2C%20O.%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20networks%202016"
        },
        {
            "id": "Safran_2018_a",
            "entry": "I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20I.%20Shamir%2C%20O.%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20I.%20Shamir%2C%20O.%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018"
        },
        {
            "id": "Sedghi_2015_a",
            "entry": "H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectivity. ICLR Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sedghi%2C%20H.%20Anandkumar%2C%20A.%20Provable%20methods%20for%20training%20neural%20networks%20with%20sparse%20connectivity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sedghi%2C%20H.%20Anandkumar%2C%20A.%20Provable%20methods%20for%20training%20neural%20networks%20with%20sparse%20connectivity%202015"
        },
        {
            "id": "Shalev-Shwartz_et+al_2017_a",
            "entry": "S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of gradient-based deep learning. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20Shammah%2C%20S.%20Failures%20of%20gradient-based%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20Shammah%2C%20S.%20Failures%20of%20gradient-based%20deep%20learning%202017"
        },
        {
            "id": "Sima_2002_a",
            "entry": "J. Sima. Training a single sigmoidal neuron is hard. Neural Computation, 14:2709\u20132728, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sima%2C%20J.%20Training%20a%20single%20sigmoidal%20neuron%20is%20hard%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sima%2C%20J.%20Training%20a%20single%20sigmoidal%20neuron%20is%20hard%202002"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Soltanolkotabi_2017_a",
            "entry": "M. Soltanolkotabi. Learning relus via gradient descent. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20M.%20Learning%20relus%20via%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20M.%20Learning%20relus%20via%20gradient%20descent%202017"
        },
        {
            "id": "Soudry_2017_a",
            "entry": "D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. ICLR Workshop 2018, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20D.%20Hoffer%2C%20E.%20Exponentially%20vanishing%20sub-optimal%20local%20minima%20in%20multilayer%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20D.%20Hoffer%2C%20E.%20Exponentially%20vanishing%20sub-optimal%20local%20minima%20in%20multilayer%20neural%20networks%202017"
        },
        {
            "id": "Soudry_et+al_2018_a",
            "entry": "D. Soudry, E. Hoffer, M. S. Nacson, and N. Srebro. The implicit bias of gradient descent on separable data. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20D.%20Hoffer%2C%20E.%20Nacson%2C%20M.S.%20Srebro%2C%20N.%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20D.%20Hoffer%2C%20E.%20Nacson%2C%20M.S.%20Srebro%2C%20N.%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Y.%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20Y.%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017"
        },
        {
            "id": "Venturi_et+al_2018_a",
            "entry": "L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in two-layer neural network optimization landscapes. arXiv:1802.06384, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06384"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "G. Wang, G. B. Giannakis, and J. Chen. Learning relu networks on linearly separable data: Algorithm, optimality, and generalization. arXiv:1808.04685, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04685"
        },
        {
            "id": "Yu_1995_a",
            "entry": "X. Yu and G. Chen. On the local minima free condition of backpropagation learning. IEEE Transaction on Neural Networks, 6:1300\u20131303, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20X.%20Chen%2C%20G.%20On%20the%20local%20minima%20free%20condition%20of%20backpropagation%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20X.%20Chen%2C%20G.%20On%20the%20local%20minima%20free%20condition%20of%20backpropagation%20learning%201995"
        },
        {
            "id": "Yun_et+al_2017_a",
            "entry": "C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yun%2C%20C.%20Sra%2C%20S.%20Jadbabaie%2C%20A.%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yun%2C%20C.%20Sra%2C%20S.%20Jadbabaie%2C%20A.%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "S. Zagoruyko and N. Komodakis. Wide residual networks. BMCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "C. Zhang, S. Bengio, M. Hardt, B. Recht, and Oriol Vinyals. Understanding deep learning requires re-thinking generalization. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20re-thinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20re-thinking%20generalization%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "H. Zhang, J. Shao, and R. Salakhutdinov. Deep neural networks with multi-branch architectures are less non-convex. arXiv:1806.01845, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01845"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "K. Zhong, Z. Song, P. Jain, P. Bartlett, and I. Dhillon. Recovery guarantees for one-hidden-layer neural networks. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20K.%20Song%2C%20Z.%20Jain%2C%20P.%20Bartlett%2C%20P.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20K.%20Song%2C%20Z.%20Jain%2C%20P.%20Bartlett%2C%20P.%20Recovery%20guarantees%20for%20one-hidden-layer%20neural%20networks%202017"
        },
        {
            "id": "We_1974_a",
            "entry": "We recall the following standard result from topology (see e.g. Apostol (1974), Theorem 4.23, p. 82), which is used in the proof of Theorem 3.4. Proposition A.2 Let f: Rm \u2192 Rn be a continuous function. If U \u2286 Rn is an open set then f \u22121(U ) is also open.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20recall%20the%20following%20standard%20result%20from%20topology%20see%20eg%20Apostol%201974%20Theorem%20423%20p%2082%20which%20is%20used%20in%20the%20proof%20of%20Theorem%2034%20Proposition%20A2%20Let%20f%20Rm%20%20Rn%20be%20a%20continuous%20function%20If%20U%20%20Rn%20is%20an%20open%20set%20then%20f%201U%20%20is%20also%20open",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20recall%20the%20following%20standard%20result%20from%20topology%20see%20eg%20Apostol%201974%20Theorem%20423%20p%2082%20which%20is%20used%20in%20the%20proof%20of%20Theorem%2034%20Proposition%20A2%20Let%20f%20Rm%20%20Rn%20be%20a%20continuous%20function%20If%20U%20%20Rn%20is%20an%20open%20set%20then%20f%201U%20%20is%20also%20open"
        }
    ]
}
