{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "DEEP DECODER: CONCISE IMAGE REPRESENTATIONS FROM UNTRAINED NON-CONVOLUTIONAL NETWORKS",
        "author": "Reinhard Heckel Department of Electrical and Computer Engineering Rice University rh,@rice.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rylV-2C9KQ"
        },
        "abstract": "Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, imagegenerating deep neural networks have a large number of parameters\u2014typically a multiple of their output dimension\u2014and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."
    },
    "keywords": [
        {
            "term": "super resolution",
            "url": "https://en.wikipedia.org/wiki/super_resolution"
        },
        {
            "term": "inverse problem",
            "url": "https://en.wikipedia.org/wiki/inverse_problem"
        },
        {
            "term": "Deep Image Prior",
            "url": "https://en.wikipedia.org/wiki/Deep_Image_Prior"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "image compression",
            "url": "https://en.wikipedia.org/wiki/image_compression"
        }
    ],
    "abbreviations": {
        "DIP": "Deep Image Prior",
        "ReLUs": "rectified linear units"
    },
    "highlights": [
        "Data models are central for signal and image processing and play a key role in compression and inverse problems such as denoising, super-resolution, and compressive sensing",
        "It has been demonstrated that for a wide range of imaging problems, from compression to denoising, deep neural networks trained on large datasets can often outperform methods based on traditional image models (<a class=\"ref-link\" id=\"cToderici_et+al_2016_a\" href=\"#rToderici_et+al_2016_a\">Toderici et al, 2016</a>; <a class=\"ref-link\" id=\"cAgustsson_et+al_2017_a\" href=\"#rAgustsson_et+al_2017_a\">Agustsson et al, 2017</a>; <a class=\"ref-link\" id=\"cTheis_et+al_2017_a\" href=\"#rTheis_et+al_2017_a\">Theis et al, 2017</a>; <a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\">Burger et al, 2012</a>; <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>)",
        "We propose a simple image model in the form of a deep neural network that can represent natural images well while using very few parameters",
        "In the previous sections we empirically showed that the deep decoder can represent images well and at the same time cannot fit noise well",
        "We formally show that the deep decoder can only fit a small proportion of the noise, relative to the degree of underparameterization",
        "We provide insights into how the components of the deep decoder contribute to representing natural images well, and we provide empirical observations on the sensitivity of the parameters and their distribution.\n6.1"
    ],
    "key_statements": [
        "Data models are central for signal and image processing and play a key role in compression and inverse problems such as denoising, super-resolution, and compressive sensing",
        "It has been demonstrated that for a wide range of imaging problems, from compression to denoising, deep neural networks trained on large datasets can often outperform methods based on traditional image models (<a class=\"ref-link\" id=\"cToderici_et+al_2016_a\" href=\"#rToderici_et+al_2016_a\">Toderici et al, 2016</a>; <a class=\"ref-link\" id=\"cAgustsson_et+al_2017_a\" href=\"#rAgustsson_et+al_2017_a\">Agustsson et al, 2017</a>; <a class=\"ref-link\" id=\"cTheis_et+al_2017_a\" href=\"#rTheis_et+al_2017_a\">Theis et al, 2017</a>; <a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\">Burger et al, 2012</a>; <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>)",
        "Three common features of the recent success stories of using deep neural network for imaging related tasks are i) that the corresponding networks are over-parameterized, ii) that the networks have a convolutional structure, and perhaps most importantly, iii) that the networks are trained on large datasets",
        "We propose a simple image model in the form of a deep neural network that can represent natural images well while using very few parameters",
        "The main point of this experiment is to demonstrate that the deep decoder is a good image model, which enables applications like solving inverse problems, as in Section 4",
        "We show that image representations of the deep decoder are not sensitive to perturbations of its coefficients, quantization does not have a detrimental effect on the image quality",
        "In the previous sections we empirically showed that the deep decoder can represent images well and at the same time cannot fit noise well",
        "We formally show that the deep decoder can only fit a small proportion of the noise, relative to the degree of underparameterization",
        "We provide insights into how the components of the deep decoder contribute to representing natural images well, and we provide empirical observations on the sensitivity of the parameters and their distribution.\n6.1",
        "We start by showing that an under-parameterized deep decoder can only fit a proportion of the noise relative to the degree of underparameterization"
    ],
    "summary": [
        "Data models are central for signal and image processing and play a key role in compression and inverse problems such as denoising, super-resolution, and compressive sensing.",
        "We propose a simple image model in the form of a deep neural network that can represent natural images well while using very few parameters.",
        "We prove that the deep decoder can only fit a small proportion of noise, which, combined with the empirical observation that it can represent natural images well, explains its denoising performance.",
        "The main point of this experiment is to demonstrate that the deep decoder is a good image model, which enables applications like solving inverse problems, as in Section 4.",
        "We use the deep decoder as a structure-enforcing model or regularizers for solving standard inverse problems: denoising, super-resolution, and inpainting.",
        "We remark that fitting an image model to observations in order to solve an inverse problem is a standard approach and is not specific to the deep decoder or deep-network-based models in general.",
        "The larger k, the larger the number of latent parameters and the smaller the representation error, i.e., the error that the deep decoder makes when representing a noise-free image.",
        "Deep learning based methods are either trained end-to-end for tasks ranging from compression (<a class=\"ref-link\" id=\"cToderici_et+al_2016_a\" href=\"#rToderici_et+al_2016_a\">Toderici et al, 2016</a>; <a class=\"ref-link\" id=\"cAgustsson_et+al_2017_a\" href=\"#rAgustsson_et+al_2017_a\">Agustsson et al, 2017</a>; <a class=\"ref-link\" id=\"cTheis_et+al_2017_a\" href=\"#rTheis_et+al_2017_a\">Theis et al, 2017</a>; <a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\"><a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\">Burger et al, 2012</a></a>; <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>) to denoising (<a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\"><a class=\"ref-link\" id=\"cBurger_et+al_2012_a\" href=\"#rBurger_et+al_2012_a\">Burger et al, 2012</a></a>; <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>), or are based on learning a generative image model) and using the resulting model to solve inverse problems such as compressed sensing (<a class=\"ref-link\" id=\"cBora_et+al_2017_a\" href=\"#rBora_et+al_2017_a\">Bora et al, 2017</a>; <a class=\"ref-link\" id=\"cHand_2018_a\" href=\"#rHand_2018_a\">Hand & Voroninski, 2018</a>), denoising (<a class=\"ref-link\" id=\"cHeckel_et+al_2018_a\" href=\"#rHeckel_et+al_2018_a\">Heckel et al, 2018</a>), phase retrieval (Hand et al, 2018; <a class=\"ref-link\" id=\"cShamshad_2018_a\" href=\"#rShamshad_2018_a\">Shamshad & Ahmed, 2018</a>), and blind deconvolution (<a class=\"ref-link\" id=\"cAsim_et+al_2018_a\" href=\"#rAsim_et+al_2018_a\">Asim et al, 2018</a>), by minimizing an associated loss.",
        "In contrast to the deep decoder, where the optimization is over the weights of the network, in all the aforementioned methods, the weights are adjusted only during training and are fixed upon solving the inverse problem.",
        "In the previous sections we empirically showed that the deep decoder can represent images well and at the same time cannot fit noise well.",
        "The proposition asserts that the deep decoder can only fit a small portion of the noise energy, precisely a proportion determined by its number of parameters relative to the output dimension, n.",
        "6.4 IMAGE GENERATION BY SUCCESSIVE APPROXIMATION The deep decoder is tasked with coverting multiple noise channels into a structured signal primarily using pixelwise linear combinations, ReLU activation funcions, and upsampling."
    ],
    "headline": "We propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters",
    "reference_links": [
        {
            "id": "Agustsson_et+al_2017_a",
            "entry": "E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli, R. Timofte, L. Benini, and L. V. Gool. Softto-hard vector quantization for end-to-end learning compressible representations. In Advances in Neural Information Processing Systems, pp. 1141\u20131151, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agustsson%2C%20E.%20Mentzer%2C%20F.%20Tschannen%2C%20M.%20Cavigelli%2C%20L.%20Softto-hard%20vector%20quantization%20for%20end-to-end%20learning%20compressible%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agustsson%2C%20E.%20Mentzer%2C%20F.%20Tschannen%2C%20M.%20Cavigelli%2C%20L.%20Softto-hard%20vector%20quantization%20for%20end-to-end%20learning%20compressible%20representations%202017"
        },
        {
            "id": "Antonini_et+al_1992_a",
            "entry": "M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies. Image coding using wavelet transform. IEEE Transactions on Image Processing, 1(2):205\u2013220, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antonini%2C%20M.%20Barlaud%2C%20M.%20Mathieu%2C%20P.%20Daubechies%2C%20I.%20Image%20coding%20using%20wavelet%20transform%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antonini%2C%20M.%20Barlaud%2C%20M.%20Mathieu%2C%20P.%20Daubechies%2C%20I.%20Image%20coding%20using%20wavelet%20transform%201992"
        },
        {
            "id": "Asim_et+al_2018_a",
            "entry": "M. Asim, F. Shamshad, and A. Ahmed. Solving bilinear inverse problems using deep generative priors. arXiv preprint arXiv:1802.04073, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04073"
        },
        {
            "id": "Bora_et+al_2017_a",
            "entry": "A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. In International Conference on Machine Learning, pp. 537\u2013546, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bora%2C%20A.%20Jalal%2C%20A.%20Price%2C%20E.%20Dimakis%2C%20A.G.%20Compressed%20sensing%20using%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bora%2C%20A.%20Jalal%2C%20A.%20Price%2C%20E.%20Dimakis%2C%20A.G.%20Compressed%20sensing%20using%20generative%20models%202017"
        },
        {
            "id": "Burger_et+al_2012_a",
            "entry": "H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with BM3d? In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2392\u20132399, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burger%2C%20H.C.%20Schuler%2C%20C.J.%20Harmeling%2C%20S.%20Image%20denoising%3A%20Can%20plain%20neural%20networks%20compete%20with%20BM3d%3F%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burger%2C%20H.C.%20Schuler%2C%20C.J.%20Harmeling%2C%20S.%20Image%20denoising%3A%20Can%20plain%20neural%20networks%20compete%20with%20BM3d%3F%202012"
        },
        {
            "id": "Dabov_et+al_2007_a",
            "entry": "K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transformdomain collaborative filtering. IEEE Transactions on Image Processing, 16(8):2080\u20132095, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dabov%2C%20K.%20Foi%2C%20A.%20Katkovnik%2C%20V.%20Egiazarian%2C%20K.%20Image%20denoising%20by%20sparse%203-D%20transformdomain%20collaborative%20filtering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dabov%2C%20K.%20Foi%2C%20A.%20Katkovnik%2C%20V.%20Egiazarian%2C%20K.%20Image%20denoising%20by%20sparse%203-D%20transformdomain%20collaborative%20filtering%202007"
        },
        {
            "id": "Donoho_1995_a",
            "entry": "D. L. Donoho. De-noising by soft-thresholding. IEEE Transactions on Information Theory, 41(3): 613\u2013627, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.L.%20De-noising%20by%20soft-thresholding%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.L.%20De-noising%20by%20soft-thresholding%201995"
        },
        {
            "id": "Glasner_et+al_2009_a",
            "entry": "D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In IEEE International Conference on Computer Vision, pp. 349\u2013356, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glasner%2C%20D.%20Bagon%2C%20S.%20Irani%2C%20M.%20Super-resolution%20from%20a%20single%20image%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glasner%2C%20D.%20Bagon%2C%20S.%20Irani%2C%20M.%20Super-resolution%20from%20a%20single%20image%202009"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672\u20132680. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Hand_2018_a",
            "entry": "P. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. In Conference on Learning Theory, pp. 970\u2013978, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hand%2C%20P.%20Voroninski%2C%20V.%20Global%20guarantees%20for%20enforcing%20deep%20generative%20priors%20by%20empirical%20risk%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hand%2C%20P.%20Voroninski%2C%20V.%20Global%20guarantees%20for%20enforcing%20deep%20generative%20priors%20by%20empirical%20risk%202018"
        },
        {
            "id": "Hand_et+al_2018_b",
            "entry": "P. Hand, O. Leong, and V. Voroninski. Phase retrieval under a generative prior. In Advances in Neural Information Processing, pp. 9136\u20139146, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hand%2C%20P.%20Leong%2C%20O.%20Voroninski%2C%20V.%20Phase%20retrieval%20under%20a%20generative%20prior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hand%2C%20P.%20Leong%2C%20O.%20Voroninski%2C%20V.%20Phase%20retrieval%20under%20a%20generative%20prior%202018"
        },
        {
            "id": "Heckel_et+al_2018_a",
            "entry": "R. Heckel, W. Huang, P. Hand, and V. Voroninski. Deep denoising: Rate-optimal recovery of structured signals with a deep prior. arXiv:1805.08855, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08855"
        },
        {
            "id": "Hinton_2006_a",
            "entry": "G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Laurent_2000_a",
            "entry": "B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302\u20131338, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20B.%20Massart%2C%20P.%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20B.%20Massart%2C%20P.%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000"
        },
        {
            "id": "Romano_et+al_2017_a",
            "entry": "Y. Romano, M. Elad, and P. Milanfar. The little engine that could: Regularization by denoising (red). SIAM Journal on Imaging Sciences, 10(4):18041844, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romano%2C%20Y.%20Elad%2C%20M.%20Milanfar%2C%20P.%20The%20little%20engine%20that%20could%3A%20Regularization%20by%20denoising%20%28red%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romano%2C%20Y.%20Elad%2C%20M.%20Milanfar%2C%20P.%20The%20little%20engine%20that%20could%3A%20Regularization%20by%20denoising%20%28red%29%202017"
        },
        {
            "id": "Shamshad_2018_a",
            "entry": "F. Shamshad and A. Ahmed. Robust compressive phase retrieval via deep generative priors. arXiv preprint arXiv:1808.05854, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05854"
        },
        {
            "id": "Starck_et+al_2002_a",
            "entry": "J.-L. Starck, E. J. Candes, and D. L. Donoho. The curvelet transform for image denoising. IEEE Transactions on Image Processing, 11(6):670\u2013684, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Starck%2C%20J.-L.%20Candes%2C%20E.J.%20Donoho%2C%20D.L.%20The%20curvelet%20transform%20for%20image%20denoising%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Starck%2C%20J.-L.%20Candes%2C%20E.J.%20Donoho%2C%20D.L.%20The%20curvelet%20transform%20for%20image%20denoising%202002"
        },
        {
            "id": "Theis_et+al_2017_a",
            "entry": "L. Theis, W. Shi, A. Cunningham, and F. Huszar. Lossy image compression with compressive autoencoders. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20L.%20Shi%2C%20W.%20Cunningham%2C%20A.%20Huszar%2C%20F.%20Lossy%20image%20compression%20with%20compressive%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20L.%20Shi%2C%20W.%20Cunningham%2C%20A.%20Huszar%2C%20F.%20Lossy%20image%20compression%20with%20compressive%20autoencoders%202017"
        },
        {
            "id": "Toderici_et+al_2016_a",
            "entry": "G. Toderici, S. M. OMalley, S. J. Hwang, D. Vincent, D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Variable rate image compression with recurrent neural networks. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toderici%2C%20G.%20OMalley%2C%20S.M.%20Hwang%2C%20S.J.%20Vincent%2C%20D.%20Variable%20rate%20image%20compression%20with%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toderici%2C%20G.%20OMalley%2C%20S.M.%20Hwang%2C%20S.J.%20Vincent%2C%20D.%20Variable%20rate%20image%20compression%20with%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Ulyanov_et+al_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. In Conference on Computer Vision and Pattern Recognition, 2018. R. O. Winder. Partitions of n-space by hyperplanes. SIAM Journal on Applied Mathematics, 14(4): 811\u2013818, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ulyanov%2C%20D.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ulyanov%2C%20D.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Published%20as%20a%20conference%20paper%20at%20ICLR%202019"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142\u20133155, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20K.%20Zuo%2C%20W.%20Chen%2C%20Y.%20Meng%2C%20D.%20Beyond%20a%20Gaussian%20denoiser%3A%20Residual%20learning%20of%20deep%20CNN%20for%20image%20denoising%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20K.%20Zuo%2C%20W.%20Chen%2C%20Y.%20Meng%2C%20D.%20Beyond%20a%20Gaussian%20denoiser%3A%20Residual%20learning%20of%20deep%20CNN%20for%20image%20denoising%202017"
        }
    ]
}
