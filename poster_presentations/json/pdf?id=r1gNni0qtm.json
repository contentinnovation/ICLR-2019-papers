{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "GENERALIZED TENSOR MODELS FOR RECURRENT NEURAL NETWORKS",
        "author": "Valentin Khrulkov, Oleksii Hrinchuk, & Ivan Oseledets, {valentin.khrulkov, oleksii.hrinchuk, i.oseledets}@skoltech.ru 1Skolkovo Institute of Science and Technology, Moscow, Russia 2Moscow Institute of Physics and Technology, Moscow, Russia 3Institute of Numerical Mathematics, Russian Academy of Sciences, Moscow, Russia",
        "date": 2018,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1gNni0qtm"
        },
        "abstract": "Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency \u2014 a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments."
    },
    "keywords": [
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "formulas",
            "url": "https://en.wikipedia.org/wiki/formulas"
        },
        {
            "term": "expressive power",
            "url": "https://en.wikipedia.org/wiki/expressive_power"
        }
    ],
    "abbreviations": {
        "RNNs": "Recurrent Neural Networks",
        "ReLU": "Rectified Linear Unit",
        "TT": "Tensor Train"
    },
    "highlights": [
        "Recurrent Neural Networks are firmly established to be one of the best deep learning techniques when the task at hand requires processing sequential data, such as text, audio, or video (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>; Mikolov et al, 2011; <a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\"><a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\">Gers et al, 1999</a></a>)",
        "Our objective is to apply the machinery of generalized tensor decompositions, and show universality and existence of depth efficiency in such Recurrent Neural Networks.\n2 RELATED WORK",
        "With grid tensors at hand we are ready to compare the expressive power of generalized Recurrent Neural Networks and generalized shallow networks",
        "We investigate whether generalized tensor networks can be used in practical settings, especially in problems typically solved by Recurrent Neural Networks",
        "We sought a more complete picture of the connection between Recurrent Neural Networks and Tensor Train decomposition, one that involves various nonlinearities applied to hidden states",
        "We showed how these nonlinearities could be incorporated into network architectures and provided complete theoretical analysis on the particular case of rectifier nonlinearity, elaborating on points of generality and expressive power"
    ],
    "key_statements": [
        "Recurrent Neural Networks are firmly established to be one of the best deep learning techniques when the task at hand requires processing sequential data, such as text, audio, or video (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>; Mikolov et al, 2011; <a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\"><a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\">Gers et al, 1999</a></a>)",
        "It was shown in (Cohen et al, 2016) that ConvNets with product pooling are exponentially more expressive than shallow networks, that is there exist functions realized by ConvNets which require an exponentially large number of parameters in order to be realized by shallow nets",
        "We aim to extend this analysis to Recurrent Neural Networks with rectifier nonlinearities which are often used in practice",
        "Our objective is to apply the machinery of generalized tensor decompositions, and show universality and existence of depth efficiency in such Recurrent Neural Networks.\n2 RELATED WORK",
        "In the previous section we showed that tensor decompositions correspond to neural networks of specific structure, which are simplified versions of those used in practice as they contain multiplicative nonlinearities only",
        "We previously considered the equality of score functions represented by tensor decomposition and tensor network on set of all possible input sequences X = x(1), . . . , x(T ) , x(t) \u2208 RN , and we restricted this set to exponentially large but finite grid of sequences consisting of template vectors only",
        "With grid tensors at hand we are ready to compare the expressive power of generalized Recurrent Neural Networks and generalized shallow networks",
        "For every value of R there exists an open set of generalized Recurrent Neural Networks with rectifier nonlinearity \u03be(x, y) = max(x, y, 0), such that for each Recurrent Neural Networks in this open set the corresponding grid tensor can be realized by a rank 1 shallow network with rectifier nonlinearity",
        "We investigate whether generalized tensor networks can be used in practical settings, especially in problems typically solved by Recurrent Neural Networks",
        "Expressivity For the second experiment we generate a number of generalized Recurrent Neural Networks with different values of Tensor Train-rank r and calculate a lower bound on the rank of shallow network necessary to realize the same grid tensor",
        "We sought a more complete picture of the connection between Recurrent Neural Networks and Tensor Train decomposition, one that involves various nonlinearities applied to hidden states",
        "We showed how these nonlinearities could be incorporated into network architectures and provided complete theoretical analysis on the particular case of rectifier nonlinearity, elaborating on points of generality and expressive power"
    ],
    "summary": [
        "Recurrent Neural Networks are firmly established to be one of the best deep learning techniques when the task at hand requires processing sequential data, such as text, audio, or video (<a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\"><a class=\"ref-link\" id=\"cGraves_et+al_2013_a\" href=\"#rGraves_et+al_2013_a\">Graves et al, 2013</a></a>; Mikolov et al, 2011; <a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\"><a class=\"ref-link\" id=\"cGers_et+al_1999_a\" href=\"#rGers_et+al_1999_a\">Gers et al, 1999</a></a>).",
        "Every tensor network had corresponding weight tensor W and we could compare expressivity of associated score functions by comparing some properties of this tensors, such as ranks (<a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\">Khrulkov et al, 2018</a></a></a>; Cohen et al, 2016).",
        "There exist a generalized shallow network and a generalized RNN with rectifier nonlinearity \u03be(x, y) = max(x, y, 0) such that grid tensor of each of the networks coincides with H.",
        "This lemma states that in the special case of rectifier nonlinearity \u03be(x, y) = max(x, y, 0) any basis tensor can be realized by some generalized RNN.",
        "The same result regarding networks with product nonlinearities considered in (<a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\">Khrulkov et al, 2018</a></a></a>) directly follows from the well\u2013known properties of tensor decompositions.",
        "It was proven in (Cohen et al, 2016; <a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\"><a class=\"ref-link\" id=\"cKhrulkov_et+al_2018_a\" href=\"#rKhrulkov_et+al_2018_a\">Khrulkov et al, 2018</a></a></a>) that ConvNets and RNNs with multiplicative nonlinearities are exponentially more expressive than the equivalent shallow networks: shallow networks of exponentially large width are required to realize the same score functions as computed by these deep architectures.",
        "To the case of ConvNets (<a class=\"ref-link\" id=\"cCohen_2016_a\" href=\"#rCohen_2016_a\">Cohen & Shashua, 2016</a>), we find that expressivity of generalized RNNs with rectifier nonlinearity holds only partially, as discussed in the following two theorems.",
        "This result states that at least for some subset of generalized RNNs expressivity holds: exponentially wide shallow networks are required to realize the same grid tensor.",
        "For every value of R there exists an open set of generalized RNNs with rectifier nonlinearity \u03be(x, y) = max(x, y, 0), such that for each RNN in this open set the corresponding grid tensor can be realized by a rank 1 shallow network with rectifier nonlinearity.",
        "Expressivity For the second experiment we generate a number of generalized RNNs with different values of TT-rank r and calculate a lower bound on the rank of shallow network necessary to realize the same grid tensor.",
        "Figure 3 shows that for different values of R and generalized RNNs of the corresponding rank there exist shallow networks of rank 1 realizing the same grid tensor, which agrees well with Theorem 5.3.",
        "We sought a more complete picture of the connection between Recurrent Neural Networks and Tensor Train decomposition, one that involves various nonlinearities applied to hidden states.",
        "We showed how these nonlinearities could be incorporated into network architectures and provided complete theoretical analysis on the particular case of rectifier nonlinearity, elaborating on points of generality and expressive power.",
        "We would like to extend the theoretical analysis to most competitive in practice architectures for processing sequential data such as LSTMs and attention mechanisms"
    ],
    "headline": "We aim to extend this analysis to Recurrent Neural Networks with rectifier nonlinearities which are often used in practice",
    "reference_links": [
        {
            "id": "Eric_2017_a",
            "entry": "Eric Bailey and Shuchin Aeron. Word embeddings via tensor factorization. arXiv preprint arXiv:1704.02686, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02686"
        },
        {
            "id": "Carroll_1970_a",
            "entry": "J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition. Psychometrika, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carroll%2C%20J.Douglas%20Chang%2C%20Jih-Jie%20Analysis%20of%20individual%20differences%20in%20multidimensional%20scaling%20via%20an%20N-way%20generalization%20of%20Eckart-Young%20decomposition%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carroll%2C%20J.Douglas%20Chang%2C%20Jih-Jie%20Analysis%20of%20individual%20differences%20in%20multidimensional%20scaling%20via%20an%20N-way%20generalization%20of%20Eckart-Young%20decomposition%201970"
        },
        {
            "id": "Cichocki_et+al_2017_a",
            "entry": "Andrzej Cichocki, Anh-Huy Phan, Qibin Zhao, Namgil Lee, Ivan Oseledets, Masashi Sugiyama, Danilo P Mandic, et al. Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives. Foundations and Trends\u00ae in Machine Learning, 9(6): 431\u2013673, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cichocki%2C%20Andrzej%20Phan%2C%20Anh-Huy%20Zhao%2C%20Qibin%20Lee%2C%20Namgil%20Tensor%20networks%20for%20dimensionality%20reduction%20and%20large-scale%20optimization%3A%20Part%202%20applications%20and%20future%20perspectives%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cichocki%2C%20Andrzej%20Phan%2C%20Anh-Huy%20Zhao%2C%20Qibin%20Lee%2C%20Namgil%20Tensor%20networks%20for%20dimensionality%20reduction%20and%20large-scale%20optimization%3A%20Part%202%20applications%20and%20future%20perspectives%202017"
        },
        {
            "id": "Cohen_2016_a",
            "entry": "Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In International Conference on Machine Learning, pp. 955\u2013963, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Nadav%20Shashua%2C%20Amnon%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Nadav%20Shashua%2C%20Amnon%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016"
        },
        {
            "id": "Cohen_et+al_2016_b",
            "entry": "Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on Learning Theory, pp. 698\u2013728, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Nadav%20Sharir%2C%20Or%20Shashua%2C%20Amnon%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Nadav%20Sharir%2C%20Or%20Shashua%2C%20Amnon%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016"
        },
        {
            "id": "Cohen_et+al_2018_a",
            "entry": "Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with mixed tensor decompositions. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=S1JHhv6TW.",
            "url": "https://openreview.net/forum?id=S1JHhv6TW",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Nadav%20Tamari%2C%20Ronen%20Shashua%2C%20Amnon%20Boosting%20dilated%20convolutional%20networks%20with%20mixed%20tensor%20decompositions%202018"
        },
        {
            "id": "Frolov_2017_a",
            "entry": "Evgeny Frolov and Ivan Oseledets. Tensor methods and recommender systems. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(3):e1201, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frolov%2C%20Evgeny%20Oseledets%2C%20Ivan%20Tensor%20methods%20and%20recommender%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frolov%2C%20Evgeny%20Oseledets%2C%20Ivan%20Tensor%20methods%20and%20recommender%20systems%202017"
        },
        {
            "id": "Gers_et+al_1999_a",
            "entry": "Felix A Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20Felix%20A.%20Schmidhuber%2C%20Jurgen%20Cummins%2C%20Fred%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%201999"
        },
        {
            "id": "Girosi_1990_a",
            "entry": "Federico Girosi and Tomaso Poggio. Networks and the best approximation property. Biological cybernetics, 63(3):169\u2013176, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girosi%2C%20Federico%20Poggio%2C%20Tomaso%20Networks%20and%20the%20best%20approximation%20property%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girosi%2C%20Federico%20Poggio%2C%20Tomaso%20Networks%20and%20the%20best%20approximation%20property%201990"
        },
        {
            "id": "Grasedyck_2010_a",
            "entry": "Lars Grasedyck. Hierarchical singular value decomposition of tensors. SIAM Journal on Matrix Analysis and Applications, 31(4):2029\u20132054, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grasedyck%2C%20Lars%20Hierarchical%20singular%20value%20decomposition%20of%20tensors%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grasedyck%2C%20Lars%20Hierarchical%20singular%20value%20decomposition%20of%20tensors%202010"
        },
        {
            "id": "Graves_et+al_2013_a",
            "entry": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645\u20136649. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Harshman_1970_a",
            "entry": "Richard A Harshman. Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multimodal factor analysis. 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harshman%2C%20Richard%20A.%20Foundations%20of%20the%20PARAFAC%20procedure%3A%20Models%20and%20conditions%20for%20an%E2%80%9Dexplanatory%E2%80%9D%20multimodal%20factor%20analysis%201970"
        },
        {
            "id": "Khrulkov_et+al_2018_a",
            "entry": "Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=S1WRibb0Z.",
            "url": "https://openreview.net/forum?id=S1WRibb0Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khrulkov%2C%20Valentin%20Novikov%2C%20Alexander%20Oseledets%2C%20Ivan%20Expressive%20power%20of%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "Kolda_2009_a",
            "entry": "Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3): 455\u2013500, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009"
        },
        {
            "id": "Springer_2002_a",
            "entry": "Springer, 2002. Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. On multiplicative integration with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 2856\u20132864, 2016. Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification. arXiv preprint arXiv:1707.01786, 2017. Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train RNNs. arXiv preprint arXiv:1711.00073, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01786"
        },
        {
            "id": "T_2011_a",
            "entry": ", 0 < t < T, and h(CT ) = ahA(T ) + bh(BT ), concluding the proof. We also note that these formulas generalize the well\u2013known formulas for addition of two tensors in the Tensor Train format (Oseledets, 2011).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=t%20%3C%20T%20and%20h%28CT%20%29%20%3D%20ahA%28T%20%29%20%2B%20bh%28BT%20%29%2C%20concluding%20the%20proof.%20We%20also%20note%20that%20these%20formulas%20generalize%20the%20well%E2%80%93known%20formulas%20for%20addition%20of%20two%20tensors%20in%20the%20Tensor%20Train%20format%202011"
        },
        {
            "id": "Proof_2016_a",
            "entry": "Proof. It is known that the statement of the lemma holds for generalized shallow networks with rectifier nonlinearities (see (Cohen & Shashua, 2016, Claim 4)). Based on Proposition A.4 and Lemma 5.1 we can conclude that it also holds for generalized RNNs with rectifier nonlinearities.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proof%20It%20is%20known%20that%20the%20statement%20of%20the%20lemma%20holds%20for%20generalized%20shallow%20networks%20with%20rectifier%20nonlinearities%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proof%20It%20is%20known%20that%20the%20statement%20of%20the%20lemma%20holds%20for%20generalized%20shallow%20networks%20with%20rectifier%20nonlinearities%202016"
        },
        {
            "id": "Note_2011_a",
            "entry": "Note that by construction for any input assembled from the template vectors we obtain (x(i1),..., x(iT )) = Hi1...iT. By taking the standard TT and CP decompositions of H which always exist (Oseledets, 2011; Kolda & Bader, 2009), and using Lemma 3.1 and eq. (6) we conclude that universality holds.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Note%20that%20by%20construction%20for%20any%20input%20assembled%20from%20the%20template%20vectors%20we%20obtain%20xi1%20xiT%20%20%20Hi1iT%20By%20taking%20the%20standard%20TT%20and%20CP%20decompositions%20of%20H%20which%20always%20exist%20Oseledets%202011%20Kolda%20%20Bader%202009%20and%20using%20Lemma%2031%20and%20eq%206%20we%20conclude%20that%20universality%20holds"
        }
    ]
}
