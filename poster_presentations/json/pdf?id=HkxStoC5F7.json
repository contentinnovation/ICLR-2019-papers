{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "META-LEARNING PROBABILISTIC INFERENCE FOR PREDICTION",
        "author": "Jonathan Gordon, John Bronskill, University of Cambridge {jg801,jfb,}@cam.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HkxStoC5F7"
        },
        "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task."
    },
    "keywords": [
        {
            "term": "forward pass",
            "url": "https://en.wikipedia.org/wiki/forward_pass"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "VERSA",
            "url": "https://en.wikipedia.org/wiki/VERSA"
        }
    ],
    "abbreviations": {
        "ML-PIP": "Meta-Learning Probabilistic Inference for Prediction",
        "VI": "Variational Inference",
        "C-VAE": "conditional variational autoencoder"
    },
    "highlights": [
        "Many applications require predictions to be made on myriad small, but related datasets",
        "We evaluate VERSA on several few-shot learning tasks",
        "We report few-shot classification results using the Omniglot and miniImageNet datasets in Section 5.2, and demonstrate VERSA\u2019s ability to retain high accuracy as the shot and way are varied at test time",
        "Meta-Learning Probabilistic Inference for Prediction unifies a broad class of recently proposed meta-learning methods, and suggests alternative approaches",
        "Building on Meta-Learning Probabilistic Inference for Prediction, we developed VERSA, a few-shot learning algorithm that avoids the use of gradient based optimization at test time by amortizing posterior inference of task-specific parameters",
        "We evaluated VERSA on several few-shot learning tasks and demonstrated state-of-the-art performance and compelling visual results on a challenging 1-shot view reconstruction task"
    ],
    "key_statements": [
        "Many applications require predictions to be made on myriad small, but related datasets",
        "2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass",
        "In this paper we develop a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP), providing this view in terms of amortizing posterior predictive distributions",
        "In Section 4, we show that Meta-Learning Probabilistic Inference for Prediction re-frames and extends existing point-estimate probabilistic interpretations of meta-learning (<a class=\"ref-link\" id=\"cGrant_et+al_2018_a\" href=\"#rGrant_et+al_2018_a\">Grant et al, 2018</a>; <a class=\"ref-link\" id=\"cFinn_et+al_2018_a\" href=\"#rFinn_et+al_2018_a\">Finn et al, 2018</a>) to cover a broader class of methods, including gradient based meta-learning (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>; <a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi and Larochelle, 2017</a>), metric based meta-learning (<a class=\"ref-link\" id=\"cSnell_et+al_2017_a\" href=\"#rSnell_et+al_2017_a\">Snell et al, 2017</a>), amortized MAP inference (<a class=\"ref-link\" id=\"cQiao_et+al_2017_a\" href=\"#rQiao_et+al_2017_a\">Qiao et al, 2017</a>) and conditional probability modelling (<a class=\"ref-link\" id=\"cGarnelo_et+al_2018_a\" href=\"#rGarnelo_et+al_2018_a\">Garnelo et al, 2018a</a>;b)",
        "Since uncertainty is rife in small datasets, we provide a procedure for metalearning probabilistic inference",
        "We propose a new method \u2013 VERSA \u2013 which substitutes optimization procedures at test time with forward passes through inference networks",
        "We evaluate VERSA on several few-shot learning tasks",
        "We begin with toy experiments to investigate the properties of the amortized posterior inference achieved by VERSA",
        "We report few-shot classification results using the Omniglot and miniImageNet datasets in Section 5.2, and demonstrate VERSA\u2019s ability to retain high accuracy as the shot and way are varied at test time",
        "We evaluate VERSA by comparing it to a conditional variational autoencoder (C-VAE) with view angles as labels (Kingma et al, 2014; <a class=\"ref-link\" id=\"cNarayanaswamy_et+al_2017_a\" href=\"#rNarayanaswamy_et+al_2017_a\">Narayanaswamy et al, 2017</a>) and identical architectures",
        "Fig. 6 shows views of unseen objects from the test set generated from a single shot with VERSA as well as a conditional variational autoencoder and compares both to ground truth views",
        "We have introduced Meta-Learning Probabilistic Inference for Prediction, a probabilistic framework for meta-learning",
        "Meta-Learning Probabilistic Inference for Prediction unifies a broad class of recently proposed meta-learning methods, and suggests alternative approaches",
        "Building on Meta-Learning Probabilistic Inference for Prediction, we developed VERSA, a few-shot learning algorithm that avoids the use of gradient based optimization at test time by amortizing posterior inference of task-specific parameters",
        "We evaluated VERSA on several few-shot learning tasks and demonstrated state-of-the-art performance and compelling visual results on a challenging 1-shot view reconstruction task"
    ],
    "summary": [
        "Many applications require predictions to be made on myriad small, but related datasets.",
        "2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass.",
        "3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time.",
        "In this paper we develop a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP), providing this view in terms of amortizing posterior predictive distributions.",
        "VERSA employs a flexible amortization network that takes few-shot learning datasets, and outputs a distribution over task-specific parameters in a single forward pass.",
        "In Section 5, we evaluate VERSA on (i) standard benchmarks where the method sets new state-of-the-art results, settings where test conditions differ from training, and a challenging one-shot view reconstruction task.",
        "In comparison to these methods, besides being distributional over \u03c8, VERSA relieves the need to back-propagate through gradient based updates during training and compute gradients at test time, as well as enables the treatment of both local and global parameters which simplifies inference.",
        "This is an example usage of hyper-networks (<a class=\"ref-link\" id=\"cHa_et+al_2016_a\" href=\"#rHa_et+al_2016_a\">Ha et al, 2016</a>) to amortize learning about weights, and can be recovered by the ML-PIP framework by pre-training \u03b8 and performing MAP inference for \u03c8.",
        "VERSA goes beyond point estimates and its amortization network is similar in spirit, it is more general, employing end-to-end training and supporting full multi-task learning by sharing information between many tasks.",
        "Fig. 4 shows the approximate posterior distributions inferred for unseen test sets by the trained amortization networks.",
        "VERSA achieves a new state-of-the-art results (67.37% - up 1.38% over the previous best) on 5-way - 5-shot classification on the miniImageNet benchmark and (97.66% - up 0.02%) on the 20-way - 1 shot Omniglot benchmark for systems using a convolution-based network architecture and an end-to-end training procedure.",
        "The time taken to evaluate 1000 test tasks with a 5-way, 5-shot miniImageNet trained model using MAML is 302.9 seconds whereas VERSA took 53.5 seconds on a NVIDIA Tesla P100-PCIE-16GB GPU.",
        "Fig. 6 shows views of unseen objects from the test set generated from a single shot with VERSA as well as a C-VAE and compares both to ground truth views.",
        "Building on ML-PIP, we developed VERSA, a few-shot learning algorithm that avoids the use of gradient based optimization at test time by amortizing posterior inference of task-specific parameters.",
        "We evaluated VERSA on several few-shot learning tasks and demonstrated state-of-the-art performance and compelling visual results on a challenging 1-shot view reconstruction task"
    ],
    "headline": "This paper introduces a new framework for data efficient and versatile learning",
    "reference_links": [
        {
            "id": "Bakker_2003_a",
            "entry": "B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal of Machine Learning Research, 4(May):83\u201399, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bakker%2C%20B.%20Heskes%2C%20T.%20Task%20clustering%20and%20gating%20for%20Bayesian%20multitask%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bakker%2C%20B.%20Heskes%2C%20T.%20Task%20clustering%20and%20gating%20for%20Bayesian%20multitask%20learning%202003"
        },
        {
            "id": "Bauer_et+al_2017_a",
            "entry": "M. Bauer, M. Rojas-Carulla, J. B. Swiatkowski, B. Scholkopf, and R. E. Turner. Discriminative k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00326"
        },
        {
            "id": "Berger_2013_a",
            "entry": "J. O. Berger. Statistical decision theory and Bayesian analysis. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berger%2C%20J.O.%20Statistical%20decision%20theory%20and%20Bayesian%20analysis%202013"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pages 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20Weight%20uncertainty%20in%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20Weight%20uncertainty%20in%20neural%20network%202015"
        },
        {
            "id": "Casella_2002_a",
            "entry": "G. Casella and R. L. Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%20Casella%20and%20R%20L%20Berger%20Statistical%20inference%20volume%202%20Duxbury%20Pacific%20Grove%20CA%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%20Casella%20and%20R%20L%20Berger%20Statistical%20inference%20volume%202%20Duxbury%20Pacific%20Grove%20CA%202002"
        },
        {
            "id": "Chang_et+al_2015_a",
            "entry": "A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03012"
        },
        {
            "id": "Cremer_et+al_2018_a",
            "entry": "C. Cremer, X. Li, and D. Duvenaud. Inference suboptimality in variational autoencoders. arXiv preprint arXiv:1801.03558, 2018. A. P. Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Mathematics, 59(1):77\u201393, 2007. H. Edwards and A. Storkey. Towards a neural statistician. In Proceedings of the International Conference on Learning Representations (ICLR), 2017. L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03558"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Finn_et+al_2018_a",
            "entry": "C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. arXiv preprint arXiv:1806.02817, 2018. V. Garcia and J. Bruna. Few-shot learning with graph neural networks. arXiv preprint arXiv:1711.04043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02817"
        },
        {
            "id": "Garnelo_et+al_2018_a",
            "entry": "M. Garnelo, D. Rosenbaum, C. J. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. J. Rezende, and S. Eslami. Conditional neural processes. arXiv preprint arXiv:1807.01613, 2018a. M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b. S. Gidaris and N. Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4367\u20134375, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01613"
        },
        {
            "id": "Grant_et+al_2018_a",
            "entry": "E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as hierarchical Bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grant%2C%20E.%20Finn%2C%20C.%20Levine%2C%20S.%20Darrell%2C%20T.%20Recasting%20gradient-based%20meta-learning%20as%20hierarchical%20Bayes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grant%2C%20E.%20Finn%2C%20C.%20Levine%2C%20S.%20Darrell%2C%20T.%20Recasting%20gradient-based%20meta-learning%20as%20hierarchical%20Bayes%202018"
        },
        {
            "id": "Ha_et+al_2016_a",
            "entry": "D. Ha, A. Dai, and Q. V. Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "Heskes_2000_a",
            "entry": "T. Heskes. Empirical bayes for learning to learn. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heskes%2C%20T.%20Empirical%20bayes%20for%20learning%20to%20learn%202000"
        },
        {
            "id": "Hinton_et+al_1995_a",
            "entry": "G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The\u201d wake-sleep\u201d algorithm for unsupervised neural networks. Science, 268(5214):1158\u20131161, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%E2%80%9D%20wake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%E2%80%9D%20wake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995"
        },
        {
            "id": "Huszar_2013_a",
            "entry": "F. Huszar. Scoring rules, divergences and information in Bayesian machine learning. PhD thesis, University of Cambridge, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huszar%2C%20F.%20Scoring%20rules%2C%20divergences%20and%20information%20in%20Bayesian%20machine%20learning%202013"
        },
        {
            "id": "Jaynes_2003_a",
            "entry": "E. T. Jaynes. Probability theory: the logic of science. Cambridge university press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaynes%2C%20E.T.%20Probability%20theory%3A%20the%20logic%20of%20science%202003"
        },
        {
            "id": "Kaiser_et+al_2017_a",
            "entry": "\u0141. Kaiser, O. Nachum, R. Aurko, and S. Bengio. Learning to remember rare events. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiser%2C%20%C5%81.%20Nachum%2C%20O.%20Aurko%2C%20R.%20Bengio%2C%20S.%20Learning%20to%20remember%20rare%20events%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiser%2C%20%C5%81.%20Nachum%2C%20O.%20Aurko%2C%20R.%20Bengio%2C%20S.%20Learning%20to%20remember%20rare%20events%202017"
        },
        {
            "id": "Kim_et+al_0000_a",
            "entry": "T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.03836"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Y. Kim, S. Wiseman, A. C. Miller, D. Sontag, and A. M. Rush. Semi-amortized variational autoencoders. In Proceedings of the 35th International Conference on Machine Learning, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Wiseman%2C%20S.%20Miller%2C%20A.C.%20Sontag%2C%20D.%20Semi-amortized%20variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.%20Wiseman%2C%20S.%20Miller%2C%20A.C.%20Sontag%2C%20D.%20Semi-amortized%20variational%20autoencoders%202018"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Kingma_et+al_2014_b",
            "entry": "D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pages 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "Kingma_et+al_2015_b",
            "entry": "D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pages 2575\u20132583, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Welling%2C%20M.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Welling%2C%20M.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "Koch_et+al_2015_a",
            "entry": "G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20G.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20G.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "Lacoste_et+al_2018_a",
            "entry": "A. Lacoste, B. Oreshkin, W. Chung, T. Boquet, N. Rostamzadeh, and D. Krueger. Uncertainty in multitask transfer learning. arXiv preprint arXiv:1806.07528, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07528"
        },
        {
            "id": "Lacoste-Julien_et+al_2011_a",
            "entry": "S. Lacoste-Julien, F. Huszar, and Z. Ghahramani. Approximate inference for the loss-calibrated Bayesian. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 416\u2013424, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20S.%20Huszar%2C%20F.%20Ghahramani%2C%20Z.%20Approximate%20inference%20for%20the%20loss-calibrated%20Bayesian%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20S.%20Huszar%2C%20F.%20Ghahramani%2C%20Z.%20Approximate%20inference%20for%20the%20loss-calibrated%20Bayesian%202011"
        },
        {
            "id": "Lake_et+al_2011_a",
            "entry": "B. Lake, R. Salakhutdinov, J. Gross, and J. Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.%20Salakhutdinov%2C%20R.%20Gross%2C%20J.%20Tenenbaum%2C%20J.%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.%20Salakhutdinov%2C%20R.%20Gross%2C%20J.%20Tenenbaum%2C%20J.%20One%20shot%20learning%20of%20simple%20visual%20concepts%202011"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Z. Li, F. Zhou, F. Chen, and H. Li. Meta-sgd: Learning to learn quickly for few shot learning. arXiv preprint arXiv:1707.09835, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09835"
        },
        {
            "id": "V_2008_a",
            "entry": "L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20N.%20Rohaninejad%2C%20M.%20Chen%2C%20X.%20Abbeel%2C%20P.%20A%20simple%20neural%20attentive%20meta-learner%202018"
        },
        {
            "id": "_0000_a",
            "entry": "http://blog.shakirm.com/2018/01/",
            "url": "http://blog.shakirm.com/2018/01/"
        },
        {
            "id": "Machine-Learning-Trick-Of-The-Day-_2018_a",
            "entry": "machine-learning-trick-of-the-day-7-density-ratio-trick/, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=machinelearningtrickoftheday7densityratiotrick%202018"
        },
        {
            "id": "Naik_1992_a",
            "entry": "D. K. Naik and R. Mammone. Meta-neural networks that learn by learning. In Neural Networks, 1992. IJCNN., International Joint Conference on, volume 1, pages 437\u2013442. IEEE, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naik%2C%20D.K.%20Mammone%2C%20R.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naik%2C%20D.K.%20Mammone%2C%20R.%20Meta-neural%20networks%20that%20learn%20by%20learning%201992"
        },
        {
            "id": "Narayanaswamy_et+al_2017_a",
            "entry": "S. Narayanaswamy, T. B. Paige, J.-W. van de Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr. Learning disentangled representations with semi-supervised deep generative models. In Advances in Neural Information Processing Systems, pages 5927\u20135937, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narayanaswamy%2C%20S.%20Paige%2C%20T.B.%20van%20de%20Meent%2C%20J.-W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narayanaswamy%2C%20S.%20Paige%2C%20T.B.%20van%20de%20Meent%2C%20J.-W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017"
        },
        {
            "id": "Ng_2002_a",
            "entry": "A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems, pages 841\u2013848, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20On%20discriminative%20vs.%20generative%20classifiers%3A%20A%20comparison%20of%20logistic%20regression%20and%20naive%20bayes%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20On%20discriminative%20vs.%20generative%20classifiers%3A%20A%20comparison%20of%20logistic%20regression%20and%20naive%20bayes%202002"
        },
        {
            "id": "Nichol_2018_a",
            "entry": "A. Nichol and J. Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Oreshkin_et+al_2018_a",
            "entry": "B. N. Oreshkin, A. Lacoste, and P. Rodriguez. Tadam: Task dependent adaptive metric for improved few-shot learning. arXiv preprint arXiv:1805.10123, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10123"
        },
        {
            "id": "Qi_et+al_2017_a",
            "entry": "C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 1(2):4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20C.R.%20Su%2C%20H.%20Mo%2C%20K.%20Guibas%2C%20L.J.%20Pointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20C.R.%20Su%2C%20H.%20Mo%2C%20K.%20Guibas%2C%20L.J.%20Pointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%202017"
        },
        {
            "id": "Qiao_et+al_2017_a",
            "entry": "S. Qiao, C. Liu, W. Shen, and A. Yuille. Few-shot image recognition by predicting parameters from activations. arXiv preprint arXiv:1706.03466, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03466"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pages 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Rusu_et+al_2018_a",
            "entry": "A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell. Meta-learning with latent embedding optimization. arXiv preprint arXiv:1807.05960, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.05960"
        },
        {
            "id": "Schmidhuber_1987_a",
            "entry": "J. Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Technische Universitat Munchen, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Evolutionary%20principles%20in%20self-referential%20learning%201987"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4080\u20134090, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Sugiyama_et+al_2012_a",
            "entry": "M. Sugiyama, T. Suzuki, and T. Kanamori. Density ratio estimation in machine learning. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Suzuki%2C%20T.%20Kanamori%2C%20T.%20Density%20ratio%20estimation%20in%20machine%20learning%202012"
        },
        {
            "id": "Thrun_2012_a",
            "entry": "S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Pratt%2C%20L.%20Learning%20to%20learn%202012"
        },
        {
            "id": "Triantafillou_et+al_2018_a",
            "entry": "E. Triantafillou, R. Zemel, and R. Urtasun. Few-shot learning through an information retrieval lens. In Advances in Neural Information Processing Systems, pages 2255\u20132265, 2017. B. Trippe and R. Turner. Overpruning in variational bayesian neural networks. arXiv preprint arXiv:1801.06230, 2018. R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. Bayesian Time series models, 1(3.1):3\u20131, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1801.06230"
        },
        {
            "id": "Wainwright_2016_a",
            "entry": "Advances in Neural Information Processing Systems, pages 3630\u20133638, 2016. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202016"
        },
        {
            "id": "Foundations_2008_a",
            "entry": "Foundations and Trends R in Machine Learning, 1(1-2):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foundations%20and%20Trends%20R%20in%20Machine%20Learning%201121305%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foundations%20and%20Trends%20R%20in%20Machine%20Learning%201121305%202008"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004. F. S. Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot learning. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202018"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In Advances in Neural Information Processing Systems, pages 3394\u20133404, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017"
        },
        {
            "id": "A_2013_a",
            "entry": "A generalization of the new inference framework presented in Section 2 is based upon Bayesian decision theory (BDT). BDT provides a recipe for making predictions yfor an unknown test variable yby combining information from observed training data D(t) (here from a single task t) and a loss function L(y, y) that encodes the cost of predicting ywhen the true value is y (Berger, 2013; Jaynes, 2003). In BDT an optimal prediction minimizes the expected loss (suppressing dependencies on the inputs and \u03b8 to reduce notational clutter):3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20generalization%20of%20the%20new%20inference%20framework%20presented%20in%20Section%202%20is%20based%20upon%20Bayesian%20decision%20theory%20BDT%20BDT%20provides%20a%20recipe%20for%20making%20predictions%20yfor%20an%20unknown%20test%20variable%20yby%20combining%20information%20from%20observed%20training%20data%20Dt%20here%20from%20a%20single%20task%20t%20and%20a%20loss%20function%20Ly%20y%20that%20encodes%20the%20cost%20of%20predicting%20ywhen%20the%20true%20value%20is%20y%20Berger%202013%20Jaynes%202003%20In%20BDT%20an%20optimal%20prediction%20minimizes%20the%20expected%20loss%20suppressing%20dependencies%20on%20the%20inputs%20and%20%CE%B8%20to%20reduce%20notational%20clutter3",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20generalization%20of%20the%20new%20inference%20framework%20presented%20in%20Section%202%20is%20based%20upon%20Bayesian%20decision%20theory%20BDT%20BDT%20provides%20a%20recipe%20for%20making%20predictions%20yfor%20an%20unknown%20test%20variable%20yby%20combining%20information%20from%20observed%20training%20data%20Dt%20here%20from%20a%20single%20task%20t%20and%20a%20loss%20function%20Ly%20y%20that%20encodes%20the%20cost%20of%20predicting%20ywhen%20the%20true%20value%20is%20y%20Berger%202013%20Jaynes%202003%20In%20BDT%20an%20optimal%20prediction%20minimizes%20the%20expected%20loss%20suppressing%20dependencies%20on%20the%20inputs%20and%20%CE%B8%20to%20reduce%20notational%20clutter3"
        },
        {
            "id": "BDT_2016_a",
            "entry": "BDT separates test and training data and so is a natural lens through which to view recent episodic approaches to training that utilize many internal training/test splits (Vinyals et al., 2016). Based on this insight, what follows is a fairly dense derivation of an ultimately simple stochastic variational objective for meta-learning probabilistic inference that is rigorously grounded in Bayesian inference and decision theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BDT%20separates%20test%20and%20training%20data%20and%20so%20is%20a%20natural%20lens%20through%20which%20to%20view%20recent%20episodic%20approaches%20to%20training%20that%20utilize%20many%20internal%20trainingtest%20splits%20Vinyals%20et%20al%202016%20Based%20on%20this%20insight%20what%20follows%20is%20a%20fairly%20dense%20derivation%20of%20an%20ultimately%20simple%20stochastic%20variational%20objective%20for%20metalearning%20probabilistic%20inference%20that%20is%20rigorously%20grounded%20in%20Bayesian%20inference%20and%20decision%20theory",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BDT%20separates%20test%20and%20training%20data%20and%20so%20is%20a%20natural%20lens%20through%20which%20to%20view%20recent%20episodic%20approaches%20to%20training%20that%20utilize%20many%20internal%20trainingtest%20splits%20Vinyals%20et%20al%202016%20Based%20on%20this%20insight%20what%20follows%20is%20a%20fairly%20dense%20derivation%20of%20an%20ultimately%20simple%20stochastic%20variational%20objective%20for%20metalearning%20probabilistic%20inference%20that%20is%20rigorously%20grounded%20in%20Bayesian%20inference%20and%20decision%20theory"
        },
        {
            "id": "where_2013_b",
            "entry": "where KL[p(y) q(y)] is the KL-divergence, and H [p(y)] is the entropy of p. Eq. (A.4) has the elegant property that the optimal q\u03c6 is the closest member of Q (in a KL sense) to the true predictive p(y|D), which is unsurprising as the log-loss is a proper scoring rule (Huszar, 2013). This is reminiscent of the sleep phase in the wake-sleep algorithm (Hinton et al., 1995). Exploration of alternative proper scoring rules (Dawid, 2007) and more task-specific losses (Lacoste-Julien et al., 2011) is left for future work.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=where%20KLpy%20qy%20is%20the%20KLdivergence%20and%20H%20py%20is%20the%20entropy%20of%20p%20Eq%20A4%20has%20the%20elegant%20property%20that%20the%20optimal%20q%CF%86%20is%20the%20closest%20member%20of%20Q%20in%20a%20KL%20sense%20to%20the%20true%20predictive%20pyD%20which%20is%20unsurprising%20as%20the%20logloss%20is%20a%20proper%20scoring%20rule%20Huszar%202013%20This%20is%20reminiscent%20of%20the%20sleep%20phase%20in%20the%20wakesleep%20algorithm%20Hinton%20et%20al%201995%20Exploration%20of%20alternative%20proper%20scoring%20rules%20Dawid%202007%20and%20more%20taskspecific%20losses%20LacosteJulien%20et%20al%202011%20is%20left%20for%20future%20work",
            "oa_query": "https://api.scholarcy.com/oa_version?query=where%20KLpy%20qy%20is%20the%20KLdivergence%20and%20H%20py%20is%20the%20entropy%20of%20p%20Eq%20A4%20has%20the%20elegant%20property%20that%20the%20optimal%20q%CF%86%20is%20the%20closest%20member%20of%20Q%20in%20a%20KL%20sense%20to%20the%20true%20predictive%20pyD%20which%20is%20unsurprising%20as%20the%20logloss%20is%20a%20proper%20scoring%20rule%20Huszar%202013%20This%20is%20reminiscent%20of%20the%20sleep%20phase%20in%20the%20wakesleep%20algorithm%20Hinton%20et%20al%201995%20Exploration%20of%20alternative%20proper%20scoring%20rules%20Dawid%202007%20and%20more%20taskspecific%20losses%20LacosteJulien%20et%20al%202011%20is%20left%20for%20future%20work"
        },
        {
            "id": "A_2018_a",
            "entry": "A principled justification for the approximation is best understood through the lens of density ratio estimation (Mohamed, 2018; Sugiyama et al., 2012). We denote the conditional density of each class as p(x|y = c) and assume equal a priori class probability p(y = c) = 1/C. Density ratio theory then uses Bayes\u2019 theorem to show that the optimal softmax classifier can be expressed in terms of the conditional densities (Mohamed, 2018; Sugiyama et al., 2012): Softmax(y = c|x) =",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20principled%20justification%20for%20the%20approximation%20is%20best%20understood%20through%20the%20lens%20of%20density%20ratio%20estimation%20Mohamed%202018%20Sugiyama%20et%20al%202012%20We%20denote%20the%20conditional%20density%20of%20each%20class%20as%20pxy%20%20c%20and%20assume%20equal%20a%20priori%20class%20probability%20py%20%20c%20%201C%20Density%20ratio%20theory%20then%20uses%20Bayes%20theorem%20to%20show%20that%20the%20optimal%20softmax%20classifier%20can%20be%20expressed%20in%20terms%20of%20the%20conditional%20densities%20Mohamed%202018%20Sugiyama%20et%20al%202012%20Softmaxy%20%20cx",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20principled%20justification%20for%20the%20approximation%20is%20best%20understood%20through%20the%20lens%20of%20density%20ratio%20estimation%20Mohamed%202018%20Sugiyama%20et%20al%202012%20We%20denote%20the%20conditional%20density%20of%20each%20class%20as%20pxy%20%20c%20and%20assume%20equal%20a%20priori%20class%20probability%20py%20%20c%20%201C%20Density%20ratio%20theory%20then%20uses%20Bayes%20theorem%20to%20show%20that%20the%20optimal%20softmax%20classifier%20can%20be%20expressed%20in%20terms%20of%20the%20conditional%20densities%20Mohamed%202018%20Sugiyama%20et%20al%202012%20Softmaxy%20%20cx"
        },
        {
            "id": "(b)_2008_b",
            "entry": "(b) Figure B.1: Visualizing the learned weights for d\u03b8 = 16. (a) Weight dimensionality is reduced using T-SNE (Maaten and Hinton, 2008). Weights are colored according to class. (b) Each weight represents one column of the image. Weights are grouped by class.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=b%20Figure%20B1%20Visualizing%20the%20learned%20weights%20for%20d%CE%B8%20%2016%20a%20Weight%20dimensionality%20is%20reduced%20using%20TSNE%20Maaten%20and%20Hinton%202008%20Weights%20are%20colored%20according%20to%20class%20b%20Each%20weight%20represents%20one%20column%20of%20the%20image%20Weights%20are%20grouped%20by%20class",
            "oa_query": "https://api.scholarcy.com/oa_version?query=b%20Figure%20B1%20Visualizing%20the%20learned%20weights%20for%20d%CE%B8%20%2016%20a%20Weight%20dimensionality%20is%20reduced%20using%20TSNE%20Maaten%20and%20Hinton%202008%20Weights%20are%20colored%20according%20to%20class%20b%20Each%20weight%20represents%20one%20column%20of%20the%20image%20Weights%20are%20grouped%20by%20class"
        }
    ]
}
