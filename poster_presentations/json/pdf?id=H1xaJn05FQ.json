{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "SLICED-WASSERSTEIN AUTO-ENCODERS",
        "author": "Soheil Kolouri, Phillip E. Pope, & Charles E. Martin, Information and Systems Sciences Laboratory HRL Laboratories, LLC. Malibu, CA, USA",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1xaJn05FQ"
        },
        "abstract": "In this paper we use the geometric properties of the optimal transport (OT) problem and the Wasserstein distances to define a prior distribution for the latent space of an auto-encoder. We introduce Sliced-Wasserstein Auto-Encoders (SWAE), that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or having a likelihood function specified. In short, we regularize the auto-encoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a samplable prior distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Auto-Encoders (WAE) and Variational Auto-Encoders (VAE), while benefiting from an embarrassingly simple implementation. We provide extensive error analysis for our algorithm, and show its merits on three benchmark datasets."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "likelihood function",
            "url": "https://en.wikipedia.org/wiki/likelihood_function"
        },
        {
            "term": "numerical solution",
            "url": "https://en.wikipedia.org/wiki/numerical_solution"
        },
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "transport problem",
            "url": "https://en.wikipedia.org/wiki/transport_problem"
        },
        {
            "term": "prior distribution",
            "url": "https://en.wikipedia.org/wiki/prior_distribution"
        }
    ],
    "abbreviations": {
        "OT": "optimal transport",
        "SWAE": "Sliced Wasserstein auto-encoders",
        "WAE": "Wasserstein Auto-Encoders",
        "VAE": "Variational auto-encoders",
        "GANs": "Generative Adversarial Networks",
        "MMD": "maximum mean discrepancy",
        "CelebA": "CelebFaces Attributes Dataset",
        "NLL": "negative log likelihood"
    },
    "highlights": [
        "Learning such generative models boils down to minimizing a dissimilarity measure between the data distribution and the output distribution of the generative model",
        "The optimal transport problem <a class=\"ref-link\" id=\"cVillani_2008_a\" href=\"#rVillani_2008_a\">Villani (2008</a>); <a class=\"ref-link\" id=\"cKolouri_et+al_2017_a\" href=\"#rKolouri_et+al_2017_a\">Kolouri et al (2017</a>) provides a way to measure the distances between probability distributions by transporting one distribution into another",
        "We introduce a new type of auto-encoders for generative modeling (Algorithm 1), which we call Sliced-Wasserstein auto-encoders (SWAE), that minimize the sliced-Wasserstein distance between the distribution of the encoded samples and a samplable prior distribution",
        "We note that <a class=\"ref-link\" id=\"cDeshpande_et+al_2018_a\" href=\"#rDeshpande_et+al_2018_a\">Deshpande et al (2018</a>) proposed to learn discriminative slices to mitigate the need for a very large number of random projections that is in essence similar to the adversarial training used in Generative Adversarial Networks, which contradicts with our goal of not using adversarial training",
        "We introduced Sliced Wasserstein auto-encoders (SWAE), which enable one to shape the distribution of the encoded samples to any samplable distribution without the need for adversarial training or having a likelihood function specified",
        "We further demonstrated the capability of our method on three image datasets, namely the MNIST, the CelebFaces Attributes Dataset face, and the LSUN Bedroom datasets, and showed competitive performance, in the sense of matching distributions pZ and qZ, to the techniques that rely on additional adversarial trainings"
    ],
    "key_statements": [
        "Learning such generative models boils down to minimizing a dissimilarity measure between the data distribution and the output distribution of the generative model",
        "The optimal transport problem <a class=\"ref-link\" id=\"cVillani_2008_a\" href=\"#rVillani_2008_a\">Villani (2008</a>); <a class=\"ref-link\" id=\"cKolouri_et+al_2017_a\" href=\"#rKolouri_et+al_2017_a\">Kolouri et al (2017</a>) provides a way to measure the distances between probability distributions by transporting one distribution into another",
        "Wasserstein distances have recently attracted a lot of interest in the learning community <a class=\"ref-link\" id=\"cFrogner_et+al_2015_a\" href=\"#rFrogner_et+al_2015_a\">Frogner et al (2015</a>); <a class=\"ref-link\" id=\"cGulrajani_et+al_2017_a\" href=\"#rGulrajani_et+al_2017_a\">Gulrajani et al (2017</a>); <a class=\"ref-link\" id=\"cBousquet_et+al_2017_a\" href=\"#rBousquet_et+al_2017_a\">Bousquet et al (2017</a>); Arjovsky et al (2017); <a class=\"ref-link\" id=\"cKolouri_et+al_2017_a\" href=\"#rKolouri_et+al_2017_a\">Kolouri et al (2017</a>) due to their exquisite geometric characteristics <a class=\"ref-link\" id=\"cSantambrogio_2015_a\" href=\"#rSantambrogio_2015_a\">Santambrogio (2015</a>)",
        "We introduce a new type of auto-encoders for generative modeling (Algorithm 1), which we call Sliced-Wasserstein auto-encoders (SWAE), that minimize the sliced-Wasserstein distance between the distribution of the encoded samples and a samplable prior distribution",
        "<a class=\"ref-link\" id=\"cDeshpande_et+al_2018_a\" href=\"#rDeshpande_et+al_2018_a\">Deshpande et al (2018</a>) use the sliced-Wasserstein distance to match the distributions of high-dimensional reconstructed images, which require large number of slices, O(104), while in our method and due to the distribution matching in the latent space we only need O(10) slices",
        "We note that <a class=\"ref-link\" id=\"cDeshpande_et+al_2018_a\" href=\"#rDeshpande_et+al_2018_a\">Deshpande et al (2018</a>) proposed to learn discriminative slices to mitigate the need for a very large number of random projections that is in essence similar to the adversarial training used in Generative Adversarial Networks, which contradicts with our goal of not using adversarial training",
        "We show that using the sliced-Wasserstein distance ameliorates the need for training an adversary network or choosing a data-dependent kernel, and provides an efficient, stable, and simple numerical implementation",
        "To further clarify why we use the sliced-Wasserstein distance to measure the difference between pZ and qZ , we reiterate that due to the lack of correspondences between zis and zjs, one cannot minimize the upper-bound in equation 4, and calculation of the Wasserstein distance requires an additional optimization step to obtain the optimal coupling between pZ and qZ",
        "Here is that while Wasserstein Auto-Encoders-Generative Adversarial Networks provides good or even slightly better generated random samples for MNIST, it fails to provide a good match between pZ and qZ for the choice of the prior distribution reported in Figure 4",
        "We introduced Sliced Wasserstein auto-encoders (SWAE), which enable one to shape the distribution of the encoded samples to any samplable distribution without the need for adversarial training or having a likelihood function specified",
        "We further demonstrated the capability of our method on three image datasets, namely the MNIST, the CelebFaces Attributes Dataset face, and the LSUN Bedroom datasets, and showed competitive performance, in the sense of matching distributions pZ and qZ, to the techniques that rely on additional adversarial trainings"
    ],
    "summary": [
        "Learning such generative models boils down to minimizing a dissimilarity measure between the data distribution and the output distribution of the generative model.",
        "Our approach avoids the need to perform adversarial training in the encoding space and is not restricted to closed-form distributions, while still benefiting from a Wasserstein-like distance measure in the latent space.",
        "Similar to variational Auto-Encoders (VAEs) Kingma & Welling (2013) and the Wasserstein Auto-Encoders (WAE) Tolstikhin et al (2017), our main objective is to encode the input data points x \u2208 X into latent codes z \u2208 Z such that: 1) x can be recovered/approximated from z, and 2) the probability density function of the encoded samples, pZ, follows a prior distribution qZ.",
        "Before explaining our proposed approach, it is worthwhile to point out the major difference between learning auto-encoders as generative models and GANs. In GANs, one needs to minimize a distance between {\u03c8|zj \u223c qZ }M j=1 and {xn}M n=1, which are high-dimensional point clouds for which there are no correspondences between \u03c8s and xns.",
        "That is obtained from integrating pX over the hyperplane orthogonal to \u03b8.Utilizing these marginal distributions in equation 10, the sliced Wasserstein distance could be defined as: SWc =",
        "To further clarify why we use the sliced-Wasserstein distance to measure the difference between pZ and qZ , we reiterate that due to the lack of correspondences between zis and zjs, one cannot minimize the upper-bound in equation 4, and calculation of the Wasserstein distance requires an additional optimization step to obtain the optimal coupling between pZ and qZ.",
        "The problem of calculating the Wasserstein distance between samples from one-dimensional densities simplifies to solving two sorting problems/O(M log(M )) best/worst case).",
        "We measured the SW distance between M = 1000 samples generated from the two Gaussian distributions using L \u2208 {1, 10, 50, 100, 500, 1000} random slices.",
        "To get a sense of the convergence behavior of SWAE, and similar to the work of <a class=\"ref-link\" id=\"cKarras_et+al_2017_a\" href=\"#rKarras_et+al_2017_a\">Karras et al (2017</a>), we calculate the Sliced Wasserstein distance between pZ and qZ as well as pX and pY at each batch iteration where we used p-LDA Wang et al (2011) to calculate projections (See supplementary material).",
        "Here is that while WAE-GAN provides good or even slightly better generated random samples for MNIST, it fails to provide a good match between pZ and qZ for the choice of the prior distribution reported in Figure 4.",
        "We introduced Sliced Wasserstein auto-encoders (SWAE), which enable one to shape the distribution of the encoded samples to any samplable distribution without the need for adversarial training or having a likelihood function specified.",
        "We envision SWAE could be effectively used in transfer learning and domain adaptation algorithms where qZ comes from a source domain and the task is to encode the target domain pX in a latent space such that the distribution follows the distribution of the target domain"
    ],
    "headline": "We introduce Sliced-Wasserstein Auto-Encoders, that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or having a likelihood function specified",
    "reference_links": [
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "Berthelot_et+al_2017_a",
            "entry": "David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "Bobkov_2014_a",
            "entry": "Sergey Bobkov and Michel Ledoux. One-dimensional empirical measures, order statistics and kantorovich transport distances. preprint, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bobkov%2C%20Sergey%20Ledoux%2C%20Michel%20One-dimensional%20empirical%20measures%2C%20order%20statistics%20and%20kantorovich%20transport%20distances%202014"
        },
        {
            "id": "Bonneel_et+al_2015_a",
            "entry": "Nicolas Bonneel, Julien Rabin, Gabriel Peyr\u00e9, and Hanspeter Pfister. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22\u201345, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonneel%2C%20Nicolas%20Rabin%2C%20Julien%20Peyr%C3%A9%2C%20Gabriel%20Pfister%2C%20Hanspeter%20Sliced%20and%20Radon%20Wasserstein%20barycenters%20of%20measures%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonneel%2C%20Nicolas%20Rabin%2C%20Julien%20Peyr%C3%A9%2C%20Gabriel%20Pfister%2C%20Hanspeter%20Sliced%20and%20Radon%20Wasserstein%20barycenters%20of%20measures%202015"
        },
        {
            "id": "Bonnotte_2013_a",
            "entry": "Nicolas Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris 11, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonnotte%2C%20Nicolas%20Unidimensional%20and%20evolution%20methods%20for%20optimal%20transportation%202013"
        },
        {
            "id": "Bousquet_et+al_2017_a",
            "entry": "Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bernhard Schoelkopf. From optimal transport to generative modeling: the VEGAN cookbook. arXiv preprint arXiv:1705.07642, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07642"
        },
        {
            "id": "Carriere_et+al_2017_a",
            "entry": "Mathieu Carriere, Marco Cuturi, and Steve Oudot. Sliced wasserstein kernel for persistence diagrams. arXiv preprint arXiv:1706.03358, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03358"
        },
        {
            "id": "Cuturi_2013_a",
            "entry": "Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pp. 2292\u20132300, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "Dedecker_et+al_2015_a",
            "entry": "J\u00e9r\u00f4me Dedecker, Aur\u00e9lie Fischer, Bertrand Michel, et al. Improved rates for wasserstein deconvolution with ordinary smooth error in dimension one. Electronic journal of statistics, 9(1):234\u2013265, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dedecker%2C%20J%C3%A9r%C3%B4me%20Fischer%2C%20Aur%C3%A9lie%20Michel%2C%20Bertrand%20Improved%20rates%20for%20wasserstein%20deconvolution%20with%20ordinary%20smooth%20error%20in%20dimension%20one%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dedecker%2C%20J%C3%A9r%C3%B4me%20Fischer%2C%20Aur%C3%A9lie%20Michel%2C%20Bertrand%20Improved%20rates%20for%20wasserstein%20deconvolution%20with%20ordinary%20smooth%20error%20in%20dimension%20one%202015"
        },
        {
            "id": "Deshpande_et+al_2018_a",
            "entry": "Ishan Deshpande, Ziyu Zhang, and Alexander Schwing. Generative modeling using the sliced wasserstein distance. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deshpande%2C%20Ishan%20Zhang%2C%20Ziyu%20Schwing%2C%20Alexander%20Generative%20modeling%20using%20the%20sliced%20wasserstein%20distance%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deshpande%2C%20Ishan%20Zhang%2C%20Ziyu%20Schwing%2C%20Alexander%20Generative%20modeling%20using%20the%20sliced%20wasserstein%20distance%202018"
        },
        {
            "id": "Frogner_et+al_2015_a",
            "entry": "Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a wasserstein loss. In Advances in Neural Information Processing Systems, pp. 2053\u20132061, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya%2C%20Mauricio%20and%20Tomaso%20A%20Poggio.%20Learning%20with%20a%20wasserstein%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya%2C%20Mauricio%20and%20Tomaso%20A%20Poggio.%20Learning%20with%20a%20wasserstein%20loss%202015"
        },
        {
            "id": "Gillespie_1983_a",
            "entry": "Daniel T Gillespie. A theorem for physicists in the theory of random variables. American Journal of Physics, 51(6):520\u2013533, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gillespie%2C%20Daniel%20T.%20A%20theorem%20for%20physicists%20in%20the%20theory%20of%20random%20variables%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gillespie%2C%20Daniel%20T.%20A%20theorem%20for%20physicists%20in%20the%20theory%20of%20random%20variables%201983"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "Karras_et+al_2017_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kolouri_2015_a",
            "entry": "Soheil Kolouri and Gustavo K Rohde. Transport-based single frame super resolution of very low resolution face images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4876\u20134884, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolouri%2C%20Soheil%20Rohde%2C%20Gustavo%20K.%20Transport-based%20single%20frame%20super%20resolution%20of%20very%20low%20resolution%20face%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolouri%2C%20Soheil%20Rohde%2C%20Gustavo%20K.%20Transport-based%20single%20frame%20super%20resolution%20of%20very%20low%20resolution%20face%20images%202015"
        },
        {
            "id": "Kolouri_et+al_0000_a",
            "entry": "Soheil Kolouri, Se Rim Park, and Gustavo K Rohde. The radon cumulative distribution transform and its application to image classification. IEEE transactions on image processing, 25(2):920\u2013934, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolouri%2C%20Soheil%20Park%2C%20Se%20Rim%20Rohde%2C%20Gustavo%20K.%20The%20radon%20cumulative%20distribution%20transform%20and%20its%20application%20to%20image%20classification",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolouri%2C%20Soheil%20Park%2C%20Se%20Rim%20Rohde%2C%20Gustavo%20K.%20The%20radon%20cumulative%20distribution%20transform%20and%20its%20application%20to%20image%20classification"
        },
        {
            "id": "Kolouri_et+al_2016_a",
            "entry": "Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced Wasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5258\u20135267, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolouri%2C%20Soheil%20Zou%2C%20Yang%20Rohde%2C%20Gustavo%20K.%20Sliced%20Wasserstein%20kernels%20for%20probability%20distributions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolouri%2C%20Soheil%20Zou%2C%20Yang%20Rohde%2C%20Gustavo%20K.%20Sliced%20Wasserstein%20kernels%20for%20probability%20distributions%202016"
        },
        {
            "id": "Kolouri_et+al_2017_a",
            "entry": "Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal mass transport: Signal processing and machine-learning applications. IEEE Signal Processing Magazine, 34(4):43\u201359, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolouri%2C%20Soheil%20Park%2C%20Se%20Rim%20Thorpe%2C%20Matthew%20Slepcev%2C%20Dejan%20Optimal%20mass%20transport%3A%20Signal%20processing%20and%20machine-learning%20applications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolouri%2C%20Soheil%20Park%2C%20Se%20Rim%20Thorpe%2C%20Matthew%20Slepcev%2C%20Dejan%20Optimal%20mass%20transport%3A%20Signal%20processing%20and%20machine-learning%20applications%202017"
        },
        {
            "id": "Kolouri_et+al_2018_a",
            "entry": "Soheil Kolouri, Gustavo K Rohde, and Heiko Hoffman. Sliced Wasserstein distance for learning Gaussian mixture models. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 3427\u2013, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolouri%2C%20Soheil%20Rohde%2C%20Gustavo%20K.%20Hoffman%2C%20Heiko%20Sliced%20Wasserstein%20distance%20for%20learning%20Gaussian%20mixture%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolouri%2C%20Soheil%20Rohde%2C%20Gustavo%20K.%20Hoffman%2C%20Heiko%20Sliced%20Wasserstein%20distance%20for%20learning%20Gaussian%20mixture%20models%202018"
        },
        {
            "id": "Lecun_1998_a",
            "entry": "Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Ledig_et+al_2016_a",
            "entry": "Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Husz%C3%A1r%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network.%20arXiv%20p%202016"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12"
        },
        {
            "id": "Makhzani_et+al_2015_a",
            "entry": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04722"
        },
        {
            "id": "Murez_et+al_2018_a",
            "entry": "Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 4500\u20134509, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murez%2C%20Zak%20Kolouri%2C%20Soheil%20Kriegman%2C%20David%20Ramamoorthi%2C%20Ravi%20Image%20to%20image%20translation%20for%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murez%2C%20Zak%20Kolouri%2C%20Soheil%20Kriegman%2C%20David%20Ramamoorthi%2C%20Ravi%20Image%20to%20image%20translation%20for%20domain%20adaptation%202018"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Oberman_2015_a",
            "entry": "Adam M Oberman and Yuanlong Ruan. An efficient linear programming method for optimal transportation. arXiv preprint arXiv:1509.03668, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.03668"
        },
        {
            "id": "Rabin_2011_a",
            "entry": "Julien Rabin and Gabriel Peyr\u00e9. Wasserstein regularization of imaging problem. In Image Processing (ICIP), 2011 18th IEEE International Conference on, pp. 1541\u20131544. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabin%2C%20Julien%20Peyr%C3%A9%2C%20Gabriel%20Wasserstein%20regularization%20of%20imaging%20problem%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rabin%2C%20Julien%20Peyr%C3%A9%2C%20Gabriel%20Wasserstein%20regularization%20of%20imaging%20problem%202011"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Santambrogio_2015_a",
            "entry": "Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, pp. 99\u2013102, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santambrogio%2C%20Filippo%20Optimal%20transport%20for%20applied%20mathematicians%202015"
        },
        {
            "id": "Simsekli_et+al_2018_a",
            "entry": "Umut Simsekli, Antoine Liutkus, Szymon Majewski, and Alain Durmus. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. arXiv preprint arXiv:1806.08141, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08141"
        },
        {
            "id": "Solomon_et+al_2015_a",
            "entry": "Justin Solomon, Fernando De Goes, Gabriel Peyr\u00e9, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (TOG), 34(4):66, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Solomon%2C%20Justin%20Goes%2C%20Fernando%20De%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Convolutional%20wasserstein%20distances%3A%20Efficient%20optimal%20transportation%20on%20geometric%20domains%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Solomon%2C%20Justin%20Goes%2C%20Fernando%20De%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Convolutional%20wasserstein%20distances%3A%20Efficient%20optimal%20transportation%20on%20geometric%20domains%202015"
        },
        {
            "id": "Tolstikhin_et+al_2017_a",
            "entry": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01558"
        },
        {
            "id": "Villani_2008_a",
            "entry": "C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C%C3%A9dric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "Wang_et+al_2011_a",
            "entry": "Wei Wang, Yilin Mo, John A Ozolek, and Gustavo K Rohde. Penalized fisher discriminant analysis and its application to image-based morphometry. Pattern recognition letters, 32(15):2128\u20132135, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Wei%20Mo%2C%20Yilin%20Ozolek%2C%20John%20A.%20Rohde%2C%20Gustavo%20K.%20Penalized%20fisher%20discriminant%20analysis%20and%20its%20application%20to%20image-based%20morphometry%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wei%20Mo%2C%20Yilin%20Ozolek%2C%20John%20A.%20Rohde%2C%20Gustavo%20K.%20Penalized%20fisher%20discriminant%20analysis%20and%20its%20application%20to%20image-based%20morphometry%202011"
        },
        {
            "id": "Yeh_et+al_2017_a",
            "entry": "Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G Schwing, Mark Hasegawa-Johnson, and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a largescale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
