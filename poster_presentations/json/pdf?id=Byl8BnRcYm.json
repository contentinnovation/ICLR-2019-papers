{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "CAPSULE GRAPH NEURAL NETWORK",
        "author": "Zhang Xinyi, Lihui Chen School of Electrical and Electronic Engineering Nanyang Technological University, Singapore xinyi001@e.ntu.edu.sg, elhchen@ntu.edu.sg",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Byl8BnRcYm"
        },
        "journal": "Here",
        "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representations may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings. Inspired by the Capsule Neural Network (CapsNet) (<a class=\"ref-link\" id=\"cSabour_et+al_2017_a\" href=\"#rSabour_et+al_2017_a\">Sabour et al., 2017</a>), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs. Our extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument."
    },
    "keywords": [
        {
            "term": "graph property",
            "url": "https://en.wikipedia.org/wiki/graph_property"
        },
        {
            "term": "SOTA",
            "url": "https://en.wikipedia.org/wiki/SOTA"
        },
        {
            "term": "convolution operation",
            "url": "https://en.wikipedia.org/wiki/convolution_operation"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "graph embedding",
            "url": "https://en.wikipedia.org/wiki/graph_embedding"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "graph laplacian",
            "url": "https://en.wikipedia.org/wiki/Graph_Laplacian"
        },
        {
            "term": "Random Walk",
            "url": "https://en.wikipedia.org/wiki/Random_Walk"
        }
    ],
    "abbreviations": {
        "GNNs": "Graph Neural Networks",
        "CapsNet": "Capsule Neural Network",
        "CapsGNN": "Capsule Graph Neural Network",
        "GCNs": "Graph Convolutional Networks",
        "WL": "Weisfeiler-Lehman subtree kernel",
        "GK": "graphlet count kernel",
        "RW": "Random Walk",
        "FC": "fully-connected layers"
    },
    "highlights": [
        "Graph Neural Networks is a general type of deep-learning architectures that can be directly applied to structured data",
        "We propose Capsule Graph Neural Network (CapsGNN), a novel deep learning architecture, which is inspired by Capsule Neural Network and uses node features extracted from Graph Neural Networks to generate high-quality graph embeddings",
        "In Capsule Graph Neural Network, we apply Attention mechanism for two purposes: 1) scaling each node capsule so that the graph capsules that are generated from different graphs are still comparable even though these graphs are vastly different in sizes. 2) guiding the model to focus on more relevant parts of graphs.\n3 CAPSULE GRAPH NEURAL NETWORK",
        "We have proposed Capsule Graph Neural Network, a novel framework that fuses capsules theory into Graph Neural Networks for more efficient graph representation learning",
        "Inspired by Capsule Neural Network, the concepts of capsules are introduced in this architecture to extract features in the form of vectors on the basis of nodes features extracted from Graph Neural Networks",
        "Our model has successfully achieved better or comparable performance when compared with other SOTA algorithms on 6 out of 10 graph classification tasks especially on social datasets"
    ],
    "key_statements": [
        "Graph Neural Networks is a general type of deep-learning architectures that can be directly applied to structured data",
        "We propose Capsule Graph Neural Network (CapsGNN), a novel deep learning architecture, which is inspired by Capsule Neural Network and uses node features extracted from Graph Neural Networks to generate high-quality graph embeddings",
        "We provide a brief introduction to Graph Convolutional Networks (GCNs) (<a class=\"ref-link\" id=\"cKipf_2017_a\" href=\"#rKipf_2017_a\">Kipf & Welling, 2017</a>), routing mechanism in Capsule Neural Network and Attention mechanism which is used in Capsule Graph Neural Network.\n2.1",
        "In Capsule Graph Neural Network, we apply Attention mechanism for two purposes: 1) scaling each node capsule so that the graph capsules that are generated from different graphs are still comparable even though these graphs are vastly different in sizes. 2) guiding the model to focus on more relevant parts of graphs.\n3 CAPSULE GRAPH NEURAL NETWORK",
        "In Capsule Graph Neural Network, primary capsules are extracted based on each node which means the number of primary capsules depends on the size of input graphs",
        "In addition to the analysis of the whole framework, we provide a comparison experiment to evaluate the contribution of each module of Capsule Graph Neural Network with classification task",
        "We construct a scalar-based neural network for each Capsule Graph Neural Network and compare the Capsule Graph Neural Network with its related scalar-based architecture by comparing their training and testing accuracy on graph classification task to demonstrate the efficiency in feature representation",
        "We have proposed Capsule Graph Neural Network, a novel framework that fuses capsules theory into Graph Neural Networks for more efficient graph representation learning",
        "Inspired by Capsule Neural Network, the concepts of capsules are introduced in this architecture to extract features in the form of vectors on the basis of nodes features extracted from Graph Neural Networks",
        "One graph is represented as multiple embeddings and each embedding captures different aspects of the graph properties",
        "Our model has successfully achieved better or comparable performance when compared with other SOTA algorithms on 6 out of 10 graph classification tasks especially on social datasets",
        "Compared with similar scalar-based architectures, Capsule Graph Neural Network is more efficient in encoding features and this would be very beneficial for processing large datasets"
    ],
    "summary": [
        "GNN is a general type of deep-learning architectures that can be directly applied to structured data.",
        "We propose Capsule Graph Neural Network (CapsGNN), a novel deep learning architecture, which is inspired by CapsNet and uses node features extracted from GNN to generate high-quality graph embeddings.",
        "Basic node features are extracted in the form of capsules through GNN and routing mechanism is applied to generate high-level graph capsules as well as class capsules.",
        "This special property inspired us to use nodes features extracted from different layers to generate the graph capsules.",
        "In CapsGNN, we apply Attention mechanism for two purposes: 1) scaling each node capsule so that the graph capsules that are generated from different graphs are still comparable even though these graphs are vastly different in sizes.",
        "In CapsGNN, primary capsules are extracted based on each node which means the number of primary capsules depends on the size of input graphs.",
        "This demonstrates that learning features in the form of capsules and modeling a graph to multiple embeddings is beneficial to capture macroscopic properties of graphs which are more important in classifying social networks.",
        "We construct a scalar-based neural network for each CapsGNN and compare the CapsGNN with its related scalar-based architecture by comparing their training and testing accuracy on graph classification task to demonstrate the efficiency in feature representation.",
        "These scalar-based architectures are designed by replacing the graph capsules block and the class capsules block in CapsGNN with fully-connected layers (FC).",
        "When we keep increasing the number of graph capsules in CapsGNN and enlarging the dimension of FC in scalar-based architectures, the difference between the dimension of graph embeddings and the size of FC becomes larger, their training accuracy will be closer.",
        "It is noted that the training accuracy of scalar-based architectures is slightly higher than the CapsGNNs when the dimension of FC is about 20% larger than the dimension of graph capsules.",
        "Since each pair of CapsGNN and its corresponding scalar-based architecture have similar structure and comparable number of trainable weights, the higher training accuracy and testing accuracy of CapsGNN demonstrate its efficiency in feature encoding and its strong capability of generalization.",
        "The generated graph and class capsules can preserve not only the classification-related information but other information with respect to graph properties which might be useful in the follow-up work and we leave this to be explored in the future.",
        "Compared with similar scalar-based architectures, CapsGNN is more efficient in encoding features and this would be very beneficial for processing large datasets."
    ],
    "headline": "Inspired by the Capsule Neural Network, we propose the Capsule Graph Neural Network, which adopts the concept of capsules to address the weakness in existing Graph Neural Networks-based graph embeddings algorithms",
    "reference_links": [
        {
            "id": "Atwood_1993_a",
            "entry": "James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1993\u20132001, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atwood%2C%20James%20Towsley%2C%20Don%20Diffusion-convolutional%20neural%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Atwood%2C%20James%20Towsley%2C%20Don%20Diffusion-convolutional%20neural%20networks%201993"
        },
        {
            "id": "More_0000_a",
            "entry": "More details of graph distribution can be found at https://sites.google.com/view/capsgnn/home where we provide more figures generated from COLLAB and REDDIT-MULTI-12K. We also plot the graph distribution based on graph embeddings learned from DGCNN as a reference and the figures can be found at https://sites.google.com/view/capsgnn/embeddings-visualization/collab/class-capsules.",
            "url": "https://sites.google.com/view/capsgnn/home",
            "oa_query": "https://api.scholarcy.com/oa_version?query=More%20details%20of%20graph%20distribution%20can%20be%20found%20at%20httpssitesgooglecomviewcapsgnnhome%20where%20we%20provide%20more%20figures%20generated%20from%20COLLAB%20and%20REDDITMULTI12K%20We%20also%20plot%20the%20graph%20distribution%20based%20on%20graph%20embeddings%20learned%20from%20DGCNN%20as%20a%20reference%20and%20the%20figures%20can%20be%20found%20at%20httpssitesgooglecomviewcapsgnnembeddingsvisualizationcollabclasscapsules"
        },
        {
            "id": "Bruna_et+al_2013_a",
            "entry": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6203"
        },
        {
            "id": "Gehring_et+al_2016_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A convolutional encoder model for neural machine translation. arXiv preprint arXiv:1611.02344, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02344"
        },
        {
            "id": "Hamilton_et+al_2017_a",
            "entry": "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024\u20131034, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "Henaff_et+al_2015_a",
            "entry": "Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.05163"
        },
        {
            "id": "Hinton_et+al_2018_a",
            "entry": "Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with EM routing. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJWLfGWRb.",
            "url": "https://openreview.net/forum?id=HJWLfGWRb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Matrix%20capsules%20with%20EM%20routing%202018"
        },
        {
            "id": "Ivanov_2018_a",
            "entry": "Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2186\u20132195, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/ivanov18a.html.",
            "url": "http://proceedings.mlr.press/v80/ivanov18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ivanov%2C%20Sergey%20Burnaev%2C%20Evgeny%20Anonymous%20walk%20embeddings%202018-07"
        },
        {
            "id": "Kipf_2017_a",
            "entry": "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Le_2014_a",
            "entry": "Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International Conference on Machine Learning, pp. 1188\u20131196, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20Mikolov%2C%20Tomas%20Distributed%20representations%20of%20sentences%20and%20documents%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20Mikolov%2C%20Tomas%20Distributed%20representations%20of%20sentences%20and%20documents%202014"
        },
        {
            "id": "Van_2008_a",
            "entry": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Narayanan_et+al_2017_a",
            "entry": "Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. CoRR, abs/1707.05005, 2017. URL http://arxiv.org/abs/1707.05005.",
            "url": "http://arxiv.org/abs/1707.05005",
            "arxiv_url": "https://arxiv.org/pdf/1707.05005"
        },
        {
            "id": "Niepert_et+al_2014_a",
            "entry": "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014\u20132023, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014"
        },
        {
            "id": "Sabour_et+al_2017_a",
            "entry": "Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pp. 3856\u20133866, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017"
        },
        {
            "id": "Nino_et+al_2009_a",
            "entry": "Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In Artificial Intelligence and Statistics, pp. 488\u2013495, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nino%20Shervashidze%2C%20S.V.N.Vishwanathan%20Petri%2C%20Tobias%20Mehlhorn%2C%20Kurt%20Borgwardt%2C%20Karsten%20Efficient%20graphlet%20kernels%20for%20large%20graph%20comparison%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nino%20Shervashidze%2C%20S.V.N.Vishwanathan%20Petri%2C%20Tobias%20Mehlhorn%2C%20Kurt%20Borgwardt%2C%20Karsten%20Efficient%20graphlet%20kernels%20for%20large%20graph%20comparison%202009"
        },
        {
            "id": "Shervashidze_et+al_2011_a",
            "entry": "Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539\u2013 2561, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shervashidze%2C%20Nino%20Schweitzer%2C%20Pascal%20van%20Leeuwen%2C%20Erik%20Jan%20Mehlhorn%2C%20Kurt%20Weisfeiler-lehman%20graph%20kernels%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shervashidze%2C%20Nino%20Schweitzer%2C%20Pascal%20van%20Leeuwen%2C%20Erik%20Jan%20Mehlhorn%2C%20Kurt%20Weisfeiler-lehman%20graph%20kernels%202011"
        },
        {
            "id": "Simonovsky_2017_a",
            "entry": "Martin Simonovsky and Nikos Komodakis. Dynamic edgeconditioned filters in convolutional neural networks on graphs. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonovsky%2C%20Martin%20Komodakis%2C%20Nikos%20Dynamic%20edgeconditioned%20filters%20in%20convolutional%20neural%20networks%20on%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonovsky%2C%20Martin%20Komodakis%2C%20Nikos%20Dynamic%20edgeconditioned%20filters%20in%20convolutional%20neural%20networks%20on%20graphs%202017"
        },
        {
            "id": "Velikovi_et+al_2018_a",
            "entry": "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.",
            "url": "https://openreview.net/forum?id=rJXMpikCZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Velikovi%2C%20Petar%20Cucurull%2C%20Guillem%20Casanova%2C%20Arantxa%20Romero%2C%20Adriana%20Graph%20attention%20networks%202018"
        },
        {
            "id": "Verma_2017_a",
            "entry": "Saurabh Verma and Zhi-Li Zhang. Hunt for the unique, stable, sparse and fast feature learning on graphs. In Advances in Neural Information Processing Systems, pp. 88\u201398, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verma%2C%20Saurabh%20Zhang%2C%20Zhi-Li%20Hunt%20for%20the%20unique%2C%20stable%2C%20sparse%20and%20fast%20feature%20learning%20on%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verma%2C%20Saurabh%20Zhang%2C%20Zhi-Li%20Hunt%20for%20the%20unique%2C%20stable%2C%20sparse%20and%20fast%20feature%20learning%20on%20graphs%202017"
        },
        {
            "id": "Verma_2018_a",
            "entry": "Saurabh Verma and Zhi-Li Zhang. Graph capsule convolutional neural networks. arXiv preprint arXiv:1805.08090, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08090"
        },
        {
            "id": "Vishwanathan_et+al_2010_a",
            "entry": "S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph kernels. Journal of Machine Learning Research, 11(Apr):1201\u20131242, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Vichy%20N%20Vishwanathan%20Nicol%20N%20Schraudolph%20Risi%20Kondor%20and%20Karsten%20M%20Borgwardt%20Graph%20kernels%20Journal%20of%20Machine%20Learning%20Research%2011Apr12011242%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Vichy%20N%20Vishwanathan%20Nicol%20N%20Schraudolph%20Risi%20Kondor%20and%20Karsten%20M%20Borgwardt%20Graph%20kernels%20Journal%20of%20Machine%20Learning%20Research%2011Apr12011242%202010"
        },
        {
            "id": "Yanardag_2015_a",
            "entry": "Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365\u20131374. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yanardag%2C%20Pinar%20Vishwanathan%2C%20S.V.N.%20Deep%20graph%20kernels%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yanardag%2C%20Pinar%20Vishwanathan%2C%20S.V.N.%20Deep%20graph%20kernels%202015"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In Proceedings of AAAI Conference on Artificial Inteligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Muhan%20Cui%2C%20Zhicheng%20Neumann%2C%20Marion%20Chen%2C%20Yixin%20An%20end-to-end%20deep%20learning%20architecture%20for%20graph%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Muhan%20Cui%2C%20Zhicheng%20Neumann%2C%20Marion%20Chen%2C%20Yixin%20An%20end-to-end%20deep%20learning%20architecture%20for%20graph%20classification%202018"
        },
        {
            "id": "Zheng_et+al_2017_a",
            "entry": "Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learning multi-attention convolutional neural network for fine-grained image recognition. In Int. Conf. on Computer Vision, volume 6, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Heliang%20Fu%2C%20Jianlong%20Mei%2C%20Tao%20Luo%2C%20Jiebo%20Learning%20multi-attention%20convolutional%20neural%20network%20for%20fine-grained%20image%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Heliang%20Fu%2C%20Jianlong%20Mei%2C%20Tao%20Luo%2C%20Jiebo%20Learning%20multi-attention%20convolutional%20neural%20network%20for%20fine-grained%20image%20recognition%202017"
        },
        {
            "id": "After_2018_a",
            "entry": "After Attention Module, coordinate addition can be used to preserve the position information of each node during the procedure of generating node capsule votes. The details of Coordinate Addition module can be found in Figure 4. This module is not necessary in some datasets. Here, we propose this module as a selective optimization. When the GNN goes deeper, the extracted nodes features contain more specific position information of each node. Inspired by Zhang et al. (2018) where the node embeddings learned from the last layer of GNN are taken to order all nodes, we also take the capsules extracted from the last layer of GNN as the position indicators of corresponding nodes by concatenating it with each capsule of the node. The procedure of calculating votes with node position indicators can be written as: v(n,i)j = [sT(n,i)Winj sT",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=After%20Attention%20Module%20coordinate%20addition%20can%20be%20used%20to%20preserve%20the%20position%20information%20of%20each%20node%20during%20the%20procedure%20of%20generating%20node%20capsule%20votes%20The%20details%20of%20Coordinate%20Addition%20module%20can%20be%20found%20in%20Figure%204%20This%20module%20is%20not%20necessary%20in%20some%20datasets%20Here%20we%20propose%20this%20module%20as%20a%20selective%20optimization%20When%20the%20GNN%20goes%20deeper%20the%20extracted%20nodes%20features%20contain%20more%20specific%20position%20information%20of%20each%20node%20Inspired%20by%20Zhang%20et%20al%202018%20where%20the%20node%20embeddings%20learned%20from%20the%20last%20layer%20of%20GNN%20are%20taken%20to%20order%20all%20nodes%20we%20also%20take%20the%20capsules%20extracted%20from%20the%20last%20layer%20of%20GNN%20as%20the%20position%20indicators%20of%20corresponding%20nodes%20by%20concatenating%20it%20with%20each%20capsule%20of%20the%20node%20The%20procedure%20of%20calculating%20votes%20with%20node%20position%20indicators%20can%20be%20written%20as%20vnij%20%20sTniWinj%20sT",
            "oa_query": "https://api.scholarcy.com/oa_version?query=After%20Attention%20Module%20coordinate%20addition%20can%20be%20used%20to%20preserve%20the%20position%20information%20of%20each%20node%20during%20the%20procedure%20of%20generating%20node%20capsule%20votes%20The%20details%20of%20Coordinate%20Addition%20module%20can%20be%20found%20in%20Figure%204%20This%20module%20is%20not%20necessary%20in%20some%20datasets%20Here%20we%20propose%20this%20module%20as%20a%20selective%20optimization%20When%20the%20GNN%20goes%20deeper%20the%20extracted%20nodes%20features%20contain%20more%20specific%20position%20information%20of%20each%20node%20Inspired%20by%20Zhang%20et%20al%202018%20where%20the%20node%20embeddings%20learned%20from%20the%20last%20layer%20of%20GNN%20are%20taken%20to%20order%20all%20nodes%20we%20also%20take%20the%20capsules%20extracted%20from%20the%20last%20layer%20of%20GNN%20as%20the%20position%20indicators%20of%20corresponding%20nodes%20by%20concatenating%20it%20with%20each%20capsule%20of%20the%20node%20The%20procedure%20of%20calculating%20votes%20with%20node%20position%20indicators%20can%20be%20written%20as%20vnij%20%20sTniWinj%20sT"
        }
    ]
}
