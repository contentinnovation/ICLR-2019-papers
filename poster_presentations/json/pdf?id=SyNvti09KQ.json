{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "VISCERAL MACHINES: RISK-AVERSION IN REINFORCEMENT LEARNING WITH INTRINSIC PHYSIOLOGICAL REWARDS",
        "author": "Daniel McDuff and Ashish Kapoor Microsoft Research Redmond, WA {damcduff,akapoor}@microsoft.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=SyNvti09KQ"
        },
        "abstract": "As people learn to navigate the world, autonomic nervous system (e.g., \u201cfight or flight\u201d) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage."
    },
    "keywords": [
        {
            "term": "mean square error",
            "url": "https://en.wikipedia.org/wiki/mean_square_error"
        },
        {
            "term": "autonomic nervous system",
            "url": "https://en.wikipedia.org/wiki/autonomic_nervous_system"
        },
        {
            "term": "decision making",
            "url": "https://en.wikipedia.org/wiki/decision_making"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "sympathetic nervous system",
            "url": "https://en.wikipedia.org/wiki/sympathetic_nervous_system"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "root mean squared error",
            "url": "https://en.wikipedia.org/wiki/root_mean_squared_error"
        }
    ],
    "abbreviations": {
        "ANS": "autonomic nervous system",
        "SNS": "sympathetic nervous system",
        "RL": "reinforcement learning",
        "CNN": "convolutional neural network",
        "MSE": "mean square error",
        "RMSE": "root mean squared error"
    },
    "highlights": [
        "The human autonomic nervous system (ANS) is composed of two branches",
        "To design a reward function based on the nervous system response of the driver in the simulated environment we collected a data set of physiological recordings and synchronized first person video frames from the car",
        "We conducted experiments to answer: (1) if we can build a deep predictive model that estimates a peripheral physiological response associated with sympathetic nervous system activity and (2) if using such predicted responses leads to sample efficiency in the reinforcement learning framework",
        "We have presented a novel reinforcement learning paradigm using an intrinsic reward function trained on peripheral physiological responses and extrinsic rewards based on mission goals",
        "We trained a neural architecture to predict a driver\u2019s peripheral blood flow modulation based on the first-person video from the vehicle. This architecture acted as the reward in our reinforcement learning step",
        "A major advantage of training a reward on a signal correlated with the sympathetic nervous system responses is that the rewards are non-sparse - the negative reward starts to show up much before the car collides"
    ],
    "key_statements": [
        "The human autonomic nervous system (ANS) is composed of two branches",
        "Driving is an everyday example of a task in which we commonly rely on both intrinsic and extrinsic motivations and experience significant physiological changes",
        "This paper provides a reinforcement learning (RL) framework that incorporates reward functions for achieving task-specific goals and minimizes a cost trained on physiological responses to",
        "We ask if such a reward function with extrinsic and intrinsic components is useful in a reinforcement learning setting",
        "We argue that a function trained on physiological responses could be used as an intrinsic reward or value function for artificially intelligent systems, or perhaps more aptly artificially emotionally intelligent systems",
        "This research could be interpreted as indicating that intrinsic rewards serve a valuable purpose in decision-making. Automatic responses both help people act quickly and in some cases help them make better decisions. While these automatic responses can be prone to mistakes, they are vital for keeping us safe",
        "We propose to use a reward signal that is trained on a physiological signal that captures sympathetic nervous system activity",
        "The extrinsic component rewards behaviors that are task specific, whereas the intrinsic component aims to predict a human physiological response to sympathetic nervous system activity and reward actions that lead to states that reduce stress and anxiety",
        "We propose to use a modified reward rthat is a convex combination of the original reward r and a component that mirrors human physiological responses r: r = \u03bbr + (1 \u2212 \u03bb)r",
        "In an autonomous driving scenario the task dependent reward r can be the velocity, while rcan correspond to physiological responses associated with safety",
        "In the rest of the paper we focus on the autonomous driving scenario as a canonical example and discuss how we can model the appropriate physiological responses and utilize them effectively in this framework",
        "To design a reward function based on the nervous system response of the driver in the simulated environment we collected a data set of physiological recordings and synchronized first person video frames from the car",
        "Intrinsic Reward Architecture: We used a convolutional neural network to predict the normalized pulse amplitude derived from the physiological response of the driver",
        "We conducted experiments to answer: (1) if we can build a deep predictive model that estimates a peripheral physiological response associated with sympathetic nervous system activity and (2) if using such predicted responses leads to sample efficiency in the reinforcement learning framework",
        "The plots are averaged over 10 different reinforcement learning runs and we show plots for different values of \u03bb",
        "We have presented a novel reinforcement learning paradigm using an intrinsic reward function trained on peripheral physiological responses and extrinsic rewards based on mission goals",
        "We trained a neural architecture to predict a driver\u2019s peripheral blood flow modulation based on the first-person video from the vehicle. This architecture acted as the reward in our reinforcement learning step",
        "A major advantage of training a reward on a signal correlated with the sympathetic nervous system responses is that the rewards are non-sparse - the negative reward starts to show up much before the car collides",
        "This leads to efficiency in training and with proper design can lead to policies that are aligned with the desired mission"
    ],
    "summary": [
        "The human autonomic nervous system (ANS) is composed of two branches.",
        "Driving is an everyday example of a task in which we commonly rely on both intrinsic and extrinsic motivations and experience significant physiological changes.",
        "This paper provides a reinforcement learning (RL) framework that incorporates reward functions for achieving task-specific goals and minimizes a cost trained on physiological responses to",
        "We ask if such a reward function with extrinsic and intrinsic components is useful in a reinforcement learning setting.",
        "We test our approach by training a model on real visceral human responses in a driving task.",
        "The key distinction in our work is that we aim to build intrinsic reward mechanisms that are visceral and trained on signals correlated with human affective responses.",
        "We contrast sparse episodic reward signals in RL agents with physiological responses in humans.",
        "The extrinsic component rewards behaviors that are task specific, whereas the intrinsic component aims to predict a human physiological response to SNS activity and reward actions that lead to states that reduce stress and anxiety.",
        "In an autonomous driving scenario the task dependent reward r can be the velocity, while rcan correspond to physiological responses associated with safety.",
        "We use high-fidelity simulations (Shah et al, 2018) to collect physiological responses of humans and train a deep neural network to predict SNS responses that will be used during the reinforcement learning process.",
        "To design a reward function based on the nervous system response of the driver in the simulated environment we collected a data set of physiological recordings and synchronized first person video frames from the car.",
        "The agent\u2019s extrinsic reward can be based on various driving related tasks, such as keeping up the velocity, making progress towards a goal, traveling large distances, and can be penalized heavily for collisions.",
        "Intrinsic Reward Architecture: We used a CNN to predict the normalized pulse amplitude derived from the physiological response of the driver.",
        "Fig 5 shows the mean extrinsic reward per episode as a function of training time.",
        "The episode length is superior with the time decaying intrinsic reward because we are directly optimizing for safety initially and the agent quickly learns not to crash.",
        "Fig 8 shows both the average reward per episode as well as average length per episode, for the velocity task, as a function of training time.",
        "We have presented a novel reinforcement learning paradigm using an intrinsic reward function trained on peripheral physiological responses and extrinsic rewards based on mission goals.",
        "Future work will consider how to balance intrinsic and extrinsic rewards and include extensions to representations that include multiple intrinsic drives"
    ],
    "headline": "We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses",
    "reference_links": [
        {
            "id": "Allen_2007_a",
            "entry": "Allen, John. Photoplethysmography and its application in clinical physiological measurement. Physiological measurement, 28(3):R1, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen%2C%20John%20Photoplethysmography%20and%20its%20application%20in%20clinical%20physiological%20measurement%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen%2C%20John%20Photoplethysmography%20and%20its%20application%20in%20clinical%20physiological%20measurement%202007"
        },
        {
            "id": "Bandura_1977_a",
            "entry": "Bandura, Albert and Walters, Richard H. Social learning theory, volume 1. Prentice-hall Englewood Cliffs, NJ, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bandura%2C%20Albert%20Walters%2C%20Richard%20H.%20Social%20learning%20theory%2C%20volume%201.%20Prentice-hall%20Englewood%20Cliffs%201977"
        },
        {
            "id": "Chang_2015_a",
            "entry": "Chang, Kai-Wei, Krishnamurthy, Akshay, Agarwal, Alekh, Daum\u00e9 III, Hal, and Langford, John. Learning to search better than your teacher. In Proceedings of the 32nd International Conference on International Conference on Machine Learning-Volume 37, pp. 2058\u20132066. JMLR. org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%20KaiWei%20Krishnamurthy%20Akshay%20Agarwal%20Alekh%20Daum%C3%A9%20III%20Hal%20and%20Langford%20John%20Learning%20to%20search%20better%20than%20your%20teacher%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%2020582066%20JMLR%20org%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%20KaiWei%20Krishnamurthy%20Akshay%20Agarwal%20Alekh%20Daum%C3%A9%20III%20Hal%20and%20Langford%20John%20Learning%20to%20search%20better%20than%20your%20teacher%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%2020582066%20JMLR%20org%202015"
        },
        {
            "id": "Chen_2018_a",
            "entry": "Chen, Weixuan and McDuff, Daniel. Deepphys: Video-based physiological measurement using convolutional attention networks. arXiv preprint arXiv:1805.07888, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07888"
        },
        {
            "id": "Chentanez_et+al_2005_a",
            "entry": "Chentanez, Nuttapong, Barto, Andrew G, and Singh, Satinder P. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 1281\u20131288, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "Daum_et+al_2009_a",
            "entry": "Daum\u00e9, Hal, Langford, John, and Marcu, Daniel. Search-based structured prediction. Machine learning, 75(3): 297\u2013325, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daum%C3%A9%2C%20Hal%20Langford%2C%20John%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daum%C3%A9%2C%20Hal%20Langford%2C%20John%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009"
        },
        {
            "id": "Gross_2002_a",
            "entry": "Gross, James J. Emotion regulation: Affective, cognitive, and social consequences. Psychophysiology, 39(3): 281\u2013291, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gross%2C%20James%20J.%20Emotion%20regulation%3A%20Affective%2C%20cognitive%2C%20and%20social%20consequences%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gross%2C%20James%20J.%20Emotion%20regulation%3A%20Affective%2C%20cognitive%2C%20and%20social%20consequences%202002"
        },
        {
            "id": "Haber_et+al_2018_a",
            "entry": "Haber, Nick, Mrowca, Damian, Fei-Fei, Li, and Yamins, Daniel L. K. Learning to play with intrinsicallymotivated self-aware agents. arXiv preprint arXiv:1802.07442, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07442"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Ho, Jonathan and Ermon, Stefano. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Jansen_et+al_1995_a",
            "entry": "Jansen, Arthur SP, Van Nguyen, Xay, Karpitskiy, Vladimir, Mettenleiter, Thomas C, and Loewy, Arthur D. Central command neurons of the sympathetic nervous system: basis of the fight-or-flight response. Science, 270(5236):644\u2013646, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jansen%2C%20Arthur%20S.P.%20Nguyen%2C%20Van%20Xay%2C%20Karpitskiy%20Vladimir%2C%20Mettenleiter%20Central%20command%20neurons%20of%20the%20sympathetic%20nervous%20system%3A%20basis%20of%20the%20fight-or-flight%20response%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jansen%2C%20Arthur%20S.P.%20Nguyen%2C%20Van%20Xay%2C%20Karpitskiy%20Vladimir%2C%20Mettenleiter%20Central%20command%20neurons%20of%20the%20sympathetic%20nervous%20system%3A%20basis%20of%20the%20fight-or-flight%20response%201995"
        },
        {
            "id": "Jaques_et+al_2018_a",
            "entry": "Jaques, Natasha, Engel, Jesse, Ha, David, Bertsch, Fred, Picard, Rosalind, and Eck, Douglas. Learning via social awareness: improving sketch representations with facial feedback. arXiv preprint arXiv:1802.04877, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04877"
        },
        {
            "id": "Fredrickson_et+al_1998_a",
            "entry": "L. Fredrickson, Barbara and Levenson, Robert W. Positive emotions speed recovery from the cardiovascular sequelae of negative emotions. Cognition & emotion, 12(2):191\u2013220, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fredrickson%2C%20L.%20Barbara%20Levenson%2C%20Robert%20W.%20Positive%20emotions%20speed%20recovery%20from%20the%20cardiovascular%20sequelae%20of%20negative%20emotions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fredrickson%2C%20L.%20Barbara%20Levenson%2C%20Robert%20W.%20Positive%20emotions%20speed%20recovery%20from%20the%20cardiovascular%20sequelae%20of%20negative%20emotions%201998"
        },
        {
            "id": "Lerner_et+al_2015_a",
            "entry": "Lerner, Jennifer S, Li, Ye, Valdesolo, Piercarlo, and Kassam, Karim S. Emotion and decision making. Annual Review of Psychology, 66, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lerner%2C%20Jennifer%20S.%20Li%2C%20Ye%20Valdesolo%2C%20Piercarlo%20Kassam%2C%20Karim%20S.%20Emotion%20and%20decision%20making%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lerner%2C%20Jennifer%20S.%20Li%2C%20Ye%20Valdesolo%2C%20Piercarlo%20Kassam%2C%20Karim%20S.%20Emotion%20and%20decision%20making%202015"
        },
        {
            "id": "Lin_1992_a",
            "entry": "Lin, Long-Ji. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293\u2013321, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Long-Ji%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Long-Ji%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992"
        },
        {
            "id": "Loewenstein_2003_a",
            "entry": "Loewenstein, George and Lerner, Jennifer S. The role of affect in decision making. Handbook of affective science, 619(642):3, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loewenstein%2C%20George%20Lerner%2C%20Jennifer%20S.%20The%20role%20of%20affect%20in%20decision%20making%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loewenstein%2C%20George%20Lerner%2C%20Jennifer%20S.%20The%20role%20of%20affect%20in%20decision%20making%202003"
        },
        {
            "id": "Mcduff_et+al_2014_a",
            "entry": "McDuff, Daniel, Gontarek, Sarah, and Picard, Rosalind W. Remote detection of photoplethysmographic systolic and diastolic peaks using a digital camera. IEEE Transactions on Biomedical Engineering, 61(12):2948\u20132954, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDuff%2C%20Daniel%20Gontarek%2C%20Sarah%20Picard%2C%20Rosalind%20W.%20Remote%20detection%20of%20photoplethysmographic%20systolic%20and%20diastolic%20peaks%20using%20a%20digital%20camera%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDuff%2C%20Daniel%20Gontarek%2C%20Sarah%20Picard%2C%20Rosalind%20W.%20Remote%20detection%20of%20photoplethysmographic%20systolic%20and%20diastolic%20peaks%20using%20a%20digital%20camera%202014"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Pathak, Deepak, Agrawal, Pulkit, Efros, Alexei A., and Darrell, Trevor. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, pp. 2778\u20132787, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017"
        },
        {
            "id": "Poh_et+al_2010_a",
            "entry": "Poh, Ming-Zher, McDuff, Daniel J, and Picard, Rosalind W. Non-contact, automated cardiac pulse measurements using video imaging and blind source separation. Optics express, 18(10):10762\u201310774, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poh%2C%20Ming-Zher%20McDuff%2C%20Daniel%20J.%20Picard%2C%20Rosalind%20W.%20Non-contact%2C%20automated%20cardiac%20pulse%20measurements%20using%20video%20imaging%20and%20blind%20source%20separation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poh%2C%20Ming-Zher%20McDuff%2C%20Daniel%20J.%20Picard%2C%20Rosalind%20W.%20Non-contact%2C%20automated%20cardiac%20pulse%20measurements%20using%20video%20imaging%20and%20blind%20source%20separation%202010"
        },
        {
            "id": "Ross_2014_a",
            "entry": "Ross, Stephane and Bagnell, J Andrew. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5979"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Ross, St\u00e9phane, Gordon, Geoffrey, and Bagnell, Drew. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Russell_1998_a",
            "entry": "Russell, Stuart. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 101\u2013103. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998"
        },
        {
            "id": "Scheirer_et+al_2002_a",
            "entry": "Scheirer, Jocelyn, Fernandez, Raul, Klein, Jonathan, and Picard, Rosalind W. Frustrating the user on purpose: a step toward building an affective computer. Interacting with computers, 14(2):93\u2013118, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scheirer%2C%20Jocelyn%20Fernandez%2C%20Raul%20Klein%2C%20Jonathan%20Picard%2C%20Rosalind%20W.%20Frustrating%20the%20user%20on%20purpose%3A%20a%20step%20toward%20building%20an%20affective%20computer.%20Interacting%20with%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scheirer%2C%20Jocelyn%20Fernandez%2C%20Raul%20Klein%2C%20Jonathan%20Picard%2C%20Rosalind%20W.%20Frustrating%20the%20user%20on%20purpose%3A%20a%20step%20toward%20building%20an%20affective%20computer.%20Interacting%20with%202002"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1): 1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Zheng_et+al_2018_a",
            "entry": "Zheng, Zeyu, Oh, Junhyuk, and Singh, Satinder. On learning intrinsic rewards for policy gradient methods. arXiv preprint arXiv:1804.06459, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06459"
        }
    ]
}
