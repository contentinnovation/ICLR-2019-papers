{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE ROLE OF OVER-PARAMETRIZATION",
        "author": "IN GENERALIZATION OF NEURAL NETWORKS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=BygfghAcYX"
        },
        "abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes (within the range reported in the experiments), and could partly explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks."
    },
    "keywords": [
        {
            "term": "complexity measure",
            "url": "https://en.wikipedia.org/wiki/complexity_measure"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "rademacher complexity",
            "url": "https://en.wikipedia.org/wiki/rademacher_complexity"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Deep neural networks have enjoyed great success in learning across a wide variety of tasks",
        "We empirically investigate the role of over-parametrization in generalization of neural networks on 3 different datasets (MNIST, CIFAR10 and SVHN), and show that the existing complexity measures increase with the number of hidden units - do not explain the generalization behavior with over-parametrization",
        "We provide a matching lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output",
        "Since the Rademacher complexity depends on the function class considered, we need to choose the right function class that only captures the real trained networks, which is potentially much smaller than networks with all possible weights, to get a complexity measure that explains the decrease in generalization error with increasing width",
        "In this paper we present a new capacity bound for neural networks that empirically decreases with the increasing number of hidden units, and could potentially explain the better generalization performance of larger networks",
        "We provided a matching lower bound for the capacity improving on the current lower bounds for neural networks"
    ],
    "key_statements": [
        "Deep neural networks have enjoyed great success in learning across a wide variety of tasks",
        "We empirically investigate the role of over-parametrization in generalization of neural networks on 3 different datasets (MNIST, CIFAR10 and SVHN), and show that the existing complexity measures increase with the number of hidden units - do not explain the generalization behavior with over-parametrization",
        "We provide a matching lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output",
        "Since the Rademacher complexity depends on the function class considered, we need to choose the right function class that only captures the real trained networks, which is potentially much smaller than networks with all possible weights, to get a complexity measure that explains the decrease in generalization error with increasing width",
        "Motivated by our empirical observations we consider the following class of two layer neural networks that depend on the capacity and impact of the hidden units of a network",
        "Given a training set S = {xi}im=1 and \u03b3 > 0, Rademacher complexity of the composition of loss function \u03b3 over the class FW defined in equations (4) and (5) is bounded as follows:",
        "We show an explicit lower bound for the Rademacher complexity (Theorem 3), matching the first term in the above generalization bound, thereby showing its tightness",
        "While the actual numerical values are very loose, we believe they are useful tools to understand the relative generalization behavior with respect to different complexity measures, and in many cases applying a set of data-dependent techniques, one can improve the numerical values of these bounds significantly (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite & Roy, 2017</a>; <a class=\"ref-link\" id=\"cArora_et+al_2018_a\" href=\"#rArora_et+al_2018_a\">Arora et al, 2018</a>)",
        "All the previous capacity lower bounds for spectral norm bounded classes of neural networks with a scalar output and element-wise activation functions correspond to the Lipschitz constant of the network",
        "Theorem 7 in <a class=\"ref-link\" id=\"cGolowich_et+al_2018_a\" href=\"#rGolowich_et+al_2018_a\">Golowich et al (2018</a>) gives a \u03a9(s1s2 c) lower bound, c is the number of outputs of the network, for the composition of 1-Lipschitz loss function and neural networks with bounded spectral norm, or \u221e-Schatten norm",
        "In this paper we present a new capacity bound for neural networks that empirically decreases with the increasing number of hidden units, and could potentially explain the better generalization performance of larger networks",
        "We provided a matching lower bound for the capacity improving on the current lower bounds for neural networks"
    ],
    "summary": [
        "Deep neural networks have enjoyed great success in learning across a wide variety of tasks.",
        "When measured in terms of layer norms, our generalization bound depends on the Frobenius norm of the top layer and the Frobenius norm of the difference of the hidden layer weights with the initialization, which decreases with increasing network size.",
        "We empirically investigate the role of over-parametrization in generalization of neural networks on 3 different datasets (MNIST, CIFAR10 and SVHN), and show that the existing complexity measures increase with the number of hidden units - do not explain the generalization behavior with over-parametrization.",
        "Since the Rademacher complexity depends on the function class considered, we need to choose the right function class that only captures the real trained networks, which is potentially much smaller than networks with all possible weights, to get a complexity measure that explains the decrease in generalization error with increasing width.",
        "Motivated by our empirical observations we consider the following class of two layer neural networks that depend on the capacity and impact of the hidden units of a network.",
        "Our empirical observations indicate that networks we learn from real data have bounded unit capacity and unit impact and studying the generalization behavior of the above function class can potentially provide us with a better understanding of these networks.",
        "Given a training set S = {xi}im=1 and \u03b3 > 0, Rademacher complexity of the composition of loss function \u03b3 over the class FW defined in equations (4) and (5) is bounded as follows:",
        "While the actual numerical values are very loose, we believe they are useful tools to understand the relative generalization behavior with respect to different complexity measures, and in many cases applying a set of data-dependent techniques, one can improve the numerical values of these bounds significantly (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite & Roy, 2017</a>; <a class=\"ref-link\" id=\"cArora_et+al_2018_a\" href=\"#rArora_et+al_2018_a\">Arora et al, 2018</a>).",
        "All the previous capacity lower bounds for spectral norm bounded classes of neural networks with a scalar output and element-wise activation functions correspond to the Lipschitz constant of the network.",
        "Theorem 7 in <a class=\"ref-link\" id=\"cGolowich_et+al_2018_a\" href=\"#rGolowich_et+al_2018_a\">Golowich et al (2018</a>) gives a \u03a9(s1s2 c) lower bound, c is the number of outputs of the network, for the composition of 1-Lipschitz loss function and neural networks with bounded spectral norm, or \u221e-Schatten norm.",
        "In this paper we present a new capacity bound for neural networks that empirically decreases with the increasing number of hidden units, and could potentially explain the better generalization performance of larger networks."
    ],
    "headline": "We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning, pp. 254\u2013263, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Neyshabur%2C%20Behnam%20Zhang%2C%20Yi%20Stronger%20generalization%20bounds%20for%20deep%20nets%20via%20a%20compression%20approach%202018"
        },
        {
            "id": "Bach_2017_a",
            "entry": "Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Bengio_et+al_2006_a",
            "entry": "Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Advances in neural information processing systems, pp. 123\u2013130, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoshua%20Bengio%20Nicolas%20L%20Roux%20Pascal%20Vincent%20Olivier%20Delalleau%20and%20Patrice%20Marcotte%20Convex%20neural%20networks%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20123130%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoshua%20Bengio%20Nicolas%20L%20Roux%20Pascal%20Vincent%20Olivier%20Delalleau%20and%20Patrice%20Marcotte%20Convex%20neural%20networks%20In%20Advances%20in%20neural%20information%20processing%20systems%20pp%20123130%202006"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017"
        },
        {
            "id": "Golowich_et+al_2018_a",
            "entry": "Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Proceedings of the 31st Conference On Learning Theory, pp. 297\u2013299, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "Gunasekar_et+al_2017_a",
            "entry": "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "Harvey_et+al_2017_a",
            "entry": "Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In Proceedings of the 30th Conference On Learning Theory, pp. 1064\u20131068, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Keskar_et+al_2017_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In Proceeding of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Steve_1998_a",
            "entry": "Steve Lawrence, C Lee Giles, and Ah Chung Tsoi. What size neural network gives optimal generalization? Convergence properties of backpropagation. Technical report, U. of Maryland, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steve%20Lawrence%2C%20C.Lee%20Giles%20Tsoi%2C%20Ah%20Chung%20What%20size%20neural%20network%20gives%20optimal%20generalization%3F%20Convergence%20properties%20of%20backpropagation%201998"
        },
        {
            "id": "Ledoux_1991_a",
            "entry": "Michel Ledoux and Michel Talagrand. Probability in banach spaces: isoperimetry and processes. 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20Talagrand%2C%20Michel%20Probability%20in%20banach%20spaces%3A%20isoperimetry%20and%20processes%201991"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jaehoon%20Sohl-dickstein%2C%20Jascha%20Pennington%2C%20Jeffrey%20Novak%2C%20Roman%20Deep%20neural%20networks%20as%20gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jaehoon%20Sohl-dickstein%2C%20Jascha%20Pennington%2C%20Jeffrey%20Novak%2C%20Roman%20Deep%20neural%20networks%20as%20gaussian%20processes%202018"
        },
        {
            "id": "Liang_et+al_2017_a",
            "entry": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01530"
        },
        {
            "id": "Maurer_2016_a",
            "entry": "Andreas Maurer. A vector-contraction inequality for Rademacher complexities. In International Conference on Algorithmic Learning Theory, pp. 3\u201317, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20A%20vector-contraction%20inequality%20for%20Rademacher%20complexities%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20A%20vector-contraction%20inequality%20for%20Rademacher%20complexities%202016"
        },
        {
            "id": "Mohri_et+al_2012_a",
            "entry": "Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Talwalkar%2C%20Ameet%20Foundations%20of%20machine%20learning%202012"
        },
        {
            "id": "Nagarajan_2017_a",
            "entry": "Vaishnavh Nagarajan and J.Zico Kolter. Generalization in deep networks: The role of distance from initialization. Advances in neural information processing systems workshop on Deep Learning: Bridging Theory and Practice, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Generalization%20in%20deep%20networks%3A%20The%20role%20of%20distance%20from%20initialization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Generalization%20in%20deep%20networks%3A%20The%20role%20of%20distance%20from%20initialization%202017"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422\u20132430, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20PathSGD%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2024222430%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20PathSGD%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2024222430%202015a"
        },
        {
            "id": "Neyshabur_et+al_2015_b",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Proceedings of the 28th Conference On Learning Theory, pp. 1376\u20131401, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015"
        },
        {
            "id": "Neyshabur_et+al_2015_c",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. International Conference on Learning Representations workshop track, 2015c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nathan%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Neyshabur_et+al_2018_a",
            "entry": "Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-Bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-Bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novak%2C%20Roman%20Bahri%2C%20Yasaman%20Abolafia%2C%20Daniel%20A.%20Pennington%2C%20Jeffrey%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20Roman%20Bahri%2C%20Yasaman%20Abolafia%2C%20Daniel%20A.%20Pennington%2C%20Jeffrey%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018"
        },
        {
            "id": "Soudry_et+al_2018_a",
            "entry": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20Daniel%20Hoffer%2C%20Elad%20Nacson%2C%20Mor%20Shpigel%20Srebro%2C%20Nathan%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20Daniel%20Hoffer%2C%20Elad%20Nacson%2C%20Mor%20Shpigel%20Srebro%2C%20Nathan%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "Evaluations_2002_a",
            "entry": "Evaluations For each generalization bound, we have calculated the exact bound including the logterms and constants. We set the margin to 5th percentile of the margin of data points. Since bounds in Bartlett & Mendelson (2002) and Neyshabur et al. (2015c) are given for binary classificatio\u221an, we multiplied Bartlett & Mendelson (2002) by factor c and Neyshabur et al. (2015c) by factor c to make sure that the bound increases linearly with the number of classes (assuming that all output units have the same norm). Furthermore, since the reference matrices can be used in the bounds given in Bartlett et al. (2017) and Neyshabur et al. (2018), we used random initialization as the reference matrix. When plotting distributions, we estimate the distribution using standard Gaussian kernel density estimation.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evaluations%20For%20each%20generalization%20bound%20we%20have%20calculated%20the%20exact%20bound%20including%20the%20logterms%20and%20constants%20We%20set%20the%20margin%20to%205th%20percentile%20of%20the%20margin%20of%20data%20points%20Since%20bounds%20in%20Bartlett%20%20Mendelson%202002%20and%20Neyshabur%20et%20al%202015c%20are%20given%20for%20binary%20classification%20we%20multiplied%20Bartlett%20%20Mendelson%202002%20by%20factor%20c%20and%20Neyshabur%20et%20al%202015c%20by%20factor%20c%20to%20make%20sure%20that%20the%20bound%20increases%20linearly%20with%20the%20number%20of%20classes%20assuming%20that%20all%20output%20units%20have%20the%20same%20norm%20Furthermore%20since%20the%20reference%20matrices%20can%20be%20used%20in%20the%20bounds%20given%20in%20Bartlett%20et%20al%202017%20and%20Neyshabur%20et%20al%202018%20we%20used%20random%20initialization%20as%20the%20reference%20matrix%20When%20plotting%20distributions%20we%20estimate%20the%20distribution%20using%20standard%20Gaussian%20kernel%20density%20estimation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evaluations%20For%20each%20generalization%20bound%20we%20have%20calculated%20the%20exact%20bound%20including%20the%20logterms%20and%20constants%20We%20set%20the%20margin%20to%205th%20percentile%20of%20the%20margin%20of%20data%20points%20Since%20bounds%20in%20Bartlett%20%20Mendelson%202002%20and%20Neyshabur%20et%20al%202015c%20are%20given%20for%20binary%20classification%20we%20multiplied%20Bartlett%20%20Mendelson%202002%20by%20factor%20c%20and%20Neyshabur%20et%20al%202015c%20by%20factor%20c%20to%20make%20sure%20that%20the%20bound%20increases%20linearly%20with%20the%20number%20of%20classes%20assuming%20that%20all%20output%20units%20have%20the%20same%20norm%20Furthermore%20since%20the%20reference%20matrices%20can%20be%20used%20in%20the%20bounds%20given%20in%20Bartlett%20et%20al%202017%20and%20Neyshabur%20et%20al%202018%20we%20used%20random%20initialization%20as%20the%20reference%20matrix%20When%20plotting%20distributions%20we%20estimate%20the%20distribution%20using%20standard%20Gaussian%20kernel%20density%20estimation"
        }
    ]
}
