{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THEORETICAL ANALYSIS OF AUTO RATE-TUNING BY BATCH NORMALIZATION",
        "author": "Sanjeev Arora",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkxQ-nA9FX"
        },
        "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T \u22121/2 in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T \u22121/4 is also shown for stochastic gradient descent."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "learning rate",
            "url": "https://en.wikipedia.org/wiki/learning_rate"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "scale invariance",
            "url": "https://en.wikipedia.org/wiki/scale_invariance"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "stationary point",
            "url": "https://en.wikipedia.org/wiki/stationary_point"
        }
    ],
    "abbreviations": {
        "BN": "Batch Normalization"
    },
    "highlights": [
        "Batch Normalization (Ioffe & Szegedy, 2015) is one of the most important innovation in deep learning, widely used in modern neural network architectures such as ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>), Inception (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), and DenseNet (<a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>)",
        "If the learning rate for g is set optimally, no matter how the learning rate for W is set, (W ; g) converges to a first-order stationary point in the rate O(T \u22121/4 polylog(T )), which asymptotically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters factor) (Theorem 4.2)",
        "We aim to show that training neural network for the original optimization problem by gradient descent can be seen as training by adaptive methods for the intrinsic optimization problem, and it converges to a first-order stationary point in the intrinsic optimization problem with no need for tuning learning rates for W .\n2.4",
        "We studied how scale-invariance in neural networks with Batch Normalization helps optimization, and showed that gradient descent can achieve the asymptotic best convergence rate without tuning learning rates for scale-invariant parameters",
        "Our analysis suggests that scale-invariance in nerual networks introduced by Batch Normalization reduces the efforts for tuning learning rate to fit the training data",
        "We only considered gradient descent in this paper"
    ],
    "key_statements": [
        "Batch Normalization (Ioffe & Szegedy, 2015) is one of the most important innovation in deep learning, widely used in modern neural network architectures such as ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>), Inception (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), and DenseNet (<a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>)",
        "O(T \u22121/2), which asymptotically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters (Theorem 3.1); 2",
        "If the learning rate for g is set optimally, no matter how the learning rate for W is set, (W ; g) converges to a first-order stationary point in the rate O(T \u22121/4 polylog(T )), which asymptotically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters factor) (Theorem 4.2)",
        "We aim to show that training neural network for the original optimization problem by gradient descent can be seen as training by adaptive methods for the intrinsic optimization problem, and it converges to a first-order stationary point in the intrinsic optimization problem with no need for tuning learning rates for W .\n2.4",
        "We analyze the effect related to the scale-invariant properties when training a neural network by stochastic gradient descent",
        "We studied how scale-invariance in neural networks with Batch Normalization helps optimization, and showed that gradient descent can achieve the asymptotic best convergence rate without tuning learning rates for scale-invariant parameters",
        "Our analysis suggests that scale-invariance in nerual networks introduced by Batch Normalization reduces the efforts for tuning learning rate to fit the training data",
        "We only considered gradient descent in this paper"
    ],
    "summary": [
        "Batch Normalization (Ioffe & Szegedy, 2015) is one of the most important innovation in deep learning, widely used in modern neural network architectures such as ResNet (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>), Inception (<a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\"><a class=\"ref-link\" id=\"cSzegedy_et+al_2017_a\" href=\"#rSzegedy_et+al_2017_a\">Szegedy et al, 2017</a></a>), and DenseNet (<a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\"><a class=\"ref-link\" id=\"cHuang_et+al_2017_a\" href=\"#rHuang_et+al_2017_a\">Huang et al, 2017</a></a>).",
        "Even though the scale of weight parameters of a linear layer proceeding a BatchNorm no longer means anything to the function represented by the neural network, their growth has an effect of reducing the learning rate.",
        "We show that the scale-invariant parameters do not require rate tuning for lowering the training loss.",
        "O(T \u22121/2), which asymptotically matches with the convergence rate of gradient descent with optimal choice of learning rates for all parameters (Theorem 3.1); 2.",
        "This means introducing scale-invariance into neural networks potentially reduces the efforts to tune learning rates, since there are less number of parameters we need to concern in order to guarantee an asymptotically fastest convergence.",
        "We include some experiments in Appendix D, showing that it is the auto-tuning behavior we analysed in this paper empowers BN to have such convergence with arbitrary learning rate for scale-invariant parameters.",
        "In order to illustrate the optimization benefits of scaleinvariance, we consider the process of training this neural network by stochastic gradient descent with separate learning rates for W and g: wt(+i)1 \u2190 wt(i) \u2212 \u03b7w,t\u2207wt(i) Fzt, gt+1 \u2190 gt \u2212 \u03b7g,t\u2207gt Fzt. 2.3 THE INTRINSIC OPTIMIZATION PROBLEM",
        "We aim to show that training neural network for the original optimization problem by gradient descent can be seen as training by adaptive methods for the intrinsic optimization problem, and it converges to a first-order stationary point in the intrinsic optimization problem with no need for tuning learning rates for W .",
        "Our theorem is more general since it holds for any normalization methods as long as it induces scale-invariant properties to the network.",
        "We rigorously analyze the effect related to the scale-invariant properties in training neural network by full-batch gradient descent.",
        "We analyze the effect related to the scale-invariant properties when training a neural network by stochastic gradient descent.",
        "Note that this learning rate schedule matches the best known convergence rate O(T \u22121/4) of SGD in the case of smooth non-convex loss functions (<a class=\"ref-link\" id=\"cGhadimi_2013_a\" href=\"#rGhadimi_2013_a\">Ghadimi & Lan, 2013</a>).",
        "Note that the auto-tuning behavior induced by scale-invariances always decreases the learning rates.",
        "We studied how scale-invariance in neural networks with BN helps optimization, and showed that gradient descent can achieve the asymptotic best convergence rate without tuning learning rates for scale-invariant parameters.",
        "Our analysis suggests that scale-invariance in nerual networks introduced by BN reduces the efforts for tuning learning rate to fit the training data.",
        "It would be interesting to use it to show similar convergence results for more gradient methods"
    ],
    "headline": "We show that the scale-invariant parameters do not require rate tuning for lowering the training loss",
    "reference_links": [
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bjorck_et+al_2018_a",
            "entry": "Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normalization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 7705\u20137716. Curran Associates, Inc., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bjorck%2C%20Nils%20Gomes%2C%20Carla%20P.%20Selman%2C%20Bart%20Weinberger%2C%20Kilian%20Q.%20Understanding%20batch%20normalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bjorck%2C%20Nils%20Gomes%2C%20Carla%20P.%20Selman%2C%20Bart%20Weinberger%2C%20Kilian%20Q.%20Understanding%20batch%20normalization%202018"
        },
        {
            "id": "Carmon_et+al_2018_a",
            "entry": "Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points of non-convex, smooth high-dimensional functions. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Yair%20Duchi%2C%20John%20C.%20Hinder%2C%20Oliver%20Sidford%2C%20Aaron%20Lower%20bounds%20for%20finding%20stationary%20points%20of%20non-convex%2C%20smooth%20high-dimensional%20functions%202018"
        },
        {
            "id": "Cho_2017_a",
            "entry": "Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 5225\u20135235. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Minhyung%20Lee%2C%20Jaehyung%20Riemannian%20approach%20to%20batch%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Minhyung%20Lee%2C%20Jaehyung%20Riemannian%20approach%20to%20batch%20normalization%202017"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Dugas_et+al_2001_a",
            "entry": "Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating second-order functional knowledge for better option pricing. In T. K. Leen, T. G. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems 13, pp. 472\u2013478. MIT Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dugas%2C%20Charles%20Bengio%2C%20Yoshua%20Belisle%2C%20Francois%20Nadeau%2C%20Claude%20Incorporating%20second-order%20functional%20knowledge%20for%20better%20option%20pricing%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dugas%2C%20Charles%20Bengio%2C%20Yoshua%20Belisle%2C%20Francois%20Nadeau%2C%20Claude%20Incorporating%20second-order%20functional%20knowledge%20for%20better%20option%20pricing%202001"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010-05"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hoffer_et+al_2018_a",
            "entry": "Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate normalization schemes in deep networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2164\u20132174. Curran Associates, Inc., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Banner%2C%20Ron%20Golan%2C%20Itay%20Soudry%2C%20Daniel%20Norm%20matters%3A%20efficient%20and%20accurate%20normalization%20schemes%20in%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Banner%2C%20Ron%20Golan%2C%20Itay%20Soudry%2C%20Daniel%20Norm%20matters%3A%20efficient%20and%20accurate%20normalization%20schemes%20in%20deep%20networks%202018"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261\u20132269. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Ioffe_2017_a",
            "entry": "Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 1945\u20131953. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batch-normalized%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batch-normalized%20models%202017"
        },
        {
            "id": "Sergey_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning-Volume 37, pp. 448\u2013456. JMLR. org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%20448456%20JMLR%20org%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sergey%20Ioffe%20and%20Christian%20Szegedy%20Batch%20normalization%20accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%20In%20Proceedings%20of%20the%2032nd%20International%20Conference%20on%20International%20Conference%20on%20Machine%20LearningVolume%2037%20pp%20448456%20JMLR%20org%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kohler_et+al_2018_a",
            "entry": "Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint arXiv:1805.10694, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10694"
        },
        {
            "id": "Krahenbuhl_et+al_2015_a",
            "entry": "Philipp Krahenbuhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations of convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06856"
        },
        {
            "id": "Li_2018_a",
            "entry": "Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. arXiv preprint arXiv:1805.08114, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08114"
        },
        {
            "id": "Maas_et+al_2013_a",
            "entry": "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Citeseer, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models.%20In%20in%20ICML%20Workshop%20on%20Deep%20Learning%20for%20Audio%2C%20Speech%20and%20Language%20Processing%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models.%20In%20in%20ICML%20Workshop%20on%20Deep%20Learning%20for%20Audio%2C%20Speech%20and%20Language%20Processing%202013"
        },
        {
            "id": "Mishkin_2015_a",
            "entry": "Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06422"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 901\u2013909. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Durk%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Durk%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "Santurkar_et+al_2018_a",
            "entry": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2488\u2013 2498. Curran Associates, Inc., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santurkar%2C%20Shibani%20Tsipras%2C%20Dimitris%20Ilyas%2C%20Andrew%20Madry%2C%20Aleksander%20How%20does%20batch%20normalization%20help%20optimization%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santurkar%2C%20Shibani%20Tsipras%2C%20Dimitris%20Ilyas%2C%20Andrew%20Madry%2C%20Aleksander%20How%20does%20batch%20normalization%20help%20optimization%3F%202018"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Szegedy_et+al_2017_a",
            "entry": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inceptionresnet and the impact of residual connections on learning. In AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20Inception-v4%2C%20inceptionresnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20Inception-v4%2C%20inceptionresnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u2013 31, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Ulyanov_et+al_2016_a",
            "entry": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "Ward_et+al_2018_a",
            "entry": "Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01811"
        },
        {
            "id": "Wu_et+al_2018_a",
            "entry": "Xiaoxia Wu, Rachel Ward, and Leon Bottou. WNGrad: Learn the Learning Rate in Gradient Descent. arXiv preprint arXiv:1803.02865, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02865"
        },
        {
            "id": "Wu_2018_b",
            "entry": "Yuxin Wu and Kaiming He. Group normalization. In The European Conference on Computer Vision (ECCV), September 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuxin%20Wu%20and%20Kaiming%20He%20Group%20normalization%20In%20The%20European%20Conference%20on%20Computer%20Vision%20ECCV%20September%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuxin%20Wu%20and%20Kaiming%20He%20Group%20normalization%20In%20The%20European%20Conference%20on%20Computer%20Vision%20ECCV%20September%202018"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05671"
        },
        {
            "id": "Zou_2018_a",
            "entry": "Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural networks. arXiv preprint arXiv:1808.03408, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.03408"
        }
    ]
}
