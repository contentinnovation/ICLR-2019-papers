{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "EXPLAINING IMAGE CLASSIFIERS BY COUNTERFACTUAL GENERATION",
        "author": "Chun-Hao Chang, Elliot Creager, Anna Goldenberg, & David Duvenaud University of Toronto, Vector Institute {kingsley,creager,duvenaud}@cs.toronto.edu anna.goldenberg@utoronto.ca",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1MXz20cYQ"
        },
        "abstract": "When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren\u2019t. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier\u2019s decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods."
    },
    "keywords": [
        {
            "term": "FIDO",
            "url": "https://en.wikipedia.org/wiki/FIDO"
        },
        {
            "term": "data distribution",
            "url": "https://en.wikipedia.org/wiki/data_distribution"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "saliency map",
            "url": "https://en.wikipedia.org/wiki/saliency_map"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "reference value",
            "url": "https://en.wikipedia.org/wiki/reference_value"
        }
    ],
    "abbreviations": {
        "SDR": "Smallest Deletion Region",
        "SSR": "substituted inside (SDR) or outside",
        "BBMP": "Black Box Meaningful Perturbations",
        "WSL": "weakly supervised localization"
    },
    "highlights": [
        "The decisions of powerful image classifiers are difficult to interpret",
        "To encourage explanations that are consistent with the data distribution, we modify the question at hand: which region, when replaced by plausible alternative values, would maximally change classifier output? In this paper we provide a new model-agnostic framework for computing and visualizing feature importance of any differentiable classifier, based on variational Bernoulli dropout (<a class=\"ref-link\" id=\"cGal_2016_a\" href=\"#rGal_2016_a\">Gal & Ghahramani, 2016</a>)",
        "The reliance on human-labeled bounding boxes makes weakly supervised localization suboptimal for evaluating saliency maps, so we evaluate the so-called Saliency Metric proposed by <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal (2017</a>), which eschews the human labeled bounding boxes",
        "We find Black Box Meaningful Perturbations-CA is brittle with respect to its threshold value, producing either too spread-out or stringent saliency maps (Figure 11)",
        "We proposed FIDO, a new framework for explaining differentiable classifiers that uses adaptive Bernoulli dropout with strong generative in-filling to combine the best properties of recently proposed methods (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a>; <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal, 2017</a>; Zintgraf et al, 2017)",
        "By quantitative comparisons we find the FIDO saliency map provides more parsimonious explanations than existing methods"
    ],
    "key_statements": [
        "The decisions of powerful image classifiers are difficult to interpret",
        "Saliency maps are a tool for interpreting differentiable classifiers that, given a particular input example and output class, computes the sensitivity of the classification with respect to each input dimension",
        "If we think of a saliency map as interrogating the neural network classifier, these approaches have to deal with a somewhat unusual question of how the classifier responds to images outside of its training distribution",
        "To encourage explanations that are consistent with the data distribution, we modify the question at hand: which region, when replaced by plausible alternative values, would maximally change classifier output? In this paper we provide a new model-agnostic framework for computing and visualizing feature importance of any differentiable classifier, based on variational Bernoulli dropout (<a class=\"ref-link\" id=\"cGal_2016_a\" href=\"#rGal_2016_a\">Gal & Ghahramani, 2016</a>)",
        "In section 4.5 we show that FIDO saliency maps outperform Black Box Meaningful Perturbations (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a>) in a successive pixel removal task where pixels are in-filled by a generative model",
        "Generative Models: Local computes xas the average value of the surrounding non-dropped-out pixels xz=0",
        "We evaluate FIDO under various in-filling strategies as well as Black Box Meaningful Perturbations with Blur and Random in-filling strategies",
        "We find that strong generative infilling (VAE and CA) yields more parsimonious saliency maps, which is consistent with our qualitative comparisons",
        "The reliance on human-labeled bounding boxes makes weakly supervised localization suboptimal for evaluating saliency maps, so we evaluate the so-called Saliency Metric proposed by <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal (2017</a>), which eschews the human labeled bounding boxes",
        "We compare FIDO-CA to a Black Box Meaningful Perturbations variant that uses CA-GAN infilling; we evaluate FIDO with heuristic infilling (FIDO-Blur, FIDO-Random)",
        "We find Black Box Meaningful Perturbations-CA is brittle with respect to its threshold value, producing either too spread-out or stringent saliency maps (Figure 11)",
        "We proposed FIDO, a new framework for explaining differentiable classifiers that uses adaptive Bernoulli dropout with strong generative in-filling to combine the best properties of recently proposed methods (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a>; <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal, 2017</a>; Zintgraf et al, 2017)",
        "By quantitative comparisons we find the FIDO saliency map provides more parsimonious explanations than existing methods"
    ],
    "summary": [
        "The decisions of powerful image classifiers are difficult to interpret.",
        "<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi (2017</a></a></a>) and <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal (2017</a>) cast saliency computation an optimization problem informally described by the following question: which inputs, when replaced by an uninformative reference value, maximally change the classifier output?",
        "Because these methods use heuristic reference values, e.g. blurred input (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a></a></a>) or random colors (<a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\"><a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal, 2017</a></a>), they ignore the context of the surrounding pixels, often producing unnatural in-filled images (Figure 2).",
        "We marginalize out the masked region, conditioning the generative model on the non-masked parts of the image to sample counterfactual inputs that either change or preserve classifier behavior.",
        "By leveraging a powerful in-filling conditional generative model we produce saliency maps on ImageNet that identify relevant and concentrated pixels better than existing methods.",
        "<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi (2017</a></a></a>) computes saliency by optimizing the change in classifier outputs with respect to a perturbed input, expressed as the pixel-wise convex combination of the original input with a reference image.",
        "We call our method FIDO because it uses a strong generative model to Fill-In the DropOut region.",
        "<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi (2017</a></a></a>) compute saliency by directly optimizing the continuous mask z \u2208 [0, 1]U under the SDR objective, with xchosen heuristically; we call this approach Black Box Meaningful Perturbations (BBMP).",
        "We instead optimize the parameters of a Bernoulli dropout distribution q\u03b8(z), which enables us to sample reference values xfrom a learned generative model.",
        "Classifier score sM, sparsity hyperparameter \u03bb, objective L \u2208 {LSSR, LSDR} Initialize single mask z \u2208 [0, 1]d while loss L is not converged do",
        "Classifier score sM, sparsity hyperparameter \u03bb, objective L \u2208 {LSSR, LSDR}, generative model pG Initialize dropout rate \u03b8 \u2208 [0, 1]d while loss L is not converged do",
        "In section 4.5 we show that FIDO saliency maps outperform BBMP (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a></a></a>) in a successive pixel removal task where pixels are in-filled by a generative model.",
        "We instead dropout pixels in saliency order and infill their values with our strongest generative model, CA-GAN.",
        "From the superior performance of FIDO-CA we conclude that a strong generative model regularizes explanations towards the natural image manifold and finds concentrated region of features relevant to the classifier\u2019s prediction.",
        "We proposed FIDO, a new framework for explaining differentiable classifiers that uses adaptive Bernoulli dropout with strong generative in-filling to combine the best properties of recently proposed methods (<a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\"><a class=\"ref-link\" id=\"cFong_2017_a\" href=\"#rFong_2017_a\">Fong & Vedaldi, 2017</a></a></a>; <a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\"><a class=\"ref-link\" id=\"cDabkowski_2017_a\" href=\"#rDabkowski_2017_a\">Dabkowski & Gal, 2017</a></a>; Zintgraf et al, 2017).",
        "We released the code in PyTorch at https://github. com/zzzace2000/FIDO-saliency"
    ],
    "headline": "To encourage explanations that are consistent with the data distribution, we modify the question at hand: which region, when replaced by plausible alternative values, would maximally change classifier output? In this paper we provide a new model-agnostic framework for computing and visualizing feature importance of any differentiable classifier, based on variational Bernoulli dropout",
    "reference_links": [
        {
            "id": "Adebayo_et+al_2018_a",
            "entry": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp. 9525\u20139536, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adebayo%2C%20Julius%20Gilmer%2C%20Justin%20Muelly%2C%20Michael%20Goodfellow%2C%20Ian%20Sanity%20checks%20for%20saliency%20maps%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adebayo%2C%20Julius%20Gilmer%2C%20Justin%20Muelly%2C%20Michael%20Goodfellow%2C%20Ian%20Sanity%20checks%20for%20saliency%20maps%202018"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An informationtheoretic perspective on model interpretation. In International Conference on Machine Learning, pp. 882\u2013891, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jianbo%20Song%2C%20Le%20Wainwright%2C%20Martin%20Jordan%2C%20Michael%20Learning%20to%20explain%3A%20An%20informationtheoretic%20perspective%20on%20model%20interpretation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jianbo%20Song%2C%20Le%20Wainwright%2C%20Martin%20Jordan%2C%20Michael%20Learning%20to%20explain%3A%20An%20informationtheoretic%20perspective%20on%20model%20interpretation%202018"
        },
        {
            "id": "Dabkowski_2017_a",
            "entry": "Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in Neural Information Processing Systems, pp. 6967\u20136976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dabkowski%2C%20Piotr%20Gal%2C%20Yarin%20Real%20time%20image%20saliency%20for%20black%20box%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dabkowski%2C%20Piotr%20Gal%2C%20Yarin%20Real%20time%20image%20saliency%20for%20black%20box%20classifiers%202017"
        },
        {
            "id": "Fong_2017_a",
            "entry": "R. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fong%2C%20R.%20Vedaldi%2C%20A.%20Interpretable%20explanations%20of%20black%20boxes%20by%20meaningful%20perturbation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fong%2C%20R.%20Vedaldi%2C%20A.%20Interpretable%20explanations%20of%20black%20boxes%20by%20meaningful%20perturbation%202017"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "Gal_et+al_2017_a",
            "entry": "Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Advances in Neural Information Processing Systems, pp. 3581\u20133590, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarin%20Gal%20Jiri%20Hron%20and%20Alex%20Kendall%20Concrete%20dropout%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2035813590%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarin%20Gal%20Jiri%20Hron%20and%20Alex%20Kendall%20Concrete%20dropout%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2035813590%202017"
        },
        {
            "id": "Iizuka_et+al_2017_a",
            "entry": "Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and Locally Consistent Image Completion. ACM Transactions on Graphics (Proc. of SIGGRAPH), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20Satoshi%20Simo-Serra%2C%20Edgar%20Ishikawa%2C%20Hiroshi%20Globally%20and%20Locally%20Consistent%20Image%20Completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20Satoshi%20Simo-Serra%2C%20Edgar%20Ishikawa%2C%20Hiroshi%20Globally%20and%20Locally%20Consistent%20Image%20Completion%202017"
        },
        {
            "id": "Jang_et+al_2017_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20gumbel-softmax%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20gumbel-softmax%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Maddison_2017_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20and%20Yee%20Whye%20Teh.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20and%20Yee%20Whye%20Teh.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017"
        },
        {
            "id": "Selvaraju_et+al_2016_a",
            "entry": "Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Why did you say that? arXiv preprint arXiv:1611.07450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07450"
        },
        {
            "id": "Shrikumar_et+al_2017_a",
            "entry": "Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3145\u20133153. JMLR. org, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Kundaje%2C%20Anshul%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Kundaje%2C%20Anshul%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017"
        },
        {
            "id": "Simonyan_et+al_2013_a",
            "entry": "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "Springenberg_et+al_2014_a",
            "entry": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6806"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5505\u20135514, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Jiahui%20Lin%2C%20Zhe%20Yang%2C%20Jimei%20Shen%2C%20Xiaohui%20Generative%20image%20inpainting%20with%20contextual%20attention%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Jiahui%20Lin%2C%20Zhe%20Yang%2C%20Jimei%20Shen%2C%20Xiaohui%20Generative%20image%20inpainting%20with%20contextual%20attention%202018"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Published as a conference paper at ICLR 2019 Jianming Zhang, Zhe Lin, Shen Xiaohui Brandt, Jonathan, and Stan Sclaroff. Top-down neural attention by excitation backprop. In European Conference on Computer Vision(ECCV), 2016. Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Jianming%20Zhang%20Zhe%20Lin%20Shen%20Xiaohui%20Brandt%20Jonathan%20and%20Stan%20Sclaroff%20Topdown%20neural%20attention%20by%20excitation%20backprop%20In%20European%20Conference%20on%20Computer%20VisionECCV%202016%20Luisa%20M%20Zintgraf%20Taco%20S%20Cohen%20Tameem%20Adel%20and%20Max%20Welling%20Visualizing%20deep%20neural%20network%20decisions%20Prediction%20difference%20analysis%20In%20International%20Conference%20on%20Learning%20Representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Jianming%20Zhang%20Zhe%20Lin%20Shen%20Xiaohui%20Brandt%20Jonathan%20and%20Stan%20Sclaroff%20Topdown%20neural%20attention%20by%20excitation%20backprop%20In%20European%20Conference%20on%20Computer%20VisionECCV%202016%20Luisa%20M%20Zintgraf%20Taco%20S%20Cohen%20Tameem%20Adel%20and%20Max%20Welling%20Visualizing%20deep%20neural%20network%20decisions%20Prediction%20difference%20analysis%20In%20International%20Conference%20on%20Learning%20Representations%202017"
        }
    ]
}
