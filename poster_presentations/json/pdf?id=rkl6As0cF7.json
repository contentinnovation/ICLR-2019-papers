{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "PROBABILISTIC RECURSIVE REASONING FOR MULTI-AGENT REINFORCEMENT LEARNING",
        "author": "Ying Wen\u00a7\u2217, Yaodong Yang\u00a7\u2217, Rui Luo\u00a7, Jun Wang\u00a7, Wei Pan \u00a7University College London, Delft University of Technology {ying.wen,yaodong.yang,rui.luo,jun.wang}@cs.ucl.ac.uk {wei.pan}@tudelft.nl",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rkl6As0cF7"
        },
        "abstract": "Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we start from level-1 recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents\u2019 conditional policy, to which each agent finds the best response and then improve their own policy. We develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "psychology",
            "url": "https://en.wikipedia.org/wiki/psychology"
        },
        {
            "term": "deep reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/deep_reinforcement_learning"
        },
        {
            "term": "Theory of Mind",
            "url": "https://en.wikipedia.org/wiki/Theory_of_Mind"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        }
    ],
    "abbreviations": {
        "AI": "artificial intelligent",
        "DRL": "deep reinforcement learning",
        "ToM": "Theory of Mind",
        "RMM": "Recursive Modeling Method\"",
        "I-POMDP": "Interactive POMDP",
        "OM": "opponent modeling",
        "IL": "independent learning",
        "SVGD": "Stein Variational Gradient Descent",
        "SGA": "Symplectic Gradient Adjustment"
    },
    "highlights": [
        "In the long journey of creating artificial intelligent (AI) that mimics human intelligence, a hallmark of an artificial intelligent agent is its capabilities of understanding and interacting with other agents (<a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\"><a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\">Lake et al, 2017</a></a>)",
        "It is known that people can use this reasoning ability recursively; that is, they engage in considering what others believe about their own beliefs",
        "Out work is different from Interactive POMDP in that we do not adjust the MDP; instead, we provide a probabilistic framework to implement the recursive reason in the MDP",
        "We present the results in Fig. 4b, PR2-AC shows superior performance that manages to converge to the global equilibrium, while all the other baselines fall into the local basin on the left, except that the MASQL has small chance to find the optimal point",
        "By comparing the learning path in Fig. 4a against Fig. 5(a-e) where the scattered blue dots are the exploration trails at the beginning, we can tell that if the PR2-AC model finds the peak point in joint action space, the agents can quickly go through the shortcut out of the local basin in a clever way, while other algorithms just converge to the local equilibrium",
        "Inspired by the recursive reasoning capability of human intelligence, in this paper, we introduce a probabilistic recursive reasoning framework for multi-agent RL that follows \"I believe that you believe that I believe\""
    ],
    "key_statements": [
        "In the long journey of creating artificial intelligent (AI) that mimics human intelligence, a hallmark of an artificial intelligent agent is its capabilities of understanding and interacting with other agents (<a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\"><a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\">Lake et al, 2017</a></a>)",
        "It is known that people can use this reasoning ability recursively; that is, they engage in considering what others believe about their own beliefs",
        "We introduce a probabilistic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks",
        "By employing variational Bayes methods to model the uncertainty of opponents\u2019 conditional policies, we develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, and prove the convergence in the self-play scenario when there is only one Nash equilibrium",
        "Out work is different from Interactive POMDP in that we do not adjust the MDP; instead, we provide a probabilistic framework to implement the recursive reason in the MDP",
        "We introduce the probabilistic recursive reasoning approach that aims to capture how the opponents believe about what the agent believes",
        "To compare against traditional method of opponent modeling, we include one baseline that is based on DDPG but with one additional opponent modeling unit that is trained in an online and supervised way to learn the most recent opponent policy, which is fed into the critic",
        "We present the results in Fig. 4b, PR2-AC shows superior performance that manages to converge to the global equilibrium, while all the other baselines fall into the local basin on the left, except that the MASQL has small chance to find the optimal point",
        "By comparing the learning path in Fig. 4a against Fig. 5(a-e) where the scattered blue dots are the exploration trails at the beginning, we can tell that if the PR2-AC model finds the peak point in joint action space, the agents can quickly go through the shortcut out of the local basin in a clever way, while other algorithms just converge to the local equilibrium",
        "Similar result can be found, that is, algorithm that has the function of taking into account the opponents (i.e. DDPG_OM & MADDPG) can converge to the local equilibrium even though not global, while DDPG and MASQL completely fails due to the inborn defect from the independent learning methods.\n5.3",
        "Inspired by the recursive reasoning capability of human intelligence, in this paper, we introduce a probabilistic recursive reasoning framework for multi-agent RL that follows \"I believe that you believe that I believe\"",
        "Our results on three kinds of testing beds with increasing complexity justify the advantages of learning to reason about the opponents in a recursive manner"
    ],
    "summary": [
        "In the long journey of creating artificial intelligent (AI) that mimics human intelligence, a hallmark of an AI agent is its capabilities of understanding and interacting with other agents (<a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\"><a class=\"ref-link\" id=\"cLake_et+al_2017_a\" href=\"#rLake_et+al_2017_a\">Lake et al, 2017</a></a>).",
        "With the recursive joint policy defined in Eq 3, the n-agent learning task can be formulated as arg max \u03b7i \u03c0\u03b8i i\u03c1\u2212\u03c6\u2212i i (a\u2212i|s, ai) , (4)",
        "Given the true opponent policy \u03c0\u03b8\u2212\u2212ii and that each agent tries to maximize its cumulative return in the stochastic game with the objective defined in Eq 1, we establish the policy gradient theorem by accounting for the PR2 joint policy decomposition in Eq 3.",
        "Under the recursive reasoning framework defined by Eq 3, the update for the multi-agent recursive reasoning policy gradient method can be derived as follows:",
        "Under the recursive reasoning framework defined by Eq 3, with the opponent policy approximated by \u03c1\u2212\u03c6\u2212i i (a\u2212i|s, ai), the update for the multi-agent recursive reasoning policy gradient method can be formulated as follows:",
        "The objective introduces an additional term of the conditional entropy on the joint policy H \u03c0\u03b8i i ati|st \u03c1\u2212\u03c6\u2212i i a\u2212i|st, ati that potentially promotes the explorations for both the agent i\u2019s best response and the opponents\u2019 conditional policy.",
        "To compare against traditional method of opponent modeling, we include one baseline that is based on DDPG but with one additional opponent modeling unit that is trained in an online and supervised way to learn the most recent opponent policy, which is fed into the critic.",
        "Our method does not need to tune the the annealing parameter at all because the each agent is acting the best response to the approximated conditional policy, considering all potential consequences of the opponent\u2019s response.",
        "By comparing the learning path in Fig. 4a against Fig. 5(a-e) where the scattered blue dots are the exploration trails at the beginning, we can tell that if the PR2-AC model finds the peak point in joint action space, the agents can quickly go through the shortcut out of the local basin in a clever way, while other algorithms just converge to the local equilibrium.",
        "Similar result can be found, that is, algorithm that has the function of taking into account the opponents (i.e. DDPG_OM & MADDPG) can converge to the local equilibrium even though not global, while DDPG and MASQL completely fails due to the inborn defect from the independent learning methods.",
        "We plan to investigate other approximation methods for the PR2 framework, and test our PR2 algorithm for the coordination task between AI agents such as coordinating autonomous cars before the traffic light"
    ],
    "headline": "We develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium",
    "reference_links": [
        {
            "id": "Albrecht_2018_a",
            "entry": "Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66\u201395, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018"
        },
        {
            "id": "Balduzzi_et+al_2018_a",
            "entry": "David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05642"
        },
        {
            "id": "Banerjee_2007_a",
            "entry": "Dipyaman Banerjee and Sandip Sen. Reaching pareto-optimality in prisoner\u2019s dilemma using conditional joint action learning. Autonomous Agents and Multi-Agent Systems, 15(1):91\u2013108, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Dipyaman%20Sen%2C%20Sandip%20Reaching%20pareto-optimality%20in%20prisoner%E2%80%99s%20dilemma%20using%20conditional%20joint%20action%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Dipyaman%20Sen%2C%20Sandip%20Reaching%20pareto-optimality%20in%20prisoner%E2%80%99s%20dilemma%20using%20conditional%20joint%20action%20learning%202007"
        },
        {
            "id": "Bolander_2011_a",
            "entry": "Thomas Bolander and Mikkel Birkegaard Andersen. Epistemic planning for single-and multi-agent systems. Journal of Applied Non-Classical Logics, 21(1):9\u201334, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bolander%2C%20Thomas%20Andersen%2C%20Mikkel%20Birkegaard%20Epistemic%20planning%20for%20single-and%20multi-agent%20systems%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bolander%2C%20Thomas%20Andersen%2C%20Mikkel%20Birkegaard%20Epistemic%20planning%20for%20single-and%20multi-agent%20systems%202011"
        },
        {
            "id": "Bowling_2005_a",
            "entry": "Michael Bowling. Convergence and no-regret in multiagent learning. In Advances in neural information processing systems, pp. 209\u2013216, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Convergence%20and%20no-regret%20in%20multiagent%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Convergence%20and%20no-regret%20in%20multiagent%20learning%202005"
        },
        {
            "id": "Bowling_0000_a",
            "entry": "Michael Bowling and Manuela Veloso. Convergence of gradient dynamics with a variable learning rate. In ICML, pp. 27\u201334, 2001a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Convergence%20of%20gradient%20dynamics%20with%20a%20variable%20learning%20rate",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Convergence%20of%20gradient%20dynamics%20with%20a%20variable%20learning%20rate"
        },
        {
            "id": "Bowling_2001_a",
            "entry": "Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence, volume 17, pp. 1021\u20131026. Lawrence Erlbaum Associates Ltd, 2001b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Rational%20and%20convergent%20learning%20in%20stochastic%20games%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Rational%20and%20convergent%20learning%20in%20stochastic%20games%202001"
        },
        {
            "id": "Bowling_2002_a",
            "entry": "Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence, 136(2):215\u2013250, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Multiagent%20learning%20using%20a%20variable%20learning%20rate%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Multiagent%20learning%20using%20a%20variable%20learning%20rate%202002"
        },
        {
            "id": "Brown_1951_a",
            "entry": "George W Brown. Iterative solution of games by fictitious play. Activity analysis of production and allocation, 13(1):374\u2013376, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20George%20W.%20Iterative%20solution%20of%20games%20by%20fictitious%20play.%20Activity%20analysis%20of%20production%20and%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20George%20W.%20Iterative%20solution%20of%20games%20by%20fictitious%20play.%20Activity%20analysis%20of%20production%20and%201951"
        },
        {
            "id": "Camerer_et+al_2004_a",
            "entry": "Colin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A cognitive hierarchy model of games. The Quarterly Journal of Economics, 119(3):861\u2013898, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camerer%2C%20Colin%20F.%20Ho%2C%20Teck-Hua%20Chong%2C%20Juin-Kuan%20A%20cognitive%20hierarchy%20model%20of%20games%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camerer%2C%20Colin%20F.%20Ho%2C%20Teck-Hua%20Chong%2C%20Juin-Kuan%20A%20cognitive%20hierarchy%20model%20of%20games%202004"
        },
        {
            "id": "Camerer_et+al_2015_a",
            "entry": "Colin F Camerer, Teck-Hua Ho, and Juin Kuan Chong. A psychological approach to strategic thinking in games. Current Opinion in Behavioral Sciences, 3:157\u2013162, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camerer%2C%20Colin%20F.%20Ho%2C%20Teck-Hua%20Chong%2C%20Juin%20Kuan%20A%20psychological%20approach%20to%20strategic%20thinking%20in%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camerer%2C%20Colin%20F.%20Ho%2C%20Teck-Hua%20Chong%2C%20Juin%20Kuan%20A%20psychological%20approach%20to%20strategic%20thinking%20in%20games%202015"
        },
        {
            "id": "Claus_1998_a",
            "entry": "Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998:746\u2013752, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Claus%2C%20Caroline%20Boutilier%2C%20Craig%20The%20dynamics%20of%20reinforcement%20learning%20in%20cooperative%20multiagent%20systems%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Claus%2C%20Caroline%20Boutilier%2C%20Craig%20The%20dynamics%20of%20reinforcement%20learning%20in%20cooperative%20multiagent%20systems%201998"
        },
        {
            "id": "Silva_et+al_2006_a",
            "entry": "Bruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and Paulo M Engel. Dealing with nonstationary environments using context detection. In Proceedings of the 23rd international conference on Machine learning, pp. 217\u2013224. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silva%2C%20Bruno%20C.Da%20Basso%2C%20Eduardo%20W.%20Bazzan%2C%20Ana%20L.C.%20Engel%2C%20Paulo%20M.%20Dealing%20with%20nonstationary%20environments%20using%20context%20detection%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silva%2C%20Bruno%20C.Da%20Basso%2C%20Eduardo%20W.%20Bazzan%2C%20Ana%20L.C.%20Engel%2C%20Paulo%20M.%20Dealing%20with%20nonstationary%20environments%20using%20context%20detection%202006"
        },
        {
            "id": "Springer,_2013_a",
            "entry": "Springer, 2013a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202013a"
        },
        {
            "id": "Weerd_et+al_2013_a",
            "entry": "Harmen De Weerd, Rineke Verbrugge, and Bart Verheij. How much does it help to know what she knows you know? an agent-based simulation study. Artificial Intelligence, 199:67\u201392, 2013b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weerd%2C%20Harmen%20De%20Verbrugge%2C%20Rineke%20Verheij%2C%20Bart%20How%20much%20does%20it%20help%20to%20know%20what%20she%20knows%20you%20know%3F%20an%20agent-based%20simulation%20study%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weerd%2C%20Harmen%20De%20Verbrugge%2C%20Rineke%20Verheij%2C%20Bart%20How%20much%20does%20it%20help%20to%20know%20what%20she%20knows%20you%20know%3F%20an%20agent-based%20simulation%20study%202013"
        },
        {
            "id": "De_et+al_2017_a",
            "entry": "Harmen de Weerd, Rineke Verbrugge, and Bart Verheij. Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information. Autonomous Agents and Multi-Agent Systems, 31(2):250\u2013287, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Weerd%2C%20Harmen%20Verbrugge%2C%20Rineke%20Verheij%2C%20Bart%20Negotiating%20with%20other%20minds%3A%20the%20role%20of%20recursive%20theory%20of%20mind%20in%20negotiation%20with%20incomplete%20information%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Weerd%2C%20Harmen%20Verbrugge%2C%20Rineke%20Verheij%2C%20Bart%20Negotiating%20with%20other%20minds%3A%20the%20role%20of%20recursive%20theory%20of%20mind%20in%20negotiation%20with%20incomplete%20information%202017"
        },
        {
            "id": "Dennett_1991_a",
            "entry": "Daniel C Dennett. Two contrasts: folk craft versus folk science, and belief versus opinion. The future of folk psychology: Intentionality and cognitive science, pp. 135\u2013148, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dennett%2C%20Daniel%20C.%20Two%20contrasts%3A%20folk%20craft%20versus%20folk%20science%2C%20and%20belief%20versus%20opinion.%20The%20future%20of%20folk%20psychology%3A%20Intentionality%20and%20cognitive%20science%201991"
        },
        {
            "id": "Doshi_1999_a",
            "entry": "Prashant Doshi and Piotr J Gmytrasiewicz. On the difficulty of achieving equilibrium in interactive pomdps. In PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, volume 21, pp. 1131. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20On%20the%20difficulty%20of%20achieving%20equilibrium%20in%20interactive%20pomdps.%20In%20PROCEEDINGS%20OF%20THE%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20On%20the%20difficulty%20of%20achieving%20equilibrium%20in%20interactive%20pomdps.%20In%20PROCEEDINGS%20OF%20THE%201999"
        },
        {
            "id": "Doshi_2009_a",
            "entry": "Prashant Doshi and Piotr J Gmytrasiewicz. Monte carlo sampling methods for approximating interactive pomdps. Journal of Artificial Intelligence Research, 34:297\u2013337, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20Monte%20carlo%20sampling%20methods%20for%20approximating%20interactive%20pomdps%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20Monte%20carlo%20sampling%20methods%20for%20approximating%20interactive%20pomdps%202009"
        },
        {
            "id": "Doshi_2008_a",
            "entry": "Prashant Doshi and Dennis Perez. Generalized point based value iteration for interactive pomdps. In AAAI, pp. 63\u201368, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi%2C%20Prashant%20Perez%2C%20Dennis%20Generalized%20point%20based%20value%20iteration%20for%20interactive%20pomdps%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi%2C%20Prashant%20Perez%2C%20Dennis%20Generalized%20point%20based%20value%20iteration%20for%20interactive%20pomdps%202008"
        },
        {
            "id": "Doshi_et+al_2009_b",
            "entry": "Prashant Doshi, Yifeng Zeng, and Qiongyu Chen. Graphical models for interactive pomdps: representations and solutions. Autonomous Agents and Multi-Agent Systems, 18(3):376, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi%2C%20Prashant%20Zeng%2C%20Yifeng%20Chen%2C%20Qiongyu%20Graphical%20models%20for%20interactive%20pomdps%3A%20representations%20and%20solutions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi%2C%20Prashant%20Zeng%2C%20Yifeng%20Chen%2C%20Qiongyu%20Graphical%20models%20for%20interactive%20pomdps%3A%20representations%20and%20solutions%202009"
        },
        {
            "id": "Foerster_et+al_2017_a",
            "entry": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08926"
        },
        {
            "id": "Foerster_et+al_2018_a",
            "entry": "Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 122\u2013130. International Foundation for Autonomous Agents and Multiagent Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202018"
        },
        {
            "id": "Fox_et+al_2016_a",
            "entry": "Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pp. 202\u2013211. AUAI Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fox%2C%20Roy%20Pakman%2C%20Ari%20Tishby%2C%20Naftali%20Taming%20the%20noise%20in%20reinforcement%20learning%20via%20soft%20updates%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fox%2C%20Roy%20Pakman%2C%20Ari%20Tishby%2C%20Naftali%20Taming%20the%20noise%20in%20reinforcement%20learning%20via%20soft%20updates%202016"
        },
        {
            "id": "Gal_2003_a",
            "entry": "Ya\u2019akov Gal and Avi Pfeffer. A language for modeling agents\u2019 decision making processes in games. In Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pp. 265\u2013272. ACM, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Ya%E2%80%99akov%20Pfeffer%2C%20Avi%20A%20language%20for%20modeling%20agents%E2%80%99%20decision%20making%20processes%20in%20games%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Ya%E2%80%99akov%20Pfeffer%2C%20Avi%20A%20language%20for%20modeling%20agents%E2%80%99%20decision%20making%20processes%20in%20games%202003"
        },
        {
            "id": "Gal_2008_a",
            "entry": "Ya\u2019akov Gal and Avi Pfeffer. Networks of influence diagrams: a formalism for representing agents\u2019 beliefs and decision-making processes. Journal of Artificial Intelligence Research, 33:109\u2013147, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Ya%E2%80%99akov%20Pfeffer%2C%20Avi%20Networks%20of%20influence%20diagrams%3A%20a%20formalism%20for%20representing%20agents%E2%80%99%20beliefs%20and%20decision-making%20processes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Ya%E2%80%99akov%20Pfeffer%2C%20Avi%20Networks%20of%20influence%20diagrams%3A%20a%20formalism%20for%20representing%20agents%E2%80%99%20beliefs%20and%20decision-making%20processes%202008"
        },
        {
            "id": "Gallese_1998_a",
            "entry": "Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mind-reading. Trends in cognitive sciences, 2(12):493\u2013501, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gallese%2C%20Vittorio%20Goldman%2C%20Alvin%20Mirror%20neurons%20and%20the%20simulation%20theory%20of%20mind-reading%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gallese%2C%20Vittorio%20Goldman%2C%20Alvin%20Mirror%20neurons%20and%20the%20simulation%20theory%20of%20mind-reading%201998"
        },
        {
            "id": "Gmytrasiewicz_2005_a",
            "entry": "Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multi-agent settings. Journal of Artificial Intelligence Research, 24:49\u201379, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Doshi%2C%20Prashant%20A%20framework%20for%20sequential%20planning%20in%20multi-agent%20settings%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Doshi%2C%20Prashant%20A%20framework%20for%20sequential%20planning%20in%20multi-agent%20settings%202005"
        },
        {
            "id": "Gmytrasiewicz_1995_a",
            "entry": "Piotr J Gmytrasiewicz and Edmund H Durfee. A rigorous, operational formalization of recursive modeling. In ICMAS, pp. 125\u2013132, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20A%20rigorous%2C%20operational%20formalization%20of%20recursive%20modeling%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20A%20rigorous%2C%20operational%20formalization%20of%20recursive%20modeling%201995"
        },
        {
            "id": "Gmytrasiewicz_2000_a",
            "entry": "Piotr J Gmytrasiewicz and Edmund H Durfee. Rational coordination in multi-agent environments. Autonomous Agents and Multi-Agent Systems, 3(4):319\u2013350, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Rational%20coordination%20in%20multi-agent%20environments%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Rational%20coordination%20in%20multi-agent%20environments%202000"
        },
        {
            "id": "Gmytrasiewicz_et+al_1991_a",
            "entry": "Piotr J Gmytrasiewicz, Edmund H Durfee, and David K Wehe. A decision-theoretic approach to coordinating multi-agent interactions. In IJCAI, volume 91, pp. 63\u201368, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Wehe%2C%20David%20K.%20A%20decision-theoretic%20approach%20to%20coordinating%20multi-agent%20interactions%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Wehe%2C%20David%20K.%20A%20decision-theoretic%20approach%20to%20coordinating%20multi-agent%20interactions%201991"
        },
        {
            "id": "Goldman_2012_a",
            "entry": "Alvin I Goldman et al. Theory of mind. The Oxford handbook of philosophy of cognitive science, pp. 402\u2013424, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldman%2C%20Alvin%20I.%20Theory%20of%20mind.%20The%20Oxford%20handbook%20of%20philosophy%20of%20cognitive%20science%202012"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goodie_et+al_2012_a",
            "entry": "Adam S Goodie, Prashant Doshi, and Diana L Young. Levels of theory-of-mind reasoning in competitive games. Journal of Behavioral Decision Making, 25(1):95\u2013108, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodie%2C%20Adam%20S.%20Doshi%2C%20Prashant%20Young%2C%20Diana%20L.%20Levels%20of%20theory-of-mind%20reasoning%20in%20competitive%20games%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodie%2C%20Adam%20S.%20Doshi%2C%20Prashant%20Young%2C%20Diana%20L.%20Levels%20of%20theory-of-mind%20reasoning%20in%20competitive%20games%202012"
        },
        {
            "id": "Gopnik_1992_a",
            "entry": "Alison Gopnik and Henry M Wellman. Why the child\u2019s theory of mind really is a theory. Mind & Language, 7(1-2):145\u2013171, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20Why%20the%20child%E2%80%99s%20theory%20of%20mind%20really%20is%20a%20theory%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20Why%20the%20child%E2%80%99s%20theory%20of%20mind%20really%20is%20a%20theory%201992"
        },
        {
            "id": "Gordon_1986_a",
            "entry": "Robert M Gordon. Folk psychology as simulation. Mind & Language, 1(2):158\u2013171, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Robert%20M.%20Folk%20psychology%20as%20simulation%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Robert%20M.%20Folk%20psychology%20as%20simulation%201986"
        },
        {
            "id": "Greenwald_et+al_2003_a",
            "entry": "Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In ICML, volume 3, pp. 242\u2013249, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greenwald%2C%20Amy%20Hall%2C%20Keith%20Serrano%2C%20Roberto%20Correlated%20q-learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greenwald%2C%20Amy%20Hall%2C%20Keith%20Serrano%2C%20Roberto%20Correlated%20q-learning%202003"
        },
        {
            "id": "Haarnoja_et+al_2017_a",
            "entry": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08165"
        },
        {
            "id": "Harsanyi_1962_a",
            "entry": "John C Harsanyi. Bargaining in ignorance of the opponent\u2019s utility function. Journal of Conflict Resolution, 6(1):29\u201338, 1962.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harsanyi%2C%20John%20C.%20Bargaining%20in%20ignorance%20of%20the%20opponent%E2%80%99s%20utility%20function%201962",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harsanyi%2C%20John%20C.%20Bargaining%20in%20ignorance%20of%20the%20opponent%E2%80%99s%20utility%20function%201962"
        },
        {
            "id": "Harsanyi_1967_a",
            "entry": "John C Harsanyi. Games with incomplete information played by bayesian players, i\u2013iii part i. the basic model. Management science, 14(3):159\u2013182, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harsanyi%2C%20John%20C.%20Games%20with%20incomplete%20information%20played%20by%20bayesian%20players%2C%20i%E2%80%93iii%20part%20i.%20the%20basic%20model%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harsanyi%2C%20John%20C.%20Games%20with%20incomplete%20information%20played%20by%20bayesian%20players%2C%20i%E2%80%93iii%20part%20i.%20the%20basic%20model%201967"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. Opponent modeling in deep reinforcement learning. In International Conference on Machine Learning, pp. 1804\u20131813, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20He%20Boyd-Graber%2C%20Jordan%20Kwok%2C%20Kevin%20Daum%C3%A9%2C%20III%2C%20Hal%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20He%20Boyd-Graber%2C%20Jordan%20Kwok%2C%20Kevin%20Daum%C3%A9%2C%20III%2C%20Hal%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Hu_2003_a",
            "entry": "Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov):1039\u20131069, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Junling%20Wellman%2C%20Michael%20P.%20Nash%20q-learning%20for%20general-sum%20stochastic%20games%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Junling%20Wellman%2C%20Michael%20P.%20Nash%20q-learning%20for%20general-sum%20stochastic%20games%202003"
        },
        {
            "id": "Jordan_et+al_1999_a",
            "entry": "Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "Lake_et+al_2017_a",
            "entry": "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017"
        },
        {
            "id": "Levine_2018_a",
            "entry": "Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00909"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "Littman_1994_a",
            "entry": "Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pp. 157\u2013163.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "Littman_2001_a",
            "entry": "Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322\u2013328, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Friend-or-foe%20q-learning%20in%20general-sum%20games%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Friend-or-foe%20q-learning%20in%20general-sum%20games%202001"
        },
        {
            "id": "Liu_2016_a",
            "entry": "Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2378\u20132386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20bayesian%20inference%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20bayesian%20inference%20algorithm%202016"
        },
        {
            "id": "Lowe_et+al_2017_a",
            "entry": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379\u20136390, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pp. 1825\u20131835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20gans%202017"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Muise_et+al_2015_a",
            "entry": "Christian J Muise, Vaishak Belle, Paolo Felli, Sheila A McIlraith, Tim Miller, Adrian R Pearce, and Liz Sonenberg. Planning over multi-agent epistemic states: A classical planning approach. In AAAI, pp. 3327\u20133334, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muise%2C%20Christian%20J.%20Belle%2C%20Vaishak%20Felli%2C%20Paolo%20McIlraith%2C%20Sheila%20A.%20Planning%20over%20multi-agent%20epistemic%20states%3A%20A%20classical%20planning%20approach%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muise%2C%20Christian%20J.%20Belle%2C%20Vaishak%20Felli%2C%20Paolo%20McIlraith%2C%20Sheila%20A.%20Planning%20over%20multi-agent%20epistemic%20states%3A%20A%20classical%20planning%20approach%202015"
        },
        {
            "id": "Panait_et+al_2006_a",
            "entry": "Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors. IEEE Transactions on Evolutionary Computation, 10(6):629\u2013645, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panait%2C%20Liviu%20Luke%2C%20Sean%20Wiegand%2C%20R.Paul%20Biasing%20coevolutionary%20search%20for%20optimal%20multiagent%20behaviors%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panait%2C%20Liviu%20Luke%2C%20Sean%20Wiegand%2C%20R.Paul%20Biasing%20coevolutionary%20search%20for%20optimal%20multiagent%20behaviors%202006"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10069"
        },
        {
            "id": "Pfeiffer_2013_a",
            "entry": "Brad E Pfeiffer and David J Foster. Hippocampal place-cell sequences depict future paths to remembered goals. Nature, 497(7447):74, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pfeiffer%2C%20Brad%20E.%20Foster%2C%20David%20J.%20Hippocampal%20place-cell%20sequences%20depict%20future%20paths%20to%20remembered%20goals%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pfeiffer%2C%20Brad%20E.%20Foster%2C%20David%20J.%20Hippocampal%20place-cell%20sequences%20depict%20future%20paths%20to%20remembered%20goals%202013"
        },
        {
            "id": "Premack_1978_a",
            "entry": "David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515\u2013526, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Premack%2C%20David%20Woodruff%2C%20Guy%20Does%20the%20chimpanzee%20have%20a%20theory%20of%20mind%3F%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Premack%2C%20David%20Woodruff%2C%20Guy%20Does%20the%20chimpanzee%20have%20a%20theory%20of%20mind%3F%201978"
        },
        {
            "id": "Pynadath_2005_a",
            "entry": "David V Pynadath and Stacy C Marsella. Psychsim: Modeling theory of mind with decision-theoretic agents. In IJCAI, volume 5, pp. 1181\u20131186, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pynadath%2C%20David%20V.%20Marsella%2C%20Stacy%20C.%20Psychsim%3A%20Modeling%20theory%20of%20mind%20with%20decision-theoretic%20agents%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pynadath%2C%20David%20V.%20Marsella%2C%20Stacy%20C.%20Psychsim%3A%20Modeling%20theory%20of%20mind%20with%20decision-theoretic%20agents%202005"
        },
        {
            "id": "Rabinowitz_et+al_2018_a",
            "entry": "Neil C Rabinowitz, Frank Perbet, H Francis Song, Chiyuan Zhang, SM Eslami, and Matthew Botvinick. Machine theory of mind. arXiv preprint arXiv:1802.07740, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07740"
        },
        {
            "id": "Robalino_2012_a",
            "entry": "Nikolaus Robalino and Arthur Robson. The economic approach to \u2019theory of mind\u2019. Phil. Trans. R. Soc. B, 367(1599):2224\u20132233, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robalino%2C%20Nikolaus%20Robson%2C%20Arthur%20The%20economic%20approach%20to%E2%80%99theory%20of%20mind%E2%80%99%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robalino%2C%20Nikolaus%20Robson%2C%20Arthur%20The%20economic%20approach%20to%E2%80%99theory%20of%20mind%E2%80%99%202012"
        },
        {
            "id": "Seuken_2008_a",
            "entry": "Sven Seuken and Shlomo Zilberstein. Formal models and algorithms for decentralized decision making under uncertainty. Autonomous Agents and Multi-Agent Systems, 17(2):190\u2013250, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seuken%2C%20Sven%20Zilberstein%2C%20Shlomo%20Formal%20models%20and%20algorithms%20for%20decentralized%20decision%20making%20under%20uncertainty%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seuken%2C%20Sven%20Zilberstein%2C%20Shlomo%20Formal%20models%20and%20algorithms%20for%20decentralized%20decision%20making%20under%20uncertainty%202008"
        },
        {
            "id": "Shapley_1953_a",
            "entry": "Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095\u20131100, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapley%2C%20Lloyd%20S.%20Stochastic%20games%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapley%2C%20Lloyd%20S.%20Stochastic%20games%201953"
        },
        {
            "id": "Shoham_et+al_2007_a",
            "entry": "Yoav Shoham, Rob Powers, Trond Grenager, et al. If multi-agent learning is the answer, what is the question? Artificial Intelligence, 171(7):365\u2013377, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shoham%2C%20Yoav%20Powers%2C%20Rob%20Grenager%2C%20Trond%20If%20multi-agent%20learning%20is%20the%20answer%2C%20what%20is%20the%20question%3F%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shoham%2C%20Yoav%20Powers%2C%20Rob%20Grenager%2C%20Trond%20If%20multi-agent%20learning%20is%20the%20answer%2C%20what%20is%20the%20question%3F%202007"
        },
        {
            "id": "Singh_et+al_2000_a",
            "entry": "Satinder Singh, Michael Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pp. 541\u2013548. Morgan Kaufmann Publishers Inc., 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Satinder%20Kearns%2C%20Michael%20Mansour%2C%20Yishay%20Nash%20convergence%20of%20gradient%20dynamics%20in%20general-sum%20games%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Satinder%20Kearns%2C%20Michael%20Mansour%2C%20Yishay%20Nash%20convergence%20of%20gradient%20dynamics%20in%20general-sum%20games%202000"
        },
        {
            "id": "Sondik_1971_a",
            "entry": "Edward Jay Sondik. The optimal control of partially observable markov processes. Technical report, STANFORD UNIV CALIF STANFORD ELECTRONICS LABS, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sondik%2C%20Edward%20Jay%20The%20optimal%20control%20of%20partially%20observable%20markov%20processes%201971"
        },
        {
            "id": "Sonu_2015_a",
            "entry": "Ekhlas Sonu and Prashant Doshi. Scalable solutions of interactive pomdps using generalized and bounded policy iteration. Autonomous Agents and Multi-Agent Systems, 29(3):455\u2013494, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sonu%2C%20Ekhlas%20Doshi%2C%20Prashant%20Scalable%20solutions%20of%20interactive%20pomdps%20using%20generalized%20and%20bounded%20policy%20iteration%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sonu%2C%20Ekhlas%20Doshi%2C%20Prashant%20Scalable%20solutions%20of%20interactive%20pomdps%20using%20generalized%20and%20bounded%20policy%20iteration%202015"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "Tolman_1948_a",
            "entry": "Edward C Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tolman%2C%20Edward%20C.%20Cognitive%20maps%20in%20rats%20and%20men%201948",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tolman%2C%20Edward%20C.%20Cognitive%20maps%20in%20rats%20and%20men%201948"
        },
        {
            "id": "Tuyls_2012_a",
            "entry": "Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. Ai Magazine, 33(3):41, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tuyls%2C%20Karl%20Weiss%2C%20Gerhard%20Multiagent%20learning%3A%20Basics%2C%20challenges%2C%20and%20prospects%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tuyls%2C%20Karl%20Weiss%2C%20Gerhard%20Multiagent%20learning%3A%20Basics%2C%20challenges%2C%20and%20prospects%202012"
        },
        {
            "id": "Osten_et+al_2017_a",
            "entry": "Friedrich Burkhard Von Der Osten, Michael Kirley, and Tim Miller. The minds of many: opponent modelling in a stochastic game. In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI), AAAI Press, pp. 3845\u20133851, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osten%2C%20Friedrich%20Burkhard%20Von%20Der%20Kirley%2C%20Michael%20Miller%2C%20Tim%20The%20minds%20of%20many%3A%20opponent%20modelling%20in%20a%20stochastic%20game%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osten%2C%20Friedrich%20Burkhard%20Von%20Der%20Kirley%2C%20Michael%20Miller%2C%20Tim%20The%20minds%20of%20many%3A%20opponent%20modelling%20in%20a%20stochastic%20game%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Dilin Wang and Qiang Liu. Learning to draw samples: With application to amortized mle for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01722"
        },
        {
            "id": "Wei_2016_a",
            "entry": "Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games. The Journal of Machine Learning Research, 17(1):2914\u20132955, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Ermo%20Luke%2C%20Sean%20Lenient%20learning%20in%20independent-learner%20stochastic%20cooperative%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Ermo%20Luke%2C%20Sean%20Lenient%20learning%20in%20independent-learner%20stochastic%20cooperative%20games%202016"
        },
        {
            "id": "Wei_et+al_2018_a",
            "entry": "Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Ermo%20Wicke%2C%20Drew%20Freelan%2C%20David%20Luke%2C%20Sean%20Multiagent%20soft%20q-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Ermo%20Wicke%2C%20Drew%20Freelan%2C%20David%20Luke%2C%20Sean%20Multiagent%20soft%20q-learning%202018"
        },
        {
            "id": "Xia_et+al_2017_a",
            "entry": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning. arXiv preprint arXiv:1707.00415, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00415"
        },
        {
            "id": "Yang_et+al_2018_a",
            "entry": "Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5571\u20135580, Stockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yaodong%20Luo%2C%20Rui%20Li%2C%20Minne%20Zhou%2C%20Ming%20Mean%20field%20multiagent%20reinforcement%20learning%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yaodong%20Luo%2C%20Rui%20Li%2C%20Minne%20Zhou%2C%20Ming%20Mean%20field%20multiagent%20reinforcement%20learning%202018-07"
        }
    ]
}
