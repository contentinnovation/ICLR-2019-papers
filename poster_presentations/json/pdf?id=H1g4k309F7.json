{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "WASSERSTEIN BARYCENTER MODEL ENSEMBLING",
        "author": "Pierre Dognin, Igor Melnyk,Youssef Mroueh, Jerret Ross, Cicero Dos Santos, & Tom Sercu, IBM Research & MIT-IBM Watson AI Lab \u2217 Alphabetical order; Equal contribution {pdognin,mroueh,rossja,cicerons}@us.ibm.com, {igor.melnyk,tom.sercu,}@ibm.com",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=H1g4k309F7"
        },
        "abstract": "In this paper we propose to perform model ensembling in a multiclass or a multilabel learning setting using Wasserstein (W.) barycenters. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. Using W. barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the W. ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "arithmetic mean",
            "url": "https://en.wikipedia.org/wiki/arithmetic_mean"
        },
        {
            "term": "Natural Language Processing",
            "url": "https://en.wikipedia.org/wiki/Natural_Language_Processing"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "mean Average Precision",
            "url": "https://en.wikipedia.org/wiki/mean_Average_Precision"
        }
    ],
    "abbreviations": {
        "DNNs": "Deep Neural Networks",
        "NLP": "Natural Language Processing",
        "OT": "Optimal transport",
        "mAP": "mean Average Precision"
    },
    "highlights": [
        "Model ensembling consists in combining many models into a stronger, more robust and more accurate model",
        "We show in this paper that W. barycenters are effective in model ensembling and in finding a semantic consensus, and can be applied to a wide range of problems in machine learning (Table 1)",
        "In this paper we propose to use the Frechet means with Wasserstein distance (d = W22) for model ensembling, i.e. use Wasserstein barycenters (<a class=\"ref-link\" id=\"cAgueh_2011_a\" href=\"#rAgueh_2011_a\">Agueh & Carlier, 2011</a>) for model ensembling: m \u03bcw",
        "We see from Proposition 1 that the W. barycenter preserves accuracy, but has a higher entropy than the individual models. This entropy increase is due to an improved smoothness on the embedding space: words that have similar semantics will have similar probability mass assigned in the barycenter",
        "We showed in this paper that W. barycenters are effective in model ensembling in machine learning",
        "We showed that they promote diversity and improve natural language generation by incorporating the knowledge of synonyms or word embeddings"
    ],
    "key_statements": [
        "Model ensembling consists in combining many models into a stronger, more robust and more accurate model",
        "Ensembling is ubiquitous in machine learning and yields improved accuracies across multiple prediction tasks such as multi-class or multi-label classification",
        "The arithmetic mean rewards confidence of the models while the geometric means seeks the consensus across models",
        "We show in this paper that W. barycenters are effective in model ensembling and in finding a semantic consensus, and can be applied to a wide range of problems in machine learning (Table 1)",
        "We show applications of Wasserstein ensembling on attribute based classification, multi-label learning and image captioning in Section 6.\n2 WASSERSTEIN BARYCENTERS FOR MODEL ENSEMBLING",
        "In this paper we propose to use the Frechet means with Wasserstein distance (d = W22) for model ensembling, i.e. use Wasserstein barycenters (<a class=\"ref-link\" id=\"cAgueh_2011_a\" href=\"#rAgueh_2011_a\">Agueh & Carlier, 2011</a>) for model ensembling: m \u03bcw",
        "As the kernel K in Algorithm 1 approaches I, the alternating Bregman projection of (<a class=\"ref-link\" id=\"cBenamou_et+al_2015_a\" href=\"#rBenamou_et+al_2015_a\">Benamou et al, 2015</a>) for balanced W. barycenter converges to the geometric mean \u03bcg = \u03a0m=1(\u03bc )\u03bb .We prove this in Appendix D",
        "We consider \u03a9S = \u03a9T = \u03a9, i.e the W. barycenters and all individual models are defined on the same label space",
        "We see from Proposition 1 that the W. barycenter preserves accuracy, but has a higher entropy than the individual models. This entropy increase is due to an improved smoothness on the embedding space: words that have similar semantics will have similar probability mass assigned in the barycenter",
        "We evaluate W. barycenter ensembling in the problems of attribute-based classification, multi-label prediction and in natural language generation in image captioning.\n6.1",
        "We report two n-gram based metrics: CIDEr and SPICE scores, as well as the WMD (Word Mover Distance) similarity (<a class=\"ref-link\" id=\"cKusner_et+al_2015_a\" href=\"#rKusner_et+al_2015_a\">Kusner et al, 2015</a>), which computes the earth mover distance between the generated and the ground truth captions using the GloVe word embedding vectors",
        "The results show that barycenter is able to recover from those perturbations, employing the side information from K, while both the arithmetic and geometric means are confused by this shuffling, displaying a significant drop in the evaluation metrics",
        "We showed in this paper that W. barycenters are effective in model ensembling in machine learning",
        "We showed that they promote diversity and improve natural language generation by incorporating the knowledge of synonyms or word embeddings"
    ],
    "summary": [
        "Model ensembling consists in combining many models into a stronger, more robust and more accurate model.",
        "We show applications of Wasserstein ensembling on attribute based classification, multi-label learning and image captioning in Section 6.",
        "In order to incorporate the semantics of the target space in model ensembling, we need to use a distance d that takes advantage of the underlying geometry of the label space via a cost matrix C when comparing positive measures.",
        "Barycenters can be naturally defined with the entropic regularized OT distance as follows: min\u03c1 m =1 \u03bb",
        "Barycenters and all individual models are defined on the same label space.",
        "This entropy increase is due to an improved smoothness on the embedding space: words that have similar semantics will have similar probability mass assigned in the barycenter.",
        "Barycenter output will be less diverse if the models have similar semantics as measured by the Wasserstein distance.",
        "Barycenter has higher entropy and is smooth along semantics and more diverse than individual models.",
        "Table 2 shows top 15 words of barycenter, arithmetic and geometric means, from which we see that the W.",
        "Model 2 car cars white black train passeng model photo truck red silver vehicle van buildin bus",
        "Model 3 car fashion black truck red photo parking city train buildin fashion bus style time old",
        "Model 4 car truck bus vehicle red van fashion passeng pickup black train style model fire white",
        "Barycenter ensembling in the problems of attribute-based classification, multi-label prediction and in natural language generation in image captioning.",
        "Barycenter outperforms arithmetic and geometric mean on this task and shows its potential in attribute based classification.",
        "Barycenters on a multi-label prediction task, we use MS-COCO (<a class=\"ref-link\" id=\"cLin_et+al_2014_a\" href=\"#rLin_et+al_2014_a\">Lin et al, 2014</a>) with 80 objects categories.",
        "This simple diagonal K gives our best results when using the top-2 scoring categories per model and outperforms arithmetic and geometric means as seen in Table 6.",
        "The smoothness of the barycenter in semantic clusters and its controllable entropy promotes diversity in the resulting captions.",
        "We report two n-gram based metrics: CIDEr and SPICE scores, as well as the WMD (Word Mover Distance) similarity (<a class=\"ref-link\" id=\"cKusner_et+al_2015_a\" href=\"#rKusner_et+al_2015_a\">Kusner et al, 2015</a>), which computes the earth mover distance between the generated and the ground truth captions using the GloVe word embedding vectors.",
        "The results show that barycenter is able to recover from those perturbations, employing the side information from K, while both the arithmetic and geometric means are confused by this shuffling, displaying a significant drop in the evaluation metrics.",
        "We showed that they promote diversity and improve natural language generation by incorporating the knowledge of synonyms or word embeddings"
    ],
    "headline": "In this paper we propose to perform model ensembling in a multiclass or a multilabel learning setting using Wasserstein barycenters",
    "reference_links": [
        {
            "id": "Agueh_2011_a",
            "entry": "Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM J. Math. Analysis, 43, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agueh%2C%20Martial%20Carlier%2C%20Guillaume%20Barycenters%20in%20the%20wasserstein%20space%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agueh%2C%20Martial%20Carlier%2C%20Guillaume%20Barycenters%20in%20the%20wasserstein%20space%202011"
        },
        {
            "id": "Altschuler_et+al_2018_a",
            "entry": "J. Altschuler, F. Bach, A. Rudi, and J. Weed. Approximating the Quadratic Transportation Metric in Near-Linear Time. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altschuler%2C%20J.%20Bach%2C%20F.%20Rudi%2C%20A.%20Weed%2C%20J.%20Approximating%20the%20Quadratic%20Transportation%20Metric%20in%20Near-Linear%20Time.%20ArXiv%20e-prints%202018"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. Arxiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20gan%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20gan%202017"
        },
        {
            "id": "Benamou_et+al_2015_a",
            "entry": "Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Iterative bregman projections for regularized transportation problems. SIAM J. Scientific Computing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benamou%2C%20Jean-David%20Carlier%2C%20Guillaume%20Cuturi%2C%20Marco%20Nenna%2C%20Luca%20Iterative%20bregman%20projections%20for%20regularized%20transportation%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benamou%2C%20Jean-David%20Carlier%2C%20Guillaume%20Cuturi%2C%20Marco%20Nenna%2C%20Luca%20Iterative%20bregman%20projections%20for%20regularized%20transportation%20problems%202015"
        },
        {
            "id": "Breiman_1996_a",
            "entry": "Leo Breiman. Bagging predictors. Machine learning, 24(2):123\u2013140, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Bagging%20predictors%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Bagging%20predictors%201996"
        },
        {
            "id": "Carlier_et+al_2017_a",
            "entry": "Guillaume Carlier, Vincent Duval, Gabriel Peyre, and Bernhard Schmitzer. Convergence of entropic schemes for optimal transport and gradient flows. SIAM J. Math. Analysis, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlier%2C%20Guillaume%20Duval%2C%20Vincent%20Peyre%2C%20Gabriel%20Schmitzer%2C%20Bernhard%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlier%2C%20Guillaume%20Duval%2C%20Vincent%20Peyre%2C%20Gabriel%20Schmitzer%2C%20Bernhard%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017"
        },
        {
            "id": "Chizat_et+al_2018_a",
            "entry": "Lena\u0131c Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. Math. Comput., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chizat%2C%20Lena%C4%B1c%20Peyre%2C%20Gabriel%20Schmitzer%2C%20Bernhard%20Vialard%2C%20Francois-Xavier%20Scaling%20algorithms%20for%20unbalanced%20optimal%20transport%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chizat%2C%20Lena%C4%B1c%20Peyre%2C%20Gabriel%20Schmitzer%2C%20Bernhard%20Vialard%2C%20Francois-Xavier%20Scaling%20algorithms%20for%20unbalanced%20optimal%20transport%20problems%202018"
        },
        {
            "id": "Cuturi_2013_a",
            "entry": "Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pp. 2292\u20132300, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "Cuturi_2014_a",
            "entry": "Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In Proceedings of the 31st International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Doucet%2C%20Arnaud%20Fast%20computation%20of%20wasserstein%20barycenters%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Doucet%2C%20Arnaud%20Fast%20computation%20of%20wasserstein%20barycenters%202014"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "Deng_et+al_2014_a",
            "entry": "Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and Hartwig Adam. Large-scale object classification using label relation graphs. In European Conference on Computer Vision, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Ding%2C%20Nan%20Jia%2C%20Yangqing%20Frome%2C%20Andrea%20Large-scale%20object%20classification%20using%20label%20relation%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Ding%2C%20Nan%20Jia%2C%20Yangqing%20Frome%2C%20Andrea%20Large-scale%20object%20classification%20using%20label%20relation%20graphs%202014"
        },
        {
            "id": "Dognin_et+al_2018_a",
            "entry": "Pierre L Dognin, Igor Melnyk, Youssef Mroueh, Jarret Ross, and Tom Sercu. Improved image captioning with adversarial semantic alignment. arXiv preprint arXiv:1805.00063, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00063"
        },
        {
            "id": "Feydy_et+al_2018_a",
            "entry": "J. Feydy, T. Sejourne, F.-X. Vialard, S.-i. Amari, A. Trouve, and G. Peyre. Interpolating between Optimal Transport and MMD using Sinkhorn Divergences. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feydy%2C%20J.%20Sejourne%2C%20T.%20Vialard%2C%20F.-X.%20Amari%2C%20S.-i%20Interpolating%20between%20Optimal%20Transport%20and%20MMD%20using%20Sinkhorn%20Divergences.%20ArXiv%20e-prints%202018"
        },
        {
            "id": "Flamary_et+al_2016_a",
            "entry": "Remi Flamary, Marco Cuturi, Nicolas Courty, and Alain Rakotomamonjy. Wasserstein discriminant analysis. arXiv preprint arXiv:1608.08063, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08063"
        },
        {
            "id": "Freund_1999_a",
            "entry": "Yoav Freund and Robert Schapire. A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Schapire%2C%20Robert%20A%20short%20introduction%20to%20boosting%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Schapire%2C%20Robert%20A%20short%20introduction%20to%20boosting%201999"
        },
        {
            "id": "Frogner_et+al_2015_a",
            "entry": "Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a wasserstein loss. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya%2C%20Mauricio%20and%20Tomaso%20A%20Poggio.%20Learning%20with%20a%20wasserstein%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya%2C%20Mauricio%20and%20Tomaso%20A%20Poggio.%20Learning%20with%20a%20wasserstein%20loss%202015"
        },
        {
            "id": "Genevay_et+al_2017_a",
            "entry": "Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn divergences. Technical report, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genevay%2C%20Aude%20Peyre%2C%20Gabriel%20Cuturi%2C%20Marco%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202017"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. JMLR, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.",
            "url": "http://arxiv.org/abs/1512.03385",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Karpathy_2015_a",
            "entry": "Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Li%2C%20Fei-Fei%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Li%2C%20Fei-Fei%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015"
        },
        {
            "id": "Karpathy_2015_b",
            "entry": "Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Li%2C%20Fei-Fei%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Li%2C%20Fei-Fei%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. URL http://arxiv.org/abs/1412.6980.",
            "url": "http://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kottur_et+al_2016_a",
            "entry": "Satwik Kottur, Ramakrishna Vedantam, Jose M. F. Moura, and Devi Parikh. Visualword2vec (visw2v): Learning visually grounded word embeddings using abstract scenes. In CVPR, pp. 4985\u2013 4994. IEEE Computer Society, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kottur%2C%20Satwik%20Vedantam%2C%20Ramakrishna%20Moura%2C%20Jose%20M.F.%20Parikh%2C%20Devi%20Visualword2vec%20%28visw2v%29%3A%20Learning%20visually%20grounded%20word%20embeddings%20using%20abstract%20scenes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kottur%2C%20Satwik%20Vedantam%2C%20Ramakrishna%20Moura%2C%20Jose%20M.F.%20Parikh%2C%20Devi%20Visualword2vec%20%28visw2v%29%3A%20Learning%20visually%20grounded%20word%20embeddings%20using%20abstract%20scenes%202016"
        },
        {
            "id": "Kusner_et+al_2015_a",
            "entry": "Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kusner%2C%20Matt%20Sun%2C%20Yu%20Kolkin%2C%20Nicholas%20Weinberger%2C%20Kilian%20From%20word%20embeddings%20to%20document%20distances%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusner%2C%20Matt%20Sun%2C%20Yu%20Kolkin%2C%20Nicholas%20Weinberger%2C%20Kilian%20From%20word%20embeddings%20to%20document%20distances%202015"
        },
        {
            "id": "Lin_et+al_2014_a",
            "entry": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. EECV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Tsung-Yi%20Maire%2C%20Michael%20Belongie%2C%20Serge%20J.%20Bourdev%2C%20Lubomir%20D.%20Microsoft%20COCO%3A%20common%20objects%20in%20context%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Tsung-Yi%20Maire%2C%20Michael%20Belongie%2C%20Serge%20J.%20Bourdev%2C%20Lubomir%20D.%20Microsoft%20COCO%3A%20common%20objects%20in%20context%202014"
        },
        {
            "id": "Marino_et+al_2017_a",
            "entry": "Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge graphs for image classification. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 20\u201328. IEEE Computer Society, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marino%2C%20Kenneth%20Salakhutdinov%2C%20Ruslan%20Gupta%2C%20Abhinav%20The%20more%20you%20know%3A%20Using%20knowledge%20graphs%20for%20image%20classification%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marino%2C%20Kenneth%20Salakhutdinov%2C%20Ruslan%20Gupta%2C%20Abhinav%20The%20more%20you%20know%3A%20Using%20knowledge%20graphs%20for%20image%20classification%202017-07-21"
        },
        {
            "id": "Muzellec_2018_a",
            "entry": "B. Muzellec and M. Cuturi. Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muzellec%2C%20B.%20Cuturi%2C%20M.%20Generalizing%20Point%20Embeddings%20using%20the%20Wasserstein%20Space%20of%20Elliptical%20Distributions.%20ArXiv%20e-prints%202018"
        },
        {
            "id": "Singh_et+al_2018_a",
            "entry": "S. Pal Singh, A. Hug, A. Dieuleveut, and M. Jaggi. Wasserstein is all you need. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Pal%20Singh%20A%20Hug%20A%20Dieuleveut%20and%20M%20Jaggi%20Wasserstein%20is%20all%20you%20need%20ArXiv%20eprints%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Pal%20Singh%20A%20Hug%20A%20Dieuleveut%20and%20M%20Jaggi%20Wasserstein%20is%20all%20you%20need%20ArXiv%20eprints%202018"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Cuturi_2017_a",
            "entry": "Gabriel Peyreand Marco Cuturi. Computational optimal transport. Technical report, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Gabriel%20Peyreand%20Marco%20Computational%20optimal%20transport%202017"
        },
        {
            "id": "Power_0000_a",
            "entry": "Power Thesaurus. Power thesaurus. https://powerthesaurus.org. Website Accessed:whenever.",
            "url": "https://powerthesaurus.org"
        },
        {
            "id": "Rios_et+al_2018_a",
            "entry": "G. Rios, J. Backhoff-Veraguas, J. Fontbona, and F. Tobar. Bayesian Learning with Wasserstein Barycenters. ArXiv e-prints, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rios%2C%20G.%20Backhoff-Veraguas%2C%20J.%20Fontbona%2C%20J.%20Tobar%2C%20F.%20Bayesian%20Learning%20with%20Wasserstein%20Barycenters.%20ArXiv%20e-prints%202018"
        },
        {
            "id": "Salimans_et+al_2018_a",
            "entry": "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal transport. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Zhang%2C%20Han%20Radford%2C%20Alec%20Metaxas%2C%20Dimitris%20Improving%20gans%20using%20optimal%20transport%202018"
        },
        {
            "id": "Santambrogio_2015_a",
            "entry": "Filippo Santambrogio. Optimal transport for applied mathematicians. May 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santambrogio%2C%20Filippo%20Optimal%20transport%20for%20applied%20mathematicians%202015-05"
        },
        {
            "id": "Schmitz_et+al_2018_a",
            "entry": "Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Maurice Ngole Mboula, David Coeurjolly, Marco Cuturi, Gabriel Peyre, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. SIAM J. Imaging Sciences, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmitz%2C%20Morgan%20A.%20Heitz%2C%20Matthieu%20Bonneel%2C%20Nicolas%20Mboula%2C%20Fred%20Maurice%20Ngole%20Wasserstein%20dictionary%20learning%3A%20Optimal%20transport-based%20unsupervised%20nonlinear%20dictionary%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmitz%2C%20Morgan%20A.%20Heitz%2C%20Matthieu%20Bonneel%2C%20Nicolas%20Mboula%2C%20Fred%20Maurice%20Ngole%20Wasserstein%20dictionary%20learning%3A%20Optimal%20transport-based%20unsupervised%20nonlinear%20dictionary%20learning%202018"
        },
        {
            "id": "Shao_et+al_2017_a",
            "entry": "Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. Generating high-quality and informative conversation responses with sequence-to-sequence models. arXiv preprint arXiv:1701.03185, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.03185"
        },
        {
            "id": "Solomon_et+al_2015_a",
            "entry": "Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (TOG), 34(4):66, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Solomon%2C%20Justin%20Goes%2C%20Fernando%20De%20Peyre%2C%20Gabriel%20Cuturi%2C%20Marco%20Convolutional%20wasserstein%20distances%3A%20Efficient%20optimal%20transportation%20on%20geometric%20domains%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Solomon%2C%20Justin%20Goes%2C%20Fernando%20De%20Peyre%2C%20Gabriel%20Cuturi%2C%20Marco%20Convolutional%20wasserstein%20distances%3A%20Efficient%20optimal%20transportation%20on%20geometric%20domains%202015"
        },
        {
            "id": "Villani_2008_a",
            "entry": "Cedric Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20Cedric%20Optimal%20Transport%3A%20Old%20and%20New.%20Grundlehren%20der%20mathematischen%20Wissenschaften%202008"
        },
        {
            "id": "Wolpert_1992_a",
            "entry": "David H Wolpert. Stacked generalization. Neural networks, 5(2):241\u2013259, 1992. Xi-Zhu Wu and Zhi-Hua Zhou. A unified view of multi-label performance measures. CoRR, abs/1609.00288, 2016. Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly.",
            "arxiv_url": "https://arxiv.org/pdf/1609.00288"
        },
        {
            "id": "Ye_et+al_2018_a",
            "entry": "Topic Modeling. ArXiv e-prints, 2018. Jianbo Ye, Panruo Wu, James Z. Wang, and Jia Li. Fast discrete distribution clustering using wasserstein barycenter with sparse support. Trans. Sig. Proc., 2017. Yoav Zemel and Victor Panaretos. Frechet means in wasserstein space theory and algorithms. 2017. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang. Learning spatial regularization with image-level supervisions for multi-label image classification. CoRR, abs/1702.05891, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05891"
        }
    ]
}
