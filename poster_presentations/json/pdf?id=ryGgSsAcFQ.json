{
    "filename": "pdf.pdf",
    "metadata": {
        "date": 2019,
        "title": "DEEP, SKINNY NEURAL NETWORKS ARE NOT UNIVERSAL APPROXIMATORS",
        "author": "Jesse Johnson Sanofi jejo.math@gmail.com",
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryGgSsAcFQ"
        },
        "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions."
    },
    "keywords": [
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "universal approximator",
            "url": "https://en.wikipedia.org/wiki/universal_approximator"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "expressive power",
            "url": "https://en.wikipedia.org/wiki/expressive_power"
        },
        {
            "term": "restricted boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_Machine"
        }
    ],
    "abbreviations": {},
    "highlights": [
        "Neural networks have become the model of choice in a variety of machine learning applications, due to their flexibility and generality",
        "The main result of the paper, Theorem 1, states that the deep, skinny neural network architectures described above cannot approximate any function with a level set that is bounded in the input space",
        "The main result of the paper is a topological constraint on the level sets of any function in the family of models N\u03c6\u2217,n",
        "In the first step, described in Section 5, we examine the family of functions defined by deep, skinny neural networks in which the activation is one-to-one and the transition matrices are all non-singular",
        "We describe topological limitations on the types of functions that can be approximated by deep, skinny neural networks, independent of the number of hidden layers"
    ],
    "key_statements": [
        "Neural networks have become the model of choice in a variety of machine learning applications, due to their flexibility and generality",
        "The main result of the paper, Theorem 1, states that the deep, skinny neural network architectures described above cannot approximate any function with a level set that is bounded in the input space",
        "The main result of the paper is a topological constraint on the level sets of any function in the family of models N\u03c6\u2217,n",
        "In the first step, described in Section 5, we examine the family of functions defined by deep, skinny neural networks in which the activation is one-to-one and the transition matrices are all non-singular",
        "We describe topological limitations on the types of functions that can be approximated by deep, skinny neural networks, independent of the number of hidden layers"
    ],
    "summary": [
        "Neural networks have become the model of choice in a variety of machine learning applications, due to their flexibility and generality.",
        "The main result of the paper, Theorem 1, states that the deep, skinny neural network architectures described above cannot approximate any function with a level set that is bounded in the input space.",
        "The main result of the paper is a topological constraint on the level sets of any function in the family of models N\u03c6\u2217,n.",
        "The main result of this paper states that deep, skinny neural networks can only approximate functions with unbounded level components.",
        "With input dimension n in which each hidden layer has dimension at most n cannot approximate any function with a level set containing a bounded path component.",
        "In the first step, described in Section 5, we examine the family of functions defined by deep, skinny neural networks in which the activation is one-to-one and the transition matrices are all non-singular.",
        "If g is approximated by N\u03c6\u2217,n for some continuous activation function \u03c6 that can be uniformly approximated by one-to-one functions it is approximated by Nn. To prove this Lemma, we will employ a technical result from point-set topology, relying on the fact that a function in N\u03c6\u2217,n can be written as a composition of linear functions defined by the weights between successive layers, and non-linear functions defined by the activation function \u03c6.",
        "Lemma 2 implies that if N\u03c6\u2217,n is universal so is Nn. So to prove Theorem 1, we will show that every function in Nn has level sets with only unbounded components, show that this property extends to any function that it approximates.",
        "This contradiction proves that g cannot be approximated by a model family M in which each function has unbounded level components.",
        "By Lemma 2, since g is approximated by N\u03c6\u2217,n, it must be approximated by Nn. By Lemma 4, every function in Nn has bounded level components, so Lemma 5 implies that every function that this family approximates has unbounded level sets.",
        "We trained two neural networks and examined the plot of the resulting functions to characterize the level sets/decision boundaries.",
        "The ideal decision boundary between the two classes of points would be a loop around the blue points in the middle, but Theorem 1 proves that such a network cannot approximate a function with such a level set.",
        "We describe topological limitations on the types of functions that can be approximated by deep, skinny neural networks, independent of the number of hidden layers.",
        "We expect that there is a great deal of remaining potential to explore further topological constraints on families of models, and to determine to what extent these topological constraints are a different way of describing more fundamental ideas that have been independently demonstrated elsewhere in other frameworks such as information theory"
    ],
    "headline": "We examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Tensorflow neural network playground. https://playground.tensorflow.org/. Accessed:2018-04-20.",
            "url": "https://playground.tensorflow.org/"
        },
        {
            "id": "2",
            "entry": "[2] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654\u20132662, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20J.%20Caruana%2C%20R.%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20J.%20Caruana%2C%20R.%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "3",
            "entry": "[3] A. R. Barron. Approximation and estimation bounds for artificial neural networks. Machine learning, 14(1):115\u2013133, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201994"
        },
        {
            "id": "4",
            "entry": "[4] N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on Learning Theory, pages 698\u2013728, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016"
        },
        {
            "id": "5",
            "entry": "[5] J. Collins, J. Sohl-Dickstein, and D. Sussillo. Capacity and trainability in recurrent neural networks. arXiv preprint arXiv:1611.09913, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09913"
        },
        {
            "id": "6",
            "entry": "[6] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989"
        },
        {
            "id": "7",
            "entry": "[7] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems, pages 666\u2013674, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Delalleau%2C%20O.%20Bengio%2C%20Y.%20Shallow%20vs.%20deep%20sum-product%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Delalleau%2C%20O.%20Bengio%2C%20Y.%20Shallow%20vs.%20deep%20sum-product%20networks%202011"
        },
        {
            "id": "8",
            "entry": "[8] K. Doya. Universality of fully connected recurrent neural networks. Dept. of Biology, UCSD, Tech. Rep, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doya%2C%20K.%20Universality%20of%20fully%20connected%20recurrent%20neural%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doya%2C%20K.%20Universality%20of%20fully%20connected%20recurrent%20neural%20networks%201993"
        },
        {
            "id": "9",
            "entry": "[9] H. Edelsbrunner and J. Harer. Persistent homology-a survey. Contemporary mathematics, 453:257\u2013282, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edelsbrunner%2C%20H.%20Harer%2C%20J.%20Persistent%20homology-a%20survey%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edelsbrunner%2C%20H.%20Harer%2C%20J.%20Persistent%20homology-a%20survey%202008"
        },
        {
            "id": "10",
            "entry": "[10] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907\u2013940, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20R.%20Shamir%2C%20O.%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20R.%20Shamir%2C%20O.%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "11",
            "entry": "[11] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183\u2013192, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Funahashi%2C%20K.-I.%20On%20the%20approximate%20realization%20of%20continuous%20mappings%20by%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Funahashi%2C%20K.-I.%20On%20the%20approximate%20realization%20of%20continuous%20mappings%20by%20neural%20networks%201989"
        },
        {
            "id": "12",
            "entry": "[12] N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02930"
        },
        {
            "id": "13",
            "entry": "[13] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359\u2013366, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989"
        },
        {
            "id": "14",
            "entry": "[14] V. Khrulkov, A. Novikov, and I. Oseledets. Expressive power of recurrent neural networks. arXiv preprint arXiv:1711.00811, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00811"
        },
        {
            "id": "15",
            "entry": "[15] S. Liang and R. Srikant. Why deep neural networks for function approximation? arXiv preprint arXiv:1610.04161, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04161"
        },
        {
            "id": "16",
            "entry": "[16] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223\u20131247, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.W.%20Tegmark%2C%20M.%20Rolnick%2C%20D.%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.W.%20Tegmark%2C%20M.%20Rolnick%2C%20D.%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017"
        },
        {
            "id": "17",
            "entry": "[17] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view from the width. In Advances in Neural Information Processing Systems, pages 6232\u20136240, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Z.%20Pu%2C%20H.%20Wang%2C%20F.%20Hu%2C%20Z.%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Z.%20Pu%2C%20H.%20Wang%2C%20F.%20Hu%2C%20Z.%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017"
        },
        {
            "id": "18",
            "entry": "[18] J. Martens, A. Chattopadhya, T. Pitassi, and R. Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages 2877\u20132885, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Chattopadhya%2C%20A.%20Pitassi%2C%20T.%20Zemel%2C%20R.%20On%20the%20representational%20efficiency%20of%20restricted%20boltzmann%20machines%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Chattopadhya%2C%20A.%20Pitassi%2C%20T.%20Zemel%2C%20R.%20On%20the%20representational%20efficiency%20of%20restricted%20boltzmann%20machines%202013"
        },
        {
            "id": "19",
            "entry": "[19] J. Martens and V. Medabalimi. On the expressive efficiency of sum product networks. arXiv preprint arXiv:1411.7717, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.7717"
        },
        {
            "id": "20",
            "entry": "[20] H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approximation theory perspective. Analysis and Applications, 14(06):829\u2013848, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Poggio%2C%20T.%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Poggio%2C%20T.%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%202016"
        },
        {
            "id": "21",
            "entry": "[21] G. Montufar, N. Ay, and K. Ghazi-Zahedi. Geometry and expressive power of conditional restricted boltzmann machines. Journal of Machine Learning Research, 16:2405\u20132436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.%20Ay%2C%20N.%20Ghazi-Zahedi%2C%20K.%20Geometry%20and%20expressive%20power%20of%20conditional%20restricted%20boltzmann%20machines%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.%20Ay%2C%20N.%20Ghazi-Zahedi%2C%20K.%20Geometry%20and%20expressive%20power%20of%20conditional%20restricted%20boltzmann%20machines%202015"
        },
        {
            "id": "22",
            "entry": "[22] G. F. Montufar. Universal approximation depth and errors of narrow belief networks with discrete units. Neural computation, 26(7):1386\u20131407, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.F.%20Universal%20approximation%20depth%20and%20errors%20of%20narrow%20belief%20networks%20with%20discrete%20units%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.F.%20Universal%20approximation%20depth%20and%20errors%20of%20narrow%20belief%20networks%20with%20discrete%20units%202014"
        },
        {
            "id": "23",
            "entry": "[23] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pages 2924\u20132932, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Bengio%2C%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Bengio%2C%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "24",
            "entry": "[24] G. F. Montufar, J. Rauh, and N. Ay. Expressive power and approximation errors of restricted boltzmann machines. In Advances in neural information processing systems, pages 415\u2013423, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.F.%20Rauh%2C%20J.%20Ay%2C%20N.%20Expressive%20power%20and%20approximation%20errors%20of%20restricted%20boltzmann%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.F.%20Rauh%2C%20J.%20Ay%2C%20N.%20Expressive%20power%20and%20approximation%20errors%20of%20restricted%20boltzmann%20machines%202011"
        },
        {
            "id": "25",
            "entry": "[25] Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08045"
        },
        {
            "id": "26",
            "entry": "[26] Q. Nguyen, M. Mukkamala, and M. Hein. Neural networks should be wide enough to learn disconnected decision regions. arXiv preprint arXiv:1803.00094, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00094"
        },
        {
            "id": "27",
            "entry": "[27] R. Pascanu, G. Montufar, and Y. Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6098"
        },
        {
            "id": "28",
            "entry": "[28] R. Rojas. Networks of width one are universal classifiers. In Neural Networks, 2003. Proceedings of the International Joint Conference on, volume 4, pages 3124\u20133127. IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rojas%2C%20R.%20Networks%20of%20width%20one%20are%20universal%20classifiers%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rojas%2C%20R.%20Networks%20of%20width%20one%20are%20universal%20classifiers%202003"
        },
        {
            "id": "29",
            "entry": "[29] D. Rolnick and M. Tegmark. The power of deeper networks for expressing natural functions. arXiv preprint arXiv:1705.05502, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05502"
        },
        {
            "id": "30",
            "entry": "[30] I. Sutskever and G. E. Hinton. Deep, narrow sigmoid belief networks are universal approximators. Neural computation, 20(11):2629\u20132636, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Deep%2C%20narrow%20sigmoid%20belief%20networks%20are%20universal%20approximators%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Deep%2C%20narrow%20sigmoid%20belief%20networks%20are%20universal%20approximators%202008"
        },
        {
            "id": "31",
            "entry": "[31] M. Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.04485"
        },
        {
            "id": "32",
            "entry": "[32] D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103\u2013114, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        }
    ]
}
