{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
        "author": "Keyulu Xu \u2217\u2020 MIT keyulu@mit.edu",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=ryGs6iA5Km"
        },
        "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance."
    },
    "keywords": [
        {
            "term": "graph isomorphism",
            "url": "https://en.wikipedia.org/wiki/graph_isomorphism"
        },
        {
            "term": "feature vector",
            "url": "https://en.wikipedia.org/wiki/feature_vector"
        },
        {
            "term": "expressive power",
            "url": "https://en.wikipedia.org/wiki/expressive_power"
        },
        {
            "term": "graph representation",
            "url": "https://en.wikipedia.org/wiki/graph_representation"
        },
        {
            "term": "multi-layer perceptrons",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptrons"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "isomorphism",
            "url": "https://en.wikipedia.org/wiki/isomorphism"
        }
    ],
    "abbreviations": {
        "GNNs": "Graph Neural Networks",
        "GNN": "Graph Neural Network",
        "GIN": "Graph Isomorphism Network",
        "GCN": "Graph Convolutional Networks",
        "MLPs": "multi-layer perceptrons",
        "DCNN": "Diffusionconvolutional neural networks",
        "DGCNN": "Deep Graph CNN",
        "AWL": "Anonymous Walk Embeddings"
    },
    "highlights": [
        "Learning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al, 2017b)",
        "Our main results are summarized as follows: 1) We show that Graph Neural Networks are at most as powerful as the WL test in distinguishing graph structures.\n2) We establish conditions on the neighbor aggregation and graph readout functions under which the resulting Graph Neural Networks is as powerful as the WL test.\n3) We identify graph structures that cannot be distinguished by popular Graph Neural Networks variants, such as Graph Convolutional Networks (<a class=\"ref-link\" id=\"cKipf_2017_a\" href=\"#rKipf_2017_a\">Kipf & Welling, 2017</a>) and GraphSAGE (<a class=\"ref-link\" id=\"cHamilton_et+al_2017_a\" href=\"#rHamilton_et+al_2017_a\">Hamilton et al, 2017a</a>), and we precisely characterize the kinds of graph structures such Graph Neural Networks-based models can capture.\n4) We develop a simple neural architecture, Graph Isomorphism Network (GIN), and show that its discriminative/representational power is equal to the power of the WL test",
        "Having developed conditions for a maximally powerful Graph Neural Networks, we develop a simple architecture, Graph Isomorphism Network (GIN), that provably satisfies the conditions in Theorem 3",
        "For the less powerful Graph Neural Networks variants, we consider architectures that replace the sum in the Graph Isomorphism Network-0 aggregation with mean or max-pooling3, or replace multi-layer perceptrons with 1-layer perceptrons, i.e., a linear mapping followed\n1The code is available at https://github.com/weihua916/powerful-gnns. 2There exist certain graphs that Graph Isomorphism Network- can distinguish but Graph Isomorphism Network-0 cannot. 3For REDDIT-BINARY, REDDIT\u2013MULTI5K, and COLLAB, we did not run experiments for max-pooling due to GPU memory constraints",
        "The Graph Neural Networks variants using mean/max pooling or 1-layer perceptrons severely underfit on many datasets",
        "We developed theoretical foundations for reasoning about the expressive power of Graph Neural Networks, and proved tight bounds on the representational capacity of popular Graph Neural Networks variants"
    ],
    "key_statements": [
        "Learning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al, 2017b)",
        "Our main results are summarized as follows: 1) We show that Graph Neural Networks are at most as powerful as the WL test in distinguishing graph structures.\n2) We establish conditions on the neighbor aggregation and graph readout functions under which the resulting Graph Neural Networks is as powerful as the WL test.\n3) We identify graph structures that cannot be distinguished by popular Graph Neural Networks variants, such as Graph Convolutional Networks (<a class=\"ref-link\" id=\"cKipf_2017_a\" href=\"#rKipf_2017_a\">Kipf & Welling, 2017</a>) and GraphSAGE (<a class=\"ref-link\" id=\"cHamilton_et+al_2017_a\" href=\"#rHamilton_et+al_2017_a\">Hamilton et al, 2017a</a>), and we precisely characterize the kinds of graph structures such Graph Neural Networks-based models can capture.\n4) We develop a simple neural architecture, Graph Isomorphism Network (GIN), and show that its discriminative/representational power is equal to the power of the WL test",
        "Having developed conditions for a maximally powerful Graph Neural Networks, we develop a simple architecture, Graph Isomorphism Network (GIN), that provably satisfies the conditions in Theorem 3",
        "We evaluate and compare the training and test performance of Graph Isomorphism Network and less powerful Graph Neural Networks variants.1",
        "We evaluate Graph Isomorphism Network (Eqs. 4.1 and 4.2) and the less powerful Graph Neural Networks variants",
        "For the less powerful Graph Neural Networks variants, we consider architectures that replace the sum in the Graph Isomorphism Network-0 aggregation with mean or max-pooling3, or replace multi-layer perceptrons with 1-layer perceptrons, i.e., a linear mapping followed\n1The code is available at https://github.com/weihua916/powerful-gnns. 2There exist certain graphs that Graph Isomorphism Network- can distinguish but Graph Isomorphism Network-0 cannot. 3For REDDIT-BINARY, REDDIT\u2013MULTI5K, and COLLAB, we did not run experiments for max-pooling due to GPU memory constraints",
        "We report the training accuracy of different Graph Neural Networks, where all the hyper-parameters were fixed across the datasets: 5 Graph Neural Networks layers, hidden units of size 64, minibatch of size 128, and 0.5 dropout ratio",
        "The Graph Neural Networks variants using mean/max pooling or 1-layer perceptrons severely underfit on many datasets",
        "We developed theoretical foundations for reasoning about the expressive power of Graph Neural Networks, and proved tight bounds on the representational capacity of popular Graph Neural Networks variants"
    ],
    "summary": [
        "Learning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al, 2017b).",
        "What makes the WL test so powerful is its injective aggregation update that maps different node neighborhoods to different feature vectors.",
        "A maximally powerful GNN would never map two different neighborhoods, i.e., multisets of feature vectors, to the same representation.",
        "In Theorem 3, is yes: if the neighbor aggregation and graph-level readout functions are injective, the resulting GNN is as powerful as the WL test.",
        "With a sufficient number of GNN layers, A maps any graphs G1 and G2 that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold: a) A aggregates and updates node features iteratively with h = \u03c6 h, f h : u \u2208 N (v)",
        "This model generalizes the WL test and achieves maximum discriminative power among GNNs. To model injective multiset functions for the neighbor aggregation, we develop a theory of \u201cdeep multisets\u201d, i.e., parameterizing universal multiset functions with neural networks.",
        "By Theorem 3 and Corollary 6, if GIN replaces READOUT in Eq 4.2 with summing all node features from the same iterations, it provably generalizes the WL test and the WL subtree kernel.",
        "Our results above provide a general framework for analyzing and characterizing the expressive power of a broad class of GNNs. Recently, many GNN-based architectures have been proposed, including sum aggregation and MLP encoding (<a class=\"ref-link\" id=\"cBattaglia_et+al_2016_a\" href=\"#rBattaglia_et+al_2016_a\">Battaglia et al, 2016</a>; <a class=\"ref-link\" id=\"cScarselli_et+al_2009_b\" href=\"#rScarselli_et+al_2009_b\">Scarselli et al, 2009b</a>; <a class=\"ref-link\" id=\"cDuvenaud_et+al_2015_a\" href=\"#rDuvenaud_et+al_2015_a\">Duvenaud et al, 2015</a>), and most without theoretical derivation.",
        "For the less powerful GNN variants, we consider architectures that replace the sum in the GIN-0 aggregation with mean or max-pooling3, or replace MLPs with 1-layer perceptrons, i.e., a linear mapping followed",
        "We apply the same graph-level readout (READOUT in Eq 4.2) for GINs and all the GNN variants, sum readout on bioinformatics datasets and mean readout on social datasets due to better test performance.",
        "Representational power: GNN variants with MLPs tend to have higher training accuracies than those with 1-layer perceptrons, and GNNs with sum aggregators tend to fit the training sets better than those with mean and max-pooling aggregators.",
        "This pattern aligns with our result that the WL test provides an upper bound for the representational capacity of the aggregation-based GNNs. the WL kernel is not able to learn how to combine node features, which might be quite informative for a given prediction task as we will see .",
        "It would be interesting to understand and improve the generalization properties of GNNs as well as better understand their optimization landscape"
    ],
    "headline": "We present a theoretical framework for analyzing the expressive power of Graph Neural Networks to capture different graph structures",
    "reference_links": [
        {
            "id": "Atwood_1993_a",
            "entry": "James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 1993\u20132001, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atwood%2C%20James%20Towsley%2C%20Don%20Diffusion-convolutional%20neural%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Atwood%2C%20James%20Towsley%2C%20Don%20Diffusion-convolutional%20neural%20networks%201993"
        },
        {
            "id": "Babai_2016_a",
            "entry": "L\u00e1szl\u00f3 Babai. Graph isomorphism in quasipolynomial time. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pp. 684\u2013697. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Babai%2C%20L%C3%A1szl%C3%B3%20Graph%20isomorphism%20in%20quasipolynomial%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Babai%2C%20L%C3%A1szl%C3%B3%20Graph%20isomorphism%20in%20quasipolynomial%20time%202016"
        },
        {
            "id": "Babai_1979_a",
            "entry": "L\u00e1szl\u00f3 Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In Foundations of Computer Science, 1979., 20th Annual Symposium on, pp. 39\u201346. IEEE, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Babai%2C%20L%C3%A1szl%C3%B3%20Kucera%2C%20Ludik%20Canonical%20labelling%20of%20graphs%20in%20linear%20average%20time%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Babai%2C%20L%C3%A1szl%C3%B3%20Kucera%2C%20Ludik%20Canonical%20labelling%20of%20graphs%20in%20linear%20average%20time%201979"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems (NIPS), pp. 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "Cai_et+al_1992_a",
            "entry": "Jin-Yi Cai, Martin F\u00fcrer, and Neil Immerman. An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4):389\u2013410, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Jin-Yi%20F%C3%BCrer%2C%20Martin%20Immerman%2C%20Neil%20An%20optimal%20lower%20bound%20on%20the%20number%20of%20variables%20for%20graph%20identification%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Jin-Yi%20F%C3%BCrer%2C%20Martin%20Immerman%2C%20Neil%20An%20optimal%20lower%20bound%20on%20the%20number%20of%20variables%20for%20graph%20identification%201992"
        },
        {
            "id": "Chih-Chung_2011_a",
            "entry": "Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ChihChung%20Chang%20and%20ChihJen%20Lin%20Libsvm%20a%20library%20for%20support%20vector%20machines%20ACM%20transactions%20on%20intelligent%20systems%20and%20technology%20TIST%202327%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ChihChung%20Chang%20and%20ChihJen%20Lin%20Libsvm%20a%20library%20for%20support%20vector%20machines%20ACM%20transactions%20on%20intelligent%20systems%20and%20technology%20TIST%202327%202011"
        },
        {
            "id": "Defferrard_et+al_2016_a",
            "entry": "Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NIPS), pp. 3844\u20133852, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016"
        },
        {
            "id": "Douglas_2011_a",
            "entry": "Brendan L Douglas. The weisfeiler-lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1101.5211"
        },
        {
            "id": "Duvenaud_et+al_2015_a",
            "entry": "David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. pp. 2224\u20132232, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duvenaud%2C%20David%20K.%20Maclaurin%2C%20Dougal%20Iparraguirre%2C%20Jorge%20Bombarell%2C%20Rafael%20Convolutional%20networks%20on%20graphs%20for%20learning%20molecular%20fingerprints%202015"
        },
        {
            "id": "Evdokimov_1999_a",
            "entry": "Sergei Evdokimov and Ilia Ponomarenko. Isomorphism of coloured graphs with slowly increasing multiplicity of jordan blocks. Combinatorica, 19(3):321\u2013333, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evdokimov%2C%20Sergei%20Ponomarenko%2C%20Ilia%20Isomorphism%20of%20coloured%20graphs%20with%20slowly%20increasing%20multiplicity%20of%20jordan%20blocks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evdokimov%2C%20Sergei%20Ponomarenko%2C%20Ilia%20Isomorphism%20of%20coloured%20graphs%20with%20slowly%20increasing%20multiplicity%20of%20jordan%20blocks%201999"
        },
        {
            "id": "Garey_1979_a",
            "entry": "Michael R Garey. A guide to the theory of np-completeness. Computers and intractability, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garey%2C%20Michael%20R.%20A%20guide%20to%20the%20theory%20of%20np-completeness.%20Computers%20and%20intractability%201979"
        },
        {
            "id": "Garey_2002_a",
            "entry": "Michael R Garey and David S Johnson. Computers and intractability, volume 29. wh freeman New York, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garey%2C%20Michael%20R.%20Johnson%2C%20David%20S.%20Computers%20and%20intractability%2C%20volume%2029.%20wh%20freeman%202002"
        },
        {
            "id": "Gilmer_et+al_2017_a",
            "entry": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), pp. 1273\u20131272, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilmer%2C%20Justin%20Schoenholz%2C%20Samuel%20S.%20Riley%2C%20Patrick%20F.%20Vinyals%2C%20Oriol%20Neural%20message%20passing%20for%20quantum%20chemistry%202017"
        },
        {
            "id": "Hamilton_et+al_2017_a",
            "entry": "William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NIPS), pp. 1025\u20131035, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20William%20L.%20Ying%2C%20Rex%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20William%20L.%20Ying%2C%20Rex%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "Hamilton_et+al_0000_a",
            "entry": "William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. IEEE Data Engineering Bulletin, 40(3):52\u201374, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20William%20L.%20Ying%2C%20Rex%20Leskovec%2C%20Jure%20Representation%20learning%20on%20graphs%3A%20Methods%20and%20applications",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20William%20L.%20Ying%2C%20Rex%20Leskovec%2C%20Jure%20Representation%20learning%20on%20graphs%3A%20Methods%20and%20applications"
        },
        {
            "id": "Hornik_1991_a",
            "entry": "Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251\u2013257, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991"
        },
        {
            "id": "Hornik_et+al_1989_a",
            "entry": "Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359\u2013366, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20Kurt%20Stinchcombe%2C%20Maxwell%20White%2C%20Halbert%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20Kurt%20Stinchcombe%2C%20Maxwell%20White%2C%20Halbert%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Ivanov_2018_a",
            "entry": "Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In International Conference on Machine Learning (ICML), pp. 2191\u20132200, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ivanov%2C%20Sergey%20Burnaev%2C%20Evgeny%20Anonymous%20walk%20embeddings%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ivanov%2C%20Sergey%20Burnaev%2C%20Evgeny%20Anonymous%20walk%20embeddings%202018"
        },
        {
            "id": "Kearnes_et+al_2016_a",
            "entry": "Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8): 595\u2013608, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearnes%2C%20Steven%20McCloskey%2C%20Kevin%20Berndl%2C%20Marc%20Pande%2C%20Vijay%20Molecular%20graph%20convolutions%3A%20moving%20beyond%20fingerprints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearnes%2C%20Steven%20McCloskey%2C%20Kevin%20Berndl%2C%20Marc%20Pande%2C%20Vijay%20Molecular%20graph%20convolutions%3A%20moving%20beyond%20fingerprints%202016"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kipf_2017_a",
            "entry": "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "Lei_et+al_2017_a",
            "entry": "Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Deriving neural architectures from sequence and graph kernels. pp. 2024\u20132033, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Tao%20Jin%2C%20Wengong%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Deriving%20neural%20architectures%20from%20sequence%20and%20graph%20kernels%202017"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "Murphy_et+al_2018_a",
            "entry": "Ryan L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. arXiv preprint arXiv:1811.01900, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1811.01900"
        },
        {
            "id": "Nelder_1972_a",
            "entry": "J. A. Nelder and R. W. M. Wedderburn. Generalized linear models. Journal of the Royal Statistical Society, Series A, General, 135:370\u2013384, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelder%2C%20J.A.%20Wedderburn%2C%20R.W.M.%20Generalized%20linear%20models%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelder%2C%20J.A.%20Wedderburn%2C%20R.W.M.%20Generalized%20linear%20models%201972"
        },
        {
            "id": "Niepert_et+al_2014_a",
            "entry": "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International Conference on Machine Learning (ICML), pp. 2014\u20132023, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014"
        },
        {
            "id": "Qi_et+al_2017_a",
            "entry": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 1(2):4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20Charles%20R.%20Su%2C%20Hao%20Mo%2C%20Kaichun%20Guibas%2C%20Leonidas%20J.%20Pointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20Charles%20R.%20Su%2C%20Hao%20Mo%2C%20Kaichun%20Guibas%2C%20Leonidas%20J.%20Pointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%202017"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4967\u20134976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017"
        },
        {
            "id": "Santoro_et+al_2018_a",
            "entry": "Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International Conference on Machine Learning, pp. 4477\u20134486, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Hill%2C%20Felix%20Barrett%2C%20David%20Morcos%2C%20Ari%20Measuring%20abstract%20reasoning%20in%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Hill%2C%20Felix%20Barrett%2C%20David%20Morcos%2C%20Ari%20Measuring%20abstract%20reasoning%20in%20neural%20networks%202018"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks, 20 (1):81\u2013102, 2009a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20Computational%20capabilities%20of%20graph%20neural%20networks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20Computational%20capabilities%20of%20graph%20neural%20networks%202009"
        },
        {
            "id": "Scarselli_et+al_2009_b",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Shervashidze_et+al_2011_a",
            "entry": "Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep): 2539\u20132561, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shervashidze%2C%20Nino%20Schweitzer%2C%20Pascal%20van%20Leeuwen%2C%20Erik%20Jan%20Mehlhorn%2C%20Kurt%20Weisfeiler-lehman%20graph%20kernels%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shervashidze%2C%20Nino%20Schweitzer%2C%20Pascal%20van%20Leeuwen%2C%20Erik%20Jan%20Mehlhorn%2C%20Kurt%20Weisfeiler-lehman%20graph%20kernels%202011"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Velickovic_et+al_2018_a",
            "entry": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Velickovic%2C%20Petar%20Cucurull%2C%20Guillem%20Casanova%2C%20Arantxa%20Romero%2C%20Adriana%20Graph%20attention%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Velickovic%2C%20Petar%20Cucurull%2C%20Guillem%20Casanova%2C%20Arantxa%20Romero%2C%20Adriana%20Graph%20attention%20networks%202018"
        },
        {
            "id": "Verma_2018_a",
            "entry": "Saurabh Verma and Zhi-Li Zhang. Graph capsule convolutional neural networks. arXiv preprint arXiv:1805.08090, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08090"
        },
        {
            "id": "Weisfeiler_1968_a",
            "entry": "Boris Weisfeiler and AA Lehman. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weisfeiler%2C%20Boris%20Lehman%2C%20A.A.%20A%20reduction%20of%20a%20graph%20to%20a%20canonical%20form%20and%20an%20algebra%20arising%20during%20this%20reduction%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weisfeiler%2C%20Boris%20Lehman%2C%20A.A.%20A%20reduction%20of%20a%20graph%20to%20a%20canonical%20form%20and%20an%20algebra%20arising%20during%20this%20reduction%201968"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning (ICML), pp. 5453\u20135462, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Keyulu%20Li%2C%20Chengtao%20Tian%2C%20Yonglong%20Sonobe%2C%20Tomohiro%20Representation%20learning%20on%20graphs%20with%20jumping%20knowledge%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Keyulu%20Li%2C%20Chengtao%20Tian%2C%20Yonglong%20Sonobe%2C%20Tomohiro%20Representation%20learning%20on%20graphs%20with%20jumping%20knowledge%20networks%202018"
        },
        {
            "id": "Yanardag_2015_a",
            "entry": "Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365\u20131374. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yanardag%2C%20Pinar%20Vishwanathan%2C%20S.V.N.%20Deep%20graph%20kernels%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yanardag%2C%20Pinar%20Vishwanathan%2C%20S.V.N.%20Deep%20graph%20kernels%202015"
        },
        {
            "id": "Ying_et+al_2018_a",
            "entry": "Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ying%2C%20Rex%20You%2C%20Jiaxuan%20Morris%2C%20Christopher%20Ren%2C%20Xiang%20Hierarchical%20graph%20representation%20learning%20with%20differentiable%20pooling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ying%2C%20Rex%20You%2C%20Jiaxuan%20Morris%2C%20Christopher%20Ren%2C%20Xiang%20Hierarchical%20graph%20representation%20learning%20with%20differentiable%20pooling%202018"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp. 3391\u20133401, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033913401%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2033913401%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In AAAI Conference on Artificial Intelligence, pp. 4438\u20134445, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Muhan%20Cui%2C%20Zhicheng%20Neumann%2C%20Marion%20Chen%2C%20Yixin%20An%20end-to-end%20deep%20learning%20architecture%20for%20graph%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Muhan%20Cui%2C%20Zhicheng%20Neumann%2C%20Marion%20Chen%2C%20Yixin%20An%20end-to-end%20deep%20learning%20architecture%20for%20graph%20classification%202018"
        },
        {
            "id": "We_2015_a",
            "entry": "We give detailed descriptions of datasets used in our experiments. Further details can be found in (Yanardag & Vishwanathan, 2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20give%20detailed%20descriptions%20of%20datasets%20used%20in%20our%20experiments%20Further%20details%20can%20be%20found%20in%20Yanardag%20%20Vishwanathan%202015"
        }
    ]
}
