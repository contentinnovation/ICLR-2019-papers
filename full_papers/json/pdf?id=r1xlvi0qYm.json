{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "Learning to Remember More  with Less Memorization",
        "author": "Hung Le, Truyen Tran and Svetha Venkatesh, Applied AI Institute, Deakin University, Geelong, Australia, {lethai,truyen.tran,svetha.venkatesh}@deakin.edu.au",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1xlvi0qYm"
        },
        "abstract": "Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks."
    },
    "keywords": [
        {
            "term": "mean square error",
            "url": "https://en.wikipedia.org/wiki/mean_square_error"
        },
        {
            "term": "external memory",
            "url": "https://en.wikipedia.org/wiki/external_memory"
        },
        {
            "term": "short term memory",
            "url": "https://en.wikipedia.org/wiki/short_term_memory"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "abbreviations": {
        "RNNs": "recurrent neural networks",
        "MANN": "memory-augmented neural networks",
        "UW": "Uniform Writing",
        "CUW": "Cached Uniform Writing",
        "RW": "random irregular writing strategy",
        "MSE": "mean square error",
        "MANNs": "Memoryaugmented neural networks"
    },
    "highlights": [
        "A core task in sequence learning is to capture long-term dependencies amongst timesteps which demands memorization of distant inputs",
        "This paper presents a new approach towards finding optimal operations for memory-augmented neural networks that serve the purpose of learning longer sequences with finite memory",
        "To relax the assumption and enable the method to work in realistic settings, we further propose Cached Uniform Writing (CUW) as an improvement over the Uniform Writing scheme",
        "We have introduced Uniform Writing (UW) and Cached Uniform Writing (CUW) as faster solutions for longer-term memorization in memory-augmented neural networks",
        "With a comprehensive suite of synthetic and practical experiments, we provide strong evidences that our simple writing mechanisms are crucial to memory-augmented neural networks to reduce computation complexity and achieve competitive performance in sequence modeling tasks",
        "In complement to the experimental results, we have proposed a meaningful measurement on memory-augmented neural networks memory capacity and provided theoretical analysis showing the optimality of our methods"
    ],
    "key_statements": [
        "A core task in sequence learning is to capture long-term dependencies amongst timesteps which demands memorization of distant inputs",
        "This paper presents a new approach towards finding optimal operations for memory-augmented neural networks that serve the purpose of learning longer sequences with finite memory",
        "To relax the assumption and enable the method to work in realistic settings, we further propose Cached Uniform Writing (CUW) as an improvement over the Uniform Writing scheme",
        "Memory-augmented neural networks can be viewed as an extension of recurrent neural networks with external memory M",
        "We provide details of the attention mechanism used in our Cached Uniform Writing",
        "We have introduced Uniform Writing (UW) and Cached Uniform Writing (CUW) as faster solutions for longer-term memorization in memory-augmented neural networks",
        "With a comprehensive suite of synthetic and practical experiments, we provide strong evidences that our simple writing mechanisms are crucial to memory-augmented neural networks to reduce computation complexity and achieve competitive performance in sequence modeling tasks",
        "In complement to the experimental results, we have proposed a meaningful measurement on memory-augmented neural networks memory capacity and provided theoretical analysis showing the optimality of our methods"
    ],
    "summary": [
        "A core task in sequence learning is to capture long-term dependencies amongst timesteps which demands memorization of distant inputs.",
        "We first define a metric to measure the memorization performance of RNNs, as well as MANNs. we solve the problem of finding the best irregular writing that optimizes the metric.",
        "With any D chosen writes at timesteps 1 \u2264K1< K2< ...<KD< T , there exist \u03bb, C \u2208 R+such that the lower bound on the average contribution of a sequence of length T with respect to a MANN having D memory slots can be quantified as the following: Algorithm 1 Cached Uniform Writing",
        "This mechanism helps the model consider the importance of each timestep input in the local interval and relax the equal contribution assumption of Theorem 3.",
        "To investigate whether our proposed writing schemes help the memory-augmented models handle these challenges, we conduct synthetic reasoning experiments which include add and max tasks.",
        "The model should weight a timestep differently based on either its content or location and maintain its memory for a long time by following uniform criteria.",
        "CUW demonstrates competitive performance against other baselines, approaching to better solution than UW for noisy task where the model should discriminate the timesteps (M SE = 0.98 for UW and 0.55 for CUW).",
        "We want to compare our proposed models with DNC and other methods designed to help recurrent networks learn longer sequence.",
        "After applying UW, the results get better and with CUW, it shows significant improvement over r-LSTM and demonstrates competitive performance against dilated-RNNs models.",
        "Our UW and CUW are built upon DNC with single layer 512-dimensional LSTM controller and the memory size is chosen in accordance with the average length of the document, which ensures 10 \u2212 20% compression ratio.",
        "Our local optimal solution to this problem is related to some known neural caching (<a class=\"ref-link\" id=\"cGrave_et+al_2017_b\" href=\"#rGrave_et+al_2017_b\">Grave et al, 2017b</a>;a; Yogatama et al, 2018) in terms of storing recent hidden states for later encoding uses.",
        "We have introduced Uniform Writing (UW) and Cached Uniform Writing (CUW) as faster solutions for longer-term memorization in MANNs. With a comprehensive suite of synthetic and practical experiments, we provide strong evidences that our simple writing mechanisms are crucial to MANNs to reduce computation complexity and achieve competitive performance in sequence modeling tasks.",
        "In complement to the experimental results, we have proposed a meaningful measurement on MANN memory capacity and provided theoretical analysis showing the optimality of our methods.",
        "Further investigations to tighten the measurement bound will be the focus of our future work"
    ],
    "headline": "We introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2016_a",
            "entry": "Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. Proceedings of the International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "Bengio_et+al_1994_a",
            "entry": "Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 77\u201387, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Shiyu%20Zhang%2C%20Yang%20Han%2C%20Wei%20Yu%2C%20Mo%20Dilated%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Shiyu%20Zhang%2C%20Yang%20Han%2C%20Wei%20Yu%2C%20Mo%20Dilated%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "Conneau_et+al_2016_a",
            "entry": "Alexis Conneau, Holger Schwenk, Lo\u0131c Barrault, and Yann Lecun. Very deep convolutional networks for natural language processing. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Lecun%2C%20Yann%20Very%20deep%20convolutional%20networks%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Schwenk%2C%20Holger%20Barrault%2C%20Lo%C4%B1c%20Lecun%2C%20Yann%20Very%20deep%20convolutional%20networks%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Elman_1990_a",
            "entry": "Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elman%2C%20Jeffrey%20L.%20Finding%20structure%20in%20time%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elman%2C%20Jeffrey%20L.%20Finding%20structure%20in%20time%201990"
        },
        {
            "id": "Franke_et+al_2018_a",
            "entry": "J\u00f6rg Franke, Jan Niehues, and Alex Waibel. Robust and scalable differentiable neural computer for question answering. In Proceedings of the Workshop on Machine Reading for Question Answering, pp. 47\u201359. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/W18-2606.",
            "url": "http://aclweb.org/anthology/W18-2606",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franke%2C%20J%C3%B6rg%20Niehues%2C%20Jan%20Waibel%2C%20Alex%20Robust%20and%20scalable%20differentiable%20neural%20computer%20for%20question%20answering%202018"
        },
        {
            "id": "Grave_et+al_2017_a",
            "entry": "Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. In Advances in Neural Information Processing Systems, pp. 6042\u20136052, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grave%2C%20Edouard%20Cisse%2C%20Moustapha%20M.%20Joulin%2C%20Armand%20Unbounded%20cache%20model%20for%20online%20language%20modeling%20with%20open%20vocabulary%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grave%2C%20Edouard%20Cisse%2C%20Moustapha%20M.%20Joulin%2C%20Armand%20Unbounded%20cache%20model%20for%20online%20language%20modeling%20with%20open%20vocabulary%202017"
        },
        {
            "id": "Grave_et+al_2017_b",
            "entry": "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. ICLR, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Usunier%2C%20Nicolas%20Improving%20neural%20language%20models%20with%20a%20continuous%20cache%202017"
        },
        {
            "id": "Graves_2016_a",
            "entry": "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08983"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_b",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi\u0144ska, Sergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471\u2013476, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Le_et+al_2018_a",
            "entry": "Hung Le, Truyen Tran, Thin Nguyen, and Svetha Venkatesh. Variational memory encoderdecoder. In Advances in Neural Information Processing Systems, pp. 1515\u20131525, 2018a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Hung%20Tran%2C%20Truyen%20Nguyen%2C%20Thin%20Venkatesh%2C%20Svetha%20Variational%20memory%20encoderdecoder%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Hung%20Tran%2C%20Truyen%20Nguyen%2C%20Thin%20Venkatesh%2C%20Svetha%20Variational%20memory%20encoderdecoder%202018"
        },
        {
            "id": "Le_et+al_2018_b",
            "entry": "Hung Le, Truyen Tran, and Svetha Venkatesh. Dual memory neural computer for asynchronous two-view sequential learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD \u201918, pp. 1637\u20131645, New York, NY, USA, 2018b. ACM. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3219981. URL http://doi.acm.org/10.1145/3219819.3219981.",
            "crossref": "https://dx.doi.org/10.1145/3219819.3219981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3219819.3219981"
        },
        {
            "id": "Le_et+al_2015_a",
            "entry": "Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "Logie_2014_a",
            "entry": "Robert H Logie. Visuo-spatial working memory. Psychology Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Logie%2C%20Robert%20H.%20Visuo-spatial%20working%20memory%202014"
        },
        {
            "id": "Miller_2018_a",
            "entry": "John Miller and Moritz Hardt. When recurrent models don\u2019t need to be recurrent. arXiv preprint arXiv:1805.10369, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10369"
        },
        {
            "id": "Pascanu_et+al_2013_a",
            "entry": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pp. 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Qui_et+al_2018_a",
            "entry": "Chao Qui, Bo Huang, Guocheng Niu, Daren Li, Daxiang Dong, Wei He, Dianhai Yu, and Hua Wu. A new method of region embedding for text classification. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkSDMA36Z.",
            "url": "https://openreview.net/forum?id=BkSDMA36Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qui%2C%20Chao%20Huang%2C%20Bo%20Niu%2C%20Guocheng%20Li%2C%20Daren%20A%20new%20method%20of%20region%20embedding%20for%20text%20classification%202018"
        },
        {
            "id": "Rae_et+al_2016_a",
            "entry": "Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pp. 3621\u20133629, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pp. 1842\u20131850, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Meta-learning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Meta-learning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Seo_et+al_2018_a",
            "entry": "Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural Speed Reading via Skim-RNN. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Neural%20Speed%20Reading%20via%20Skim-RNN%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Minjoon%20Min%2C%20Sewon%20Farhadi%2C%20Ali%20Hajishirzi%2C%20Hannaneh%20Neural%20Speed%20Reading%20via%20Skim-RNN%202018"
        },
        {
            "id": "Trinh_et+al_2018_a",
            "entry": "Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies in rnns with auxiliary losses. In Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trinh%2C%20Trieu%20H.%20Dai%2C%20Andrew%20M.%20Luong%2C%20Thang%20Le%2C%20Quoc%20V.%20Learning%20longer-term%20dependencies%20in%20rnns%20with%20auxiliary%20losses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trinh%2C%20Trieu%20H.%20Dai%2C%20Andrew%20M.%20Luong%2C%20Thang%20Le%2C%20Quoc%20V.%20Learning%20longer-term%20dependencies%20in%20rnns%20with%20auxiliary%20losses%202018"
        },
        {
            "id": "Oord_et+al_2016_a",
            "entry": "A\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In SSW, pp. 125, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oord%2C%20A%C3%A4ron%20Van%20Den%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oord%2C%20A%C3%A4ron%20Van%20Den%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998\u20136008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017"
        },
        {
            "id": "Wisdom_et+al_2016_a",
            "entry": "Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Fullcapacity unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 4880\u20134888, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Fullcapacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Fullcapacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Yogatama_et+al_2017_a",
            "entry": "Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blunsom. Generative and discriminative text classification with recurrent neural networks. arXiv preprint arXiv:1703.01898, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01898"
        },
        {
            "id": "Published_2019_a",
            "entry": "Published as a conference paper at ICLR 2019 Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Published%20as%20a%20conference%20paper%20at%20ICLR%202019%20Dani%20Yogatama%20Yishu%20Miao%20Gabor%20Melis%20Wang%20Ling%20Adhiguna%20Kuncoro%20Chris%20Dyer%20and"
        },
        {
            "id": "Blunsom_2018_a",
            "entry": "Phil Blunsom. Memory architectures in recurrent neural network language models. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SkFqf0lAZ. Adams Wei Yu, Hongrae Lee, and Quoc Le. Learning to skim text. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp.1880\u20131890, 2017. Keyi Yu, Yang Liu, Alexander G Schwing, and Jian Peng. Fast and accurate text classification: Skimming, rereading and early stopping. ICLR Workshop Track, 2018.",
            "url": "https://openreview.net/forum?id=SkFqf0lAZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blunsom%2C%20Phil%20Memory%20architectures%20in%20recurrent%20neural%20network%20language%20models%202018"
        }
    ]
}
