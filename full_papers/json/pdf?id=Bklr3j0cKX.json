{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LEARNING DEEP REPRESENTATIONS BY MUTUAL IN-",
        "author": "FORMATION ESTIMATION AND MAXIMIZATION",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bklr3j0cKX"
        },
        "abstract": "This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation\u2019s suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals."
    },
    "keywords": [
        {
            "term": "self organization",
            "url": "https://en.wikipedia.org/wiki/self_organization"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "Jensen-Shannon divergence",
            "url": "https://en.wikipedia.org/wiki/Jensen-Shannon_divergence"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "representation learning",
            "url": "https://en.wikipedia.org/wiki/representation_learning"
        }
    ],
    "abbreviations": {
        "DIM": "Deep InfoMax",
        "MI": "mutual information",
        "NDM": "neural dependence measure",
        "JSD": "Jensen-Shannon divergence",
        "SVM": "support vector machine",
        "MINE": "Mutual information neural estimate"
    },
    "highlights": [
        "One core objective of deep learning is to discover useful representations, and the simple idea explored here is to train a representation-learning function, i.e. an encoder, to maximize the mutual information (MI) between its inputs and outputs",
        "Structure matters: maximizing the average mutual information between the representation and local regions of the input can greatly improve the representation\u2019s quality for, e.g., classification tasks, while global mutual information plays a stronger role in the ability to reconstruct the full input given the representation",
        "Deep InfoMax (DIM) follows Mutual information neural estimate in this regard, though we find that the generator is unnecessary",
        "We investigate Jensen-Shannon divergence and infoNCE in our experiments, and find that using infoNCE often outperforms Jensen-Shannon divergence on downstream tasks, though this effect diminishes with more challenging data",
        "Our results show that infoNCE tends to perform best, but differences between infoNCE and Jensen-Shannon divergence diminish with larger datasets",
        "Augmenting Deep InfoMax with these tasks helps move our method further towards learning representations which encode images not just in terms of compressing their low-level content, but in terms of distributions over relations among higher-level features extracted from their lower-level content.\n5 CONCLUSION In this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised representations by maximizing mutual information, allowing for representations that contain locally-consistent information across structural \u201clocations\u201d"
    ],
    "key_statements": [
        "One core objective of deep learning is to discover useful representations, and the simple idea explored here is to train a representation-learning function, i.e. an encoder, to maximize the mutual information (MI) between its inputs and outputs",
        "Structure matters: maximizing the average mutual information between the representation and local regions of the input can greatly improve the representation\u2019s quality for, e.g., classification tasks, while global mutual information plays a stronger role in the ability to reconstruct the full input given the representation",
        "Our mutual information maximization procedure can prioritize global or local information, which we show can be used to tune the suitability of learned representations for classification or reconstruction-style tasks",
        "Deep InfoMax (DIM) follows Mutual information neural estimate in this regard, though we find that the generator is unnecessary",
        "Our work investigates the suitability of representations learned across two different mutual information objectives that focus on local or global structure, a flexibility we believe is necessary for training representations intended for different applications",
        "We investigate Jensen-Shannon divergence and infoNCE in our experiments, and find that using infoNCE often outperforms Jensen-Shannon divergence on downstream tasks, though this effect diminishes with more challenging data",
        "Our results show that infoNCE tends to perform best, but differences between infoNCE and Jensen-Shannon divergence diminish with larger datasets",
        "Augmenting Deep InfoMax with these tasks helps move our method further towards learning representations which encode images not just in terms of compressing their low-level content, but in terms of distributions over relations among higher-level features extracted from their lower-level content.\n5 CONCLUSION In this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised representations by maximizing mutual information, allowing for representations that contain locally-consistent information across structural \u201clocations\u201d"
    ],
    "summary": [
        "One core objective of deep learning is to discover useful representations, and the simple idea explored here is to train a representation-learning function, i.e. an encoder, to maximize the mutual information (MI) between its inputs and outputs.",
        "We formalize Deep InfoMax (DIM), which simultaneously estimates and maximizes the mutual information between input data and learned high-level representations.",
        "Our mutual information maximization procedure can prioritize global or local information, which we show can be used to tune the suitability of learned representations for classification or reconstruction-style tasks.",
        "We outline the general setting of training an encoder to maximize mutual information between its input and output.",
        "All three objectives \u2013 global and local MI maximization and prior matching \u2013 can be used together, and doing so we arrive at our complete objective for Deep InfoMax (DIM): arg max \u03c91 ,\u03c92 ,\u03c8 \u03b1I\u03c91,\u03c8(X; E\u03c8(X)) +",
        "Other works (<a class=\"ref-link\" id=\"cBojanowski_2017_a\" href=\"#rBojanowski_2017_a\">Bojanowski & Joulin, 2017</a>) have looked at transfer learning classification tasks by freezing the weights of the encoder and training a small fully-connected neural network classifier using the representation as input.",
        "We can use mutual information neural estimation (MINE, Belghazi et al, 2018) to more directly measure the MI between the input and output of the encoder.",
        "Mutual information neural estimate (MINE), I\u03c1(X, E\u03c8(x)), between the input, X, and the output representation, E\u03c8(x), by training a discriminator with parameters \u03c1 to maximize the DV estimator of the KL-divergence.",
        "Extended comparisons Tables 4 shows results on linear separability, reconstruction (MS-SSIM), mutual information, and dependence (NDM) with the CIFAR10 dataset.",
        "All models showed much lower dependence than BiGAN, indicating the marginal of the encoder output is not matching to the generator\u2019s spherical Gaussian input prior, though the mixed local/global version of DIM is close.",
        "We consider augmenting DIM by adding input occlusion when computing global features and by adding auxiliary tasks which maximize MI between local features and absolute or relative spatial coordinates given a global feature.",
        "Augmenting DIM with these tasks helps move our method further towards learning representations which encode images not just in terms of compressing their low-level content, but in terms of distributions over relations among higher-level features extracted from their lower-level content.",
        "5 CONCLUSION In this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised representations by maximizing mutual information, allowing for representations that contain locally-consistent information across structural \u201clocations\u201d.",
        "We believe that this is an important direction in learning higher-level representations"
    ],
    "headline": "We show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation\u2019s suitability for downstream tasks",
    "reference_links": [
        {
            "id": "Alemi_et+al_2016_a",
            "entry": "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "Almeida_2003_a",
            "entry": "Lu\u0131s B Almeida. Linear and nonlinear ica based on mutual information. The Journal of Machine Learning Research, 4:1297\u20131318, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Almeida%2C%20Lu%C4%B1s%20B.%20Linear%20and%20nonlinear%20ica%20based%20on%20mutual%20information%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Almeida%2C%20Lu%C4%B1s%20B.%20Linear%20and%20nonlinear%20ica%20based%20on%20mutual%20information%202003"
        },
        {
            "id": "Arjovsky_2017_a",
            "entry": "Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Bottou%2C%20Leon%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Bottou%2C%20Leon%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Becker_1992_a",
            "entry": "Suzanna Becker. An information-theoretic unsupervised learning algorithm for neural networks. University of Toronto, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Becker%2C%20Suzanna%20An%20information-theoretic%20unsupervised%20learning%20algorithm%20for%20neural%20networks%201992"
        },
        {
            "id": "Becker_1996_a",
            "entry": "Suzanna Becker. Mutual information maximization: models of cortical self-organization. Network: Computation in neural systems, 7(1):7\u201331, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Becker%2C%20Suzanna%20Mutual%20information%20maximization%3A%20models%20of%20cortical%20self-organization.%20Network%3A%20Computation%20in%20neural%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Becker%2C%20Suzanna%20Mutual%20information%20maximization%3A%20models%20of%20cortical%20self-organization.%20Network%3A%20Computation%20in%20neural%201996"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, ICML\u20192018, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04062"
        },
        {
            "id": "Bell_1995_a",
            "entry": "Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6):1129\u20131159, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bell%2C%20Anthony%20J.%20Sejnowski%2C%20Terrence%20J.%20An%20information-maximization%20approach%20to%20blind%20separation%20and%20blind%20deconvolution%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bell%2C%20Anthony%20J.%20Sejnowski%2C%20Terrence%20J.%20An%20information-maximization%20approach%20to%20blind%20separation%20and%20blind%20deconvolution%201995"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Bojanowski_2017_a",
            "entry": "Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint arXiv:1704.05310, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05310"
        },
        {
            "id": "Brakel_2017_a",
            "entry": "Philemon Brakel and Yoshua Bengio. Learning independent features with adversarial nets for non-linear ica. arXiv preprint arXiv:1710.05050, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05050"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep adaptive image clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5879\u20135887, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Jianlong%20Wang%2C%20Lingfeng%20Meng%2C%20Gaofeng%20Xiang%2C%20Shiming%20Deep%20adaptive%20image%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Jianlong%20Wang%2C%20Lingfeng%20Meng%2C%20Gaofeng%20Xiang%2C%20Shiming%20Deep%20adaptive%20image%20clustering%202017"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04942"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Coates_et+al_2011_a",
            "entry": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Lee%2C%20Honglak%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "Dinh_et+al_2014_a",
            "entry": "Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.8516"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Doersch_2017_a",
            "entry": "Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In The IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Zisserman%2C%20Andrew%20Multi-task%20self-supervised%20visual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Zisserman%2C%20Andrew%20Multi-task%20self-supervised%20visual%20learning%202017"
        },
        {
            "id": "Doersch_2015_a",
            "entry": "Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20and%20Alexei%20A%20Efros.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20and%20Alexei%20A%20Efros.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015"
        },
        {
            "id": "Donahue_et+al_2016_a",
            "entry": "Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09782"
        },
        {
            "id": "Donsker_1983_a",
            "entry": "M.D Donsker and S.R.S Varadhan. Asymptotic evaluation of certain markov process expectations for large time, iv. Communications on Pure and Applied Mathematics, 36(2):183\u2013212, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donsker%2C%20M.D.%20Varadhan%2C%20S.R.S.%20Asymptotic%20evaluation%20of%20certain%20markov%20process%20expectations%20for%20large%20time%2C%20iv%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donsker%2C%20M.D.%20Varadhan%2C%20S.R.S.%20Asymptotic%20evaluation%20of%20certain%20markov%20process%20expectations%20for%20large%20time%2C%20iv%201983"
        },
        {
            "id": "Dosovitskiy_et+al_2016_a",
            "entry": "Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intelligence, 38(9):1734\u20131747, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20Alexey%20Fischer%2C%20Philipp%20Springenberg%2C%20Jost%20Tobias%20Riedmiller%2C%20Martin%20Discriminative%20unsupervised%20feature%20learning%20with%20exemplar%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20Alexey%20Fischer%2C%20Philipp%20Springenberg%2C%20Jost%20Tobias%20Riedmiller%2C%20Martin%20Discriminative%20unsupervised%20feature%20learning%20with%20exemplar%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Dumoulin_et+al_2016_a",
            "entry": "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "Gonzalez-Garcia_et+al_2018_a",
            "entry": "Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain disentanglement. arXiv preprint arXiv:1805.09730, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09730"
        },
        {
            "id": "Gretton_et+al_2012_a",
            "entry": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00028"
        },
        {
            "id": "Gutmann_2010_a",
            "entry": "Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297\u2013304, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gutmann%2C%20Michael%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%3A%20A%20new%20estimation%20principle%20for%20unnormalized%20statistical%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gutmann%2C%20Michael%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%3A%20A%20new%20estimation%20principle%20for%20unnormalized%20statistical%20models%202010"
        },
        {
            "id": "Gutmann_2012_a",
            "entry": "Michael U Gutmann and Aapo Hyvarinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of Machine Learning Research, 13 (Feb):307\u2013361, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gutmann%2C%20Michael%20U.%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%20of%20unnormalized%20statistical%20models%2C%20with%20applications%20to%20natural%20image%20statistics%202012-02-13",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gutmann%2C%20Michael%20U.%20Hyvarinen%2C%20Aapo%20Noise-contrastive%20estimation%20of%20unnormalized%20statistical%20models%2C%20with%20applications%20to%20natural%20image%20statistics%202012-02-13"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Higgins_et+al_2016_a",
            "entry": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. Openreview, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20beta-vae%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20beta-vae%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202016"
        },
        {
            "id": "Hinton_2002_a",
            "entry": "Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Training%20products%20of%20experts%20by%20minimizing%20contrastive%20divergence%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Training%20products%20of%20experts%20by%20minimizing%20contrastive%20divergence%202002"
        },
        {
            "id": "Hjelm_et+al_2018_a",
            "entry": "R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, and Yoshua Bengio. Boundary-seeking generative adversarial networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hjelm%2C%20R.Devon%20Jacob%2C%20Athul%20Paul%20Che%2C%20Tong%20Trischler%2C%20Adam%20Boundary-seeking%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hjelm%2C%20R.Devon%20Jacob%2C%20Athul%20Paul%20Che%2C%20Tong%20Trischler%2C%20Adam%20Boundary-seeking%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. arXiv preprint arXiv:1702.08720, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08720"
        },
        {
            "id": "Hyvarinen_2000_a",
            "entry": "Aapo Hyvarinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural networks, 13(4):411\u2013430, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyvarinen%2C%20Aapo%20Oja%2C%20Erkki%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyvarinen%2C%20Aapo%20Oja%2C%20Erkki%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%202000"
        },
        {
            "id": "Hyvarinen_1999_a",
            "entry": "Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429\u2013439, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyvarinen%2C%20Aapo%20Pajunen%2C%20Petteri%20Nonlinear%20independent%20component%20analysis%3A%20Existence%20and%20uniqueness%20results%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyvarinen%2C%20Aapo%20Pajunen%2C%20Petteri%20Nonlinear%20independent%20component%20analysis%3A%20Existence%20and%20uniqueness%20results%201999"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Ji_et+al_2018_a",
            "entry": "Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information distillation for unsupervised image segmentation and clustering. arXiv preprint arXiv:1807.06653, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06653"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2014_a",
            "entry": "Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "Kohonen_1998_a",
            "entry": "Teuvo Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1\u20136, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohonen%2C%20Teuvo%20The%20self-organizing%20map%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohonen%2C%20Teuvo%20The%20self-organizing%20map%201998"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Linsker_1988_a",
            "entry": "Ralph Linsker. Self-organization in a perceptual network. IEEE Computer, 21(3):105\u2013117, 1988. doi: 10.1109/2.36. URL https://doi.org/10.1109/2.36.",
            "crossref": "https://dx.doi.org/10.1109/2.36",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/2.36"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12"
        },
        {
            "id": "Ma_2018_a",
            "entry": "Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.01812"
        },
        {
            "id": "Makhzani_et+al_2015_a",
            "entry": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "Mescheder_et+al_2018_a",
            "entry": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International Conference on Machine Learning, pp. 3478\u20133487, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20gans%20do%20actually%20converge%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20gans%20do%20actually%20converge%3F%202018"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mnih_2013_a",
            "entry": "Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in neural information processing systems, pp. 2265\u20132273, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Andriy%20Kavukcuoglu%2C%20Koray%20Learning%20word%20embeddings%20efficiently%20with%20noise-contrastive%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Andriy%20Kavukcuoglu%2C%20Koray%20Learning%20word%20embeddings%20efficiently%20with%20noise-contrastive%20estimation%202013"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Pathak_et+al_2016_a",
            "entry": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Rezende_et+al_2016_a",
            "entry": "Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. Oneshot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05106"
        },
        {
            "id": "Rifai_et+al_0000_a",
            "entry": "Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pp. 833\u2013840.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rifai%2C%20Salah%20Vincent%2C%20Pascal%20Muller%2C%20Xavier%20Glorot%2C%20Xavier%20Contractive%20autoencoders%3A%20Explicit%20invariance%20during%20feature%20extraction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rifai%2C%20Salah%20Vincent%2C%20Pascal%20Muller%2C%20Xavier%20Glorot%2C%20Xavier%20Contractive%20autoencoders%3A%20Explicit%20invariance%20during%20feature%20extraction"
        },
        {
            "id": "Omnipress_2011_a",
            "entry": "Omnipress, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omnipress%202011"
        },
        {
            "id": "Sajjadi_et+al_2016_a",
            "entry": "Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163\u20131171, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863\u2013879, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Learning%20factorial%20codes%20by%20predictability%20minimization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20Jurgen%20Learning%20factorial%20codes%20by%20predictability%20minimization%201992"
        },
        {
            "id": "Thomas_et+al_2017_a",
            "entry": "Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features. arXiv preprint arXiv:1708.01289, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.01289"
        },
        {
            "id": "Vincent_et+al_2008_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "Vincent_et+al_2010_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(Dec):3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "Wang_et+al_2004_a",
            "entry": "Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh Asilomar Conference on, volume 2, pp. 1398\u20131402.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhou%20Simoncelli%2C%20Eero%20P.%20Bovik%2C%20Alan%20C.%20Multiscale%20structural%20similarity%20for%20image%20quality%20assessment%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhou%20Simoncelli%2C%20Eero%20P.%20Bovik%2C%20Alan%20C.%20Multiscale%20structural%20similarity%20for%20image%20quality%20assessment%202004"
        },
        {
            "id": "Wiskott_2002_a",
            "entry": "Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural computation, 14(4):715\u2013770, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiskott%2C%20Laurenz%20Sejnowski%2C%20Terrence%20J.%20Slow%20feature%20analysis%3A%20Unsupervised%20learning%20of%20invariances%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiskott%2C%20Laurenz%20Sejnowski%2C%20Terrence%20J.%20Slow%20feature%20analysis%3A%20Unsupervised%20learning%20of%20invariances%202002"
        },
        {
            "id": "Xie_et+al_2016_a",
            "entry": "Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In International conference on machine learning, pp. 478\u2013487, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Junyuan%20Girshick%2C%20Ross%20Farhadi%2C%20Ali%20Unsupervised%20deep%20embedding%20for%20clustering%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Junyuan%20Girshick%2C%20Ross%20Farhadi%2C%20Ali%20Unsupervised%20deep%20embedding%20for%20clustering%20analysis%202016"
        },
        {
            "id": "Yang_et+al_2015_a",
            "entry": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. From facial parts responses to face detection: A deep learning approach. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3676\u20133684, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Shuo%20Luo%2C%20Ping%20Loy%2C%20Chen-Change%20Tang%2C%20Xiaoou%20From%20facial%20parts%20responses%20to%20face%20detection%3A%20A%20deep%20learning%20approach%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Shuo%20Luo%2C%20Ping%20Loy%2C%20Chen-Change%20Tang%2C%20Xiaoou%20From%20facial%20parts%20responses%20to%20face%20detection%3A%20A%20deep%20learning%20approach%202015"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a largescale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
