{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ENABLING FACTORIZED PIANO MUSIC MODELING AND GENERATION WITH THE MAESTRO DATASET",
        "author": "Curtis Hawthorne , Andriy Stasyuk , Adam Roberts , Ian Simon , Cheng-Zhi Anna Huang , Sander Dieleman, Erich Elsen , Jesse Engel & Douglas Eck Google Brain, \u2020DeepMind",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=r1lYRjC9F7"
        },
        "abstract": "Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (\u223c0.1 ms to \u223c100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (\u22483 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music."
    },
    "keywords": [
        {
            "term": "piano transcription",
            "url": "https://en.wikipedia.org/wiki/piano_transcription"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "abbreviations": {
        "SMD": "Saarland Music Data"
    },
    "highlights": [
        "Since the beginning of the recent wave of deep learning research, there have been many attempts to create generative models of expressive musical audio de novo",
        "Using an existing transcription model architecture trained on our new dataset, we achieve state-of-the-art results on a piano transcription benchmark",
        "When evaluating against the MAESTRO dataset, we found that audio augmentation made results slightly worse, so results in table 5 are presented without audio augmentation",
        "We have demonstrated the Wave2Midi2Wave system of models for factorized piano music modeling, all enabled by the new MAESTRO dataset",
        "In this paper we have demonstrated all capabilities on the same dataset, but thanks to the new state-of-the-art piano transcription capabilities, any large set of piano recordings could be used,6 which we plan to do in future work"
    ],
    "key_statements": [
        "Since the beginning of the recent wave of deep learning research, there have been many attempts to create generative models of expressive musical audio de novo",
        "Using an existing transcription model architecture trained on our new dataset, we achieve state-of-the-art results on a piano transcription benchmark",
        "When evaluating against the MAESTRO dataset, we found that audio augmentation made results slightly worse, so results in table 5 are presented without audio augmentation",
        "We have demonstrated the Wave2Midi2Wave system of models for factorized piano music modeling, all enabled by the new MAESTRO dataset",
        "In this paper we have demonstrated all capabilities on the same dataset, but thanks to the new state-of-the-art piano transcription capabilities, any large set of piano recordings could be used,6 which we plan to do in future work"
    ],
    "summary": [
        "Since the beginning of the recent wave of deep learning research, there have been many attempts to create generative models of expressive musical audio de novo.",
        "We make the new dataset (MIDI, audio, metadata, and train/validation/test split configuration) available at https://g.co/magenta/maestro-datasetunder a Creative Commons Attribution Non-Commercial Share-Alike 4.0 license.",
        "Our goal in processing the data from International Piano-e-Competition was to produce pairs of audio and MIDI files time-aligned to represent the same musical events.",
        "The large MAESTRO dataset enables training an automatic piano music transcription model that achieves a new state of the art.",
        "We present our results on the train, validation, and test splits of the MAESTRO dataset as a new baseline score in table 5.",
        "While it is true that the audio transcribed for MAESTRO-T was used to train the transcription model, table 5 shows that the model performance is not significantly different between the training split and the validation or test splits, and we needed the larger split to enable training the other models.",
        "One on MIDI data from the MAESTRO dataset and another on MIDI transcriptions inferred by Onsets and Frames from audio in MAESTRO, referred to as MAESTRO-T in section 4.",
        "To separately assess the effects of transcription, language modeling, and synthesis on the listeners\u2019 responses, we presented users with two 20-second clips5 drawn from the following sets, each relying on an additional model from our factorization: Ground Truth Recordings Clips randomly selected from the MAESTRO validation audio split.",
        "WaveNet Ground/Test Clips generated by the Ground WaveNet model described in section 6, conditioned on random 20-second MIDI subsequences from the MAESTRO test split.",
        "WaveNet Transcribed/Test Clips generated by the Transcribed WaveNet model described in section 6, conditioned on random 20-second subsequences from the MAESTRO test split.",
        "The final set of samples demonstrates the full end-to-end ability of taking unlabeled piano performances, inferring MIDI labels via transcription, generating new performances with a language model trained on the inferred MIDI, and rendering new audio as though it were played on a similar piano\u2014all without any information other than raw audio recordings of piano performances.",
        "The transcriptions could be used to train a WaveNet and a Music Transformer model, and new compositions could be generated with the Transformer and rendered with the WaveNet. These new compositions would have similar musical characteristics to the music in the original dataset, and the audio renderings would have similar acoustical characteristics to the source piano.",
        "The new dataset (MIDI, audio, metadata, and train/validation/test split configurations) is available at https://g.co/magenta/maestro-datasetunder a Creative Commons Attribution NonCommercial Share-Alike 4.0 license.",
        "These new compositions would have similar musical characteristics to the music in the original dataset, and the audio renderings would have similar acoustical characteristics to the source piano"
    ],
    "headline": "We show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude, a process we call Wave2Midi2Wave",
    "reference_links": [
        {
            "id": "Bank_et+al_2010_a",
            "entry": "B. Bank, S. Zambon, and F. Fontana. A modal-based real-time piano synthesizer. IEEE Transactions on Audio, Speech, and Language Processing, 18(4):809\u2013821, May 2010. ISSN 1558-7916. doi: 10.1109/TASL.2010.2040524.",
            "crossref": "https://dx.doi.org/10.1109/TASL.2010.2040524",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TASL.2010.2040524"
        },
        {
            "id": "Bay_et+al_2009_a",
            "entry": "Mert Bay, Andreas F Ehmann, and J Stephen Downie. Evaluation of multiple-f0 estimation and tracking systems. In ISMIR, pp. 315\u2013320, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bay%2C%20Mert%20Ehmann%2C%20Andreas%20F.%20Downie%2C%20J.Stephen%20Evaluation%20of%20multiple-f0%20estimation%20and%20tracking%20systems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bay%2C%20Mert%20Ehmann%2C%20Andreas%20F.%20Downie%2C%20J.Stephen%20Evaluation%20of%20multiple-f0%20estimation%20and%20tracking%20systems%202009"
        },
        {
            "id": "Bittner_et+al_2016_a",
            "entry": "Rachel Bittner, Eric Humphrey, and Juan Bello. Pysox: Leveraging the audio signal processing power of sox in python. In Proceedings of the International Society for Music Information Retrieval Conference Late Breaking and Demo Papers, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bittner%2C%20Rachel%20Humphrey%2C%20Eric%20Bello%2C%20Juan%20Pysox%3A%20Leveraging%20the%20audio%20signal%20processing%20power%20of%20sox%20in%20python%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bittner%2C%20Rachel%20Humphrey%2C%20Eric%20Bello%2C%20Juan%20Pysox%3A%20Leveraging%20the%20audio%20signal%20processing%20power%20of%20sox%20in%20python%202016"
        },
        {
            "id": "Brown_1991_a",
            "entry": "Judith C. Brown. Calculation of a constant q spectral transform. The Journal of the Acoustical Society of America, 89(1):425\u2013434, 1991. doi: 10.1121/1.400476.",
            "crossref": "https://dx.doi.org/10.1121/1.400476",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1121/1.400476"
        },
        {
            "id": "Dieleman_et+al_2018_a",
            "entry": "Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music generation: modelling raw audio at scale. arXiv preprint arXiv:1806.10474, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.10474"
        },
        {
            "id": "Emiya_et+al_2010_a",
            "entry": "Valentin Emiya, Nancy Bertin, Bertrand David, and Roland Badeau. MAPS - A piano database for multipitch estimation and automatic transcription of music. Research report, July 2010. URL https://hal.inria.fr/inria-00544155.",
            "url": "https://hal.inria.fr/inria-00544155",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Emiya%2C%20Valentin%20Bertin%2C%20Nancy%20David%2C%20Bertrand%20Badeau%2C%20Roland%20MAPS%20-%20A%20piano%20database%20for%20multipitch%20estimation%20and%20automatic%20transcription%20of%20music%202010-07"
        },
        {
            "id": "Hawthorne_et+al_2018_a",
            "entry": "Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, and Douglas Eck. Onsets and frames: Dual-objective piano transcription. In Proceedings of the 19th International Society for Music Information Retrieval Conference, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hawthorne%2C%20Curtis%20Elsen%2C%20Erich%20Song%2C%20Jialin%20Roberts%2C%20Adam%20Onsets%20and%20frames%3A%20Dual-objective%20piano%20transcription%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hawthorne%2C%20Curtis%20Elsen%2C%20Erich%20Song%2C%20Jialin%20Roberts%2C%20Adam%20Onsets%20and%20frames%3A%20Dual-objective%20piano%20transcription%202018"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, and Douglas Eck. An improved relative self-attention mechanism for transformer with application to music generation. arXiv preprint arXiv:1809.04281, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.04281"
        },
        {
            "id": "Kelz_et+al_2018_a",
            "entry": "Rainer Kelz, Sebastian Bock, and Gerhard Widmer. Deep polyphonic adsr piano note transcription. In Late Breaking/Demos, Proceedings of the 19th International Society for Music Information Retrieval Conference, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelz%2C%20Rainer%20Bock%2C%20Sebastian%20Widmer%2C%20Gerhard%20Deep%20polyphonic%20adsr%20piano%20note%20transcription%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelz%2C%20Rainer%20Bock%2C%20Sebastian%20Widmer%2C%20Gerhard%20Deep%20polyphonic%20adsr%20piano%20note%20transcription%202018"
        },
        {
            "id": "Thakkar_2018_a",
            "entry": "Thakkar Vijay Manzelli, Rachel and, Ali Siahkamari, and Brian Kulis. Combining deep generative raw audio models for structured automatic music. In 19th International Society for Music Information Retrieval Conference, ISMIR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thakkar%20Vijay%20Manzelli%2C%20Rachel%20and%2C%20Ali%20Siahkamari%20Kulis%2C%20Brian%20Combining%20deep%20generative%20raw%20audio%20models%20for%20structured%20automatic%20music%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thakkar%20Vijay%20Manzelli%2C%20Rachel%20and%2C%20Ali%20Siahkamari%20Kulis%2C%20Brian%20Combining%20deep%20generative%20raw%20audio%20models%20for%20structured%20automatic%20music%202018"
        },
        {
            "id": "Mcfee_et+al_2017_a",
            "entry": "Brian McFee, Matt McVicar, Oriol Nieto, Stefan Balke, Carl Thome, Dawen Liang, Eric Battenberg, Josh Moore, Rachel Bittner, Ryuichi Yamamoto, Dan Ellis, Fabian-Robert Stoter, Douglas Repetto, Simon Waloschek, CJ Carr, Seth Kranzler, Keunwoo Choi, Petr Viktorin, Joao Felipe Santos, Adrian Holovaty, Waldir Pimenta, Hojin Lee, and Paul Brossier. librosa 0.5.1, May 2017. URL https://doi.org/10.5281/zenodo.1022770.",
            "crossref": "https://dx.doi.org/10.5281/zenodo.1022770"
        },
        {
            "id": "Muller_2015_a",
            "entry": "Meinard Muller. Fundamentals of music processing: Audio, analysis, algorithms, applications. Springer, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20Meinard%20Fundamentals%20of%20music%20processing%3A%20Audio%2C%20analysis%2C%20algorithms%2C%20applications%202015"
        },
        {
            "id": "Muller_et+al_2011_a",
            "entry": "Meinard Muller, Verena Konz, Wolfgang Bogler, and Vlora Arifi-Muller. Saarland music data (SMD). 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20Meinard%20Konz%2C%20Verena%20Bogler%2C%20Wolfgang%20Arifi-Muller%2C%20Vlora%20Saarland%20music%20data%20%28SMD%29%202011"
        },
        {
            "id": "Raffel_2014_a",
            "entry": "Colin Raffel and Daniel P W Ellis. Intuitive analysis, creation and manipulation of midi data with pretty midi. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raffel%2C%20Colin%20Ellis%2C%20Daniel%20P.W.%20Intuitive%20analysis%2C%20creation%20and%20manipulation%20of%20midi%20data%20with%20pretty%20midi%202014"
        },
        {
            "id": "Sakoe_1978_a",
            "entry": "Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE transactions on acoustics, speech, and signal processing, 26(1):43\u201349, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakoe%2C%20Hiroaki%20Chiba%2C%20Seibi%20Dynamic%20programming%20algorithm%20optimization%20for%20spoken%20word%20recognition%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakoe%2C%20Hiroaki%20Chiba%2C%20Seibi%20Dynamic%20programming%20algorithm%20optimization%20for%20spoken%20word%20recognition%201978"
        },
        {
            "id": "Schorkhuber_2010_a",
            "entry": "Christian Schorkhuber and Anssi Klapuri. Constant-q transform toolbox for music processing. In Proceedings of the 7th Sound and Music Computing Conference, Barcelona, Spain, July 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schorkhuber%2C%20Christian%20Klapuri%2C%20Anssi%20Constant-q%20transform%20toolbox%20for%20music%20processing%202010-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schorkhuber%2C%20Christian%20Klapuri%2C%20Anssi%20Constant-q%20transform%20toolbox%20for%20music%20processing%202010-07"
        },
        {
            "id": "Thickstun_et+al_2017_a",
            "entry": "John Thickstun, Zaid Harchaoui, and Sham Kakade. Learning features of music from scratch. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thickstun%2C%20John%20Harchaoui%2C%20Zaid%20Kakade%2C%20Sham%20Learning%20features%20of%20music%20from%20scratch%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thickstun%2C%20John%20Harchaoui%2C%20Zaid%20Kakade%2C%20Sham%20Learning%20features%20of%20music%20from%20scratch%202017"
        },
        {
            "id": "Valimaki_et+al_2012_a",
            "entry": "Vesa Valimaki, Julian D Parker, Lauri Savioja, Julius O Smith, and Jonathan S Abel. Fifty years of artificial reverberation. IEEE Transactions on Audio, Speech, and Language Processing, 20(5): 1421\u20131448, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valimaki%2C%20Vesa%20Parker%2C%20Julian%20D.%20Savioja%2C%20Lauri%20Smith%2C%20Julius%20O.%20Fifty%20years%20of%20artificial%20reverberation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valimaki%2C%20Vesa%20Parker%2C%20Julian%20D.%20Savioja%2C%20Lauri%20Smith%2C%20Julius%20O.%20Fifty%20years%20of%20artificial%20reverberation%202012"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. In SSW, pp. 125, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20WaveNet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20WaveNet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, and Demis Hassabis. Parallel WaveNet: Fast high-fidelity speech synthesis. In Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Li%2C%20Yazhe%20Babuschkin%2C%20Igor%20Simonyan%2C%20Karen%20Parallel%20WaveNet%3A%20Fast%20high-fidelity%20speech%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Li%2C%20Yazhe%20Babuschkin%2C%20Igor%20Simonyan%2C%20Karen%20Parallel%20WaveNet%3A%20Fast%20high-fidelity%20speech%20synthesis%202018"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998\u20136008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2059986008%202017"
        },
        {
            "id": "4)._2006_a",
            "entry": "4). Cambridge University, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cambridge%20University%202006"
        }
    ]
}
