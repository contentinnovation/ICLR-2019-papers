{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES",
        "author": "ACCURACY AND ROBUSTNESS",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=Bygh9j09KX"
        },
        "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \u2018StylizedImageNet\u2019, a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation."
    },
    "keywords": [
        {
            "term": "Intelligence Advanced Research Projects Activity",
            "url": "https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        },
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "Convolutional Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        }
    ],
    "abbreviations": {
        "CNNs": "Convolutional Neural Networks",
        "IARPA": "Intelligence Advanced Research Projects Activity",
        "DoI/IBC": "Department of Interior/Interior Business Center"
    },
    "highlights": [
        "How are Convolutional Neural Networks (CNNs) able to reach impressive performance on complex perceptual tasks such as object recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) and semantic segmentation (<a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\"><a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\">Long et al, 2015</a></a>)? One widely accepted intuition is that Convolutional Neural Networks combine low-level features to increasingly complex shapes until the object can be readily classified",
        "When object outlines were filled in with black colour to generate a silhouette, Convolutional Neural Networks recognition accuracies were much lower than human accuracies",
        "On the basis of extensive experiments on both Convolutional Neural Networks and human observers in a controlled psychophysical lab setting, we provide evidence that unlike humans, ImageNet-trained Convolutional Neural Networks tend to classify objects according to local textures instead of global object shapes",
        "In combination with previous work which showed that changing other major object dimensions such as colour (<a class=\"ref-link\" id=\"cGeirhos_et+al_2018_a\" href=\"#rGeirhos_et+al_2018_a\">Geirhos et al, 2018</a>) and object size relative to the context (<a class=\"ref-link\" id=\"cEckstein_et+al_2017_a\" href=\"#rEckstein_et+al_2017_a\">Eckstein et al, 2017</a>) do not have a strong detrimental impact on Convolutional Neural Networks recognition performance, this highlights the special role that local cues such as textures seem to play in Convolutional Neural Networks object recognition",
        "We demonstrated that a ResNet-50 architecture can learn to recognise objects based on object shape, revealing that the texture bias in current Convolutional Neural Networks is not by design but induced by ImageNet training data",
        "We provided evidence that machine recognition today overly relies on object textures rather than global object shapes as commonly assumed"
    ],
    "key_statements": [
        "How are Convolutional Neural Networks (CNNs) able to reach impressive performance on complex perceptual tasks such as object recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) and semantic segmentation (<a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\"><a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\">Long et al, 2015</a></a>)? One widely accepted intuition is that Convolutional Neural Networks combine low-level features to increasingly complex shapes until the object can be readily classified",
        "When object outlines were filled in with black colour to generate a silhouette, Convolutional Neural Networks recognition accuracies were much lower than human accuracies",
        "This was even more pronounced for edge stimuli, indicating that human observers cope much better with images that have little to no texture information",
        "One confound in these experiments is that Convolutional Neural Networks tend not to cope well with domain shifts, i.e. the large change in image statistics from natural images to sketches",
        "On the basis of extensive experiments on both Convolutional Neural Networks and human observers in a controlled psychophysical lab setting, we provide evidence that unlike humans, ImageNet-trained Convolutional Neural Networks tend to classify objects according to local textures instead of global object shapes",
        "In combination with previous work which showed that changing other major object dimensions such as colour (<a class=\"ref-link\" id=\"cGeirhos_et+al_2018_a\" href=\"#rGeirhos_et+al_2018_a\">Geirhos et al, 2018</a>) and object size relative to the context (<a class=\"ref-link\" id=\"cEckstein_et+al_2017_a\" href=\"#rEckstein_et+al_2017_a\">Eckstein et al, 2017</a>) do not have a strong detrimental impact on Convolutional Neural Networks recognition performance, this highlights the special role that local cues such as textures seem to play in Convolutional Neural Networks object recognition",
        "We demonstrated that a ResNet-50 architecture can learn to recognise objects based on object shape, revealing that the texture bias in current Convolutional Neural Networks is not by design but induced by ImageNet training data",
        "This indicates that standard ImageNet-trained models may be taking a \u201cshortcut\u201d by focusing on local textures, which could be seen as a version of Occam\u2019s razor: If textures are sufficient, why should a Convolutional Neural Networks learn much else? While texture classification may be easier than shape recognition, we found that shape-based features trained on SIN generalise well to natural images",
        "We provided evidence that machine recognition today overly relies on object textures rather than global object shapes as commonly assumed"
    ],
    "summary": [
        "How are Convolutional Neural Networks (CNNs) able to reach impressive performance on complex perceptual tasks such as object recognition (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) and semantic segmentation (<a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\"><a class=\"ref-link\" id=\"cLong_et+al_2015_a\" href=\"#rLong_et+al_2015_a\">Long et al, 2015</a></a>)? One widely accepted intuition is that CNNs combine low-level features to increasingly complex shapes until the object can be readily classified.",
        "Networks with a higher shape bias are inherently more robust to many different image distortions and reach higher performance on classification and object recognition tasks.",
        "Almost all object and texture images (Original and Texture data set) were recognised correctly by both CNNs and humans (Figure 2).",
        "The psychophysical experiments suggest that ImageNet-trained CNNs, but not humans, exhibit a strong texture bias.",
        "In order to test this hypothesis we train a ResNet-50 on our Stylized-ImageNet (SIN) data set in which we replaced the object-related local texture information with the uninformative style of randomly selected artistic paintings.",
        "Robustness against distortions We systematically tested how model accuracies degrade if images are distorted by uniform or phase noise, contrast changes, high- and low-pass filtering or eidolon perturbations.4 The results of this comparison, including human data for reference, are visualised in Figure 6.",
        "As noted in the Introduction, there seems to be a large discrepancy between the common assumption that CNNs use increasingly complex shape features to recognise objects and recent empirical findings which suggest a crucial role of object textures instead.",
        "On the basis of extensive experiments on both CNNs and human observers in a controlled psychophysical lab setting, we provide evidence that unlike humans, ImageNet-trained CNNs tend to classify objects according to local textures instead of global object shapes.",
        "Texture-based generative modelling approaches such as style transfer (<a class=\"ref-link\" id=\"cGatys_et+al_2016_a\" href=\"#rGatys_et+al_2016_a\">Gatys et al, 2016</a>), single image super-resolution (<a class=\"ref-link\" id=\"cGondal_et+al_2018_a\" href=\"#rGondal_et+al_2018_a\">Gondal et al, 2018</a>) as well as static and dynamic texture synthesis (<a class=\"ref-link\" id=\"cGatys_et+al_2015_a\" href=\"#rGatys_et+al_2015_a\">Gatys et al, 2015</a>; <a class=\"ref-link\" id=\"cFunke_et+al_2017_a\" href=\"#rFunke_et+al_2017_a\">Funke et al, 2017</a>) all produce excellent results using standard CNNs, while CNNbased shape transfer seems to be very difficult (<a class=\"ref-link\" id=\"cGokaslan_et+al_2018_a\" href=\"#rGokaslan_et+al_2018_a\">Gokaslan et al, 2018</a>).",
        "In order to reduce the texture bias of CNNs we introduced Stylized-ImageNet (SIN), a data set that removes local cues through style transfer and thereby forces networks to go beyond texture recognition.",
        "We demonstrated that a ResNet-50 architecture can learn to recognise objects based on object shape, revealing that the texture bias in current CNNs is not by design but induced by ImageNet training data.",
        "Our results indicate that a more shape-based representation can be beneficial for recognition tasks that rely on pre-trained ImageNet CNNs. while ImageNet-trained CNNs generalise poorly towards a wide range of image distortions (e.g. <a class=\"ref-link\" id=\"cDodge_2017_a\" href=\"#rDodge_2017_a\">Dodge & Karam, 2017</a>; <a class=\"ref-link\" id=\"cGeirhos_et+al_2017_a\" href=\"#rGeirhos_et+al_2017_a\">Geirhos et al, 2017</a>; 2018), our ResNet-50 trained on Stylized-ImageNet often reaches or even surpasses human-level robustness.",
        "A useful starting point for future undertakings where domain knowledge suggests that a shape-based representation may be more beneficial than a texture-based one"
    ],
    "headline": "We show that ImageNettrained Convolutional Neural Networks are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies",
    "reference_links": [
        {
            "id": "Ballester_2016_a",
            "entry": "Pedro Ballester and Ricardo Matsumura de Araujo. On the performance of GoogLeNet and AlexNet applied to sketches. In AAAI, pp. 1124\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ballester%2C%20Pedro%20de%20Araujo%2C%20Ricardo%20Matsumura%20On%20the%20performance%20of%20GoogLeNet%20and%20AlexNet%20applied%20to%20sketches%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ballester%2C%20Pedro%20de%20Araujo%2C%20Ricardo%20Matsumura%20On%20the%20performance%20of%20GoogLeNet%20and%20AlexNet%20applied%20to%20sketches%202016"
        },
        {
            "id": "Brendel_2019_a",
            "entry": "Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brendel%2C%20Wieland%20Bethge%2C%20Matthias%20Approximating%20CNNs%20with%20bag-of-local-features%20models%20works%20surprisingly%20well%20on%20ImageNet%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brendel%2C%20Wieland%20Bethge%2C%20Matthias%20Approximating%20CNNs%20with%20bag-of-local-features%20models%20works%20surprisingly%20well%20on%20ImageNet%202019"
        },
        {
            "id": "Cadieu_et+al_2014_a",
            "entry": "Charles F Cadieu, H Hong, D L K Yamins, N Pinto, D Ardila, E A Solomon, N J Majaj, and J J DiCarlo. Deep neural networks rival the representation of primate IT cortex for core visual object recognition. PLoS Computational Biology, 10(12), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cadieu%2C%20Charles%20F.%20Hong%2C%20H.%20Yamins%2C%20D.L.K.%20Pinto%2C%20N.%20E%20A%20Solomon%2C%20N%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cadieu%2C%20Charles%20F.%20Hong%2C%20H.%20Yamins%2C%20D.L.K.%20Pinto%2C%20N.%20E%20A%20Solomon%2C%20N%202014"
        },
        {
            "id": "Dodge_2017_a",
            "entry": "Samuel Dodge and Lina Karam. A study and comparison of human and deep learning recognition performance under visual distortions. arXiv preprint arXiv:1705.02498, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02498"
        },
        {
            "id": "Eckstein_et+al_2017_a",
            "entry": "Miguel P Eckstein, Kathryn Koehler, Lauren E Welbourne, and Emre Akbas. Humans, but not deep neural networks, often miss giant targets in scenes. Current Biology, 27(18):2827\u20132832, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eckstein%2C%20Miguel%20P.%20Koehler%2C%20Kathryn%20Welbourne%2C%20Lauren%20E.%20Akbas%2C%20Emre%20Humans%2C%20but%20not%20deep%20neural%20networks%2C%20often%20miss%20giant%20targets%20in%20scenes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eckstein%2C%20Miguel%20P.%20Koehler%2C%20Kathryn%20Welbourne%2C%20Lauren%20E.%20Akbas%2C%20Emre%20Humans%2C%20but%20not%20deep%20neural%20networks%2C%20often%20miss%20giant%20targets%20in%20scenes%202017"
        },
        {
            "id": "Funke_et+al_2017_a",
            "entry": "Christina M Funke, Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Synthesising dynamic textures using convolutional neural networks. arXiv preprint arXiv:1702.07006, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07006"
        },
        {
            "id": "Gatys_et+al_2015_a",
            "entry": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 262\u2013270, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Texture%20synthesis%20using%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Texture%20synthesis%20using%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "Gatys_et+al_2016_a",
            "entry": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2414\u20132423, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Gatys_et+al_2017_a",
            "entry": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Texture and art with deep neural networks. Current Opinion in Neurobiology, 46:178\u2013186, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Texture%20and%20art%20with%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20Leon%20A.%20Ecker%2C%20Alexander%20S.%20Bethge%2C%20Matthias%20Texture%20and%20art%20with%20deep%20neural%20networks%202017"
        },
        {
            "id": "Geirhos_et+al_2017_a",
            "entry": "Robert Geirhos, David HJ Janssen, Heiko H Schutt, Jonas Rauber, Matthias Bethge, and Felix A Wichmann. Comparing deep neural networks against humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06969"
        },
        {
            "id": "Geirhos_et+al_2018_a",
            "entry": "Robert Geirhos, Carlos M. Medina Temme, Jonas Rauber, Heiko H Schutt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. arXiv preprint arXiv:1808.08750, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.08750"
        },
        {
            "id": "Gokaslan_et+al_2018_a",
            "entry": "Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie, Kwang In Kim, and James Tompkin. Improving shape deformation in unsupervised image-to-image translation. arXiv preprint arXiv:1808.04325, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.04325"
        },
        {
            "id": "Gondal_et+al_2018_a",
            "entry": "Muhammad W Gondal, Bernhard Scholkopf, and Michael Hirsch. The unreasonable effectiveness of texture transfer for single image super-resolution. arXiv preprint arXiv:1808.00043, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.00043"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "Hendrycks_2019_a",
            "entry": "Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendrycks%2C%20Dan%20Dietterich%2C%20Thomas%20Benchmarking%20neural%20network%20robustness%20to%20common%20corruptions%20and%20perturbations%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendrycks%2C%20Dan%20Dietterich%2C%20Thomas%20Benchmarking%20neural%20network%20robustness%20to%20common%20corruptions%20and%20perturbations%202019"
        },
        {
            "id": "Hosseini_et+al_2018_a",
            "entry": "Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, and Radha Poovendran. Assessing shape bias property of Convolutional Neural Networks. arXiv preprint arXiv:1803.07739, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07739"
        },
        {
            "id": "Huang_2017_a",
            "entry": "Xun Huang and Serge J Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, pp. 1510\u20131519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xun%20Belongie%2C%20Serge%20J.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xun%20Belongie%2C%20Serge%20J.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017"
        },
        {
            "id": "Jia_et+al_2014_a",
            "entry": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675\u2013678. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014"
        },
        {
            "id": "Krasin_et+al_2017_a",
            "entry": "Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. OpenImages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017.",
            "url": "https://github.com/openimages",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krasin%2C%20Ivan%20Duerig%2C%20Tom%20Alldrin%2C%20Neil%20Ferrari%2C%20Vittorio%20OpenImages%3A%20A%20public%20dataset%20for%20large-scale%20multi-label%20and%20multi-class%20image%20classification%202017"
        },
        {
            "id": "Kriegeskorte_2015_a",
            "entry": "N. Kriegeskorte. Deep neural networks: A new framework for modeling biological vision and brain information processing. Annual Review of Vision Science, 1(15):417\u2013446, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kriegeskorte%2C%20N.%20Deep%20neural%20networks%3A%20A%20new%20framework%20for%20modeling%20biological%20vision%20and%20brain%20information%20processing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kriegeskorte%2C%20N.%20Deep%20neural%20networks%3A%20A%20new%20framework%20for%20modeling%20biological%20vision%20and%20brain%20information%20processing%202015"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Kubilius_et+al_2016_a",
            "entry": "Jonas Kubilius, Stefania Bracci, and Hans P Op de Beeck. Deep neural networks as a computational model for human shape sensitivity. PLoS Computational Biology, 12(4):e1004896, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kubilius%2C%20Jonas%20Bracci%2C%20Stefania%20de%20Beeck%2C%20Hans%20P.Op%20Deep%20neural%20networks%20as%20a%20computational%20model%20for%20human%20shape%20sensitivity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kubilius%2C%20Jonas%20Bracci%2C%20Stefania%20de%20Beeck%2C%20Hans%20P.Op%20Deep%20neural%20networks%20as%20a%20computational%20model%20for%20human%20shape%20sensitivity%202016"
        },
        {
            "id": "Landau_et+al_1988_a",
            "entry": "Barbara Landau, Linda B Smith, and Susan S Jones. The importance of shape in early lexical learning. Cognitive Development, 3(3):299\u2013321, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Landau%2C%20Barbara%20Smith%2C%20Linda%20B.%20Jones%2C%20Susan%20S.%20The%20importance%20of%20shape%20in%20early%20lexical%20learning%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Landau%2C%20Barbara%20Smith%2C%20Linda%20B.%20Jones%2C%20Susan%20S.%20The%20importance%20of%20shape%20in%20early%20lexical%20learning%201988"
        },
        {
            "id": "Laskar_et+al_2018_a",
            "entry": "Md Nasir Uddin Laskar, Luis G Sanchez Giraldo, and Odelia Schwartz. Correspondence of deep neural networks and the brain for visual textures. arXiv preprint arXiv:1806.02888, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02888"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015"
        },
        {
            "id": "Long_2018_a",
            "entry": "Bria Long and Talia Konkle. The role of textural statistics vs. outer contours in deep CNN and neural responses to objects. http://konklab.fas.harvard.edu/ ConferenceProceedings/Long_2018_CCN.pdf, 2018.",
            "url": "http://konklab.fas.harvard.edu/ConferenceProceedings/Long_2018_CCN.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Bria%20Konkle%2C%20Talia%20The%20role%20of%20textural%20statistics%20vs.%20outer%20contours%20in%20deep%20CNN%20and%20neural%20responses%20to%20objects%202018"
        },
        {
            "id": "Long_et+al_2015_a",
            "entry": "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u20133440, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "George_1995_a",
            "entry": "George A Miller. WordNet: a lexical database for English. Communications of the ACM, 38(11): 39\u201341, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=George%20A%20Miller%20WordNet%20a%20lexical%20database%20for%20English%20Communications%20of%20the%20ACM%203811%203941%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=George%20A%20Miller%20WordNet%20a%20lexical%20database%20for%20English%20Communications%20of%20the%20ACM%203811%203941%201995"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%202017"
        },
        {
            "id": "Ren_et+al_2017_a",
            "entry": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis & Machine Intelligence, pp. 1137\u20131149, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202017"
        },
        {
            "id": "Ritter_et+al_2017_a",
            "entry": "Samuel Ritter, David GT Barrett, Adam Santoro, and Matt M Botvinick. Cognitive psychology for deep neural networks: A shape bias case study. arXiv preprint arXiv:1706.08606, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08606"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Ustyuzhaninov_et+al_2018_a",
            "entry": "Ivan Ustyuzhaninov, Claudio Michaelis, Wieland Brendel, and Matthias Bethge. One-shot texture segmentation. arXiv preprint arXiv:1807.02654, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.02654"
        },
        {
            "id": "Wallis_et+al_2017_a",
            "entry": "Thomas SA Wallis, Christina M Funke, Alexander S Ecker, Leon A Gatys, Felix A Wichmann, and Matthias Bethge. A parametric texture model based on deep convolutional features closely matches texture appearance for humans. Journal of Vision, 17(12):5\u20135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wallis%2C%20Thomas%20S.A.%20Funke%2C%20Christina%20M.%20Ecker%2C%20Alexander%20S.%20Leon%20A%20Gatys%2C%20Felix%20A%20Wichmann%2C%20and%20Matthias%20Bethge.%20A%20parametric%20texture%20model%20based%20on%20deep%20convolutional%20features%20closely%20matches%20texture%20appearance%20for%20humans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wallis%2C%20Thomas%20S.A.%20Funke%2C%20Christina%20M.%20Ecker%2C%20Alexander%20S.%20Leon%20A%20Gatys%2C%20Felix%20A%20Wichmann%2C%20and%20Matthias%20Bethge.%20A%20parametric%20texture%20model%20based%20on%20deep%20convolutional%20features%20closely%20matches%20texture%20appearance%20for%20humans%202017"
        },
        {
            "id": "Yamins_2014_a",
            "entry": "Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences, 111(23):8619\u20138624, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamins%2C%20Daniel%20L.K.%20Hong%2C%20Ha%20Charles%20F%20Cadieu%2C%20Ethan%20A%20Solomon%2C%20Darren%20Seibert%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamins%2C%20Daniel%20L.K.%20Hong%2C%20Ha%20Charles%20F%20Cadieu%2C%20Ethan%20A%20Solomon%2C%20Darren%20Seibert%202014"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang, and Timothy M Hospedales. Sketcha-net: A deep neural network that beats humans. International Journal of Computer Vision, 122 (3):411\u2013425, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Qian%20Yang%2C%20Yongxin%20Liu%2C%20Feng%20Song%2C%20Yi-Zhe%20Sketcha-net%3A%20A%20deep%20neural%20network%20that%20beats%20humans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Qian%20Yang%2C%20Yongxin%20Liu%2C%20Feng%20Song%2C%20Yi-Zhe%20Sketcha-net%3A%20A%20deep%20neural%20network%20that%20beats%20humans%202017"
        },
        {
            "id": "In_0000_a",
            "entry": "In this Appendix, we report experimental details for human and CNN experiments. All trained model weights reported in this paper as well as our human behavioural data set (48,560 psychophysical trials across 97 observers) are openly available from this repository: https://github.com/rgeirhos/texture-vs-shape",
            "url": "https://github.com/rgeirhos/texture-vs-shape"
        },
        {
            "id": "We_2018_a",
            "entry": "We followed the paradigm of Geirhos et al. (2018) for maximal comparability. A trial consisted of 300 ms presentation of a fixation square and a 200 ms presentation of the stimulus image, which was followed by a full-contrast pink noise mask (1/f spectral shape) of the same size lasting for 200 ms. Participants had to choose one of 16 entry-level categories by clicking on a response screen shown for 1500 ms. On this screen, icons of all 16 categories were arranged in a 4 \u00d7 4 grid. The experiments were not self-paced and therefore one trial always lasted 2200 ms (300 ms + 200 ms + 200 ms + 1500 ms = 2200 ms). The necessary time to complete an experiment with 1280 stimuli was 47 minutes, for 160 stimuli six minutes, and for 48 stimuli two minutes. In the experiments with 1280 trials, observers were given the possibility of taking a brief break after every block of 256 trials (five blocks in total).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20followed%20the%20paradigm%20of%20Geirhos%20et%20al%202018%20for%20maximal%20comparability%20A%20trial%20consisted%20of%20300%20ms%20presentation%20of%20a%20fixation%20square%20and%20a%20200%20ms%20presentation%20of%20the%20stimulus%20image%20which%20was%20followed%20by%20a%20fullcontrast%20pink%20noise%20mask%201f%20spectral%20shape%20of%20the%20same%20size%20lasting%20for%20200%20ms%20Participants%20had%20to%20choose%20one%20of%2016%20entrylevel%20categories%20by%20clicking%20on%20a%20response%20screen%20shown%20for%201500%20ms%20On%20this%20screen%20icons%20of%20all%2016%20categories%20were%20arranged%20in%20a%204%20%204%20grid%20The%20experiments%20were%20not%20selfpaced%20and%20therefore%20one%20trial%20always%20lasted%202200%20ms%20300%20ms%20%20200%20ms%20%20200%20ms%20%201500%20ms%20%202200%20ms%20The%20necessary%20time%20to%20complete%20an%20experiment%20with%201280%20stimuli%20was%2047%20minutes%20for%20160%20stimuli%20six%20minutes%20and%20for%2048%20stimuli%20two%20minutes%20In%20the%20experiments%20with%201280%20trials%20observers%20were%20given%20the%20possibility%20of%20taking%20a%20brief%20break%20after%20every%20block%20of%20256%20trials%20five%20blocks%20in%20total",
            "oa_query": "https://api.scholarcy.com/oa_version?query=We%20followed%20the%20paradigm%20of%20Geirhos%20et%20al%202018%20for%20maximal%20comparability%20A%20trial%20consisted%20of%20300%20ms%20presentation%20of%20a%20fixation%20square%20and%20a%20200%20ms%20presentation%20of%20the%20stimulus%20image%20which%20was%20followed%20by%20a%20fullcontrast%20pink%20noise%20mask%201f%20spectral%20shape%20of%20the%20same%20size%20lasting%20for%20200%20ms%20Participants%20had%20to%20choose%20one%20of%2016%20entrylevel%20categories%20by%20clicking%20on%20a%20response%20screen%20shown%20for%201500%20ms%20On%20this%20screen%20icons%20of%20all%2016%20categories%20were%20arranged%20in%20a%204%20%204%20grid%20The%20experiments%20were%20not%20selfpaced%20and%20therefore%20one%20trial%20always%20lasted%202200%20ms%20300%20ms%20%20200%20ms%20%20200%20ms%20%201500%20ms%20%202200%20ms%20The%20necessary%20time%20to%20complete%20an%20experiment%20with%201280%20stimuli%20was%2047%20minutes%20for%20160%20stimuli%20six%20minutes%20and%20for%2048%20stimuli%20two%20minutes%20In%20the%20experiments%20with%201280%20trials%20observers%20were%20given%20the%20possibility%20of%20taking%20a%20brief%20break%20after%20every%20block%20of%20256%20trials%20five%20blocks%20in%20total"
        },
        {
            "id": "Observers_120_b",
            "entry": "Observers were shown the 224\u00d7224 pixels stimuli in a dark cabin on a 22\u201d, 120 Hz VIEWPixx LCD monitor (VPixx Technologies, Saint-Bruno, Canada). The screen of size 484 \u00d7 302 mm corresponds to 1920 \u00d7 1200 pixels, although stimuli were only presented foveally at the center of the screen (3 \u00d7 3 degrees of visual angle at a viewing distance of 107 cm) while the background was set to a grey value of 0.7614 in the [0, 1] range, the average greyscale value of all stimuli used in the original experiment. Participants used a chin rest to keep their head position static during an experiment. Stimulus presentation was conducted with the Psychophysics Toolbox (version 3.0.12) in MATLAB (Release 2016a, The MathWorks, Inc., Natick, Massachusetts, United States) using a 12-core desktop computer (AMD HD7970 graphics card \u201cTahiti\u201d by AMD, Sunnyvale, California, United States) running Kubuntu 14.04 LTS. Participants clicked on a response screen, showing an experiment instruction # p. #\u2640 #\u2642 age range mean age",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Observers%20were%20shown%20the%20224224%20pixels%20stimuli%20in%20a%20dark%20cabin%20on%20a%2022%20120%20Hz%20VIEWPixx%20LCD%20monitor%20VPixx%20Technologies%20SaintBruno%20Canada%20The%20screen%20of%20size%20484%20%20302%20mm%20corresponds%20to%201920%20%201200%20pixels%20although%20stimuli%20were%20only%20presented%20foveally%20at%20the%20center%20of%20the%20screen%203%20%203%20degrees%20of%20visual%20angle%20at%20a%20viewing%20distance%20of%20107%20cm%20while%20the%20background%20was%20set%20to%20a%20grey%20value%20of%2007614%20in%20the%200%201%20range%20the%20average%20greyscale%20value%20of%20all%20stimuli%20used%20in%20the%20original%20experiment%20Participants%20used%20a%20chin%20rest%20to%20keep%20their%20head%20position%20static%20during%20an%20experiment%20Stimulus%20presentation%20was%20conducted%20with%20the%20Psychophysics%20Toolbox%20version%203012%20in%20MATLAB%20Release%202016a%20The%20MathWorks%20Inc%20Natick%20Massachusetts%20United%20States%20using%20a%2012core%20desktop%20computer%20AMD%20HD7970%20graphics%20card%20Tahiti%20by%20AMD%20Sunnyvale%20California%20United%20States%20running%20Kubuntu%201404%20LTS%20Participants%20clicked%20on%20a%20response%20screen%20showing%20an%20experiment%20instruction%20%20p%20%20%20age%20range%20mean%20age",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Observers%20were%20shown%20the%20224224%20pixels%20stimuli%20in%20a%20dark%20cabin%20on%20a%2022%20120%20Hz%20VIEWPixx%20LCD%20monitor%20VPixx%20Technologies%20SaintBruno%20Canada%20The%20screen%20of%20size%20484%20%20302%20mm%20corresponds%20to%201920%20%201200%20pixels%20although%20stimuli%20were%20only%20presented%20foveally%20at%20the%20center%20of%20the%20screen%203%20%203%20degrees%20of%20visual%20angle%20at%20a%20viewing%20distance%20of%20107%20cm%20while%20the%20background%20was%20set%20to%20a%20grey%20value%20of%2007614%20in%20the%200%201%20range%20the%20average%20greyscale%20value%20of%20all%20stimuli%20used%20in%20the%20original%20experiment%20Participants%20used%20a%20chin%20rest%20to%20keep%20their%20head%20position%20static%20during%20an%20experiment%20Stimulus%20presentation%20was%20conducted%20with%20the%20Psychophysics%20Toolbox%20version%203012%20in%20MATLAB%20Release%202016a%20The%20MathWorks%20Inc%20Natick%20Massachusetts%20United%20States%20using%20a%2012core%20desktop%20computer%20AMD%20HD7970%20graphics%20card%20Tahiti%20by%20AMD%20Sunnyvale%20California%20United%20States%20running%20Kubuntu%201404%20LTS%20Participants%20clicked%20on%20a%20response%20screen%20showing%20an%20experiment%20instruction%20%20p%20%20%20age%20range%20mean%20age"
        }
    ]
}
