{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
        "author": "Andrew Brock, Heriot-Watt University ajb,@hw.ac.uk",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=B1xsqj09Fm"
        },
        "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \u201ctruncation trick,\u201d allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator\u2019s input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128\u00d7128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65."
    },
    "keywords": [
        {
            "term": "image synthesis",
            "url": "https://en.wikipedia.org/wiki/image_synthesis"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        }
    ],
    "abbreviations": {
        "IS": "Inception Score",
        "FID": "Frechet Inception Distance",
        "GAN": "Generative Adversarial Network"
    },
    "highlights": [
        "The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al (2014)) at the forefront of efforts to generate highfidelity, diverse images with models learned directly from data",
        "We demonstrate that Generative Adversarial Network benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art",
        "We discover instabilities specific to large scale Generative Adversarial Network, and characterize them empirically",
        "We have demonstrated that Generative Adversarial Networks trained to model natural images of multiple categories highly benefit from scaling up, both in terms of fidelity and variety of the generated samples",
        "Our models set a new level of performance among ImageNet Generative Adversarial Network models, improving on the state of the art by a large margin",
        "We have presented an analysis of the training behavior of large scale Generative Adversarial Network, characterized their stability in terms of the singular values of their weights, and discussed the interplay between stability and performance"
    ],
    "key_statements": [
        "The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al (2014)) at the forefront of efforts to generate highfidelity, diverse images with models learned directly from data",
        "We set out to close the gap in fidelity and variety between images generated by Generative Adversarial Network and real-world images from the ImageNet dataset",
        "We demonstrate that Generative Adversarial Network benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art",
        "We discover instabilities specific to large scale Generative Adversarial Network, and characterize them empirically",
        "Leveraging insights from this analysis, we demonstrate that a combination of novel and existing techniques can reduce these instabilities, but complete training stability can only be achieved at a dramatic cost to performance",
        "We extend on these analyses to gain further insight into the pathology of Generative Adversarial Network training",
        "We find that current Generative Adversarial Network techniques are sufficient to enable scaling to large models and distributed, large-batch training",
        "Much previous work has investigated Generative Adversarial Network stability from a variety of analytical angles and on toy problems, but the instabilities we observe occur for settings which are stable at small scale, necessitating direct analysis at large scale",
        "To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of imposing additional conditioning on G to explicitly counteract spectral explosion",
        "We report Inception Score and Frechet Inception Distance in Table 2",
        "As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we report values at three settings, with complete curves in Appendix D",
        "We report the Frechet Inception Distance at the truncation setting for which our model\u2019s Inception Score is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of \u201cobjectness.\u201d Third, we report Frechet Inception Distance at the maximum Inception Score achieved by each model, to demonstrate how much variety must be traded off to maximize quality",
        "In addition to the BigGAN model introduced in the first version of the paper and used in the majority of experiments, we present a 4x deeper model (BigGAN-deep) which uses a different configuration of residual blocks",
        "We find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs than crowds",
        "We compare an ablated version of our model \u2013 comparable to SA-Generative Adversarial Network (<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a>) but with the larger batch size \u2013 against a \u201cfull\u201d BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet",
        "Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels)",
        "We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet Generative Adversarial Network that additional capacity was not beneficial",
        "In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset",
        "The improvement over the baseline Generative Adversarial Network model that we achieve on this dataset without changes to the underlying models or training and regularization techniques demonstrates that our findings extend from ImageNet to datasets with scale and complexity far unprecedented for generative models of images",
        "We have demonstrated that Generative Adversarial Networks trained to model natural images of multiple categories highly benefit from scaling up, both in terms of fidelity and variety of the generated samples",
        "Our models set a new level of performance among ImageNet Generative Adversarial Network models, improving on the state of the art by a large margin",
        "We have presented an analysis of the training behavior of large scale Generative Adversarial Network, characterized their stability in terms of the singular values of their weights, and discussed the interplay between stability and performance"
    ],
    "summary": [
        "The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al (2014)) at the forefront of efforts to generate highfidelity, diverse images with models learned directly from data.",
        "Our modifications substantially improve class-conditional GANs. When trained on ImageNet at 128\u00d7128 resolution, our models (BigGANs) improve the state-of-the-art Inception Score (IS) and Frechet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively.",
        "Much previous work has investigated GAN stability from a variety of analytical angles and on toy problems, but the instabilities we observe occur for settings which are stable at small scale, necessitating direct analysis at large scale.",
        "As a simple test for D\u2019s memorization), we evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what percentage of samples are classified as real or generated.",
        "As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we report values at three settings, with complete curves in Appendix D.",
        "We report the FID at the truncation setting for which our model\u2019s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of \u201cobjectness.\u201d Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality.",
        "We compare an ablated version of our model \u2013 comparable to SA-GAN (<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a>) but with the larger batch size \u2013 against a \u201cfull\u201d BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet. Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).",
        "The improvement over the baseline GAN model that we achieve on this dataset without changes to the underlying models or training and regularization techniques demonstrates that our findings extend from ImageNet to datasets with scale and complexity far unprecedented for generative models of images.",
        "We have demonstrated that Generative Adversarial Networks trained to model natural images of multiple categories highly benefit from scaling up, both in terms of fidelity and variety of the generated samples.",
        "Our models set a new level of performance among ImageNet GAN models, improving on the state of the art by a large margin.",
        "We have presented an analysis of the training behavior of large scale GANs, characterized their stability in terms of the singular values of their weights, and discussed the interplay between stability and performance."
    ],
    "headline": "We find that applying orthogonal regularization to the generator renders it amenable to a simple \u201ctruncation trick,\u201d allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator\u2019s input",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale machine learning. In OSDI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C4%B1n%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Arjovsky_et+al_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Chintala%2C%20Soumith%20Bottou%2C%20Leon%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Barratt_2018_a",
            "entry": "Shane Barratt and Rishi Sharma. A note on the Inception Score. In arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "Bellemare_et+al_2017_a",
            "entry": "Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein gradients. In arXiv preprint arXiv:1705.10743, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10743"
        },
        {
            "id": "Binkowski_et+al_2018_a",
            "entry": "Mikolaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolaj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolaj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018"
        },
        {
            "id": "Brock_et+al_2017_a",
            "entry": "Andrew Brock, Theodore Lim, J.M. Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Theodore%20Lim%2C%20J.M.Ritchie%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Theodore%20Lim%2C%20J.M.Ritchie%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "De_et+al_2017_a",
            "entry": "Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron Courville. Modulating early visual processing by language. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Vries%2C%20Harm%20Strub%2C%20Florian%20Mary%2C%20Jeremie%20Larochelle%2C%20Hugo%20Modulating%20early%20visual%20processing%20by%20language%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Vries%2C%20Harm%20Strub%2C%20Florian%20Mary%2C%20Jeremie%20Larochelle%2C%20Hugo%20Modulating%20early%20visual%20processing%20by%20language%202017"
        },
        {
            "id": "Denton_et+al_2015_a",
            "entry": "Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20Emily%20Chintala%2C%20Soumith%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20Emily%20Chintala%2C%20Soumith%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015"
        },
        {
            "id": "Dumoulin_et+al_2017_a",
            "entry": "Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017"
        },
        {
            "id": "Fedus_et+al_2018_a",
            "entry": "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Golub_2000_a",
            "entry": "Gene Golub and Henk Van der Vorst. Eigenvalue computation in the 20th century. Journal of Computational and Applied Mathematics, 123:35\u201365, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20Gene%20der%20Vorst%2C%20Henk%20Van%20Eigenvalue%20computation%20in%20the%2020th%20century%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golub%2C%20Gene%20der%20Vorst%2C%20Henk%20Van%20Eigenvalue%20computation%20in%20the%2020th%20century%202000"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, and Aaron Courville Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Sherjil%20Ozair%2C%20and%20Aaron%20Courville%20Yoshua%20Bengio.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Sherjil%20Ozair%2C%20and%20Aaron%20Courville%20Yoshua%20Bengio.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Google_2018_a",
            "entry": "Google. Cloud TPUs. https://cloud.google.com/tpu/, 2018.",
            "url": "https://cloud.google.com/tpu/"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Mart\u0131n Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of Wasserstein GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Mart%C4%B1n%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Mart%C4%B1n%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Karras_et+al_2018_a",
            "entry": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Kodali_et+al_2017_a",
            "entry": "Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of GANs. In arXiv preprint arXiv:1705.07215, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07215"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lim_2017_a",
            "entry": "Jae Hyun Lim and Jong Chul Ye. Geometric GAN. In arXiv preprint arXiv:1705.02894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02894"
        },
        {
            "id": "Mao_et+al_2016_a",
            "entry": "Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang. Least squares generative adversarial networks. In arXiv preprint arXiv:1611.04076, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04076"
        },
        {
            "id": "Marchesi_2016_a",
            "entry": "Marco Marchesi. Megapixel size image creation using generative adversarial networks. In arXiv preprint arXiv:1706.00082, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00082"
        },
        {
            "id": "Mescheder_et+al_2018_a",
            "entry": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Geiger%2C%20Andreas%20Nowozin%2C%20Sebastian%20Which%20training%20methods%20for%20GANs%20do%20actually%20converge%3F%202018"
        },
        {
            "id": "Mirza_2014_a",
            "entry": "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. In arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "Miyato_2018_a",
            "entry": "Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cGANs%20with%20projection%20discriminator%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Koyama%2C%20Masanori%20cGANs%20with%20projection%20discriminator%202018"
        },
        {
            "id": "Miyato_et+al_2018_b",
            "entry": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "Nowozin_et+al_2016_a",
            "entry": "Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "Odena_et+al_2016_a",
            "entry": "Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016"
        },
        {
            "id": "Odena_et+al_2017_a",
            "entry": "Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "Odena_et+al_2018_a",
            "entry": "Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raffel, and Ian Goodfellow. Is generator conditioning causally related to GAN performance? In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Buckman%2C%20Jacob%20Olsson%2C%20Catherine%20Brown%2C%20Tom%20B.%20Is%20generator%20conditioning%20causally%20related%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Buckman%2C%20Jacob%20Olsson%2C%20Catherine%20Brown%2C%20Tom%20B.%20Is%20generator%20conditioning%20causally%20related%202018"
        },
        {
            "id": "Perez_et+al_2018_a",
            "entry": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual reasoning with a general conditioning layer. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perez%2C%20Ethan%20Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Dumoulin%2C%20Vincent%20FiLM%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perez%2C%20Ethan%20Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Dumoulin%2C%20Vincent%20FiLM%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018"
        },
        {
            "id": "Pieters_2014_a",
            "entry": "Mathijs Pieters and Marco Wiering. Comparing generative adversarial network techniques for image creation and modificatio. In arXiv preprint arXiv:1803.09093, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09093"
        },
        {
            "id": "Radford_et+al_2016_a",
            "entry": "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein. ImageNet large scale visual recognition challenge. IJCV, 115:211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Tim Salimans and Diederik Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "Salimans_et+al_2016_b",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "Salimans_et+al_2018_a",
            "entry": "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal transport. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Zhang%2C%20Han%20Radford%2C%20Alec%20Metaxas%2C%20Dimitris%20Improving%20GANs%20using%20optimal%20transport%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Zhang%2C%20Han%20Radford%2C%20Alec%20Metaxas%2C%20Dimitris%20Improving%20GANs%20using%20optimal%20transport%202018"
        },
        {
            "id": "Saxe_et+al_2014_a",
            "entry": "Andrew Saxe, James McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20Andrew%20McClelland%2C%20James%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20Andrew%20McClelland%2C%20James%20Ganguli%2C%20Surya%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "S_et+al_2017_a",
            "entry": "Casper Kaae S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszr. Amortised map inference for image super-resolution. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%B8nderby%2C%20Casper%20Kaae%20Caballero%2C%20Jose%20Theis%2C%20Lucas%20Shi%2C%20Wenzhe%20Amortised%20map%20inference%20for%20image%20super-resolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%B8nderby%2C%20Casper%20Kaae%20Caballero%2C%20Jose%20Theis%2C%20Lucas%20Shi%2C%20Wenzhe%20Amortised%20map%20inference%20for%20image%20super-resolution%202017"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Sun_et+al_2017_a",
            "entry": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Chen%20Shrivastava%2C%20Abhinav%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Revisiting%20unreasonable%20effectiveness%20of%20data%20in%20deep%20learning%20era%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Chen%20Shrivastava%2C%20Abhinav%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Revisiting%20unreasonable%20effectiveness%20of%20data%20in%20deep%20learning%20era%202017"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jonathon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jonathon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Theis_et+al_2015_a",
            "entry": "Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. In arXiv preprint arXiv:1511.01844, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01844"
        },
        {
            "id": "Tran_et+al_2017_a",
            "entry": "Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free variational inference. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Blei%2C%20David%20M.%20Hierarchical%20implicit%20models%20and%20likelihood-free%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Blei%2C%20David%20M.%20Hierarchical%20implicit%20models%20and%20likelihood-free%20variational%20inference%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiaolong%20Wang%20Ross%20B%20Girshick%20Abhinav%20Gupta%20and%20Kaiming%20He%20Nonlocal%20neural%20networks%20In%20CVPR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiaolong%20Wang%20Ross%20B%20Girshick%20Abhinav%20Gupta%20and%20Kaiming%20He%20Nonlocal%20neural%20networks%20In%20CVPR%202018"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis of decoder-based generative models. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20B.%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20B.%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        },
        {
            "id": "Yazc_et+al_2018_a",
            "entry": "Yasin Yazc, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chandrasekhar. The unusual effectiveness of averaging in gan training. In arXiv preprint arXiv:1806.04498, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.04498"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        }
    ]
}
