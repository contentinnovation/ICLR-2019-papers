{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "ON RANDOM DEEP WEIGHT-TIED AUTOENCODERS: EXACT ASYMPTOTIC ANALYSIS, PHASE TRANSITIONS, AND IMPLICATIONS TO TRAINING",
        "author": "Ping Li, Phan-Minh Nguyen",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=HJx54i05tX"
        },
        "abstract": "We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs \u201capproximate inference\u201d as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization. Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability."
    },
    "keywords": [
        {
            "term": "dynamics",
            "url": "https://en.wikipedia.org/wiki/dynamics"
        },
        {
            "term": "isometry",
            "url": "https://en.wikipedia.org/wiki/isometry"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "edge of chaos",
            "url": "https://en.wikipedia.org/wiki/edge_of_chaos"
        }
    ],
    "abbreviations": {
        "EOC": "edge of chaos"
    },
    "highlights": [
        "The autoencoder is a cornerstone in machine learning, first as a response to the unsupervised learning problem (<a class=\"ref-link\" id=\"cRumelhart_1985_a\" href=\"#rRumelhart_1985_a\"><a class=\"ref-link\" id=\"cRumelhart_1985_a\" href=\"#rRumelhart_1985_a\">Rumelhart & Zipser (1985</a></a>)), with applications to dimensionality reduction (<a class=\"ref-link\" id=\"cHinton_2006_a\" href=\"#rHinton_2006_a\"><a class=\"ref-link\" id=\"cHinton_2006_a\" href=\"#rHinton_2006_a\">Hinton & Salakhutdinov (2006</a></a>)), unsupervised pre-training (<a class=\"ref-link\" id=\"cErhan_et+al_2010_a\" href=\"#rErhan_et+al_2010_a\"><a class=\"ref-link\" id=\"cErhan_et+al_2010_a\" href=\"#rErhan_et+al_2010_a\">Erhan et al (2010</a></a>)), and as a precursor to many modern generative models (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2016_a\" href=\"#rGoodfellow_et+al_2016_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2016_a\" href=\"#rGoodfellow_et+al_2016_a\">Goodfellow et al (2016</a></a>))",
        "The results are in good agreement with our hypothesis. (Recall we test the hypothesis separately for each pair \u03c6 and \u03c3, for which the involved schemes share the same architecture and only differ in the initialization.) Note that as predicted, in Setting 2, Scheme 3 and 6 are trapped with numerical errors, and in Setting 1, they saturate quickly at a high loss",
        "We found from our experiments that these two losses perform comparably, with the normalized loss typically yielding slight improvements, provided that the learning rates are scaled appropriately",
        "This paper has shown quantitative answers to the three questions posed in Section 1",
        "This paper has shown quantitative answers to the three questions posed in Section 1. This feat is enabled by an exact analysis via Theorem 1",
        "The theorem is stated in a general setting, allowing varying activations, weight variances, etc, but our analyses in Section 3 have made several simplifications"
    ],
    "key_statements": [
        "The autoencoder is a cornerstone in machine learning, first as a response to the unsupervised learning problem (<a class=\"ref-link\" id=\"cRumelhart_1985_a\" href=\"#rRumelhart_1985_a\">Rumelhart & Zipser (1985</a>)), with applications to dimensionality reduction (<a class=\"ref-link\" id=\"cHinton_2006_a\" href=\"#rHinton_2006_a\">Hinton & Salakhutdinov (2006</a>)), unsupervised pre-training (<a class=\"ref-link\" id=\"cErhan_et+al_2010_a\" href=\"#rErhan_et+al_2010_a\">Erhan et al (2010</a>)), and as a precursor to many modern generative models (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2016_a\" href=\"#rGoodfellow_et+al_2016_a\">Goodfellow et al (2016</a>))",
        "Its reconstruction power is well utilized in applications such as anomaly detection (<a class=\"ref-link\" id=\"cChandola_et+al_2009_a\" href=\"#rChandola_et+al_2009_a\">Chandola et al (2009</a>)) and image recovery (<a class=\"ref-link\" id=\"cMousavi_et+al_2015_a\" href=\"#rMousavi_et+al_2015_a\">Mousavi et al (2015</a>))",
        "One enjoys much analytical tractability from the randomness assumption, whereas weight tying enforces the random autoencoder to perform \u201cautoencoding\u201d. We study this in the high-dimensional setting, where all dimensions are comparably large and ideally jointly approaching infinity",
        "We show that the deep autoencoder exhibits a higher order of sensitivity to perturbations of the parameters (Section 3.4)",
        "We introduce some convenient quantities inductively: x0 = x, x = W \u03c3 \u22121 (x \u22121) + b , = 1, ..., L, xL = W L \u03c6L + vL, x = W \u03c6 (x +1) + v , = L \u2212 1, ..., 1",
        "Taking a parallel setting with Section 3.1, by Theorem 1, x \u223c= Ssigx + Svarz, Ssig = \u03b2\u03b3, Svar = \u03b2\u03c1, in which, with \u03c6hid being the activation in the hidden layer, 1 \u03b3 = \u03c42 E {\u03c4z\u03c6hid} , \u03c1 = E \u03c6hid2",
        "\u0392SRIns,eiLgac/noUSdm,vphaaaesrrn=iocsbeo\u03a9sneSrswvi\u221ageitd\u03b2t.rhoIe=mn ipn\u03a9Fafiireu3l6,ad,rei,Snpnvtcehaear rrcene,=prfehoga\u03a9risme\u03c6\u221aetrs=a\u03b2,n\u03b3s,\u03c3,itai\u03c1on=ndanottdhfae\u03b3n\u03b3sh,/iS\u221agosn\u03c1riagl/\u03c6cScaonv=margprt=ooawnn\u03a9ehnaglwwlitiinhtthhge that \u03b2 = \u03b1\u03c3W 2 , this implies for the infinite depth case, as compared to the shallow one, firstly a slight perturbation in \u03c3W 2 may result in a larger perturbation in the signal\u2019s strength, and secondly an architecture using larger \u03b1 may gain more in terms of amplification of the signals",
        "At the expense of much care in the selection of parameters, since there are continuous regimes in which the infinite depth diminishes Ssig and Svar to zero or boost them to infinity, a situation that never occurs in the shallow case",
        "We examine the implications of Interpretation 1 in Section 3.1 to trainability of the weight-tied autoencoder",
        "Setting 1: The output activation \u03c60 is tanh, MNIST images are normalized to [\u22121, +1], and the learning rate is fixed at 5 \u00d7 10\u22123",
        "The results are in good agreement with our hypothesis. (Recall we test the hypothesis separately for each pair \u03c6 and \u03c3, for which the involved schemes share the same architecture and only differ in the initialization.) Note that as predicted, in Setting 2, Scheme 3 and 6 are trapped with numerical errors, and in Setting 1, they saturate quickly at a high loss",
        "We found from our experiments that these two losses perform comparably, with the normalized loss typically yielding slight improvements, provided that the learning rates are scaled appropriately",
        "We observe that in Setting 2, the tanh network under Scheme 7 is best performing in terms of the reconstruction loss, and its progress does not seem to reach a plateau after 5 \u00d7 105 iterations",
        "Overall we see that our experiments confirm the hypothesis, showing an intimate connection between the phase transition behaviors found by our theory and trainability of the autoencoders",
        "This paper has shown quantitative answers to the three questions posed in Section 1",
        "This paper has shown quantitative answers to the three questions posed in Section 1. This feat is enabled by an exact analysis via Theorem 1",
        "The theorem is stated in a general setting, allowing varying activations, weight variances, etc, but our analyses in Section 3 have made several simplifications",
        "The theorem is stated in a general setting, allowing varying activations, weight variances, etc, but our analyses in Section 3 have made several simplifications. This leaves a question of whether these simplifications can be relaxed, and how the picture changes for instance, when the parameters vary across layers, similar to <a class=\"ref-link\" id=\"cYang_2018_a\" href=\"#rYang_2018_a\">Yang & Schoenholz (2018</a>)",
        "What would be the covariance structure between the outputs of two distinct inputs? How does the network\u2019s Jacobian matrix look like? These questions have been answered in the feedforward case (<a class=\"ref-link\" id=\"cPoole_et+al_2016_a\" href=\"#rPoole_et+al_2016_a\">Poole et al (2016</a>); Pennington et al (2017)), but we believe answering them is more technically involved in our case"
    ],
    "summary": [
        "The autoencoder is a cornerstone in machine learning, first as a response to the unsupervised learning problem (<a class=\"ref-link\" id=\"cRumelhart_1985_a\" href=\"#rRumelhart_1985_a\"><a class=\"ref-link\" id=\"cRumelhart_1985_a\" href=\"#rRumelhart_1985_a\">Rumelhart & Zipser (1985</a></a>)), with applications to dimensionality reduction (<a class=\"ref-link\" id=\"cHinton_2006_a\" href=\"#rHinton_2006_a\"><a class=\"ref-link\" id=\"cHinton_2006_a\" href=\"#rHinton_2006_a\">Hinton & Salakhutdinov (2006</a></a>)), unsupervised pre-training (<a class=\"ref-link\" id=\"cErhan_et+al_2010_a\" href=\"#rErhan_et+al_2010_a\"><a class=\"ref-link\" id=\"cErhan_et+al_2010_a\" href=\"#rErhan_et+al_2010_a\">Erhan et al (2010</a></a>)), and as a precursor to many modern generative models (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2016_a\" href=\"#rGoodfellow_et+al_2016_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2016_a\" href=\"#rGoodfellow_et+al_2016_a\">Goodfellow et al (2016</a></a>)).",
        "Theorem 1 gives a quantitatively exact sense of how the random weight-tied autoencoder performs \u201capproximate inference\u201d.",
        "Our analysis mitigates the shortcoming of the in-expectation approach, and gives a more precise understanding of what the random weight-tied autoencoder can and cannot achieve when the depth becomes large.",
        "Taking a parallel setting with Section 3.1, by Theorem 1, x \u223c= Ssigx + Svarz, Ssig = \u03b2\u03b3, Svar = \u03b2\u03c1, in which, with \u03c6hid being the activation in the hidden layer, 1 \u03b3 = \u03c42 E {\u03c4z\u03c6hid} , \u03c1 = E \u03c6hid2 .",
        "\u0392SRIns,eiLgac/noUSdm,vphaaaesrrn=iocsbeo\u03a9sneSrswvi\u221ageitd\u03b2t.rhoIe=mn ipn\u03a9Fafiireu3l6,ad,rei,Snpnvtcehaear rrcene,=prfehoga\u03a9risme\u03c6\u221aetrs=a\u03b2,n\u03b3s,\u03c3,itai\u03c1on=ndanottdhfae\u03b3n\u03b3sh,/iS\u221agosn\u03c1riagl/\u03c6cScaonv=margprt=ooawnn\u03a9ehnaglwwlitiinhtthhge that \u03b2 = \u03b1\u03c3W 2 , this implies for the infinite depth case, as compared to the shallow one, firstly a slight perturbation in \u03c3W 2 may result in a larger perturbation in the signal\u2019s strength, and secondly an architecture using larger \u03b1 may gain more in terms of amplification of the signals.",
        "At the expense of much care in the selection of parameters, since there are continuous regimes in which the infinite depth diminishes Ssig and Svar to zero or boost them to infinity, a situation that never occurs in the shallow case.",
        "Since the majority of intermediate layers can be described approximately by \u03b3 and \u03c1 and the random weight-tied autoencoder is one at initialization, appropriate values of \u03b3, \u03c1 and \u03c4 should lead to better trainability.",
        "We perform simple experiments on a weight-tied vanilla autoencoder as described in Section 3.1: L = 100, all hidden dimensions of 400, identity input activation \u03c30, and decoder biases initialized to zero.",
        "Setting 1: The output activation \u03c60 is tanh, MNIST images are normalized to [\u22121, +1], and the learning rate is fixed at 5 \u00d7 10\u22123.",
        "We observe that in Setting 2, the tanh network under Scheme 7 is best performing in terms of the reconstruction loss, and its progress does not seem to reach a plateau after 5 \u00d7 105 iterations.",
        "This extends the conclusion in Pennington et al (2017) to the context of weight-tied autoencoders: reasonable training at a large depth is possible even for the notoriously difficult tanh activation, and this necessarily requires careful initializations.",
        "Overall we see that our experiments confirm the hypothesis, showing an intimate connection between the phase transition behaviors found by our theory and trainability of the autoencoders.",
        "The theorem is stated in a general setting, allowing varying activations, weight variances, etc, but our analyses in Section 3 have made several simplifications."
    ],
    "headline": "We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights",
    "reference_links": [
        {
            "id": "Alain_2014_a",
            "entry": "Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. Journal of Machine Learning Research, 15(1):3563\u20133593, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alain%2C%20Guillaume%20Bengio%2C%20Yoshua%20What%20regularized%20auto-encoders%20learn%20from%20the%20data-generating%20distribution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alain%2C%20Guillaume%20Bengio%2C%20Yoshua%20What%20regularized%20auto-encoders%20learn%20from%20the%20data-generating%20distribution%202014"
        },
        {
            "id": "Amari_1972_a",
            "entry": "Shun-Ichi Amari. Characteristics of random nets of analog neuron-like elements. IEEE Transactions on systems, man, and cybernetics, (5):643\u2013657, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-Ichi%20Characteristics%20of%20random%20nets%20of%20analog%20neuron-like%20elements%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-Ichi%20Characteristics%20of%20random%20nets%20of%20analog%20neuron-like%20elements%201972"
        },
        {
            "id": "Amari_et+al_2018_a",
            "entry": "Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi. Statistical neurodynamics of deep networks: Geometry of signal spaces. arXiv preprint arXiv:1808.07169, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07169"
        },
        {
            "id": "Arora_et+al_2014_a",
            "entry": "Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584\u2013592, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Bhaskara%2C%20Aditya%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Provable%20bounds%20for%20learning%20some%20deep%20representations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Bhaskara%2C%20Aditya%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Provable%20bounds%20for%20learning%20some%20deep%20representations%202014"
        },
        {
            "id": "Arora_2015_a",
            "entry": "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with implications for training. arXiv preprint arXiv:1511.05653, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05653"
        },
        {
            "id": "Arpit_et+al_2015_a",
            "entry": "Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders learn sparse representation? arXiv preprint arXiv:1505.05561, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05561"
        },
        {
            "id": "Baity-Jesi_et+al_2018_a",
            "entry": "Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06969"
        },
        {
            "id": "Autoencoders_2012_a",
            "entry": "Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 37\u201349, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Autoencoders%2C%20Pierre%20Baldi%20unsupervised%20learning%2C%20and%20deep%20architectures%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Autoencoders%2C%20Pierre%20Baldi%20unsupervised%20learning%2C%20and%20deep%20architectures%202012"
        },
        {
            "id": "Bayati_2011_a",
            "entry": "Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899\u2013907, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Yao%2C%20Li%20Alain%2C%20Guillaume%20Vincent%2C%20Pascal%20Generalized%20denoising%20auto-encoders%20as%20generative%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Yao%2C%20Li%20Alain%2C%20Guillaume%20Vincent%2C%20Pascal%20Generalized%20denoising%20auto-encoders%20as%20generative%20models%202013"
        },
        {
            "id": "Berthier_et+al_2017_a",
            "entry": "Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen. State evolution for approximate message passing with non-separable functions. arXiv preprint arXiv:1708.03950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03950"
        },
        {
            "id": "Bolthausen_2014_a",
            "entry": "Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington\u2013 Kirkpatrick model. Communications in Mathematical Physics, 325(1):333\u2013366, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bolthausen%2C%20Erwin%20An%20iterative%20construction%20of%20solutions%20of%20the%20TAP%20equations%20for%20the%20Sherrington%E2%80%93%20Kirkpatrick%20model%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bolthausen%2C%20Erwin%20An%20iterative%20construction%20of%20solutions%20of%20the%20TAP%20equations%20for%20the%20Sherrington%E2%80%93%20Kirkpatrick%20model%202014"
        },
        {
            "id": "Buehlmann_2011_a",
            "entry": "Peter B\u00fchlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science &amp; Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BChlmann%2C%20Peter%20Geer%2C%20Sara%20Van%20De%20Statistics%20for%20high-dimensional%20data%3A%20methods%2C%20theory%20and%20applications%202011"
        },
        {
            "id": "Burkholz_2018_a",
            "entry": "Rebekka Burkholz and Alina Dubatovka. Exact information propagation through fully-connected feed forward neural networks. arXiv preprint arXiv:1806.06362, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06362"
        },
        {
            "id": "Cessac_1995_a",
            "entry": "Bruno Cessac. Increase in complexity in random neural networks. Journal de Physique I, 5(3): 409\u2013432, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cessac%2C%20Bruno%20Increase%20in%20complexity%20in%20random%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cessac%2C%20Bruno%20Increase%20in%20complexity%20in%20random%20neural%20networks%201995"
        },
        {
            "id": "Cessac_et+al_1994_a",
            "entry": "Bruno Cessac, Bernard Doyon, Mathias Quoy, and Manuel Samuelides. Mean-field equations, bifurcation map and route to chaos in discrete time neural networks. Physica D: Nonlinear Phenomena, 74(1-2):24\u201344, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cessac%2C%20Bruno%20Doyon%2C%20Bernard%20Quoy%2C%20Mathias%20Samuelides%2C%20Manuel%20Mean-field%20equations%2C%20bifurcation%20map%20and%20route%20to%20chaos%20in%20discrete%20time%20neural%20networks%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cessac%2C%20Bruno%20Doyon%2C%20Bernard%20Quoy%2C%20Mathias%20Samuelides%2C%20Manuel%20Mean-field%20equations%2C%20bifurcation%20map%20and%20route%20to%20chaos%20in%20discrete%20time%20neural%20networks%201994"
        },
        {
            "id": "Chandola_et+al_2009_a",
            "entry": "Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):15, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandola%2C%20Varun%20Banerjee%2C%20Arindam%20Kumar%2C%20Vipin%20Anomaly%20detection%3A%20A%20survey%202009"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Minmin Chen, Jeffrey Pennington, and Samuel S Schoenholz. Dynamical isometry and a mean field theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv preprint arXiv:1806.05394, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05394"
        },
        {
            "id": "Chizat_2018_a",
            "entry": "Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09545"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "Erhan_et+al_2010_a",
            "entry": "Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625\u2013660, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Manzagol%2C%20Pierre-Antoine%20Why%20does%20unsupervised%20pre-training%20help%20deep%20learning%3F%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Manzagol%2C%20Pierre-Antoine%20Why%20does%20unsupervised%20pre-training%20help%20deep%20learning%3F%202010"
        },
        {
            "id": "Fletcher_et+al_2018_a",
            "entry": "Alyson K Fletcher, Sundeep Rangan, and Philip Schniter. Inference in deep networks in high dimensions. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1884\u20131888. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fletcher%2C%20Alyson%20K.%20Rangan%2C%20Sundeep%20Schniter%2C%20Philip%20Inference%20in%20deep%20networks%20in%20high%20dimensions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fletcher%2C%20Alyson%20K.%20Rangan%2C%20Sundeep%20Schniter%2C%20Philip%20Inference%20in%20deep%20networks%20in%20high%20dimensions%202018"
        },
        {
            "id": "Gabri_et+al_2018_a",
            "entry": "Marylou Gabri\u00e9, Andre Manoel, Cl\u00e9ment Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, and Lenka Zdeborov\u00e1. Entropy and mutual information in models of deep neural networks. arXiv preprint arXiv:1805.09785, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09785"
        },
        {
            "id": "Gilbert_et+al_2017_a",
            "entry": "Anna C Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding the invertibility of convolutional neural networks. arXiv preprint arXiv:1705.08664, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08664"
        },
        {
            "id": "Giryes_et+al_2016_a",
            "entry": "Raja Giryes, Guillermo Sapiro, and Alexander M Bronstein. Deep neural networks with random gaussian weights: a universal classification strategy? IEEE Trans. Signal Processing, 64(13): 3444\u20133457, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Bronstein%2C%20Alexander%20M.%20Deep%20neural%20networks%20with%20random%20gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Bronstein%2C%20Alexander%20M.%20Deep%20neural%20networks%20with%20random%20gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%2C%20volume%201%202016"
        },
        {
            "id": "Hanin_2018_a",
            "entry": "Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? arXiv preprint arXiv:1801.03744, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03744"
        },
        {
            "id": "Hanin_2018_b",
            "entry": "Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. arXiv preprint arXiv:1803.01719, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01719"
        },
        {
            "id": "Hayou_et+al_2018_a",
            "entry": "Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activation function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08266"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "Hinton_2006_a",
            "entry": "Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "Javanmard_2013_a",
            "entry": "Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115\u2013144, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Javanmard%2C%20Adel%20Montanari%2C%20Andrea%20State%20evolution%20for%20general%20approximate%20message%20passing%20algorithms%2C%20with%20applications%20to%20spatial%20coupling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Javanmard%2C%20Adel%20Montanari%2C%20Andrea%20State%20evolution%20for%20general%20approximate%20message%20passing%20algorithms%2C%20with%20applications%20to%20spatial%20coupling%202013"
        },
        {
            "id": "Roux_2008_a",
            "entry": "Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and deep belief networks. Neural computation, 20(6):1631\u20131649, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20Le%20Bengio%2C%20Yoshua%20Representational%20power%20of%20restricted%20boltzmann%20machines%20and%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20Le%20Bengio%2C%20Yoshua%20Representational%20power%20of%20restricted%20boltzmann%20machines%20and%20deep%20belief%20networks%202008"
        },
        {
            "id": "Li_2018_a",
            "entry": "Bo Li and David Saad. Exploring the function space of deep-learning machines. Physical Review Letters, 120(24):248301, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Bo%20Saad%2C%20David%20Exploring%20the%20function%20space%20of%20deep-learning%20machines%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Bo%20Saad%2C%20David%20Exploring%20the%20function%20space%20of%20deep-learning%20machines%202018"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7: 13276, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016"
        },
        {
            "id": "Louart_et+al_2018_a",
            "entry": "Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks. Ann. Appl. Probab., 28(2):1190\u20131248, 04 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louart%2C%20Cosme%20Liao%2C%20Zhenyu%20Couillet%2C%20Romain%20A%20random%20matrix%20approach%20to%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louart%2C%20Cosme%20Liao%2C%20Zhenyu%20Couillet%2C%20Romain%20A%20random%20matrix%20approach%20to%20neural%20networks%202018"
        },
        {
            "id": "Manoel_et+al_2017_a",
            "entry": "Andre Manoel, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. Multi-layer generalized linear estimation. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 2098\u20132102. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manoel%2C%20Andre%20Krzakala%2C%20Florent%20M%C3%A9zard%2C%20Marc%20Zdeborov%C3%A1%2C%20Lenka%20Multi-layer%20generalized%20linear%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manoel%2C%20Andre%20Krzakala%2C%20Florent%20M%C3%A9zard%2C%20Marc%20Zdeborov%C3%A1%2C%20Lenka%20Multi-layer%20generalized%20linear%20estimation%202017"
        },
        {
            "id": "Mei_et+al_2018_a",
            "entry": "Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 07 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018"
        },
        {
            "id": "Montufar_2011_a",
            "entry": "Guido Montufar and Nihat Ay. Refinements of universal approximation results for deep belief networks and restricted boltzmann machines. Neural Computation, 23(5):1306\u20131319, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20Guido%20Ay%2C%20Nihat%20Refinements%20of%20universal%20approximation%20results%20for%20deep%20belief%20networks%20and%20restricted%20boltzmann%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20Guido%20Ay%2C%20Nihat%20Refinements%20of%20universal%20approximation%20results%20for%20deep%20belief%20networks%20and%20restricted%20boltzmann%20machines%202011"
        },
        {
            "id": "Mousavi_et+al_2015_a",
            "entry": "Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal recovery. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1336\u20131343. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mousavi%2C%20Ali%20Patel%2C%20Ankit%20B.%20Baraniuk%2C%20Richard%20G.%20A%20deep%20learning%20approach%20to%20structured%20signal%20recovery%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mousavi%2C%20Ali%20Patel%2C%20Ankit%20B.%20Baraniuk%2C%20Richard%20G.%20A%20deep%20learning%20approach%20to%20structured%20signal%20recovery%202015"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Thanh V Nguyen, Raymond KW Wong, and Chinmay Hegde. Autoencoders learn generative linear models. arXiv preprint arXiv:1806.00572, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00572"
        },
        {
            "id": "Pennington_2017_a",
            "entry": "Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances in Neural Information Processing Systems, pp. 2637\u20132646, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Worah%2C%20Pratik%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Worah%2C%20Pratik%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017"
        },
        {
            "id": "Pennington_et+al_2017_b",
            "entry": "Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pp. 4785\u20134795, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pp. 3360\u20133368, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016"
        },
        {
            "id": "Rangamani_et+al_2017_a",
            "entry": "Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu, Sang Chin, and Trac D Tran. Sparse coding and autoencoders. arXiv preprint arXiv:1708.03735, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03735"
        },
        {
            "id": "Reeves_2017_a",
            "entry": "Galen Reeves. Additivity of information in multilayer networks via additive gaussian noise transforms. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1064\u20131070. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reeves%2C%20Galen%20Additivity%20of%20information%20in%20multilayer%20networks%20via%20additive%20gaussian%20noise%20transforms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reeves%2C%20Galen%20Additivity%20of%20information%20in%20multilayer%20networks%20via%20additive%20gaussian%20noise%20transforms%202017"
        },
        {
            "id": "Grant_2018_a",
            "entry": "Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. arXiv preprint arXiv:1805.00915, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00915"
        },
        {
            "id": "Rumelhart_1985_a",
            "entry": "David E Rumelhart and David Zipser. Feature discovery by competitive learning. Cognitive science, 9(1):75\u2013112, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20David%20E.%20Zipser%2C%20David%20Feature%20discovery%20by%20competitive%20learning%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20David%20E.%20Zipser%2C%20David%20Feature%20discovery%20by%20competitive%20learning%201985"
        },
        {
            "id": "Scellier_et+al_2018_a",
            "entry": "Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Extending the framework of equilibrium propagation to general dynamics. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scellier%2C%20Benjamin%20Goyal%2C%20Anirudh%20Binas%2C%20Jonathan%20Mesnard%2C%20Thomas%20Extending%20the%20framework%20of%20equilibrium%20propagation%20to%20general%20dynamics%202018"
        },
        {
            "id": "Schoenholz_et+al_2016_a",
            "entry": "Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "Sirignano_2018_a",
            "entry": "Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv preprint arXiv:1805.01053, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01053"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20Bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "Sompolinsky_et+al_1988_a",
            "entry": "Haim Sompolinsky, Andrea Crisanti, and Hans-Jurgen Sommers. Chaos in random neural networks. Physical review letters, 61(3):259, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sompolinsky%2C%20Haim%20Crisanti%2C%20Andrea%20Sommers%2C%20Hans-Jurgen%20Chaos%20in%20random%20neural%20networks%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sompolinsky%2C%20Haim%20Crisanti%2C%20Andrea%20Sommers%2C%20Hans-Jurgen%20Chaos%20in%20random%20neural%20networks%201988"
        },
        {
            "id": "Sutskever_2008_a",
            "entry": "Ilya Sutskever and Geoffrey E Hinton. Deep, narrow sigmoid belief networks are universal approximators. Neural computation, 20(11):2629\u20132636, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Deep%2C%20narrow%20sigmoid%20belief%20networks%20are%20universal%20approximators%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Deep%2C%20narrow%20sigmoid%20belief%20networks%20are%20universal%20approximators%202008"
        },
        {
            "id": "Vershynin_2012_a",
            "entry": "Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, pp. 210\u2013268. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20Introduction%20to%20the%20non-asymptotic%20analysis%20of%20random%20matrices%202012"
        },
        {
            "id": "Vincent_et+al_2010_a",
            "entry": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "Xiao_et+al_2018_a",
            "entry": "Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05393"
        },
        {
            "id": "Yang_2018_a",
            "entry": "Greg Yang and Sam S Schoenholz. Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Greg%20Schoenholz%2C%20Sam%20S.%20Deep%20mean%20field%20theory%3A%20Layerwise%20variance%20and%20width%20variation%20as%20methods%20to%20control%20gradient%20explosion%202018"
        },
        {
            "id": "Yang_2017_a",
            "entry": "Greg Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103\u20137114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Greg%20Schoenholz%2C%20Samuel%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Greg%20Schoenholz%2C%20Samuel%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%202017"
        }
    ]
}
