{
    "filename": "pdf.pdf",
    "metadata": {
        "title": "THE NEURO-SYMBOLIC CONCEPT LEARNER: INTERPRETING SCENES, WORDS, AND SENTENCES FROM NATURAL SUPERVISION",
        "author": "Jiayuan Mao MIT CSAIL and IIIS, Tsinghua University mjy,@mails.tsinghua.edu.cn",
        "date": 2019,
        "identifiers": {
            "url": "https://openreview.net/pdf?id=rJgMlhRctm"
        },
        "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "visual reasoning",
            "url": "https://en.wikipedia.org/wiki/visual_reasoning"
        },
        {
            "term": "semantic parsing",
            "url": "https://en.wikipedia.org/wiki/semantic_parsing"
        },
        {
            "term": "domain-specific language",
            "url": "https://en.wikipedia.org/wiki/domain-specific_language"
        },
        {
            "term": "semantics",
            "url": "https://en.wikipedia.org/wiki/semantics"
        }
    ],
    "abbreviations": {
        "NS-CL": "neuro-symbolic concept learner",
        "VQA": "Visual question answering",
        "DSL": "domain-specific language"
    },
    "highlights": [
        "Humans are capable of learning visual concepts by jointly understanding vision and language (<a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\"><a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\">Fazly et al, 2010</a></a>; Chrupa\u0142a et al, 2015; <a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\"><a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\">Gauthier et al, 2018</a></a>)",
        "We propose the neuro-symbolic concept learner (NS-CL), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs",
        "We present our neuro-symbolic concept learner, which uses a symbolic reasoning process to bridge the learning of visual concepts, words, and semantic parsing of sentences without explicit annotations",
        "We first use a visual perception module to construct an object-based representation for a scene, and run a semantic parsing module to translate a question into an executable program",
        "A separate semantic parser is trained for the Visual question answering baselines, which translates captions into a CLEVR QA-compatible program (e.g., Exist(Filter(Box, Relate(Right, Filter(Cylinder)))",
        "We presented a method that jointly learns visual concepts, words, and semantic parsing of sentences from natural supervision"
    ],
    "key_statements": [
        "Humans are capable of learning visual concepts by jointly understanding vision and language (<a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\"><a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\">Fazly et al, 2010</a></a>; Chrupa\u0142a et al, 2015; <a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\"><a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\">Gauthier et al, 2018</a></a>)",
        "Starting from there, humans are able to inductively learn the correspondence between visual concepts and word semantics, and unravel compositional logic from complex questions assisted by the learned visual concepts (Figure 1-III, see <a class=\"ref-link\" id=\"cAbend_et+al_2017_a\" href=\"#rAbend_et+al_2017_a\">Abend et al (2017</a>))",
        "We propose the neuro-symbolic concept learner (NS-CL), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs",
        "We present our neuro-symbolic concept learner, which uses a symbolic reasoning process to bridge the learning of visual concepts, words, and semantic parsing of sentences without explicit annotations",
        "We first use a visual perception module to construct an object-based representation for a scene, and run a semantic parsing module to translate a question into an executable program",
        "Our goal is to find the optimal parameters \u0398v of the visual perception module Perception and \u0398s of the semantic parsing module SemanticParse, to maximize the likelihood of answering the question Q correctly: \u0398v, \u0398s \u2190 arg max EP [Pr[A = Executor(Perception(S; \u0398v), P )]], (1)",
        "We found that this is essential to the learning of our neuro-symbolic concept learner",
        "We evaluate the performance on all concepts appeared in the CLEVR dataset",
        "A separate semantic parser is trained for the Visual question answering baselines, which translates captions into a CLEVR QA-compatible program (e.g., Exist(Filter(Box, Relate(Right, Filter(Cylinder)))",
        "We presented a method that jointly learns visual concepts, words, and semantic parsing of sentences from natural supervision"
    ],
    "summary": [
        "Humans are capable of learning visual concepts by jointly understanding vision and language (<a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\"><a class=\"ref-link\" id=\"cFazly_et+al_2010_a\" href=\"#rFazly_et+al_2010_a\">Fazly et al, 2010</a></a>; Chrupa\u0142a et al, 2015; <a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\"><a class=\"ref-link\" id=\"cGauthier_et+al_2018_a\" href=\"#rGauthier_et+al_2018_a\">Gauthier et al, 2018</a></a>).",
        "We propose the neuro-symbolic concept learner (NS-CL), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs.",
        "It naturally learns disentangled visual and language concepts, enabling combinatorial generalization w.r.t. both visual scenes and semantic programs.",
        "Our model learns general visual concepts and their association with symbolic representations of language.",
        "We first use a visual perception module to construct an object-based representation for a scene, and run a semantic parsing module to translate a question into an executable program.",
        "The semantic parsing module translates a natural language question into an executable program with a hierarchy of primitive operations, represented in a domain-specific language (DSL) designed for VQA.",
        "Given the latent program recovered from the question in natural language, a symbolic program executor executes the program and derives the answer based on the object-based visual representation.",
        "It learns visual concepts with remarkable accuracy; second, it allows data-efficient visual reasoning on the CLEVR dataset (<a class=\"ref-link\" id=\"cJohnson_et+al_2017_a\" href=\"#rJohnson_et+al_2017_a\">Johnson et al, 2017a</a>); third, it generalizes well to new attributes, visual composition, and language domains.",
        "To evaluate the visual concepts learned by such models, we generate a synthetic question set.",
        "Our approach outperforms IEP by a significant margin (8%) and attention-based baselines by >2%, suggesting object-based visual representations and symbolic reasoning helps to interpret visual concepts.",
        "NS-CL jointly learns visual concepts, words and semantic parsing by watching images and reading paired questions and answers.",
        "The recent NS-VQA model from Yi et al (2018) achieves better performance on CLEVR; their system requires annotated visual attributes and program traces during training, while our NS-CL needs no extra labels.",
        "Besides achieving a competitive performance on the visual reasoning testbeds, by leveraging both object-based representation and symbolic reasoning, out model learns fully interpretable visual concepts: see Appendix H for qualitative results on various datasets.",
        "We jointly train the concept embeddings (e.g., Red, Cube, etc.) as well as the semantic parser on split A, keeping pretrained, frozen attribute operators.",
        "A separate semantic parser is trained for the VQA baselines, which translates captions into a CLEVR QA-compatible program (e.g., Exist(Filter(Box, Relate(Right, Filter(Cylinder))).",
        "Figure 8 shows examples of the learned visual concepts, including object categories, attributes, and relations.",
        "Based upon the learned concepts, our model achieves good results on question answering, and more importantly, generalizes well to new visual compositions, new visual concepts, and new domain specific languages."
    ],
    "headline": "We propose the Neuro-Symbolic Concept Learner, a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by looking at images and reading paired questions and answers",
    "reference_links": [
        {
            "id": "Abend_et+al_2017_a",
            "entry": "Omri Abend, Tom Kwiatkowski, Nathaniel J Smith, Sharon Goldwater, and Mark Steedman. Bootstrapping language acquisition. Cognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abend%2C%20Omri%20Kwiatkowski%2C%20Tom%20Smith%2C%20Nathaniel%20J.%20Goldwater%2C%20Sharon%20Bootstrapping%20language%20acquisition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abend%2C%20Omri%20Kwiatkowski%2C%20Tom%20Smith%2C%20Nathaniel%20J.%20Goldwater%2C%20Sharon%20Bootstrapping%20language%20acquisition%202017"
        },
        {
            "id": "Anderson_et+al_2018_a",
            "entry": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018"
        },
        {
            "id": "Andreas_et+al_2016_a",
            "entry": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. In NAACL-HLT, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Learning%20to%20compose%20neural%20networks%20for%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Learning%20to%20compose%20neural%20networks%20for%20question%20answering%202016"
        },
        {
            "id": "Antol_et+al_2015_a",
            "entry": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20VQA%20Visual%20question%20answering%20In%20ICCV%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20VQA%20Visual%20question%20answering%20In%20ICCV%202015"
        },
        {
            "id": "Artzi_2013_a",
            "entry": "Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013"
        },
        {
            "id": "Baradel_et+al_2018_a",
            "entry": "Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, and Greg Mori. Object level visual reasoning in videos. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baradel%2C%20Fabien%20Neverova%2C%20Natalia%20Wolf%2C%20Christian%20Mille%2C%20Julien%20Object%20level%20visual%20reasoning%20in%20videos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baradel%2C%20Fabien%20Neverova%2C%20Natalia%20Wolf%2C%20Christian%20Mille%2C%20Julien%20Object%20level%20visual%20reasoning%20in%20videos%202018"
        },
        {
            "id": "Chen_et+al_0000_a",
            "entry": "Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia. Abc-cnn: An attention based convolutional neural network for visual question answering. arXiv:1511.05960, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05960"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20Merrienboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Chrupa_et+al_2015_a",
            "entry": "Grzegorz Chrupa\u0142a, Akos Kadar, and Afra Alishahi. Learning language through pictures. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chrupa%C5%82a%2C%20Grzegorz%20Kadar%2C%20Akos%20Alishahi%2C%20Afra%20Learning%20language%20through%20pictures%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chrupa%C5%82a%2C%20Grzegorz%20Kadar%2C%20Akos%20Alishahi%2C%20Afra%20Learning%20language%20through%20pictures%202015"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Donahue_et+al_2015_a",
            "entry": "Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description.%20In%20CVPR%202015"
        },
        {
            "id": "Dong_2016_a",
            "entry": "Li Dong and Mirella Lapata. Language to logical form with neural attention. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Li%20Lapata%2C%20Mirella%20Language%20to%20logical%20form%20with%20neural%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Li%20Lapata%2C%20Mirella%20Language%20to%20logical%20form%20with%20neural%20attention%202016"
        },
        {
            "id": "Fazly_et+al_2010_a",
            "entry": "Afsaneh Fazly, Afra Alishahi, and Suzanne Stevenson. A probabilistic computational model of cross-situational word learning. Cognit. Sci., 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fazly%2C%20Afsaneh%20Alishahi%2C%20Afra%20Stevenson%2C%20Suzanne%20A%20probabilistic%20computational%20model%20of%20cross-situational%20word%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fazly%2C%20Afsaneh%20Alishahi%2C%20Afra%20Stevenson%2C%20Suzanne%20A%20probabilistic%20computational%20model%20of%20cross-situational%20word%20learning%202010"
        },
        {
            "id": "Gan_et+al_2017_a",
            "entry": "Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. VQS: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Chuang%20Li%2C%20Yandong%20Li%2C%20Haoxiang%20Sun%2C%20Chen%20VQS%3A%20Linking%20segmentations%20to%20questions%20and%20answers%20for%20supervised%20attention%20in%20vqa%20and%20question-focused%20semantic%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Chuang%20Li%2C%20Yandong%20Li%2C%20Haoxiang%20Sun%2C%20Chen%20VQS%3A%20Linking%20segmentations%20to%20questions%20and%20answers%20for%20supervised%20attention%20in%20vqa%20and%20question-focused%20semantic%20segmentation%202017"
        },
        {
            "id": "Ganju_et+al_2017_a",
            "entry": "Siddha Ganju, Olga Russakovsky, and Abhinav Gupta. What\u2019s in a question: Using visual questions as a form of supervision. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganju%2C%20Siddha%20Russakovsky%2C%20Olga%20Gupta%2C%20Abhinav%20What%E2%80%99s%20in%20a%20question%3A%20Using%20visual%20questions%20as%20a%20form%20of%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganju%2C%20Siddha%20Russakovsky%2C%20Olga%20Gupta%2C%20Abhinav%20What%E2%80%99s%20in%20a%20question%3A%20Using%20visual%20questions%20as%20a%20form%20of%20supervision%202017"
        },
        {
            "id": "Gauthier_et+al_2018_a",
            "entry": "Jon Gauthier, Roger Levy, and Joshua B Tenenbaum. Word learning and the acquisition of syntactic\u2013 semantic overhypotheses. In CogSci, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gauthier%2C%20Jon%20Levy%2C%20Roger%20Tenenbaum%2C%20Joshua%20B.%20Word%20learning%20and%20the%20acquisition%20of%20syntactic%E2%80%93%20semantic%20overhypotheses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gauthier%2C%20Jon%20Levy%2C%20Roger%20Tenenbaum%2C%20Joshua%20B.%20Word%20learning%20and%20the%20acquisition%20of%20syntactic%E2%80%93%20semantic%20overhypotheses%202018"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202015"
        },
        {
            "id": "He_et+al_2017_a",
            "entry": "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Dollar%20and%20Ross%20Girshick%20Mask%20RCNN%20In%20ICCV%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Dollar%20and%20Ross%20Girshick%20Mask%20RCNN%20In%20ICCV%202017"
        },
        {
            "id": "Higgins_et+al_2018_a",
            "entry": "Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: learning abstract hierarchical compositional visual concepts. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Sonnerat%2C%20Nicolas%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Scan%3A%20learning%20abstract%20hierarchical%20compositional%20visual%20concepts%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Sonnerat%2C%20Nicolas%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Scan%3A%20learning%20abstract%20hierarchical%20compositional%20visual%20concepts%202018"
        },
        {
            "id": "Hu_et+al_2018_a",
            "entry": "Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stack neural module networks. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Ronghang%20Andreas%2C%20Jacob%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Explainable%20neural%20computation%20via%20stack%20neural%20module%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Ronghang%20Andreas%2C%20Jacob%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Explainable%20neural%20computation%20via%20stack%20neural%20module%20networks%202018"
        },
        {
            "id": "Hudson_2018_a",
            "entry": "Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hudson%2C%20Drew%20A.%20Manning%2C%20Christopher%20D.%20Compositional%20attention%20networks%20for%20machine%20reasoning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hudson%2C%20Drew%20A.%20Manning%2C%20Christopher%20D.%20Compositional%20attention%20networks%20for%20machine%20reasoning%202018"
        },
        {
            "id": "Jabri_et+al_2016_a",
            "entry": "Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jabri%2C%20Allan%20Joulin%2C%20Armand%20van%20der%20Maaten%2C%20Laurens%20Revisiting%20visual%20question%20answering%20baselines%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jabri%2C%20Allan%20Joulin%2C%20Armand%20van%20der%20Maaten%2C%20Laurens%20Revisiting%20visual%20question%20answering%20baselines%202016"
        },
        {
            "id": "Johnson_et+al_2015_a",
            "entry": "Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Krishna%2C%20Ranjay%20Stark%2C%20Michael%20Li%2C%20Li-Jia%20Image%20retrieval%20using%20scene%20graphs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Krishna%2C%20Ranjay%20Stark%2C%20Michael%20Li%2C%20Li-Jia%20Image%20retrieval%20using%20scene%20graphs%202015"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Justin Johnson, Andrej Karpathy, and Li Fei-Fei. DenseCap: Fully convolutional localization networks for dense captioning. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20DenseCap%3A%20Fully%20convolutional%20localization%20networks%20for%20dense%20captioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20DenseCap%3A%20Fully%20convolutional%20localization%20networks%20for%20dense%20captioning%202016"
        },
        {
            "id": "Johnson_et+al_2017_a",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20CLEVR%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20CLEVR%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%202017"
        },
        {
            "id": "Johnson_et+al_2017_b",
            "entry": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In ICCV, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Hoffman%2C%20Judy%20Inferring%20and%20executing%20programs%20for%20visual%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Hoffman%2C%20Judy%20Inferring%20and%20executing%20programs%20for%20visual%20reasoning%202017"
        },
        {
            "id": "Kiros_et+al_2014_a",
            "entry": "Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv:1411.2539, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.2539"
        },
        {
            "id": "Konidaris_et+al_2018_a",
            "entry": "George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez. From skills to symbols: Learning symbolic representations for abstract high-level planning. JAIR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Kaelbling%2C%20Leslie%20Pack%20Lozano-Perez%2C%20Tomas%20From%20skills%20to%20symbols%3A%20Learning%20symbolic%20representations%20for%20abstract%20high-level%20planning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20George%20Kaelbling%2C%20Leslie%20Pack%20Lozano-Perez%2C%20Tomas%20From%20skills%20to%20symbols%3A%20Learning%20symbolic%20representations%20for%20abstract%20high-level%20planning%202018"
        },
        {
            "id": "Kulkarni_et+al_2015_a",
            "entry": "Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In NeurIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015"
        },
        {
            "id": "Levin_1993_a",
            "entry": "Beth Levin. English verb classes and alternations, volume 1. University of Chicago Press, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20Beth%20English%20verb%20classes%20and%20alternations%2C%20volume%201%201993"
        },
        {
            "id": "Lin_et+al_2014_a",
            "entry": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Dollar%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Dollar%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014"
        },
        {
            "id": "Malinowski_2014_a",
            "entry": "M Malinowski and M Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In NeurIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malinowski%2C%20M.%20Fritz%2C%20M.%20A%20multi-world%20approach%20to%20question%20answering%20about%20real-world%20scenes%20based%20on%20uncertain%20input%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malinowski%2C%20M.%20Fritz%2C%20M.%20A%20multi-world%20approach%20to%20question%20answering%20about%20real-world%20scenes%20based%20on%20uncertain%20input%202014"
        },
        {
            "id": "Mao_et+al_2016_a",
            "entry": "Junhua Mao, Jiajing Xu, Kevin Jing, and Alan L Yuille. Training and evaluating multimodal word embeddings with large-scale web annotated images. In NeurIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Junhua%20Xu%2C%20Jiajing%20Jing%2C%20Kevin%20Yuille%2C%20Alan%20L.%20Training%20and%20evaluating%20multimodal%20word%20embeddings%20with%20large-scale%20web%20annotated%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Junhua%20Xu%2C%20Jiajing%20Jing%2C%20Kevin%20Yuille%2C%20Alan%20L.%20Training%20and%20evaluating%20multimodal%20word%20embeddings%20with%20large-scale%20web%20annotated%20images%202016"
        },
        {
            "id": "Mascharka_et+al_2018_a",
            "entry": "David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mascharka%2C%20David%20Tran%2C%20Philip%20Soklaski%2C%20Ryan%20Majumdar%2C%20Arjun%20Transparency%20by%20design%3A%20Closing%20the%20gap%20between%20performance%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mascharka%2C%20David%20Tran%2C%20Philip%20Soklaski%2C%20Ryan%20Majumdar%2C%20Arjun%20Transparency%20by%20design%3A%20Closing%20the%20gap%20between%20performance%202018"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Kohli%2C%20Pushmeet%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Kohli%2C%20Pushmeet%20Zero-shot%20task%20generalization%20with%20multi-task%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Perez_et+al_2018_a",
            "entry": "Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perez%2C%20Ethan%20Strub%2C%20Florian%20Vries%2C%20Harm%20De%20Dumoulin%2C%20Vincent%20Film%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perez%2C%20Ethan%20Strub%2C%20Florian%20Vries%2C%20Harm%20De%20Dumoulin%2C%20Vincent%20Film%3A%20Visual%20reasoning%20with%20a%20general%20conditioning%20layer%202018"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In NeurIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.T.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.T.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017"
        },
        {
            "id": "Schuster_et+al_2015_a",
            "entry": "Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher D Manning. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In EMNLP Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuster%2C%20Sebastian%20Krishna%2C%20Ranjay%20Chang%2C%20Angel%20Fei-Fei%2C%20Li%20Generating%20semantically%20precise%20scene%20graphs%20from%20textual%20descriptions%20for%20improved%20image%20retrieval%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuster%2C%20Sebastian%20Krishna%2C%20Ranjay%20Chang%2C%20Angel%20Fei-Fei%2C%20Li%20Generating%20semantically%20precise%20scene%20graphs%20from%20textual%20descriptions%20for%20improved%20image%20retrieval%202015"
        },
        {
            "id": "Shi_et+al_2018_a",
            "entry": "Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, and Jian Sun. Learning visually-grounded semantics from contrastive adversarial samples. In COLING, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Haoyue%20Mao%2C%20Jiayuan%20Xiao%2C%20Tete%20Jiang%2C%20Yuning%20Learning%20visually-grounded%20semantics%20from%20contrastive%20adversarial%20samples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Haoyue%20Mao%2C%20Jiayuan%20Xiao%2C%20Tete%20Jiang%2C%20Yuning%20Learning%20visually-grounded%20semantics%20from%20contrastive%20adversarial%20samples%202018"
        },
        {
            "id": "Siddharth_et+al_2017_a",
            "entry": "N Siddharth, T. B. Paige, J.W. Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr. Learning disentangled representations with semi-supervised deep generative models. In NeurIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siddharth%2C%20N.%20Paige%2C%20T.B.%20Meent%2C%20J.W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siddharth%2C%20N.%20Paige%2C%20T.B.%20Meent%2C%20J.W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017"
        },
        {
            "id": "Suarez_et+al_2018_a",
            "entry": "Joseph Suarez, Justin Johnson, and Fei-Fei Li. DDRprog: A clevr differentiable dynamic reasoning programmer. arXiv:1803.11361, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11361"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NeurIPS, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "Vendrov_et+al_2016_a",
            "entry": "Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ivan%20Vendrov%20Ryan%20Kiros%20Sanja%20Fidler%20and%20Raquel%20Urtasun%20Orderembeddings%20of%20images%20and%20language%20In%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ivan%20Vendrov%20Ryan%20Kiros%20Sanja%20Fidler%20and%20Raquel%20Urtasun%20Orderembeddings%20of%20images%20and%20language%20In%20ICLR%202016"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. MLJ, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In CVPR, 2017. Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, 2016. Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling with recurrent transformations for 3d view synthesis. In NeurIPS, 2015. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016. Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B Tenenbaum.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiajun%20Tenenbaum%2C%20Joshua%20B.%20Kohli%2C%20Pushmeet%20Huijuan%20Xu%20and%20Kate%20Saenko.%20Ask%2C%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiajun%20Tenenbaum%2C%20Joshua%20B.%20Kohli%2C%20Pushmeet%20Huijuan%20Xu%20and%20Kate%20Saenko.%20Ask%2C%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "Zhu_et+al_2018_a",
            "entry": "Neural-Symbolic VQA: Disentangling reasoning from vision and language understanding. In NeurIPS, 2018. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Ryan%20Kiros%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Urtasun%2C%20Raquel%20Neural-Symbolic%20VQA%3A%20Disentangling%20reasoning%20from%20vision%20and%20language%20understanding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Ryan%20Kiros%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Urtasun%2C%20Raquel%20Neural-Symbolic%20VQA%3A%20Disentangling%20reasoning%20from%20vision%20and%20language%20understanding%202018"
        },
        {
            "id": "We_2017_a",
            "entry": "We first introduce the domain-specific language (DSL) designed for the CLEVR VQA dataset (Johnson et al., 2017a). Table 6 shows the available operations in the DSL, while Table 7 explains the type system.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=We%20first%20introduce%20the%20domainspecific%20language%20DSL%20designed%20for%20the%20CLEVR%20VQA%20dataset%20Johnson%20et%20al%202017a%20Table%206%20shows%20the%20available%20operations%20in%20the%20DSL%20while%20Table%207%20explains%20the%20type%20system"
        }
    ]
}
